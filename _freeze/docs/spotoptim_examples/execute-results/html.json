{
  "hash": "dacd9434d9728ddd34c25d8b04766545",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"SpotOptim: Living Examples\"\ndescription: \"Executable examples for the SpotOptim class, validated by pytest.\"\n---\n\nThis document contains fully executable code examples for the `SpotOptim` class.\nEvery example is a `{python}` code block and is covered by a corresponding pytest\nin `tests/test_spotoptim_deep.py`.\n\nRun all examples with:\n\n```bash\nuv run pytest tests/test_spotoptim_deep.py -v\n```\n\n---\n\n## Quick Start: Sphere Function\n\nThe sphere function $f(x) = \\sum_{i=1}^{n} x_i^2$ has its global minimum at\n$x^* = 0$ with $f(x^*) = 0$.\n\n::: {#ec5c9d4e .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom spotoptim import SpotOptim\n\ndef sphere(X):\n    X = np.atleast_2d(X)\n    return np.sum(X**2, axis=1)\n\nopt = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=20,\n    n_initial=10,\n    seed=0,\n)\nresult = opt.optimize()\n\nprint(f\"Best x    : {result.x}\")\nprint(f\"Best f(x) : {result.fun:.6f}\")\nprint(f\"Evaluations: {result.nfev}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest x    : [-3.78209374e-05  1.47505079e-04]\nBest f(x) : 0.000000\nEvaluations: 20\n```\n:::\n:::\n\n\nCorresponding test (`test_sphere_2d_converges_near_origin`):\n\n::: {#0b51718a .cell execution_count=2}\n``` {.python .cell-code}\nassert result.fun < 0.5, f\"Expected convergence near 0, got f={result.fun}\"\nprint(\"Convergence check passed.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConvergence check passed.\n```\n:::\n:::\n\n\n---\n\n## Result Contract\n\nEvery call to `optimize()` returns a `scipy.optimize.OptimizeResult` satisfying\nthese invariants:\n\n::: {#2a27d9e9 .cell execution_count=3}\n``` {.python .cell-code}\nfrom scipy.optimize import OptimizeResult\n\nassert isinstance(result, OptimizeResult)\n\n# Required fields\nfor field in (\"x\", \"fun\", \"nfev\", \"nit\", \"success\", \"message\", \"X\", \"y\"):\n    assert hasattr(result, field), f\"Missing: {field}\"\n\n# Shape invariants\nassert result.x.ndim == 1            # best point is 1-D\nassert result.X.shape == (20, 2)     # (max_iter, n_dim)\nassert result.y.shape == (20,)       # one y per evaluation\n\n# Optimality\nassert np.isclose(result.fun, np.min(result.y))\nbest_idx = np.argmin(result.y)\nnp.testing.assert_array_almost_equal(result.x, result.X[best_idx])\n\nprint(\"All result contract checks passed.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAll result contract checks passed.\n```\n:::\n:::\n\n\n---\n\n## Acquisition Functions\n\nSpotOptim supports three acquisition functions:\n\n| Acquisition | Description |\n|---|---|\n| `\"y\"` (default) | Best observed value |\n| `\"ei\"` | Expected Improvement |\n| `\"pi\"` | Probability of Improvement |\n\n::: {#55d27ddd .cell execution_count=4}\n``` {.python .cell-code}\nresults = {}\nfor acq in [\"y\", \"ei\", \"pi\"]:\n    r = SpotOptim(\n        fun=sphere,\n        bounds=[(-3, 3), (-3, 3)],\n        max_iter=12,\n        n_initial=7,\n        acquisition=acq,\n        seed=0,\n    ).optimize()\n    results[acq] = r\n    print(f\"  acq={acq!r:4s}  f(x*)={r.fun:.4f}  nfev={r.nfev}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  acq='y'   f(x*)=0.0000  nfev=12\n  acq='ei'  f(x*)=0.0000  nfev=12\n  acq='pi'  f(x*)=0.0218  nfev=12\n```\n:::\n:::\n\n\n::: {#b59ef715 .cell execution_count=5}\n``` {.python .cell-code}\nfor acq, r in results.items():\n    assert isinstance(r, OptimizeResult), f\"{acq} did not return OptimizeResult\"\n    assert r.success is True\n    assert np.isfinite(r.fun)\nprint(\"All acquisition checks passed.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAll acquisition checks passed.\n```\n:::\n:::\n\n\n---\n\n## Seed Reproducibility\n\nThe same seed produces byte-identical results across independent runs:\n\n::: {#5bb77e3a .cell execution_count=6}\n``` {.python .cell-code}\ncommon_kwargs = dict(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=12,\n    n_initial=6,\n    seed=42,\n)\nr1 = SpotOptim(**common_kwargs).optimize()\nr2 = SpotOptim(**common_kwargs).optimize()\n\nnp.testing.assert_array_equal(r1.X, r2.X)\nnp.testing.assert_array_equal(r1.y, r2.y)\nassert r1.fun == r2.fun\nprint(f\"Seed 42 result: f(x*)={r1.fun:.6f} — fully reproducible.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeed 42 result: f(x*)=0.000006 — fully reproducible.\n```\n:::\n:::\n\n\n---\n\n## Integer Variables\n\nUse `var_type=[\"int\", ...]` to restrict variables to integer domains.\n\nThe optimum of $f(x_1, x_2) = x_1^2 + x_2^2$ over $\\mathbb{Z}^2$ is at $x^* = (0, 0)$.\n\n::: {#d2632ca9 .cell execution_count=7}\n``` {.python .cell-code}\nopt_int = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=12,\n    n_initial=7,\n    var_type=[\"int\", \"int\"],\n    seed=0,\n)\nresult_int = opt_int.optimize()\n\nprint(f\"Best x    : {result_int.x}  (should be integers)\")\nprint(f\"Best f(x) : {result_int.fun}\")\n\n# All stored X values must be integers\nassert np.allclose(result_int.X, np.round(result_int.X), atol=1e-9)\nassert np.allclose(result_int.x, np.round(result_int.x), atol=1e-9)\nprint(\"Integer domain check passed.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest x    : [-0.  0.]  (should be integers)\nBest f(x) : 0.0\nInteger domain check passed.\n```\n:::\n:::\n\n\n---\n\n## Log-Scaled Variables (var_trans)\n\nFor variables spanning multiple orders of magnitude, use `var_trans`:\n\n::: {#21210e42 .cell execution_count=8}\n``` {.python .cell-code}\ndef log_objective(X):\n    \"\"\"Minimum at x = 10 (log10 scale: minimum at log10(x) = 1).\"\"\"\n    X = np.atleast_2d(X)\n    return (np.log10(X[:, 0]) - 1.0) ** 2\n\nopt_log = SpotOptim(\n    fun=log_objective,\n    bounds=[(1e-3, 1e3)],\n    max_iter=12,\n    n_initial=6,\n    var_trans=[\"log10\"],\n    seed=0,\n)\nresult_log = opt_log.optimize()\n\nprint(f\"Best x      : {result_log.x[0]:.4f}  (expected ~10)\")\nprint(f\"Best f(x)   : {result_log.fun:.6f}\")\n\nassert 1e-3 <= result_log.x[0] <= 1e3, \"Result out of bounds\"\nassert np.isfinite(result_log.fun)\nprint(\"Log-transform checks passed.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest x      : 10.0034  (expected ~10)\nBest f(x)   : 0.000000\nLog-transform checks passed.\n```\n:::\n:::\n\n\n---\n\n## Factor (Categorical) Variables\n\nTuple bounds define categorical levels that are mapped to integers internally:\n\n::: {#511c0b98 .cell execution_count=9}\n``` {.python .cell-code}\ncall_log = []\n\ndef cat_sphere(X):\n    \"\"\"Records (color, value) pairs; minimises value regardless of color.\"\"\"\n    X = np.atleast_2d(X)\n    call_log.append(X.copy())\n    return X[:, 1].astype(float) ** 2\n\nopt_cat = SpotOptim(\n    fun=cat_sphere,\n    bounds=[(\"red\", \"green\", \"blue\"), (-5.0, 5.0)],\n    max_iter=10,\n    n_initial=6,\n    seed=0,\n)\n\n# Factor dimension mapped to integers 0..2 internally\nassert opt_cat.bounds[0] == (0, 2)\nassert opt_cat._factor_maps[0] == {0: \"red\", 1: \"green\", 2: \"blue\"}\n\nresult_cat = opt_cat.optimize()\nprint(f\"Best value : {result_cat.fun:.4f}\")\nprint(\"Factor variable checks passed.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest value : 0.0000\nFactor variable checks passed.\n```\n:::\n:::\n\n\n---\n\n## Custom Kriging Surrogate\n\nReplace the default GP surrogate with `Kriging`:\n\n::: {#7188bde4 .cell execution_count=10}\n``` {.python .cell-code}\nfrom spotoptim import Kriging\n\nkriging = Kriging(noise=1e-6, min_theta=-3.0, max_theta=2.0, seed=42)\n\nopt_k = SpotOptim(\n    fun=sphere,\n    bounds=[(-3, 3), (-3, 3)],\n    surrogate=kriging,\n    max_iter=12,\n    n_initial=6,\n    seed=0,\n)\nresult_k = opt_k.optimize()\n\nprint(f\"Kriging best f(x) : {result_k.fun:.6f}\")\nassert isinstance(result_k, OptimizeResult)\nassert result_k.success is True\nassert np.isfinite(result_k.fun)\nprint(\"Kriging surrogate checks passed.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKriging best f(x) : 0.000952\nKriging surrogate checks passed.\n```\n:::\n:::\n\n\n---\n\n## get_best_hyperparameters\n\nAfter optimization, retrieve the best point as a labelled dict:\n\n::: {#e7e6a6ec .cell execution_count=11}\n``` {.python .cell-code}\nopt_named = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=12,\n    n_initial=6,\n    var_name=[\"x0\", \"x1\"],\n    seed=0,\n)\nresult_named = opt_named.optimize()\nbest = opt_named.get_best_hyperparameters(as_dict=True)\n\nprint(f\"result.x           : {result_named.x}\")\nprint(f\"get_best_hyperparams: {best}\")\n\n# Values in the dict must match result.x\nvalues_arr = np.array(list(best.values()), dtype=float)\nnp.testing.assert_array_almost_equal(values_arr, result_named.x)\nprint(\"get_best_hyperparameters check passed.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nresult.x           : [0.03916876 0.02590388]\nget_best_hyperparams: {'x0': np.float64(0.03916876059627464), 'x1': np.float64(0.025903884165880253)}\nget_best_hyperparameters check passed.\n```\n:::\n:::\n\n\n---\n\n## transform_value Roundtrip\n\nAll supported variable transformations satisfy the roundtrip identity\n$\\text{inverse}(\\text{transform}(x)) = x$:\n\n::: {#58f48727 .cell execution_count=12}\n``` {.python .cell-code}\nopt_t = SpotOptim(fun=sphere, bounds=[(1e-6, 100)])\n\ntransforms = [\"log10\", \"log\", \"ln\", \"sqrt\", \"exp\", \"square\", \"cube\", \"inv\", None]\nx_test = 4.0\n\nfor trans in transforms:\n    tx = opt_t.transform_value(x_test, trans)\n    x_back = opt_t.inverse_transform_value(tx, trans)\n    ok = np.isclose(x_back, x_test, rtol=1e-9)\n    status = \"OK\" if ok else \"FAIL\"\n    print(f\"  trans={str(trans):<10}  {x_test} -> {tx:.4f} -> {x_back:.6f}  [{status}]\")\n    assert ok, f\"Roundtrip failed for {trans!r}\"\n\nprint(\"All transform roundtrip checks passed.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  trans=log10       4.0 -> 0.6021 -> 4.000000  [OK]\n  trans=log         4.0 -> 1.3863 -> 4.000000  [OK]\n  trans=ln          4.0 -> 1.3863 -> 4.000000  [OK]\n  trans=sqrt        4.0 -> 2.0000 -> 4.000000  [OK]\n  trans=exp         4.0 -> 54.5982 -> 4.000000  [OK]\n  trans=square      4.0 -> 16.0000 -> 4.000000  [OK]\n  trans=cube        4.0 -> 64.0000 -> 4.000000  [OK]\n  trans=inv         4.0 -> 0.2500 -> 4.000000  [OK]\n  trans=None        4.0 -> 4.0000 -> 4.000000  [OK]\nAll transform roundtrip checks passed.\n```\n:::\n:::\n\n\n---\n\n## Convergence on Rosenbrock\n\nThe Rosenbrock function $f(x,y) = (1-x)^2 + 100(y-x^2)^2$ has its minimum at\n$(1, 1)$ with $f(1,1) = 0$.\n\n::: {#c182a5d3 .cell execution_count=13}\n``` {.python .cell-code}\ndef rosenbrock(X):\n    X = np.atleast_2d(X)\n    x, y = X[:, 0], X[:, 1]\n    return (1.0 - x)**2 + 100.0 * (y - x**2)**2\n\nopt_rb = SpotOptim(\n    fun=rosenbrock,\n    bounds=[(-2, 2), (-2, 2)],\n    max_iter=30,\n    n_initial=10,\n    acquisition=\"ei\",\n    seed=0,\n)\nresult_rb = opt_rb.optimize()\n\nprint(f\"Best x    : {result_rb.x}\")\nprint(f\"Best f(x) : {result_rb.fun:.6f}  (global min = 0 at [1, 1])\")\n\nassert result_rb.fun < 10.0, f\"Rosenbrock did not converge: f={result_rb.fun}\"\nprint(\"Rosenbrock convergence check passed.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest x    : [0.8746919  0.83489777]\nBest f(x) : 0.503071  (global min = 0 at [1, 1])\nRosenbrock convergence check passed.\n```\n:::\n:::\n\n\n",
    "supporting": [
      "spotoptim_examples_files"
    ],
    "filters": [],
    "includes": {}
  }
}