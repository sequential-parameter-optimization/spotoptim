{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Hyperparameter Tuning for Physics-Informed Neural Networks\"\n",
        "subtitle: \"Using SpotOptim to Optimize PINN Architecture and Training\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Overview\n",
        "\n",
        "This tutorial demonstrates how to use SpotOptim for hyperparameter optimization of Physics-Informed Neural Networks (PINNs). We'll optimize the network architecture and training parameters to find the best configuration for solving an ordinary differential equation.\n",
        "\n",
        "Building on the basic PINN demo, we'll now systematically search for optimal:\n",
        "\n",
        "- Number of neurons per hidden layer\n",
        "- Number of hidden layers\n",
        "- Activation function (categorical)\n",
        "- Optimizer algorithm (categorical)\n",
        "- Learning rate (log-scale)\n",
        "- Physics loss weight (log-scale)\n",
        "\n",
        "## Key Features\n",
        "\n",
        "### 1. PyTorch Dataset and DataLoader\n",
        "\n",
        "Following PyTorch best practices from the [official tutorial](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html), this tutorial implements:\n",
        "\n",
        "- **Custom Dataset Classes**: Separate classes for supervised data (`PINNDataset`) and collocation points (`CollocationDataset`)\n",
        "- **DataLoader Integration**: Efficient batch processing with configurable batch size, shuffling, and parallel loading\n",
        "- **Proper Data Separation**: Clean separation of training, validation, and collocation data\n",
        "- **Gradient Tracking**: Automatic gradient handling for collocation points needed in physics loss\n",
        "\n",
        "Benefits:\n",
        "\n",
        "- **Modularity**: Clean separation between data and model code\n",
        "- **Efficiency**: Batch processing and optional parallel data loading\n",
        "- **Scalability**: Easy to extend to larger datasets\n",
        "- **Best Practices**: Follows PyTorch conventions used across the ecosystem\n",
        "\n",
        "### 2. Automatic Transformation Handling\n",
        "\n",
        "This tutorial also showcases SpotOptim's `var_trans` feature for automatic variable transformations. Learning rates and regularization parameters are often best explored on a log scale, but manually transforming values is tedious and error-prone. With `var_trans`, you simply specify:\n",
        "\n",
        "```python\n",
        "var_trans = [None, None, \"log10\", \"log10\"]\n",
        "```\n",
        "\n",
        "SpotOptim then:\n",
        "\n",
        "- Optimizes internally in log-transformed space (efficient exploration)\n",
        "- Passes original-scale values to your objective function (no manual conversion needed)\n",
        "- Displays all results in original scale (easy interpretation)\n",
        "\n",
        "This eliminates the need for manual `10**x` conversions throughout your code!\n",
        "\n",
        "# The Problem\n",
        "\n",
        "We're solving the same ODE as in the basic PINN demo:\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dt} + 0.1 y - \\sin\\left(\\frac{\\pi t}{2}\\right) = 0\n",
        "$$\n",
        "\n",
        "with initial condition $y(0) = 0$.\n",
        "\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: setup-pinn2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple\n",
        "from spotoptim import SpotOptim\n",
        "from spotoptim.nn.linear_regressor import LinearRegressor\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set number of epochs for training\n",
        "N_EPOCHS=10000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Generation\n",
        "\n",
        "Following PyTorch best practices, we'll create custom Dataset classes for our PINN data.\n",
        "\n",
        "## Custom Dataset Classes\n",
        "\n",
        "We'll create two dataset types:\n",
        "\n",
        "1. `PINNDataset` for supervised data (training and validation)\n",
        "2. `CollocationDataset` for physics-informed collocation points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: data-generation-pinn2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def oscillator(\n",
        "    n_steps: int = 3000,\n",
        "    t_min: float = 0.0,\n",
        "    t_max: float = 30.0,\n",
        "    y0: float = 0.0,\n",
        "    alpha: float = 0.1,\n",
        "    omega: float = np.pi / 2\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Solve ODE: dy/dt + alpha*y - sin(omega*t) = 0\n",
        "    using RK2 (midpoint method).\n",
        "    \n",
        "    Returns:\n",
        "        t_tensor: Time points, shape (n_steps, 1)\n",
        "        y_tensor: Solution values, shape (n_steps, 1)\n",
        "    \"\"\"\n",
        "    t_step = (t_max - t_min) / n_steps\n",
        "    t_points = np.arange(t_min, t_min + n_steps * t_step, t_step)[:n_steps]\n",
        "    \n",
        "    y = [y0]\n",
        "    \n",
        "    for t_current_step_end in t_points[1:]:\n",
        "        t_midpoint = t_current_step_end - t_step / 2.0\n",
        "        y_prev = y[-1]\n",
        "        \n",
        "        slope_at_t_mid = -alpha * y_prev + np.sin(omega * t_midpoint)\n",
        "        y_intermediate = y_prev + (t_step / 2.0) * slope_at_t_mid\n",
        "        \n",
        "        slope_at_t_end = -alpha * y_intermediate + np.sin(omega * t_current_step_end)\n",
        "        y_next = y_prev + t_step * slope_at_t_end\n",
        "        y.append(y_next)\n",
        "    \n",
        "    t_tensor = torch.tensor(t_points, dtype=torch.float32).view(-1, 1)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "    \n",
        "    return t_tensor, y_tensor\n",
        "\n",
        "\n",
        "class PINNDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for PINN supervised data (training/validation).\n",
        "    \n",
        "    This dataset stores time-solution pairs (t, y) for supervised learning.\n",
        "    \n",
        "    Args:\n",
        "        t (torch.Tensor): Time points, shape (n_samples, 1)\n",
        "        y (torch.Tensor): Solution values, shape (n_samples, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, t: torch.Tensor, y: torch.Tensor):\n",
        "        self.t = t\n",
        "        self.y = y\n",
        "        \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.t)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        return self.t[idx], self.y[idx]\n",
        "\n",
        "\n",
        "class CollocationDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for PINN collocation points.\n",
        "    \n",
        "    This dataset stores time points where physics loss is evaluated.\n",
        "    Gradients are required for computing derivatives in the PDE.\n",
        "    \n",
        "    Args:\n",
        "        t (torch.Tensor): Collocation time points, shape (n_points, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, t: torch.Tensor):\n",
        "        # Store collocation points with gradient tracking\n",
        "        self.t = t.requires_grad_(True)\n",
        "        \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.t)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        # Return single collocation point (still requires_grad)\n",
        "        return self.t[idx].unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate exact solution using RK2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset size: 26\n",
            "Validation dataset size: 25\n",
            "Collocation dataset size: 50\n",
            "\n",
            "Sample from training dataset:\n",
            "  t: 0.0000, y: 0.0000\n"
          ]
        }
      ],
      "source": [
        "x_exact, y_exact = oscillator()\n",
        "\n",
        "# Create training data (sparse sampling)\n",
        "t_train = x_exact[0:3000:119]\n",
        "y_train = y_exact[0:3000:119]\n",
        "\n",
        "# Create validation data (different sampling for unbiased evaluation)\n",
        "t_val = x_exact[50:3000:120]\n",
        "y_val = y_exact[50:3000:120]\n",
        "\n",
        "# Create collocation points for physics loss\n",
        "t_physics = torch.linspace(0, 30, 50).view(-1, 1)\n",
        "\n",
        "# Create Dataset objects\n",
        "train_dataset = PINNDataset(t_train, y_train)\n",
        "val_dataset = PINNDataset(t_val, y_val)\n",
        "collocation_dataset = CollocationDataset(t_physics)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Collocation dataset size: {len(collocation_dataset)}\")\n",
        "print(f\"\\nSample from training dataset:\")\n",
        "t_sample, y_sample = train_dataset[0]\n",
        "print(f\"  t: {t_sample.item():.4f}, y: {y_sample.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define the PINN Training Function\n",
        "\n",
        "This function creates DataLoaders and trains a PINN with given hyperparameters.\n",
        "Following PyTorch best practices, we use DataLoader for efficient batch processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing PINN training function with DataLoaders...\n",
            "  Epoch 2000/10000: Avg Loss = 0.185009\n",
            "  Epoch 4000/10000: Avg Loss = 0.153585\n",
            "  Epoch 6000/10000: Avg Loss = 0.111497\n",
            "  Epoch 8000/10000: Avg Loss = 0.077971\n",
            "  Epoch 10000/10000: Avg Loss = 0.050399\n",
            "\n",
            "Test validation MSE: 0.051722\n"
          ]
        }
      ],
      "source": [
        "#| label: pinn-training-function-pinn2\n",
        "def train_pinn(\n",
        "    l1: int,\n",
        "    num_layers: int,\n",
        "    activation: str,\n",
        "    optimizer_name: str,\n",
        "    lr_unified: float,\n",
        "    alpha: float,\n",
        "    n_epochs: int = N_EPOCHS,\n",
        "    batch_size: int = 16,\n",
        "    verbose: bool = False\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Train a PINN with specified hyperparameters using DataLoaders.\n",
        "    \n",
        "    Args:\n",
        "        l1: Number of neurons per hidden layer\n",
        "        num_layers: Number of hidden layers\n",
        "        activation: Activation function (\"Tanh\", \"ReLU\", \"Sigmoid\", \"GELU\")\n",
        "        optimizer_name: Optimizer algorithm (\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\")\n",
        "        lr_unified: Unified learning rate multiplier\n",
        "        alpha: Weight for physics loss\n",
        "        n_epochs: Number of training epochs\n",
        "        batch_size: Batch size for DataLoader\n",
        "        verbose: Whether to print progress\n",
        "    \n",
        "    Returns:\n",
        "        Validation mean squared error\n",
        "    \"\"\"\n",
        "    # Set seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    \n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        pin_memory=False\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=False\n",
        "    )\n",
        "    \n",
        "    # For collocation points, we can use full batch since it's small\n",
        "    collocation_loader = DataLoader(\n",
        "        collocation_dataset,\n",
        "        batch_size=len(collocation_dataset),\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "    \n",
        "    # Create model\n",
        "    model = LinearRegressor(\n",
        "        input_dim=1,\n",
        "        output_dim=1,\n",
        "        l1=l1,\n",
        "        num_hidden_layers=num_layers,\n",
        "        activation=activation,\n",
        "        lr=lr_unified\n",
        "    )\n",
        "    \n",
        "    # Get optimizer\n",
        "    optimizer = model.get_optimizer(optimizer_name)\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        \n",
        "        # Get collocation points (full batch)\n",
        "        t_physics_batch = next(iter(collocation_loader))\n",
        "        # Ensure gradients are enabled\n",
        "        t_physics_batch = t_physics_batch.requires_grad_(True)\n",
        "        \n",
        "        # Iterate over training batches\n",
        "        for batch_t, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Data Loss\n",
        "            y_pred = model(batch_t)\n",
        "            loss_data = torch.mean((y_pred - batch_y)**2)\n",
        "            \n",
        "            # Physics Loss (computed on full collocation set)\n",
        "            y_physics = model(t_physics_batch)\n",
        "            dy_dt = torch.autograd.grad(\n",
        "                y_physics,\n",
        "                t_physics_batch,\n",
        "                torch.ones_like(y_physics),\n",
        "                create_graph=True,\n",
        "                retain_graph=True\n",
        "            )[0]\n",
        "            \n",
        "            # PDE residual: dy/dt + 0.1*y - sin(pi*t/2) = 0\n",
        "            physics_residual = dy_dt + 0.1 * y_physics - torch.sin(np.pi * t_physics_batch / 2)\n",
        "            loss_physics = torch.mean(physics_residual**2)\n",
        "            \n",
        "            # Total Loss\n",
        "            loss = loss_data + alpha * loss_physics\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        if verbose and (epoch + 1) % 2000 == 0:\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            print(f\"  Epoch {epoch+1}/{n_epochs}: Avg Loss = {avg_loss:.6f}\")\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    model.eval()\n",
        "    val_mse = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_t, batch_y in val_loader:\n",
        "            y_pred = model(batch_t)\n",
        "            val_mse += torch.mean((batch_y - y_pred)**2).item()\n",
        "    \n",
        "    val_mse /= len(val_loader)\n",
        "    return val_mse\n",
        "\n",
        "# Test the function with default parameters\n",
        "print(\"Testing PINN training function with DataLoaders...\")\n",
        "test_error = train_pinn(\n",
        "    l1=32, \n",
        "    num_layers=2,\n",
        "    activation=\"Tanh\",\n",
        "    optimizer_name=\"Adam\",\n",
        "    lr_unified=3.0, \n",
        "    alpha=0.06,\n",
        "    batch_size=16,\n",
        "    n_epochs=N_EPOCHS, \n",
        "    verbose=True\n",
        ")\n",
        "print(f\"\\nTest validation MSE: {test_error:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Optimization with SpotOptim\n",
        "\n",
        "Now we'll use SpotOptim to find the best hyperparameters:\n",
        "\n",
        "## Define the Objective Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing objective function with 2 configurations...\n",
            "\n",
            "Configuration 1/2:\n",
            "  l1=32, num_layers=2, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=3.0000, alpha=0.0600\n",
            "  Validation MSE: 0.051722\n",
            "\n",
            "Configuration 2/2:\n",
            "  l1=64, num_layers=3, activation=ReLU, \n",
            "  optimizer=AdamW, lr_unified=2.0000, alpha=0.0400\n",
            "  Validation MSE: 0.118984\n",
            "\n",
            "Test results: [0.05172164 0.11898382]\n"
          ]
        }
      ],
      "source": [
        "#| label: pinn-objective-function-pinn2\n",
        "def objective_pinn(X):\n",
        "    \"\"\"\n",
        "    Objective function for SpotOptim.\n",
        "    \n",
        "    Args:\n",
        "        X: Array of hyperparameter configurations, shape (n_configs, 6)\n",
        "           Each row: [l1, num_layers, activation, optimizer, lr_unified, alpha]\n",
        "           Note: SpotOptim handles log transformations and factor mapping automatically\n",
        "    \n",
        "    Returns:\n",
        "        Array of validation errors\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i, params in enumerate(X):\n",
        "        # Extract parameters (already in original scale thanks to var_trans)\n",
        "        # Factor variables (activation, optimizer) are returned as strings\n",
        "        l1 = int(params[0])                    # Number of neurons\n",
        "        num_layers = int(params[1])            # Number of hidden layers\n",
        "        activation = params[2]                 # Activation function\n",
        "        optimizer_name = params[3]             # Optimizer algorithm\n",
        "        lr_unified = params[4]                 # Learning rate\n",
        "        alpha = params[5]                      # Physics weight\n",
        "        \n",
        "        print(f\"\\nConfiguration {i+1}/{len(X)}:\")\n",
        "        print(f\"  l1={l1}, num_layers={num_layers}, activation={activation}, \")\n",
        "        print(f\"  optimizer={optimizer_name}, lr_unified={lr_unified:.4f}, alpha={alpha:.4f}\")\n",
        "        \n",
        "        # Train PINN with these hyperparameters\n",
        "        val_error = train_pinn(\n",
        "            l1=l1,\n",
        "            num_layers=num_layers,\n",
        "            activation=activation,\n",
        "            optimizer_name=optimizer_name,\n",
        "            lr_unified=lr_unified,\n",
        "            alpha=alpha,\n",
        "            n_epochs=N_EPOCHS,\n",
        "            verbose=False\n",
        "        )\n",
        "        \n",
        "        print(f\"  Validation MSE: {val_error:.6f}\")\n",
        "        results.append(val_error)\n",
        "    \n",
        "    return np.array(results)\n",
        "\n",
        "# Test the objective function\n",
        "print(\"Testing objective function with 2 configurations...\")\n",
        "X_test = np.array([\n",
        "    [32, 2, \"Tanh\", \"Adam\", 3.0, 0.06],    # Baseline config\n",
        "    [64, 3, \"ReLU\", \"AdamW\", 2.0, 0.04]    # Alternative config\n",
        "], dtype=object)\n",
        "test_results = objective_pinn(X_test)\n",
        "print(f\"\\nTest results: {test_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Factor variable at dimension 2:\n",
            "  Levels: ['Tanh', 'ReLU', 'Sigmoid', 'GELU']\n",
            "  Mapped to integers: 0 to 3\n",
            "Factor variable at dimension 3:\n",
            "  Levels: ['Adam', 'SGD', 'RMSprop', 'AdamW']\n",
            "  Mapped to integers: 0 to 3\n",
            "'runs' directory does not exist, nothing to clean\n",
            "TensorBoard logging enabled: runs/spotoptim_20251117_003143\n"
          ]
        }
      ],
      "source": [
        "#| label: pinn-hyperparameter-optimization-pinn2\n",
        "# Define search space with var_trans for automatic log-scale handling\n",
        "bounds = [\n",
        "    (16, 128),                                      # l1: neurons per layer (16 to 128)\n",
        "    (1, 4),                                         # num_layers: 1 to 4 hidden layers\n",
        "    (\"Tanh\", \"ReLU\", \"Sigmoid\", \"GELU\"),         # activation: activation function\n",
        "    (\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\"),          # optimizer: optimizer algorithm\n",
        "    (0.1, 10.0),                                    # lr_unified: learning rate (0.1 to 10)\n",
        "    (0.01, 1.0)                                     # alpha: physics weight (0.01 to 1.0)\n",
        "]\n",
        "\n",
        "var_type = [\"int\", \"int\", \"factor\", \"factor\", \"num\", \"num\"]\n",
        "var_name = [\"l1\", \"num_layers\", \"activation\", \"optimizer\", \"lr_unified\", \"alpha\"]\n",
        "\n",
        "# Use var_trans to handle log-scale transformations automatically\n",
        "# Factor variables don't need transformations (None)\n",
        "var_trans = [None, None, None, None, \"log10\", \"log10\"]\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = SpotOptim(\n",
        "    fun=objective_pinn,\n",
        "    bounds=bounds,\n",
        "    var_type=var_type,\n",
        "    var_name=var_name,\n",
        "    var_trans=var_trans,  # Automatic log-scale handling!\n",
        "    max_iter=100,\n",
        "    n_initial=20,\n",
        "    seed=42,\n",
        "    verbose=True,\n",
        "    tensorboard_clean=True,\n",
        "    tensorboard_log=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display search space configuration.\n",
        "The `trans`column shows applied transformations. `lr_unified` and `alpha` use log10 transformation internally.\n",
        "This enables efficient exploration of log-scale parameters. All values shown are in original scale (not transformed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| name       | type   | lower   | upper   | default   | trans   |\n",
            "|------------|--------|---------|---------|-----------|---------|\n",
            "| l1         | int    | 16.0    | 128.0   | 72        | -       |\n",
            "| num_layers | int    | 1.0     | 4.0     | 2         | -       |\n",
            "| activation | factor | Tanh    | GELU    | Sigmoid   | -       |\n",
            "| optimizer  | factor | Adam    | AdamW   | RMSprop   | -       |\n",
            "| lr_unified | num    | 0.1     | 10.0    | 5.05      | log10   |\n",
            "| alpha      | num    | 0.01    | 1.0     | 0.505     | log10   |\n"
          ]
        }
      ],
      "source": [
        "# Display search space configuration\n",
        "design_table = optimizer.print_design_table(tablefmt=\"github\")\n",
        "print(design_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Configuration 1/20:\n",
            "  l1=124, num_layers=2, activation=Tanh, \n",
            "  optimizer=AdamW, lr_unified=0.1232, alpha=0.7988\n",
            "  Validation MSE: 0.184257\n",
            "\n",
            "Configuration 2/20:\n",
            "  l1=85, num_layers=1, activation=GELU, \n",
            "  optimizer=Adam, lr_unified=1.8320, alpha=0.3216\n",
            "  Validation MSE: 0.190136\n",
            "\n",
            "Configuration 3/20:\n",
            "  l1=119, num_layers=3, activation=ReLU, \n",
            "  optimizer=RMSprop, lr_unified=3.5038, alpha=0.1562\n",
            "  Validation MSE: 0.227455\n",
            "\n",
            "Configuration 4/20:\n",
            "  l1=51, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=SGD, lr_unified=1.0068, alpha=0.0647\n",
            "  Validation MSE: 0.207611\n",
            "\n",
            "Configuration 5/20:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=7.6660, alpha=0.2702\n",
            "  Validation MSE: 0.001272\n",
            "\n",
            "Configuration 6/20:\n",
            "  l1=96, num_layers=1, activation=Sigmoid, \n",
            "  optimizer=Adam, lr_unified=0.2254, alpha=0.0604\n",
            "  Validation MSE: 0.187897\n",
            "\n",
            "Configuration 7/20:\n",
            "  l1=43, num_layers=3, activation=ReLU, \n",
            "  optimizer=RMSprop, lr_unified=9.0424, alpha=0.0131\n",
            "  Validation MSE: 0.227926\n",
            "\n",
            "Configuration 8/20:\n",
            "  l1=34, num_layers=3, activation=ReLU, \n",
            "  optimizer=RMSprop, lr_unified=0.7265, alpha=0.7433\n",
            "  Validation MSE: 0.170226\n",
            "\n",
            "Configuration 9/20:\n",
            "  l1=57, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=SGD, lr_unified=0.2638, alpha=0.0216\n",
            "  Validation MSE: 0.204072\n",
            "\n",
            "Configuration 10/20:\n",
            "  l1=23, num_layers=1, activation=Sigmoid, \n",
            "  optimizer=RMSprop, lr_unified=1.5347, alpha=0.2447\n",
            "  Validation MSE: 0.166504\n",
            "\n",
            "Configuration 11/20:\n",
            "  l1=113, num_layers=1, activation=ReLU, \n",
            "  optimizer=SGD, lr_unified=2.1703, alpha=0.0111\n",
            "  Validation MSE: 0.229240\n",
            "\n",
            "Configuration 12/20:\n",
            "  l1=63, num_layers=4, activation=GELU, \n",
            "  optimizer=RMSprop, lr_unified=0.3789, alpha=0.1816\n",
            "  Validation MSE: 0.144710\n",
            "\n",
            "Configuration 13/20:\n",
            "  l1=17, num_layers=2, activation=Tanh, \n",
            "  optimizer=RMSprop, lr_unified=5.8971, alpha=0.0430\n",
            "  Validation MSE: 0.635350\n",
            "\n",
            "Configuration 14/20:\n",
            "  l1=108, num_layers=2, activation=GELU, \n",
            "  optimizer=SGD, lr_unified=0.1314, alpha=0.4823\n",
            "  Validation MSE: 0.187704\n",
            "\n",
            "Configuration 15/20:\n",
            "  l1=33, num_layers=3, activation=ReLU, \n",
            "  optimizer=RMSprop, lr_unified=0.1923, alpha=0.0891\n",
            "  Validation MSE: 0.145513\n",
            "\n",
            "Configuration 16/20:\n",
            "  l1=71, num_layers=3, activation=ReLU, \n",
            "  optimizer=SGD, lr_unified=2.9502, alpha=0.5457\n",
            "  Validation MSE: nan\n",
            "\n",
            "Configuration 17/20:\n",
            "  l1=81, num_layers=4, activation=Sigmoid, \n",
            "  optimizer=SGD, lr_unified=0.5118, alpha=0.0170\n",
            "  Validation MSE: 0.233001\n",
            "\n",
            "Configuration 18/20:\n",
            "  l1=93, num_layers=3, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.9017, alpha=0.0374\n",
            "  Validation MSE: 0.002114\n",
            "\n",
            "Configuration 19/20:\n",
            "  l1=105, num_layers=4, activation=Tanh, \n",
            "  optimizer=SGD, lr_unified=4.6709, alpha=0.0277\n",
            "  Validation MSE: 0.172364\n",
            "\n",
            "Configuration 20/20:\n",
            "  l1=77, num_layers=4, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=0.4537, alpha=0.1090\n",
            "  Validation MSE: 0.169493\n",
            "Warning: Found 1 NaN/inf value(s), replacing with inf + noise\n",
            "Warning: Removed 1 sample(s) with NaN/inf values\n",
            "Initial best: f(x) = 0.001272\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=46, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=7.3902, alpha=0.7494\n",
            "  Validation MSE: 0.004757\n",
            "Iteration 1: f(x) = 0.004757\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=92, num_layers=4, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=1.6518, alpha=0.0302\n",
            "  Validation MSE: 0.007043\n",
            "Iteration 2: f(x) = 0.007043\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=1, activation=GELU, \n",
            "  optimizer=AdamW, lr_unified=7.9231, alpha=1.0000\n",
            "  Validation MSE: 0.156382\n",
            "Iteration 3: f(x) = 0.156382\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=92, num_layers=3, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=1.0174, alpha=0.0331\n",
            "  Validation MSE: 0.004263\n",
            "Iteration 4: f(x) = 0.004263\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=46, num_layers=3, activation=ReLU, \n",
            "  optimizer=AdamW, lr_unified=7.1316, alpha=0.2415\n",
            "  Validation MSE: 0.126561\n",
            "Iteration 5: f(x) = 0.126561\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=3, activation=Tanh, \n",
            "  optimizer=SGD, lr_unified=0.9275, alpha=0.0281\n",
            "  Validation MSE: 0.189201\n",
            "Iteration 6: f(x) = 0.189201\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=46, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=7.2648, alpha=0.1691\n",
            "  Validation MSE: 0.017152\n",
            "Iteration 7: f(x) = 0.017152\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=2.6850, alpha=0.4854\n",
            "  Validation MSE: 0.065754\n",
            "Iteration 8: f(x) = 0.065754\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=92, num_layers=3, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=1.0796, alpha=0.0951\n",
            "  Validation MSE: 0.002019\n",
            "Iteration 9: f(x) = 0.002019\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=4, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.5463, alpha=0.0717\n",
            "  Validation MSE: 0.034104\n",
            "Iteration 10: f(x) = 0.034104\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=3, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=4.3913, alpha=0.0640\n",
            "  Validation MSE: 0.120202\n",
            "Iteration 11: f(x) = 0.120202\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=92, num_layers=3, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.2825, alpha=0.0492\n",
            "  Validation MSE: 0.145098\n",
            "Iteration 12: f(x) = 0.145098\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=10.0000, alpha=0.3499\n",
            "  Validation MSE: 0.002661\n",
            "Iteration 13: f(x) = 0.002661\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=10.0000, alpha=0.2760\n",
            "  Validation MSE: 0.018335\n",
            "Iteration 14: f(x) = 0.018335\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=7.5032, alpha=0.4661\n",
            "  Validation MSE: 0.007400\n",
            "Iteration 15: f(x) = 0.007400\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=46, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=6.9499, alpha=0.3519\n",
            "  Validation MSE: 0.046002\n",
            "Iteration 16: f(x) = 0.046002\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=46, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=10.0000, alpha=1.0000\n",
            "  Validation MSE: 0.000608\n",
            "Iteration 17: New best f(x) = 0.000608\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=3.8459, alpha=0.1335\n",
            "  Validation MSE: 0.072070\n",
            "Iteration 18: f(x) = 0.072070\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=7.5539, alpha=0.3634\n",
            "  Validation MSE: 0.042696\n",
            "Iteration 19: f(x) = 0.042696\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=10.0000, alpha=0.6149\n",
            "  Validation MSE: 0.022363\n",
            "Iteration 20: f(x) = 0.022363\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=ReLU, \n",
            "  optimizer=RMSprop, lr_unified=0.8998, alpha=0.3008\n",
            "  Validation MSE: 0.168257\n",
            "Iteration 21: f(x) = 0.168257\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=47, num_layers=3, activation=Sigmoid, \n",
            "  optimizer=SGD, lr_unified=8.7779, alpha=0.2198\n",
            "  Validation MSE: 0.192845\n",
            "Iteration 22: f(x) = 0.192845\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=46, num_layers=1, activation=Sigmoid, \n",
            "  optimizer=RMSprop, lr_unified=2.2793, alpha=0.0406\n",
            "  Validation MSE: 0.168656\n",
            "Iteration 23: f(x) = 0.168656\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=1, activation=ReLU, \n",
            "  optimizer=AdamW, lr_unified=5.2500, alpha=0.0130\n",
            "  Validation MSE: 0.157059\n",
            "Iteration 24: f(x) = 0.157059\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=3, activation=ReLU, \n",
            "  optimizer=Adam, lr_unified=8.0895, alpha=0.6516\n",
            "  Validation MSE: 0.141514\n",
            "Iteration 25: f(x) = 0.141514\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=90, num_layers=3, activation=ReLU, \n",
            "  optimizer=RMSprop, lr_unified=9.4996, alpha=0.0114\n",
            "  Validation MSE: 0.240721\n",
            "Iteration 26: f(x) = 0.240721\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=GELU, \n",
            "  optimizer=RMSprop, lr_unified=1.0065, alpha=0.7313\n",
            "  Validation MSE: 0.142543\n",
            "Iteration 27: f(x) = 0.142543\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=4, activation=ReLU, \n",
            "  optimizer=RMSprop, lr_unified=1.7067, alpha=0.0330\n",
            "  Validation MSE: 0.150795\n",
            "Iteration 28: f(x) = 0.150795\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=91, num_layers=1, activation=Sigmoid, \n",
            "  optimizer=Adam, lr_unified=0.7603, alpha=0.0103\n",
            "  Validation MSE: 0.187357\n",
            "Iteration 29: f(x) = 0.187357\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=46, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=10.0000, alpha=0.7335\n",
            "  Validation MSE: 0.001032\n",
            "Iteration 30: f(x) = 0.001032\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=69, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=SGD, lr_unified=0.4627, alpha=0.5375\n",
            "  Validation MSE: 0.201114\n",
            "Iteration 31: f(x) = 0.201114\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=29, num_layers=1, activation=Sigmoid, \n",
            "  optimizer=SGD, lr_unified=6.4584, alpha=0.0172\n",
            "  Validation MSE: 0.300731\n",
            "Iteration 32: f(x) = 0.300731\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=94, num_layers=3, activation=ReLU, \n",
            "  optimizer=Adam, lr_unified=6.6543, alpha=0.0202\n",
            "  Validation MSE: 0.134128\n",
            "Iteration 33: f(x) = 0.134128\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=44, num_layers=2, activation=GELU, \n",
            "  optimizer=RMSprop, lr_unified=3.5341, alpha=0.0402\n",
            "  Validation MSE: 0.196220\n",
            "Iteration 34: f(x) = 0.196220\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=3, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.9499, alpha=0.0402\n",
            "  Validation MSE: 0.009200\n",
            "Iteration 35: f(x) = 0.009200\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=4, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.6331, alpha=0.0578\n",
            "  Validation MSE: 0.001360\n",
            "Iteration 36: f(x) = 0.001360\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=4, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.7386, alpha=0.0472\n",
            "  Validation MSE: 0.001029\n",
            "Iteration 37: f(x) = 0.001029\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=4, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.6616, alpha=0.0494\n",
            "  Validation MSE: 0.001509\n",
            "Iteration 38: f(x) = 0.001509\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=4, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.7847, alpha=0.0558\n",
            "  Validation MSE: 0.000910\n",
            "Iteration 39: f(x) = 0.000910\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=4, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.7405, alpha=0.0510\n",
            "  Validation MSE: 0.003418\n",
            "Iteration 40: f(x) = 0.003418\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=4, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.7430, alpha=0.0352\n",
            "  Validation MSE: 0.007661\n",
            "Iteration 41: f(x) = 0.007661\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=4, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.8475, alpha=0.0516\n",
            "  Validation MSE: 0.001419\n",
            "Iteration 42: f(x) = 0.001419\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=4, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.8183, alpha=0.0517\n",
            "  Validation MSE: 0.001149\n",
            "Iteration 43: f(x) = 0.001149\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=4, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.6891, alpha=0.0467\n",
            "  Validation MSE: 0.001094\n",
            "Iteration 44: f(x) = 0.001094\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=4, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.7078, alpha=0.0473\n",
            "  Validation MSE: 0.001137\n",
            "Iteration 45: f(x) = 0.001137\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=92, num_layers=3, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=1.3055, alpha=0.0557\n",
            "  Validation MSE: 0.002005\n",
            "Iteration 46: f(x) = 0.002005\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=93, num_layers=4, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.7433, alpha=0.0487\n",
            "  Validation MSE: 0.001516\n",
            "Iteration 47: f(x) = 0.001516\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=94, num_layers=4, activation=ReLU, \n",
            "  optimizer=SGD, lr_unified=0.1692, alpha=0.0185\n",
            "  Validation MSE: 0.191693\n",
            "Iteration 48: f(x) = 0.191693\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=91, num_layers=4, activation=ReLU, \n",
            "  optimizer=SGD, lr_unified=1.5362, alpha=0.1961\n",
            "  Validation MSE: 0.154496\n",
            "Iteration 49: f(x) = 0.154496\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=128, num_layers=3, activation=Tanh, \n",
            "  optimizer=SGD, lr_unified=7.7228, alpha=0.6513\n",
            "  Validation MSE: 0.146123\n",
            "Iteration 50: f(x) = 0.146123\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=6.8643, alpha=0.1979\n",
            "  Validation MSE: 0.001014\n",
            "Iteration 51: f(x) = 0.001014\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=10.0000, alpha=0.4381\n",
            "  Validation MSE: 0.001337\n",
            "Iteration 52: f(x) = 0.001337\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=5.8306, alpha=0.6987\n",
            "  Validation MSE: 0.000632\n",
            "Iteration 53: f(x) = 0.000632\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=7.0862, alpha=0.5880\n",
            "  Validation MSE: 0.000587\n",
            "Iteration 54: New best f(x) = 0.000587\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=6.7583, alpha=0.6020\n",
            "  Validation MSE: 0.000615\n",
            "Iteration 55: f(x) = 0.000615\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=45, num_layers=2, activation=Sigmoid, \n",
            "  optimizer=AdamW, lr_unified=6.8660, alpha=0.5955\n",
            "  Validation MSE: 0.018555\n",
            "Iteration 56: f(x) = 0.018555\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=92, num_layers=3, activation=Tanh, \n",
            "  optimizer=Adam, lr_unified=0.8984, alpha=0.0352\n",
            "  Validation MSE: 0.003791\n",
            "Iteration 57: f(x) = 0.003791\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=52, num_layers=3, activation=ReLU, \n",
            "  optimizer=RMSprop, lr_unified=0.1454, alpha=0.0135\n",
            "  Validation MSE: 0.156813\n",
            "Iteration 58: f(x) = 0.156813\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=60, num_layers=1, activation=ReLU, \n",
            "  optimizer=RMSprop, lr_unified=0.1454, alpha=0.0135\n",
            "  Validation MSE: 0.165294\n",
            "Iteration 59: f(x) = 0.165294\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=94, num_layers=1, activation=ReLU, \n",
            "  optimizer=RMSprop, lr_unified=0.7199, alpha=0.3696\n",
            "  Validation MSE: 0.167290\n",
            "Iteration 60: f(x) = 0.167290\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=16, num_layers=2, activation=ReLU, \n",
            "  optimizer=RMSprop, lr_unified=0.7199, alpha=0.3696\n",
            "  Validation MSE: 0.186946\n",
            "Iteration 61: f(x) = 0.186946\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=80, num_layers=3, activation=ReLU, \n",
            "  optimizer=RMSprop, lr_unified=6.1768, alpha=0.0158\n",
            "  Validation MSE: 0.237630\n",
            "Iteration 62: f(x) = 0.237630\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=89, num_layers=3, activation=Tanh, \n",
            "  optimizer=RMSprop, lr_unified=6.1768, alpha=0.0158\n",
            "  Validation MSE: 15.165651\n",
            "Iteration 63: f(x) = 15.165651\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=128, num_layers=3, activation=Tanh, \n",
            "  optimizer=SGD, lr_unified=3.4118, alpha=0.2106\n",
            "  Validation MSE: 0.179169\n",
            "Iteration 64: f(x) = 0.179169\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=128, num_layers=3, activation=Tanh, \n",
            "  optimizer=SGD, lr_unified=5.6648, alpha=0.4244\n",
            "  Validation MSE: 0.149132\n",
            "Iteration 65: f(x) = 0.149132\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=128, num_layers=3, activation=Tanh, \n",
            "  optimizer=SGD, lr_unified=6.0671, alpha=0.4666\n",
            "  Validation MSE: 0.152812\n",
            "Iteration 66: f(x) = 0.152812\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=128, num_layers=3, activation=Tanh, \n",
            "  optimizer=SGD, lr_unified=6.1392, alpha=0.4743\n",
            "  Validation MSE: 0.152596\n",
            "Iteration 67: f(x) = 0.152596\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=128, num_layers=3, activation=Tanh, \n",
            "  optimizer=SGD, lr_unified=1.8251, alpha=1.0000\n",
            "  Validation MSE: 0.169831\n",
            "Iteration 68: f(x) = 0.169831\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=128, num_layers=3, activation=Tanh, \n",
            "  optimizer=SGD, lr_unified=3.7577, alpha=0.5980\n",
            "  Validation MSE: 0.154101\n",
            "Iteration 69: f(x) = 0.154101\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=128, num_layers=3, activation=Tanh, \n",
            "  optimizer=SGD, lr_unified=4.4943, alpha=0.5273\n",
            "  Validation MSE: 0.153600\n",
            "Iteration 70: f(x) = 0.153600\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=128, num_layers=4, activation=Tanh, \n",
            "  optimizer=SGD, lr_unified=4.5856, alpha=0.5219\n",
            "  Validation MSE: 0.146409\n",
            "Iteration 71: f(x) = 0.146409\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=128, num_layers=3, activation=Tanh, \n",
            "  optimizer=SGD, lr_unified=4.5945, alpha=0.5216\n",
            "  Validation MSE: 0.153745\n",
            "Iteration 72: f(x) = 0.153745\n",
            "\n",
            "Configuration 1/1:\n",
            "  l1=128, num_layers=3, activation=Tanh, \n",
            "  optimizer=SGD, lr_unified=4.7404, alpha=0.5037\n"
          ]
        }
      ],
      "source": [
        "#| label: pinn-run-optimization-pinn2\n",
        "result = optimizer.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results Analysis\n",
        "\n",
        "## Best Configuration\n",
        "\n",
        "Display best hyperparameters using print_best() method.\n",
        "With `var_trans`, results are already in original scale!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-best-configuration-pinn2\n",
        "optimizer.print_best(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Store values for later use in visualizations.  Values are already in original scale thanks to `var_trans`.\n",
        "Factor variables are returned as strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_l1 = int(result.x[0])\n",
        "best_num_layers = int(result.x[1])\n",
        "best_activation = result.x[2]\n",
        "best_optimizer = result.x[3]\n",
        "best_lr_unified = result.x[4]\n",
        "best_alpha = result.x[5]\n",
        "best_val_error = result.fun\n",
        "\n",
        "print(f\"Best activation: {best_activation}\")\n",
        "print(f\"Best optimizer: {best_optimizer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results Table with Importance Scores\n",
        "\n",
        "Display comprehensive results table with importance scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-results-table-pinn2\n",
        "table = optimizer.print_results_table(show_importance=True, tablefmt=\"github\")\n",
        "print(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-optimization-history-pinn2\n",
        "optimizer.plot_progress(log_y=True, ylabel=\"Validation MSE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Surrogate Visualization\n",
        "\n",
        "Visualize the surrogate model's learned response surface for the most important hyperparameter combinations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-surrogate-visualization-pinn2\n",
        "# Plot top 3 most important hyperparameter combinations\n",
        "optimizer.plot_important_hyperparameter_contour(max_imp=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameter Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-parameter-scatter-pinn2\n",
        "optimizer.plot_parameter_scatter(\n",
        "    result,\n",
        "    ylabel=\"Validation MSE\",\n",
        "    cmap=\"plasma\",\n",
        "    figsize=(14, 12)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Final Model with Best Hyperparameters\n",
        "\n",
        "Now let's train a final model with the optimized hyperparameters using DataLoaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-final-model-training-pinn2\n",
        "print(\"Training final model with best hyperparameters using DataLoaders...\")\n",
        "print(f\"Training for 30,000 epochs...\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create DataLoaders for final training\n",
        "final_batch_size = 16\n",
        "train_loader_final = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=final_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "collocation_loader_final = DataLoader(\n",
        "    collocation_dataset,\n",
        "    batch_size=len(collocation_dataset),\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# Create model with best hyperparameters\n",
        "final_model = LinearRegressor(\n",
        "    input_dim=1,\n",
        "    output_dim=1,\n",
        "    l1=best_l1,\n",
        "    num_hidden_layers=best_num_layers,\n",
        "    activation=best_activation,\n",
        "    lr=best_lr_unified\n",
        ")\n",
        "\n",
        "optimizer_final = final_model.get_optimizer(best_optimizer)\n",
        "\n",
        "# Training with history tracking\n",
        "loss_history = []\n",
        "n_epochs_final = 30000\n",
        "\n",
        "for epoch in range(n_epochs_final):\n",
        "    final_model.train()\n",
        "    epoch_loss = 0.0\n",
        "    \n",
        "    # Get collocation points\n",
        "    t_physics_batch = next(iter(collocation_loader_final))\n",
        "    t_physics_batch = t_physics_batch.requires_grad_(True)\n",
        "    \n",
        "    # Iterate over training batches\n",
        "    for batch_t, batch_y in train_loader_final:\n",
        "        optimizer_final.zero_grad()\n",
        "        \n",
        "        # Data Loss\n",
        "        y_pred = final_model(batch_t)\n",
        "        loss_data = torch.mean((y_pred - batch_y)**2)\n",
        "        \n",
        "        # Physics Loss\n",
        "        y_physics = final_model(t_physics_batch)\n",
        "        dy_dt = torch.autograd.grad(\n",
        "            y_physics,\n",
        "            t_physics_batch,\n",
        "            torch.ones_like(y_physics),\n",
        "            create_graph=True,\n",
        "            retain_graph=True\n",
        "        )[0]\n",
        "        \n",
        "        physics_residual = dy_dt + 0.1 * y_physics - torch.sin(np.pi * t_physics_batch / 2)\n",
        "        loss_physics = torch.mean(physics_residual**2)\n",
        "        \n",
        "        # Total Loss\n",
        "        loss = loss_data + best_alpha * loss_physics\n",
        "        loss.backward()\n",
        "        optimizer_final.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    # Record average loss every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        avg_loss = epoch_loss / len(train_loader_final)\n",
        "        loss_history.append(avg_loss)\n",
        "    \n",
        "    if (epoch + 1) % 5000 == 0:\n",
        "        avg_loss = epoch_loss / len(train_loader_final)\n",
        "        print(f\"  Epoch {epoch+1}/{n_epochs_final}: Avg Loss = {avg_loss:.6f}\")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_history, linewidth=1.5)\n",
        "plt.xlabel('Iteration (100)', fontsize=11)\n",
        "plt.ylabel('Average Total Loss per Batch', fontsize=11)\n",
        "plt.title('Final Model Training History', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-final-model-evaluation-pinn2\n",
        "# Create validation DataLoader for evaluation\n",
        "val_loader_final = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=len(val_dataset),\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# Evaluate on validation set\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    # Validation MSE using DataLoader\n",
        "    val_mse_total = 0.0\n",
        "    for batch_t, batch_y in val_loader_final:\n",
        "        y_pred = final_model(batch_t)\n",
        "        val_mse_total += torch.mean((y_pred - batch_y)**2).item()\n",
        "    final_val_mse = val_mse_total / len(val_loader_final)\n",
        "    \n",
        "    # Predict on full domain for visualization\n",
        "    y_pred_full = final_model(x_exact)\n",
        "    full_mse = torch.mean((y_pred_full - y_exact)**2).item()\n",
        "    \n",
        "    # Compute maximum absolute error\n",
        "    max_error = torch.max(torch.abs(y_pred_full - y_exact)).item()\n",
        "\n",
        "print(\"\\nFinal Model Performance:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"  Validation MSE: {final_val_mse:.6f}\")\n",
        "print(f\"  Full domain MSE: {full_mse:.6f}\")\n",
        "print(f\"  Maximum absolute error: {max_error:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize Final Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-final-model-visualization-pinn2\n",
        "# Generate predictions\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = final_model(x_exact)\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# Plot 1: Solution comparison\n",
        "ax1 = axes[0]\n",
        "ax1.plot(x_exact.numpy(), y_exact.numpy(), 'b-', linewidth=2.5, \n",
        "         label='Exact solution', alpha=0.8)\n",
        "ax1.plot(x_exact.numpy(), y_pred.numpy(), 'r--', linewidth=2, \n",
        "         label='PINN prediction (optimized)', alpha=0.8)\n",
        "\n",
        "# Plot training data from dataset\n",
        "ax1.scatter(train_dataset.t.numpy(), train_dataset.y.numpy(), \n",
        "            color='tab:orange', s=80, label='Training data', \n",
        "            zorder=5, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "# Plot collocation points\n",
        "t_collocation = collocation_dataset.t.detach()\n",
        "ax1.scatter(t_collocation.numpy(), \n",
        "            final_model(t_collocation).detach().numpy(),\n",
        "            color='green', marker='x', s=50, \n",
        "            label='Collocation points', alpha=0.7, zorder=4)\n",
        "\n",
        "ax1.set_xlabel('Time t', fontsize=12)\n",
        "ax1.set_ylabel('Solution y(t)', fontsize=12)\n",
        "ax1.set_title('Optimized PINN Solution vs Exact Solution', fontsize=13, fontweight='bold')\n",
        "ax1.legend(fontsize=11, loc='best')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Error\n",
        "ax2 = axes[1]\n",
        "error = torch.abs(y_pred - y_exact)\n",
        "ax2.plot(x_exact.numpy(), error.numpy(), 'r-', linewidth=2)\n",
        "ax2.axhline(y=max_error, color='gray', linestyle='--', linewidth=1, \n",
        "            label=f'Max error = {max_error:.6f}')\n",
        "ax2.set_xlabel('Time t', fontsize=12)\n",
        "ax2.set_ylabel('Absolute Error |y_exact - y_PINN|', fontsize=12)\n",
        "ax2.set_title('Approximation Error (Optimized Model)', fontsize=13, fontweight='bold')\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison with Baseline\n",
        "\n",
        "Let's compare the optimized configuration with a baseline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-baseline-comparison-pinn2\n",
        "print(\"Training baseline model for comparison...\")\n",
        "\n",
        "# Baseline configuration (from basic PINN demo)\n",
        "baseline_config = {\n",
        "    'l1': 32,\n",
        "    'num_layers': 3,\n",
        "    'activation': 'Tanh',\n",
        "    'optimizer': 'Adam',\n",
        "    'lr_unified': 3.0,\n",
        "    'alpha': 0.06\n",
        "}\n",
        "\n",
        "print(f\"\\nBaseline Configuration:\")\n",
        "for key, val in baseline_config.items():\n",
        "    print(f\"  {key}: {val}\")\n",
        "\n",
        "# Train baseline\n",
        "baseline_error = train_pinn(\n",
        "    l1=baseline_config['l1'],\n",
        "    num_layers=baseline_config['num_layers'],\n",
        "    activation=baseline_config['activation'],\n",
        "    optimizer_name=baseline_config['optimizer'],\n",
        "    lr_unified=baseline_config['lr_unified'],\n",
        "    alpha=baseline_config['alpha'],\n",
        "    n_epochs=N_EPOCHS,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(f\"\\nValidation MSE Comparison:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"  Baseline: {baseline_error:.6f}\")\n",
        "print(f\"  Optimized: {best_val_error:.6f}\")\n",
        "print(f\"  Improvement: {(1 - best_val_error/baseline_error)*100:.1f}%\")\n",
        "\n",
        "# Bar plot comparison\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "configs = ['Baseline', 'Optimized']\n",
        "errors = [baseline_error, best_val_error]\n",
        "colors = ['tab:blue', 'tab:green']\n",
        "\n",
        "bars = ax.bar(configs, errors, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, error in zip(bars, errors):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{error:.6f}',\n",
        "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Validation MSE', fontsize=12)\n",
        "ax.set_title('Model Comparison: Baseline vs Optimized', fontsize=13, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Sensitivity Analysis\n",
        "\n",
        "Let's analyze how sensitive the model is to each hyperparameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-sensitivity-analysis-pinn2\n",
        "# Compute correlation between parameters and error\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Get optimization history and parameters\n",
        "history = optimizer.y_\n",
        "all_params = optimizer.X_\n",
        "\n",
        "# Define parameter metadata\n",
        "param_names = ['l1 (neurons)', 'num_layers', 'activation', 'optimizer', 'lr_unified', 'alpha']\n",
        "param_indices = [0, 1, 2, 3, 4, 5]\n",
        "transforms = [lambda x: x, lambda x: x, lambda x: x, lambda x: x, lambda x: x, lambda x: x]\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "for idx, (ax, name, param_idx, transform) in enumerate(\n",
        "    zip(axes.flat, param_names, param_indices, transforms)\n",
        "):\n",
        "    param_values = all_params[:, param_idx]\n",
        "    errors = history\n",
        "    \n",
        "    # Handle different parameter types\n",
        "    if idx < 2:  # Integer parameters (l1, num_layers)\n",
        "        corr, p_value = spearmanr(param_values, errors)\n",
        "        ax.scatter(param_values, errors, alpha=0.6, s=40, edgecolors='black', linewidth=0.5)\n",
        "        ax.scatter([result.x[param_idx]], [best_val_error], \n",
        "                  color='red', s=200, marker='*', edgecolors='black', linewidth=1.5, \n",
        "                  label='Best', zorder=5)\n",
        "        ax.set_yscale('log')\n",
        "    elif idx < 4:  # Factor parameters (activation, optimizer)\n",
        "        # For categorical variables, create a box plot or strip plot\n",
        "        unique_vals = np.unique(param_values)\n",
        "        positions = {val: i for i, val in enumerate(unique_vals)}\n",
        "        numeric_vals = np.array([positions[val] for val in param_values])\n",
        "        \n",
        "        ax.scatter(numeric_vals, errors, alpha=0.6, s=40, edgecolors='black', linewidth=0.5)\n",
        "        \n",
        "        # Get best value - handle both string and potential numeric representations\n",
        "        best_val = result.x[param_idx]\n",
        "        # Ensure best_val is in positions dict, if not, add it\n",
        "        if best_val not in positions:\n",
        "            # This shouldn't happen, but handle gracefully\n",
        "            positions[best_val] = len(positions)\n",
        "            unique_vals = np.append(unique_vals, best_val)\n",
        "        \n",
        "        best_pos = positions[best_val]\n",
        "        ax.scatter([best_pos], [best_val_error], \n",
        "                  color='red', s=200, marker='*', edgecolors='black', linewidth=1.5, \n",
        "                  label='Best', zorder=5)\n",
        "        ax.set_xticks(range(len(unique_vals)))\n",
        "        ax.set_xticklabels(unique_vals, rotation=45, ha='right')\n",
        "        ax.set_yscale('log')\n",
        "        \n",
        "        # For categorical, correlation doesn't apply\n",
        "        corr, p_value = np.nan, np.nan\n",
        "    else:  # Log-scale parameters (lr_unified, alpha)\n",
        "        corr, p_value = spearmanr(np.log10(param_values), np.log10(errors))\n",
        "        ax.scatter(param_values, errors, alpha=0.6, s=40, edgecolors='black', linewidth=0.5)\n",
        "        ax.scatter([result.x[param_idx]], [best_val_error], \n",
        "                  color='red', s=200, marker='*', edgecolors='black', linewidth=1.5, \n",
        "                  label='Best', zorder=5)\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_yscale('log')\n",
        "    \n",
        "    ax.set_xlabel(name, fontsize=11)\n",
        "    ax.set_ylabel('Validation MSE', fontsize=11)\n",
        "    if not np.isnan(corr):\n",
        "        ax.set_title(f'{name} Sensitivity\\nCorrelation: {corr:.3f} (p={p_value:.3f})', \n",
        "                    fontsize=11)\n",
        "    else:\n",
        "        ax.set_title(f'{name} Sensitivity', fontsize=11)\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSensitivity Analysis (Spearman Correlation):\")\n",
        "print(\"-\" * 50)\n",
        "for name, param_idx in zip(param_names, param_indices):\n",
        "    param_values = all_params[:, param_idx]\n",
        "    \n",
        "    # Handle different parameter types\n",
        "    if param_idx < 2:  # Integer parameters\n",
        "        corr, p_value = spearmanr(param_values, history)\n",
        "    elif param_idx < 4:  # Factor parameters (skip correlation)\n",
        "        print(f\"  {name:20s}: (categorical variable, use visual inspection)\")\n",
        "        continue\n",
        "    else:  # Log-scale parameters\n",
        "        corr, p_value = spearmanr(np.log10(param_values), np.log10(history))\n",
        "    \n",
        "    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
        "    print(f\"  {name:20s}: {corr:+.3f} (p={p_value:.3f}) {significance}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary\n",
        "\n",
        "## Key Findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: pinn-summary-pinn2\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"HYPERPARAMETER OPTIMIZATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. BEST CONFIGURATION FOUND:\")\n",
        "print(f\"   - Neurons per layer (l1): {best_l1}\")\n",
        "print(f\"   - Number of hidden layers: {best_num_layers}\")\n",
        "print(f\"   - Activation function: {best_activation}\")\n",
        "print(f\"   - Optimizer: {best_optimizer}\")\n",
        "print(f\"   - Learning rate: {best_lr_unified:.4f}\")\n",
        "print(f\"   - Physics weight (alpha): {best_alpha:.4f}\")\n",
        "\n",
        "print(\"\\n2. PERFORMANCE:\")\n",
        "print(f\"   - Validation MSE: {best_val_error:.6f}\")\n",
        "print(f\"   - Full domain MSE: {full_mse:.6f}\")\n",
        "print(f\"   - Maximum absolute error: {max_error:.6f}\")\n",
        "\n",
        "print(\"\\n3. OPTIMIZATION STATISTICS:\")\n",
        "print(f\"   - Total evaluations: {result.nfev}\")\n",
        "print(f\"   - Initial best: {history[0]:.6f}\")\n",
        "print(f\"   - Final best: {best_val_error:.6f}\")\n",
        "print(f\"   - Improvement: {(1 - best_val_error/history[0])*100:.1f}%\")\n",
        "\n",
        "print(\"\\n4. COMPARISON TO BASELINE:\")\n",
        "print(f\"   - Baseline MSE: {baseline_error:.6f}\")\n",
        "print(f\"   - Optimized MSE: {best_val_error:.6f}\")\n",
        "print(f\"   - Improvement: {(1 - best_val_error/baseline_error)*100:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendations\n",
        "\n",
        "Based on the hyperparameter optimization results:\n",
        "\n",
        "1. **Network Architecture**:\n",
        "   - The optimal architecture was found with `{best_l1}` neurons and `{best_num_layers}` hidden layers\n",
        "   - Best activation function: `{best_activation}`\n",
        "   - This balances model capacity with training efficiency\n",
        "\n",
        "2. **Optimizer Selection**:\n",
        "   - Best optimizer: `{best_optimizer}`\n",
        "   - Different optimizers have different convergence characteristics for PINNs\n",
        "\n",
        "3. **Learning Rate**:\n",
        "   - Optimal unified learning rate: `{best_lr_unified:.4f}`\n",
        "   - This translates to an actual Adam learning rate of `{best_lr_unified * 0.001:.6f}`\n",
        "\n",
        "4. **Physics Loss Weight**:\n",
        "   - Optimal alpha: `{best_alpha:.4f}`\n",
        "   - This balances data fitting with physics constraint satisfaction\n",
        "\n",
        "5. **Training Strategy**:\n",
        "   - Start with a broad search space to explore different architectures\n",
        "   - Use `var_trans` with \"log10\" for learning rate and physics weight parameters\n",
        "   - This enables efficient exploration of log-scale parameters without manual transformations\n",
        "   - Validate on held-out data to prevent overfitting to training points\n",
        "\n",
        "6. **Benefits of var_trans and Factor Variables**:\n",
        "   - **Factor variables**: Categorical choices (activation, optimizer) handled automatically\n",
        "   - SpotOptim maps strings to integers internally and back to strings in results\n",
        "   - **Cleaner code**: No manual `10**x` conversions in objective function\n",
        "   - **Fewer errors**: Eliminates confusion about which scale values are in\n",
        "   - **Better optimization**: Searches efficiently in transformed space\n",
        "   - **Easier interpretation**: All results displayed in original scale\n",
        "\n",
        "## Using These Results\n",
        "\n",
        "To use the optimized configuration in your own PINN problems:\n",
        "\n",
        "```python\n",
        "# Create optimized PINN\n",
        "model = LinearRegressor(\n",
        "    input_dim=1,\n",
        "    output_dim=1,\n",
        "    l1={best_l1},\n",
        "    num_hidden_layers={best_num_layers},\n",
        "    activation=\"{best_activation}\",\n",
        "    lr={best_lr_unified:.4f}\n",
        ")\n",
        "\n",
        "optimizer = model.get_optimizer(\"{best_optimizer}\")\n",
        "\n",
        "# Use alpha={best_alpha:.4f} for physics loss weight\n",
        "loss = data_loss + {best_alpha:.4f} * physics_loss\n",
        "```\n",
        "\n",
        "## Using var_trans for Your Hyperparameter Optimization\n",
        "\n",
        "When setting up optimization for your own PINN problems:\n",
        "\n",
        "```python\n",
        "from spotoptim import SpotOptim\n",
        "\n",
        "# Define search space with factor variables and log-scale parameters\n",
        "bounds = [\n",
        "    (16, 128),                                    # neurons (integer)\n",
        "    (1, 4),                                       # layers (integer)\n",
        "    (\"Tanh\", \"ReLU\", \"Sigmoid\", \"GELU\"),       # activation (factor)\n",
        "    (\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\"),        # optimizer (factor)\n",
        "    (0.1, 10.0),                                  # learning rate (log-scale)\n",
        "    (0.01, 1.0)                                   # physics weight (log-scale)\n",
        "]\n",
        "\n",
        "var_type = [\"int\", \"int\", \"factor\", \"factor\", \"num\", \"num\"]\n",
        "var_trans = [None, None, None, None, \"log10\", \"log10\"]\n",
        "\n",
        "opt = SpotOptim(\n",
        "    fun=your_objective_function,\n",
        "    bounds=bounds,\n",
        "    var_type=var_type,\n",
        "    var_trans=var_trans,  # Automatic log-scale and factor handling!\n",
        "    max_iter=20,\n",
        "    n_initial=10\n",
        ")\n",
        "\n",
        "result = opt.optimize()\n",
        "```\n",
        "\n",
        "Your objective function receives parameters in **original scale** - no manual transformations needed!\n",
        "\n",
        "## Future Directions\n",
        "\n",
        "Consider exploring:\n",
        "\n",
        "1. **Adaptive physics weights** that change during training\n",
        "2. **Architecture search** including skip connections or residual blocks\n",
        "3. **Batch size optimization** as an additional hyperparameter\n",
        "4. **Multi-objective optimization** balancing accuracy and computational cost\n",
        "5. **Transfer learning** from pre-optimized configurations\n",
        "6. **Learning rate schedules** with different decay strategies\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: The specific optimal values depend on the problem, data distribution, and computational budget. Always validate results on held-out test data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "spotoptim",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
