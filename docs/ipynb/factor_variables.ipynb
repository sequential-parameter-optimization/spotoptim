{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0091fec",
   "metadata": {},
   "source": [
    "# Factor Variables for Categorical Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0c92d",
   "metadata": {},
   "source": [
    "SpotOptim supports factor variables for optimizing categorical hyperparameters, such as activation functions, optimizers, or any discrete string-based choices. Factor variables are automatically converted between string values (external interface) and integers (internal optimization), making categorical optimization seamless.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**What are Factor Variables?**\n",
    "\n",
    "Factor variables allow you to specify categorical choices as tuples of strings in the bounds. SpotOptim handles the conversion:\n",
    "\n",
    "1. **String tuples in bounds** → Internal integer mapping (0, 1, 2, ...)\n",
    "2. **Optimization uses integers** internally for surrogate modeling\n",
    "3. **Objective function receives strings** after automatic conversion\n",
    "4. **Results return strings** (not integers)\n",
    "\n",
    "**Module**: `spotoptim.SpotOptim`\n",
    "\n",
    "**Key Features**:\n",
    "\n",
    "- Define categorical choices as string tuples: `(\"ReLU\", \"Sigmoid\", \"Tanh\")`\n",
    "- Automatic integer↔string conversion\n",
    "- Seamless integration with neural network hyperparameters\n",
    "- Mix factor variables with numeric/integer variables\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### Basic Factor Variable Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed22767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing activation: Tanh\n",
      "Testing activation: ReLU\n",
      "Testing activation: Sigmoid\n",
      "Testing activation: Sigmoid\n",
      "Testing activation: Sigmoid\n",
      "Testing activation: Tanh\n",
      "Testing activation: ReLU\n",
      "Testing activation: Tanh\n",
      "Testing activation: Tanh\n",
      "Testing activation: LeakyReLU\n",
      "Testing activation: LeakyReLU\n",
      "Testing activation: LeakyReLU\n",
      "Testing activation: LeakyReLU\n",
      "Testing activation: LeakyReLU\n",
      "Testing activation: LeakyReLU\n",
      "Testing activation: LeakyReLU\n",
      "Testing activation: ReLU\n",
      "Testing activation: LeakyReLU\n",
      "Testing activation: LeakyReLU\n",
      "Testing activation: LeakyReLU\n",
      "Testing activation: LeakyReLU\n",
      "Testing activation: ReLU\n",
      "Testing activation: LeakyReLU\n",
      "Testing activation: ReLU\n",
      "Testing activation: LeakyReLU\n",
      "\n",
      "Best activation: ReLU\n",
      "Best score: 3386.3000\n",
      "Testing activation: ReLU\n",
      "Testing activation: LeakyReLU\n",
      "\n",
      "Best activation: ReLU\n",
      "Best score: 3386.3000\n"
     ]
    }
   ],
   "source": [
    "from spotoptim import SpotOptim\n",
    "import numpy as np\n",
    "\n",
    "def objective_function(X):\n",
    "    \"\"\"Objective function receives string values.\"\"\"\n",
    "    results = []\n",
    "    for params in X:\n",
    "        activation = params[0]  # This is a string!\n",
    "        print(f\"Testing activation: {activation}\")\n",
    "        \n",
    "        # Simple scoring based on activation choice (for demonstration)\n",
    "        # In real use, you would train a model and return actual performance\n",
    "        scores = {\n",
    "            \"ReLU\": 3500.0,\n",
    "            \"Sigmoid\": 4200.0,\n",
    "            \"Tanh\": 3800.0,\n",
    "            \"LeakyReLU\": 3600.0\n",
    "        }\n",
    "        score = scores.get(activation, 5000.0) + np.random.normal(0, 100)\n",
    "        results.append(score)\n",
    "    return np.array(results)  # Return numpy array\n",
    "\n",
    "# Define bounds with factor variable\n",
    "optimizer = SpotOptim(\n",
    "    fun=objective_function,\n",
    "    bounds=[(\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\")],\n",
    "    var_type=[\"factor\"],\n",
    "    max_iter=20,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "result = optimizer.optimize()\n",
    "print(f\"\\nBest activation: {result.x[0]}\")  # Returns string, e.g., \"ReLU\"\n",
    "print(f\"Best score: {result.fun:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f7427",
   "metadata": {},
   "source": [
    "### Neural Network Activation Function Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a715b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best activation function: ELU\n",
      "Best test MSE: 2460.3892\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from spotoptim import SpotOptim\n",
    "from spotoptim.data import get_diabetes_dataloaders\n",
    "from spotoptim.nn.linear_regressor import LinearRegressor\n",
    "import numpy as np\n",
    "\n",
    "def train_and_evaluate(X):\n",
    "    \"\"\"Train models with different activation functions.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for params in X:\n",
    "        activation = params[0]  # String: \"ReLU\", \"Sigmoid\", etc.\n",
    "        \n",
    "        # Load data\n",
    "        train_loader, test_loader, _ = get_diabetes_dataloaders()\n",
    "        \n",
    "        # Create model with the activation function\n",
    "        model = LinearRegressor(\n",
    "            input_dim=10,\n",
    "            output_dim=1,\n",
    "            l1=64,\n",
    "            num_hidden_layers=2,\n",
    "            activation=activation  # Pass string directly!\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(50):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                predictions = model(batch_X)\n",
    "                test_loss += criterion(predictions, batch_y).item()\n",
    "        \n",
    "        avg_loss = test_loss / len(test_loader)\n",
    "        results.append(avg_loss)\n",
    "    \n",
    "    return np.array(results)  # Return numpy array\n",
    "\n",
    "# Optimize activation function choice\n",
    "optimizer = SpotOptim(\n",
    "    fun=train_and_evaluate,\n",
    "    bounds=[(\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\", \"ELU\")],\n",
    "    var_type=[\"factor\"],\n",
    "    max_iter=30\n",
    ")\n",
    "\n",
    "result = optimizer.optimize()\n",
    "print(f\"Best activation function: {result.x[0]}\")\n",
    "print(f\"Best test MSE: {result.fun:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e81ee",
   "metadata": {},
   "source": [
    "## Mixed Variable Types\n",
    "\n",
    "### Combining Factor, Integer, and Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ccd3dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.001000, l1=127, layers=3, activation=Sigmoid\n",
      "lr=0.001000, l1=103, layers=3, activation=LeakyReLU\n",
      "lr=0.000100, l1=113, layers=2, activation=Sigmoid\n",
      "lr=0.010000, l1=88, layers=4, activation=Sigmoid\n",
      "lr=0.000100, l1=113, layers=2, activation=Sigmoid\n",
      "lr=0.010000, l1=88, layers=4, activation=Sigmoid\n",
      "lr=0.000100, l1=35, layers=3, activation=Tanh\n",
      "lr=0.010000, l1=78, layers=2, activation=Tanh\n",
      "lr=0.000100, l1=35, layers=3, activation=Tanh\n",
      "lr=0.010000, l1=78, layers=2, activation=Tanh\n",
      "lr=0.010000, l1=18, layers=2, activation=LeakyReLU\n",
      "lr=0.000100, l1=66, layers=0, activation=Tanh\n",
      "lr=0.001000, l1=51, layers=1, activation=ReLU\n",
      "lr=0.001000, l1=42, layers=1, activation=ReLU\n",
      "lr=0.010000, l1=18, layers=2, activation=LeakyReLU\n",
      "lr=0.000100, l1=66, layers=0, activation=Tanh\n",
      "lr=0.001000, l1=51, layers=1, activation=ReLU\n",
      "lr=0.001000, l1=42, layers=1, activation=ReLU\n",
      "lr=0.010000, l1=82, layers=3, activation=Tanh\n",
      "lr=0.010000, l1=82, layers=3, activation=Tanh\n",
      "lr=0.001000, l1=98, layers=3, activation=LeakyReLU\n",
      "lr=0.001000, l1=98, layers=3, activation=LeakyReLU\n",
      "lr=0.000100, l1=16, layers=0, activation=ReLU\n",
      "lr=0.001000, l1=101, layers=3, activation=LeakyReLU\n",
      "lr=0.000100, l1=16, layers=0, activation=ReLU\n",
      "lr=0.001000, l1=101, layers=3, activation=LeakyReLU\n",
      "lr=0.010000, l1=20, layers=4, activation=LeakyReLU\n",
      "lr=0.010000, l1=95, layers=4, activation=ReLU\n",
      "lr=0.010000, l1=20, layers=4, activation=LeakyReLU\n",
      "lr=0.010000, l1=95, layers=4, activation=ReLU\n",
      "lr=0.010000, l1=94, layers=0, activation=Tanh\n",
      "lr=0.001000, l1=98, layers=4, activation=Sigmoid\n",
      "lr=0.010000, l1=94, layers=0, activation=Tanh\n",
      "lr=0.001000, l1=98, layers=4, activation=Sigmoid\n",
      "lr=0.010000, l1=19, layers=3, activation=LeakyReLU\n",
      "lr=0.010000, l1=19, layers=3, activation=LeakyReLU\n",
      "lr=0.001000, l1=99, layers=2, activation=LeakyReLU\n",
      "lr=0.001000, l1=99, layers=2, activation=LeakyReLU\n",
      "lr=0.010000, l1=102, layers=2, activation=LeakyReLU\n",
      "lr=0.010000, l1=102, layers=2, activation=LeakyReLU\n",
      "lr=0.010000, l1=80, layers=2, activation=ReLU\n",
      "lr=0.010000, l1=80, layers=0, activation=Tanh\n",
      "lr=0.010000, l1=80, layers=2, activation=ReLU\n",
      "lr=0.010000, l1=80, layers=0, activation=Tanh\n",
      "lr=0.010000, l1=79, layers=3, activation=Sigmoid\n",
      "lr=0.010000, l1=79, layers=3, activation=Sigmoid\n",
      "lr=0.010000, l1=81, layers=3, activation=ReLU\n",
      "lr=0.010000, l1=81, layers=3, activation=ReLU\n",
      "lr=0.010000, l1=76, layers=3, activation=Tanh\n",
      "lr=0.010000, l1=76, layers=3, activation=Tanh\n",
      "lr=0.010000, l1=77, layers=2, activation=ReLU\n",
      "lr=0.010000, l1=77, layers=2, activation=ReLU\n",
      "lr=0.010000, l1=84, layers=4, activation=ReLU\n",
      "lr=0.010000, l1=84, layers=4, activation=ReLU\n",
      "lr=0.000100, l1=83, layers=4, activation=ReLU\n",
      "lr=0.000100, l1=83, layers=4, activation=ReLU\n",
      "lr=0.000100, l1=102, layers=1, activation=LeakyReLU\n",
      "lr=0.000100, l1=102, layers=1, activation=LeakyReLU\n",
      "lr=0.010000, l1=102, layers=3, activation=LeakyReLU\n",
      "lr=0.001000, l1=85, layers=4, activation=Tanh\n",
      "lr=0.010000, l1=102, layers=3, activation=LeakyReLU\n",
      "lr=0.001000, l1=85, layers=4, activation=Tanh\n",
      "lr=0.010000, l1=83, layers=4, activation=ReLU\n",
      "lr=0.010000, l1=83, layers=4, activation=ReLU\n",
      "lr=0.001000, l1=77, layers=2, activation=Sigmoid\n",
      "lr=0.001000, l1=77, layers=2, activation=Sigmoid\n",
      "lr=0.010000, l1=100, layers=3, activation=LeakyReLU\n",
      "lr=0.010000, l1=100, layers=3, activation=LeakyReLU\n",
      "lr=0.001000, l1=83, layers=3, activation=ReLU\n",
      "lr=0.001000, l1=83, layers=3, activation=ReLU\n",
      "lr=0.010000, l1=20, layers=2, activation=LeakyReLU\n",
      "lr=0.010000, l1=20, layers=3, activation=Tanh\n",
      "lr=0.010000, l1=20, layers=2, activation=LeakyReLU\n",
      "lr=0.010000, l1=20, layers=3, activation=Tanh\n",
      "lr=0.010000, l1=104, layers=3, activation=LeakyReLU\n",
      "lr=0.010000, l1=104, layers=3, activation=LeakyReLU\n",
      "lr=0.001000, l1=104, layers=4, activation=Tanh\n",
      "lr=0.001000, l1=104, layers=4, activation=Tanh\n",
      "lr=0.010000, l1=19, layers=1, activation=LeakyReLU\n",
      "lr=0.010000, l1=19, layers=1, activation=LeakyReLU\n",
      "lr=0.001000, l1=81, layers=3, activation=ReLU\n",
      "lr=0.001000, l1=81, layers=3, activation=ReLU\n",
      "lr=0.001000, l1=19, layers=2, activation=LeakyReLU\n",
      "lr=0.001000, l1=19, layers=2, activation=LeakyReLU\n",
      "lr=0.010000, l1=103, layers=2, activation=LeakyReLU\n",
      "lr=0.010000, l1=103, layers=2, activation=LeakyReLU\n",
      "lr=0.010000, l1=22, layers=0, activation=LeakyReLU\n",
      "lr=0.010000, l1=22, layers=0, activation=LeakyReLU\n",
      "lr=0.010000, l1=93, layers=4, activation=ReLU\n",
      "lr=0.010000, l1=93, layers=4, activation=ReLU\n",
      "lr=0.010000, l1=90, layers=4, activation=ReLU\n",
      "lr=0.010000, l1=90, layers=4, activation=ReLU\n",
      "lr=0.000100, l1=91, layers=4, activation=ReLU\n",
      "lr=0.000100, l1=91, layers=4, activation=ReLU\n",
      "lr=0.010000, l1=78, layers=3, activation=LeakyReLU\n",
      "lr=0.010000, l1=78, layers=3, activation=LeakyReLU\n",
      "lr=0.010000, l1=91, layers=4, activation=Tanh\n",
      "lr=0.010000, l1=91, layers=4, activation=Tanh\n",
      "\n",
      "Optimization Results:\n",
      "Best learning rate: 0.010000\n",
      "Best layer size: 81\n",
      "Best num layers: 3\n",
      "Best activation: ReLU\n",
      "Best test MSE: 2582.3544\n",
      "\n",
      "Optimization Results:\n",
      "Best learning rate: 0.010000\n",
      "Best layer size: 81\n",
      "Best num layers: 3\n",
      "Best activation: ReLU\n",
      "Best test MSE: 2582.3544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from spotoptim import SpotOptim\n",
    "from spotoptim.data import get_diabetes_dataloaders\n",
    "from spotoptim.nn.linear_regressor import LinearRegressor\n",
    "\n",
    "def comprehensive_optimization(X):\n",
    "    \"\"\"Optimize learning rate, layer size, depth, and activation.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for params in X:\n",
    "        log_lr = params[0]      # Continuous (log scale)\n",
    "        l1 = int(params[1])     # Integer\n",
    "        n_layers = int(params[2])  # Integer\n",
    "        activation = params[3]   # Factor (string)\n",
    "        \n",
    "        lr = 10 ** log_lr  # Convert from log scale\n",
    "        \n",
    "        print(f\"lr={lr:.6f}, l1={l1}, layers={n_layers}, activation={activation}\")\n",
    "        \n",
    "        # Load data\n",
    "        train_loader, test_loader, _ = get_diabetes_dataloaders(\n",
    "            batch_size=32,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        model = LinearRegressor(\n",
    "            input_dim=10,\n",
    "            output_dim=1,\n",
    "            l1=l1,\n",
    "            num_hidden_layers=n_layers,\n",
    "            activation=activation\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        optimizer = model.get_optimizer(\"Adam\", lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(30):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                predictions = model(batch_X)\n",
    "                test_loss += criterion(predictions, batch_y).item()\n",
    "        \n",
    "        results.append(test_loss / len(test_loader))\n",
    "    \n",
    "    return np.array(results)\n",
    "\n",
    "# Optimize all four hyperparameters simultaneously\n",
    "optimizer = SpotOptim(\n",
    "    fun=comprehensive_optimization,\n",
    "    bounds=[\n",
    "        (-4, -2),                                    # log10(learning_rate)\n",
    "        (16, 128),                                   # l1 (neurons per layer)\n",
    "        (0, 4),                                      # num_hidden_layers\n",
    "        (\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\")   # activation function\n",
    "    ],\n",
    "    var_type=[\"num\", \"int\", \"int\", \"factor\"],\n",
    "    max_iter=50\n",
    ")\n",
    "\n",
    "result = optimizer.optimize()\n",
    "\n",
    "# Results contain original string values\n",
    "print(\"\\nOptimization Results:\")\n",
    "print(f\"Best learning rate: {10**result.x[0]:.6f}\")\n",
    "print(f\"Best layer size: {int(result.x[1])}\")\n",
    "print(f\"Best num layers: {int(result.x[2])}\")\n",
    "print(f\"Best activation: {result.x[3]}\")  # String value!\n",
    "print(f\"Best test MSE: {result.fun:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f2b125",
   "metadata": {},
   "source": [
    "## Multiple Factor Variables\n",
    "\n",
    "### Optimizing Both Activation and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3213cd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best activation: Tanh\n",
      "Best optimizer: SGD\n",
      "Best learning rate: 0.001000\n"
     ]
    }
   ],
   "source": [
    "from spotoptim import SpotOptim\n",
    "from spotoptim.data import get_diabetes_dataloaders\n",
    "from spotoptim.nn.linear_regressor import LinearRegressor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def optimize_activation_and_optimizer(X):\n",
    "    \"\"\"Optimize both activation function and optimizer choice.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for params in X:\n",
    "        activation = params[0]      # Factor variable 1\n",
    "        optimizer_name = params[1]  # Factor variable 2\n",
    "        lr = 10 ** params[2]        # Continuous variable\n",
    "        \n",
    "        train_loader, test_loader, _ = get_diabetes_dataloaders()\n",
    "        \n",
    "        model = LinearRegressor(\n",
    "            input_dim=10,\n",
    "            output_dim=1,\n",
    "            l1=64,\n",
    "            num_hidden_layers=2,\n",
    "            activation=activation\n",
    "        )\n",
    "        \n",
    "        # Use the optimizer string\n",
    "        optimizer = model.get_optimizer(optimizer_name, lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Train\n",
    "        for epoch in range(30):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                predictions = model(batch_X)\n",
    "                test_loss += criterion(predictions, batch_y).item()\n",
    "        \n",
    "        results.append(test_loss / len(test_loader))\n",
    "    \n",
    "    return np.array(results)  # Return numpy array\n",
    "\n",
    "# Two factor variables + one continuous\n",
    "opt = SpotOptim(\n",
    "    fun=optimize_activation_and_optimizer,\n",
    "    bounds=[\n",
    "        (\"ReLU\", \"Tanh\", \"Sigmoid\", \"LeakyReLU\"),    # Activation\n",
    "        (\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\"),         # Optimizer\n",
    "        (-4, -2)                                      # log10(lr)\n",
    "    ],\n",
    "    var_type=[\"factor\", \"factor\", \"num\"],\n",
    "    max_iter=40\n",
    ")\n",
    "\n",
    "result = opt.optimize()\n",
    "print(f\"Best activation: {result.x[0]}\")\n",
    "print(f\"Best optimizer: {result.x[1]}\")\n",
    "print(f\"Best learning rate: {10**result.x[2]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf4185",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "### Custom Categorical Choices\n",
    "\n",
    "Factor variables work with any string values, not just activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ef2c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration:\n",
      "  Dropout: light\n",
      "  Batch norm: before\n",
      "  Weight init: kaiming\n",
      "  Score: 2862.3384\n"
     ]
    }
   ],
   "source": [
    "from spotoptim import SpotOptim\n",
    "import numpy as np\n",
    "\n",
    "def train_model_with_config(dropout_policy, batch_norm, weight_init):\n",
    "    \"\"\"Simulate model training with different configurations.\"\"\"\n",
    "    # In real use, this would train an actual model\n",
    "    # Here we return synthetic scores for demonstration\n",
    "    base_score = 3000.0\n",
    "    \n",
    "    # Dropout impact\n",
    "    dropout_scores = {\"none\": 200, \"light\": 0, \"heavy\": 100}\n",
    "    # Batch norm impact\n",
    "    bn_scores = {\"before\": -50, \"after\": 0, \"none\": 150}\n",
    "    # Weight init impact\n",
    "    init_scores = {\"xavier\": 0, \"kaiming\": -30, \"normal\": 100}\n",
    "    \n",
    "    score = (base_score + \n",
    "             dropout_scores.get(dropout_policy, 0) + \n",
    "             bn_scores.get(batch_norm, 0) + \n",
    "             init_scores.get(weight_init, 0) +\n",
    "             np.random.normal(0, 50))\n",
    "    \n",
    "    return score\n",
    "\n",
    "def train_with_config(X):\n",
    "    \"\"\"Objective function with various categorical choices.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for params in X:\n",
    "        dropout_policy = params[0]  # \"none\", \"light\", \"heavy\"\n",
    "        batch_norm = params[1]       # \"before\", \"after\", \"none\"\n",
    "        weight_init = params[2]      # \"xavier\", \"kaiming\", \"normal\"\n",
    "        \n",
    "        # Use these strings to configure your model\n",
    "        score = train_model_with_config(\n",
    "            dropout_policy=dropout_policy,\n",
    "            batch_norm=batch_norm,\n",
    "            weight_init=weight_init\n",
    "        )\n",
    "        results.append(score)\n",
    "    \n",
    "    return np.array(results)  # Return numpy array\n",
    "\n",
    "optimizer = SpotOptim(\n",
    "    fun=train_with_config,\n",
    "    bounds=[\n",
    "        (\"none\", \"light\", \"heavy\"),           # Dropout policy\n",
    "        (\"before\", \"after\", \"none\"),          # Batch norm position\n",
    "        (\"xavier\", \"kaiming\", \"normal\")       # Weight initialization\n",
    "    ],\n",
    "    var_type=[\"factor\", \"factor\", \"factor\"],\n",
    "    max_iter=25,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "result = optimizer.optimize()\n",
    "print(\"Best configuration:\")\n",
    "print(f\"  Dropout: {result.x[0]}\")\n",
    "print(f\"  Batch norm: {result.x[1]}\")\n",
    "print(f\"  Weight init: {result.x[2]}\")\n",
    "print(f\"  Score: {result.fun:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b6e78",
   "metadata": {},
   "source": [
    "### Viewing All Evaluated Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "959f0dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All evaluated configurations:\n",
      "Layer Size | Activation | Test MSE\n",
      "------------------------------------------\n",
      "        41 | Tanh       | 3786.7542\n",
      "       118 | Sigmoid    | 2767.9953\n",
      "        26 | Tanh       | 5594.0517\n",
      "       108 | Sigmoid    | 2713.4416\n",
      "        71 | LeakyReLU  | 2988.5792\n",
      "        34 | Tanh       | 5356.3825\n",
      "        87 | ReLU       | 2648.2992\n",
      "       101 | Tanh       | 2706.8905\n",
      "        55 | Sigmoid    | 3179.8775\n",
      "        74 | ReLU       | 2771.7615\n",
      "\n",
      "Top 5 configurations:\n",
      "l1=113, activation=LeakyReLU , MSE=2470.4270\n",
      "l1=115, activation=LeakyReLU , MSE=2604.7826\n",
      "l1=123, activation=LeakyReLU , MSE=2626.1193\n",
      "l1= 87, activation=ReLU      , MSE=2648.2992\n",
      "l1=125, activation=ReLU      , MSE=2694.6366\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from spotoptim import SpotOptim\n",
    "from spotoptim.data import get_diabetes_dataloaders\n",
    "from spotoptim.nn.linear_regressor import LinearRegressor\n",
    "import numpy as np\n",
    "\n",
    "def train_and_evaluate(X):\n",
    "    \"\"\"Train models with different activation functions.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for params in X:\n",
    "        l1 = int(params[0])         # Integer: layer size\n",
    "        activation = params[1]       # String: activation function\n",
    "        \n",
    "        # Load data\n",
    "        train_loader, test_loader, _ = get_diabetes_dataloaders()\n",
    "        \n",
    "        # Create model with the activation function\n",
    "        model = LinearRegressor(\n",
    "            input_dim=10,\n",
    "            output_dim=1,\n",
    "            l1=l1,\n",
    "            num_hidden_layers=2,\n",
    "            activation=activation  # Pass string directly!\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(50):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                predictions = model(batch_X)\n",
    "                test_loss += criterion(predictions, batch_y).item()\n",
    "        \n",
    "        avg_loss = test_loss / len(test_loader)\n",
    "        results.append(avg_loss)\n",
    "    \n",
    "    return np.array(results)\n",
    "\n",
    "optimizer = SpotOptim(\n",
    "    fun=train_and_evaluate,\n",
    "    bounds=[\n",
    "        (16, 128),                                   # Layer size\n",
    "        (\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\")   # Activation\n",
    "    ],\n",
    "    var_type=[\"int\", \"factor\"],  # IMPORTANT: Specify variable types!\n",
    "    max_iter=30,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "result = optimizer.optimize()\n",
    "\n",
    "# Access all evaluated configurations\n",
    "print(\"\\nAll evaluated configurations:\")\n",
    "print(\"Layer Size | Activation | Test MSE\")\n",
    "print(\"-\" * 42)\n",
    "for i in range(min(10, len(result.X))):  # Show first 10\n",
    "    l1 = int(result.X[i, 0])\n",
    "    activation = result.X[i, 1]  # String value!\n",
    "    loss = result.y[i]\n",
    "    print(f\"{l1:10d} | {activation:10s} | {loss:.4f}\")\n",
    "\n",
    "# Find top 5 configurations\n",
    "sorted_indices = result.y.argsort()[:5]\n",
    "print(\"\\nTop 5 configurations:\")\n",
    "for idx in sorted_indices:\n",
    "    print(f\"l1={int(result.X[idx, 0]):3d}, \"\n",
    "          f\"activation={result.X[idx, 1]:10s}, \"\n",
    "          f\"MSE={result.y[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af17f6",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "### Internal Mechanism\n",
    "\n",
    "SpotOptim handles factor variables through automatic conversion:\n",
    "\n",
    "1. **Initialization**: String tuples in bounds are detected\n",
    "   ```python\n",
    "   bounds = [(\"ReLU\", \"Sigmoid\", \"Tanh\")]\n",
    "   # Internally mapped to: {0: \"ReLU\", 1: \"Sigmoid\", 2: \"Tanh\"}\n",
    "   # Bounds become: [(0, 2)]\n",
    "   ```\n",
    "\n",
    "2. **Sampling**: Initial design samples from `[0, n_levels-1]` and rounds to integers\n",
    "   ```python\n",
    "   # Samples might be: [0.3, 1.8, 2.1]\n",
    "   # After rounding: [0, 2, 2]\n",
    "   ```\n",
    "\n",
    "3. **Evaluation**: Before calling objective function, integers → strings\n",
    "   ```python\n",
    "   # [0, 2, 2] → [\"ReLU\", \"Tanh\", \"Tanh\"]\n",
    "   # Objective function receives strings\n",
    "   ```\n",
    "\n",
    "4. **Optimization**: Surrogate model works with integers `[0, n_levels-1]`\n",
    "\n",
    "5. **Results**: Final results mapped back to strings\n",
    "   ```python\n",
    "   result.x[0]  # Returns \"ReLU\", not 0\n",
    "   result.X     # All rows contain strings for factor variables\n",
    "   ```\n",
    "\n",
    "### Variable Type Auto-Detection\n",
    "\n",
    "If you don't specify `var_type`, SpotOptim automatically detects factor variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82491285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lr: 0.010000, Best activation: ReLU\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Explicit var_type (recommended)\n",
    "# This shows the syntax - replace my_function with your actual function\n",
    "\n",
    "# optimizer = SpotOptim(\n",
    "#     fun=my_function,\n",
    "#     bounds=[(-4, -2), (\"ReLU\", \"Tanh\")],\n",
    "#     var_type=[\"num\", \"factor\"]  # Explicit\n",
    "# )\n",
    "\n",
    "# Example 2: Auto-detection (works but less explicit)\n",
    "# optimizer = SpotOptim(\n",
    "#     fun=my_function,\n",
    "#     bounds=[(-4, -2), (\"ReLU\", \"Tanh\")]\n",
    "#     # var_type automatically set to [\"float\", \"factor\"]\n",
    "# )\n",
    "\n",
    "# Here's a working example:\n",
    "from spotoptim import SpotOptim\n",
    "import numpy as np\n",
    "\n",
    "def demo_function(X):\n",
    "    results = []\n",
    "    for params in X:\n",
    "        lr = 10 ** params[0]  # Continuous parameter\n",
    "        activation = params[1]  # Factor parameter\n",
    "        score = 3000 + lr * 100 + {\"ReLU\": 0, \"Tanh\": 50}.get(activation, 100)\n",
    "        results.append(score + np.random.normal(0, 10))\n",
    "    return np.array(results)\n",
    "\n",
    "# With explicit var_type (recommended)\n",
    "optimizer = SpotOptim(\n",
    "    fun=demo_function,\n",
    "    bounds=[(-4, -2), (\"ReLU\", \"Tanh\")],\n",
    "    var_type=[\"num\", \"factor\"],  # Explicit is clearer\n",
    "    max_iter=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "result = optimizer.optimize()\n",
    "print(f\"Best lr: {10**result.x[0]:.6f}, Best activation: {result.x[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457207db",
   "metadata": {},
   "source": [
    "## Complete Example: Full Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445e859b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Neural Network Hyperparameter Optimization with Factor Variables\n",
      "================================================================================\n",
      "\n",
      "Starting optimization...\n",
      "Testing: lr=0.010000, l1=101, layers=2, activation=ReLU\n",
      "  → Test MSE: 2749.3840\n",
      "Testing: lr=0.001000, l1=50, layers=2, activation=ReLU\n",
      "  → Test MSE: 4250.1239\n",
      "Testing: lr=0.000100, l1=67, layers=1, activation=Tanh\n",
      "  → Test MSE: 26543.4342\n",
      "Testing: lr=0.000100, l1=40, layers=0, activation=Tanh\n",
      "  → Test MSE: 26512.4831\n",
      "Testing: lr=0.010000, l1=116, layers=2, activation=Sigmoid\n",
      "  → Test MSE: 4250.1239\n",
      "Testing: lr=0.000100, l1=67, layers=1, activation=Tanh\n",
      "  → Test MSE: 26543.4342\n",
      "Testing: lr=0.000100, l1=40, layers=0, activation=Tanh\n",
      "  → Test MSE: 26512.4831\n",
      "Testing: lr=0.010000, l1=116, layers=2, activation=Sigmoid\n",
      "  → Test MSE: 3155.1196\n",
      "Testing: lr=0.001000, l1=124, layers=3, activation=Sigmoid\n",
      "  → Test MSE: 17794.4932\n",
      "Testing: lr=0.001000, l1=36, layers=4, activation=Sigmoid\n",
      "  → Test MSE: 3155.1196\n",
      "Testing: lr=0.001000, l1=124, layers=3, activation=Sigmoid\n",
      "  → Test MSE: 17794.4932\n",
      "Testing: lr=0.001000, l1=36, layers=4, activation=Sigmoid\n",
      "  → Test MSE: 23270.2598\n",
      "Testing: lr=0.010000, l1=20, layers=1, activation=LeakyReLU\n",
      "  → Test MSE: 3669.6982\n",
      "Testing: lr=0.001000, l1=90, layers=1, activation=Tanh\n",
      "  → Test MSE: 22400.3063\n",
      "Testing: lr=0.000100, l1=78, layers=3, activation=Tanh\n",
      "  → Test MSE: 23270.2598\n",
      "Testing: lr=0.010000, l1=20, layers=1, activation=LeakyReLU\n",
      "  → Test MSE: 3669.6982\n",
      "Testing: lr=0.001000, l1=90, layers=1, activation=Tanh\n",
      "  → Test MSE: 22400.3063\n",
      "Testing: lr=0.000100, l1=78, layers=3, activation=Tanh\n",
      "  → Test MSE: 24658.6191\n",
      "Testing: lr=0.010000, l1=105, layers=2, activation=ReLU\n",
      "  → Test MSE: 2824.7159\n",
      "  → Test MSE: 24658.6191\n",
      "Testing: lr=0.010000, l1=105, layers=2, activation=ReLU\n",
      "  → Test MSE: 2824.7159\n",
      "Testing: lr=0.010000, l1=112, layers=1, activation=Tanh\n",
      "  → Test MSE: 2990.5002\n",
      "Testing: lr=0.010000, l1=112, layers=1, activation=Tanh\n",
      "  → Test MSE: 2990.5002\n",
      "Testing: lr=0.000100, l1=112, layers=4, activation=ReLU\n",
      "  → Test MSE: 3704.0166\n",
      "Testing: lr=0.000100, l1=112, layers=4, activation=ReLU\n",
      "  → Test MSE: 3704.0166\n",
      "Testing: lr=0.010000, l1=16, layers=4, activation=ReLU\n",
      "  → Test MSE: 2822.2253\n",
      "Testing: lr=0.010000, l1=16, layers=4, activation=ReLU\n",
      "  → Test MSE: 2822.2253\n",
      "Testing: lr=0.000100, l1=16, layers=0, activation=LeakyReLU\n",
      "  → Test MSE: 26621.8034\n",
      "Testing: lr=0.010000, l1=20, layers=4, activation=Sigmoid\n",
      "Testing: lr=0.000100, l1=16, layers=0, activation=LeakyReLU\n",
      "  → Test MSE: 26621.8034\n",
      "Testing: lr=0.010000, l1=20, layers=4, activation=Sigmoid\n",
      "  → Test MSE: 11135.6807\n",
      "Testing: lr=0.010000, l1=113, layers=2, activation=Sigmoid\n",
      "  → Test MSE: 11135.6807\n",
      "Testing: lr=0.010000, l1=113, layers=2, activation=Sigmoid\n",
      "  → Test MSE: 3245.6501\n",
      "Testing: lr=0.001000, l1=109, layers=2, activation=Sigmoid\n",
      "  → Test MSE: 3245.6501\n",
      "Testing: lr=0.001000, l1=109, layers=2, activation=Sigmoid\n",
      "  → Test MSE: 17415.8874\n",
      "Testing: lr=0.010000, l1=103, layers=2, activation=ReLU\n",
      "  → Test MSE: 17415.8874\n",
      "Testing: lr=0.010000, l1=103, layers=2, activation=ReLU\n",
      "  → Test MSE: 2719.3151\n",
      "Testing: lr=0.001000, l1=114, layers=1, activation=LeakyReLU\n",
      "  → Test MSE: 12990.9105\n",
      "  → Test MSE: 2719.3151\n",
      "Testing: lr=0.001000, l1=114, layers=1, activation=LeakyReLU\n",
      "  → Test MSE: 12990.9105\n",
      "Testing: lr=0.001000, l1=115, layers=4, activation=ReLU\n",
      "  → Test MSE: 3011.1618\n",
      "Testing: lr=0.001000, l1=115, layers=4, activation=ReLU\n",
      "  → Test MSE: 3011.1618\n",
      "Testing: lr=0.010000, l1=104, layers=0, activation=ReLU\n",
      "  → Test MSE: 25324.5807\n",
      "Testing: lr=0.010000, l1=104, layers=3, activation=ReLU\n",
      "Testing: lr=0.010000, l1=104, layers=0, activation=ReLU\n",
      "  → Test MSE: 25324.5807\n",
      "Testing: lr=0.010000, l1=104, layers=3, activation=ReLU\n",
      "  → Test MSE: 3008.4267\n",
      "Testing: lr=0.001000, l1=114, layers=3, activation=ReLU\n",
      "  → Test MSE: 3008.4267\n",
      "Testing: lr=0.001000, l1=114, layers=3, activation=ReLU\n",
      "  → Test MSE: 3260.0366\n",
      "Testing: lr=0.010000, l1=102, layers=3, activation=ReLU\n",
      "  → Test MSE: 3260.0366\n",
      "Testing: lr=0.010000, l1=102, layers=3, activation=ReLU\n",
      "  → Test MSE: 2842.4390\n",
      "Testing: lr=0.010000, l1=114, layers=4, activation=Sigmoid\n",
      "  → Test MSE: 2842.4390\n",
      "Testing: lr=0.010000, l1=114, layers=4, activation=Sigmoid\n",
      "  → Test MSE: 5284.8105\n",
      "Testing: lr=0.010000, l1=102, layers=2, activation=Sigmoid\n",
      "  → Test MSE: 5284.8105\n",
      "Testing: lr=0.010000, l1=102, layers=2, activation=Sigmoid\n",
      "  → Test MSE: 3475.8301\n",
      "Testing: lr=0.010000, l1=117, layers=3, activation=ReLU\n",
      "  → Test MSE: 3475.8301\n",
      "Testing: lr=0.010000, l1=117, layers=3, activation=ReLU\n",
      "  → Test MSE: 2633.9051\n",
      "Testing: lr=0.000100, l1=116, layers=3, activation=ReLU\n",
      "  → Test MSE: 2633.9051\n",
      "Testing: lr=0.000100, l1=116, layers=3, activation=ReLU\n",
      "  → Test MSE: 10605.2269\n",
      "Testing: lr=0.000100, l1=102, layers=3, activation=ReLU\n",
      "  → Test MSE: 10605.2269\n",
      "Testing: lr=0.000100, l1=102, layers=3, activation=ReLU\n",
      "  → Test MSE: 11600.1868\n",
      "Testing: lr=0.010000, l1=22, layers=0, activation=LeakyReLU\n",
      "  → Test MSE: 25286.9245\n",
      "  → Test MSE: 11600.1868\n",
      "Testing: lr=0.010000, l1=22, layers=0, activation=LeakyReLU\n",
      "  → Test MSE: 25286.9245\n",
      "Testing: lr=0.010000, l1=115, layers=3, activation=ReLU\n",
      "  → Test MSE: 3291.7816\n",
      "Testing: lr=0.010000, l1=115, layers=3, activation=ReLU\n",
      "  → Test MSE: 3291.7816\n",
      "Testing: lr=0.010000, l1=113, layers=0, activation=ReLU\n",
      "  → Test MSE: 25351.6471\n",
      "Testing: lr=0.010000, l1=112, layers=2, activation=Tanh\n",
      "Testing: lr=0.010000, l1=113, layers=0, activation=ReLU\n",
      "  → Test MSE: 25351.6471\n",
      "Testing: lr=0.010000, l1=112, layers=2, activation=Tanh\n",
      "  → Test MSE: 3223.9716\n",
      "Testing: lr=0.010000, l1=116, layers=3, activation=Sigmoid\n",
      "  → Test MSE: 3223.9716\n",
      "Testing: lr=0.010000, l1=116, layers=3, activation=Sigmoid\n",
      "  → Test MSE: 5286.0335\n",
      "  → Test MSE: 5286.0335\n",
      "Testing: lr=0.001000, l1=113, layers=4, activation=ReLU\n",
      "  → Test MSE: 2671.7316\n",
      "Testing: lr=0.001000, l1=113, layers=4, activation=ReLU\n",
      "  → Test MSE: 2671.7316\n",
      "Testing: lr=0.010000, l1=118, layers=1, activation=ReLU\n",
      "  → Test MSE: 2977.5250\n",
      "Testing: lr=0.010000, l1=118, layers=1, activation=ReLU\n",
      "  → Test MSE: 2977.5250\n",
      "Testing: lr=0.010000, l1=118, layers=2, activation=Sigmoid\n",
      "  → Test MSE: 3182.1468\n",
      "Testing: lr=0.010000, l1=118, layers=2, activation=Sigmoid\n",
      "  → Test MSE: 3182.1468\n",
      "Testing: lr=0.010000, l1=19, layers=2, activation=LeakyReLU\n",
      "  → Test MSE: 2930.6335\n",
      "Testing: lr=0.001000, l1=52, layers=3, activation=ReLU\n",
      "Testing: lr=0.010000, l1=19, layers=2, activation=LeakyReLU\n",
      "  → Test MSE: 2930.6335\n",
      "Testing: lr=0.001000, l1=52, layers=3, activation=ReLU\n",
      "  → Test MSE: 3075.5275\n",
      "Testing: lr=0.010000, l1=52, layers=1, activation=ReLU\n",
      "  → Test MSE: 2961.7695\n",
      "  → Test MSE: 3075.5275\n",
      "Testing: lr=0.010000, l1=52, layers=1, activation=ReLU\n",
      "  → Test MSE: 2961.7695\n",
      "Testing: lr=0.010000, l1=52, layers=2, activation=Tanh\n",
      "  → Test MSE: 4914.0295\n",
      "Testing: lr=0.010000, l1=52, layers=2, activation=Tanh\n",
      "  → Test MSE: 4914.0295\n",
      "Testing: lr=0.000100, l1=52, layers=1, activation=ReLU\n",
      "  → Test MSE: 26459.7812\n",
      "Testing: lr=0.010000, l1=51, layers=3, activation=ReLU\n",
      "Testing: lr=0.000100, l1=52, layers=1, activation=ReLU\n",
      "  → Test MSE: 26459.7812\n",
      "Testing: lr=0.010000, l1=51, layers=3, activation=ReLU\n",
      "  → Test MSE: 2964.5138\n",
      "Testing: lr=0.010000, l1=53, layers=3, activation=ReLU\n",
      "  → Test MSE: 2964.5138\n",
      "Testing: lr=0.010000, l1=53, layers=3, activation=ReLU\n",
      "  → Test MSE: 2907.8923\n",
      "Testing: lr=0.001000, l1=52, layers=4, activation=Sigmoid\n",
      "  → Test MSE: 2907.8923\n",
      "Testing: lr=0.001000, l1=52, layers=4, activation=Sigmoid\n",
      "  → Test MSE: 22350.0423\n",
      "  → Test MSE: 22350.0423\n",
      "Testing: lr=0.010000, l1=52, layers=2, activation=ReLU\n",
      "  → Test MSE: 2876.1516\n",
      "Testing: lr=0.010000, l1=52, layers=2, activation=ReLU\n",
      "  → Test MSE: 2876.1516\n",
      "Testing: lr=0.010000, l1=50, layers=1, activation=Sigmoid\n",
      "  → Test MSE: 4409.1695\n",
      "Testing: lr=0.010000, l1=50, layers=1, activation=Sigmoid\n",
      "  → Test MSE: 4409.1695\n",
      "Testing: lr=0.010000, l1=117, layers=2, activation=ReLU\n",
      "  → Test MSE: 2800.5202\n",
      "Testing: lr=0.010000, l1=117, layers=2, activation=ReLU\n",
      "  → Test MSE: 2800.5202\n",
      "Testing: lr=0.010000, l1=49, layers=2, activation=ReLU\n",
      "  → Test MSE: 2778.6850\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZATION RESULTS\n",
      "================================================================================\n",
      "Best learning rate: 0.010000\n",
      "Best layer size (l1): 117\n",
      "Best num hidden layers: 3\n",
      "Best activation function: ReLU\n",
      "Best test MSE: 2633.9051\n",
      "\n",
      "================================================================================\n",
      "TOP 5 CONFIGURATIONS\n",
      "================================================================================\n",
      "Rank   LR           L1     Layers   Activation   MSE       \n",
      "--------------------------------------------------------------------------------\n",
      "1      0.010000     117    3        ReLU         2633.9051 \n",
      "2      0.001000     113    4        ReLU         2671.7316 \n",
      "3      0.010000     103    2        ReLU         2719.3151 \n",
      "4      0.010000     101    2        ReLU         2749.3840 \n",
      "5      0.010000     49     2        ReLU         2778.6850 \n",
      "\n",
      "================================================================================\n",
      "TRAINING FINAL MODEL\n",
      "================================================================================\n",
      "Configuration: lr=0.010000, l1=117, layers=3, activation=ReLU\n",
      "\n",
      "Training for 100 epochs...\n",
      "Epoch 20/100: Train MSE = 2492.9515\n",
      "Testing: lr=0.010000, l1=49, layers=2, activation=ReLU\n",
      "  → Test MSE: 2778.6850\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZATION RESULTS\n",
      "================================================================================\n",
      "Best learning rate: 0.010000\n",
      "Best layer size (l1): 117\n",
      "Best num hidden layers: 3\n",
      "Best activation function: ReLU\n",
      "Best test MSE: 2633.9051\n",
      "\n",
      "================================================================================\n",
      "TOP 5 CONFIGURATIONS\n",
      "================================================================================\n",
      "Rank   LR           L1     Layers   Activation   MSE       \n",
      "--------------------------------------------------------------------------------\n",
      "1      0.010000     117    3        ReLU         2633.9051 \n",
      "2      0.001000     113    4        ReLU         2671.7316 \n",
      "3      0.010000     103    2        ReLU         2719.3151 \n",
      "4      0.010000     101    2        ReLU         2749.3840 \n",
      "5      0.010000     49     2        ReLU         2778.6850 \n",
      "\n",
      "================================================================================\n",
      "TRAINING FINAL MODEL\n",
      "================================================================================\n",
      "Configuration: lr=0.010000, l1=117, layers=3, activation=ReLU\n",
      "\n",
      "Training for 100 epochs...\n",
      "Epoch 20/100: Train MSE = 2492.9515\n",
      "Epoch 40/100: Train MSE = 2344.3117\n",
      "Epoch 60/100: Train MSE = 2582.2915\n",
      "Epoch 80/100: Train MSE = 2931.2874\n",
      "Epoch 40/100: Train MSE = 2344.3117\n",
      "Epoch 60/100: Train MSE = 2582.2915\n",
      "Epoch 80/100: Train MSE = 2931.2874\n",
      "Epoch 100/100: Train MSE = 2308.9831\n",
      "\n",
      "Final Test MSE: 2995.4425\n",
      "================================================================================\n",
      "Epoch 100/100: Train MSE = 2308.9831\n",
      "\n",
      "Final Test MSE: 2995.4425\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete example: Neural network hyperparameter optimization with factor variables.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from spotoptim import SpotOptim\n",
    "from spotoptim.data import get_diabetes_dataloaders\n",
    "from spotoptim.nn.linear_regressor import LinearRegressor\n",
    "\n",
    "\n",
    "def objective_function(X):\n",
    "    \"\"\"Train and evaluate models with given hyperparameters.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for params in X:\n",
    "        # Extract hyperparameters\n",
    "        log_lr = params[0]\n",
    "        l1 = int(params[1])\n",
    "        num_layers = int(params[2])\n",
    "        activation = params[3]  # String!\n",
    "        \n",
    "        lr = 10 ** log_lr\n",
    "        \n",
    "        print(f\"Testing: lr={lr:.6f}, l1={l1}, layers={num_layers}, \"\n",
    "              f\"activation={activation}\")\n",
    "        \n",
    "        # Load data\n",
    "        train_loader, test_loader, _ = get_diabetes_dataloaders(\n",
    "            test_size=0.2,\n",
    "            batch_size=32,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create and train model\n",
    "        model = LinearRegressor(\n",
    "            input_dim=10,\n",
    "            output_dim=1,\n",
    "            l1=l1,\n",
    "            num_hidden_layers=num_layers,\n",
    "            activation=activation\n",
    "        )\n",
    "        \n",
    "        optimizer = model.get_optimizer(\"Adam\", lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        num_epochs = 30\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        results.append(avg_test_loss)\n",
    "        print(f\"  → Test MSE: {avg_test_loss:.4f}\")\n",
    "    \n",
    "    return np.array(results)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Neural Network Hyperparameter Optimization with Factor Variables\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Define optimization problem\n",
    "    optimizer = SpotOptim(\n",
    "        fun=objective_function,\n",
    "        bounds=[\n",
    "            (-4, -2),                                    # log10(learning_rate)\n",
    "            (16, 128),                                   # l1 (neurons)\n",
    "            (0, 4),                                      # num_hidden_layers\n",
    "            (\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\")   # activation (factor!)\n",
    "        ],\n",
    "        var_type=[\"num\", \"int\", \"int\", \"factor\"],\n",
    "        max_iter=50,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    print(\"\\nStarting optimization...\")\n",
    "    result = optimizer.optimize()\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OPTIMIZATION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Best learning rate: {10**result.x[0]:.6f}\")\n",
    "    print(f\"Best layer size (l1): {int(result.x[1])}\")\n",
    "    print(f\"Best num hidden layers: {int(result.x[2])}\")\n",
    "    print(f\"Best activation function: {result.x[3]}\")  # String value!\n",
    "    print(f\"Best test MSE: {result.fun:.4f}\")\n",
    "    \n",
    "    # Show top 5 configurations\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TOP 5 CONFIGURATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    sorted_indices = result.y.argsort()[:5]\n",
    "    print(f\"{'Rank':<6} {'LR':<12} {'L1':<6} {'Layers':<8} \"\n",
    "          f\"{'Activation':<12} {'MSE':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for rank, idx in enumerate(sorted_indices, 1):\n",
    "        lr = 10 ** result.X[idx, 0]\n",
    "        l1 = int(result.X[idx, 1])\n",
    "        layers = int(result.X[idx, 2])\n",
    "        activation = result.X[idx, 3]\n",
    "        mse = result.y[idx]\n",
    "        print(f\"{rank:<6} {lr:<12.6f} {l1:<6} {layers:<8} \"\n",
    "              f\"{activation:<12} {mse:<10.4f}\")\n",
    "    \n",
    "    # Train final model with best configuration\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TRAINING FINAL MODEL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    best_lr = 10 ** result.x[0]\n",
    "    best_l1 = int(result.x[1])\n",
    "    best_layers = int(result.x[2])\n",
    "    best_activation = result.x[3]\n",
    "    \n",
    "    print(f\"Configuration: lr={best_lr:.6f}, l1={best_l1}, \"\n",
    "          f\"layers={best_layers}, activation={best_activation}\")\n",
    "    \n",
    "    train_loader, test_loader, _ = get_diabetes_dataloaders(\n",
    "        test_size=0.2,\n",
    "        batch_size=32,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    final_model = LinearRegressor(\n",
    "        input_dim=10,\n",
    "        output_dim=1,\n",
    "        l1=best_l1,\n",
    "        num_hidden_layers=best_layers,\n",
    "        activation=best_activation\n",
    "    )\n",
    "    \n",
    "    optimizer_final = final_model.get_optimizer(\"Adam\", lr=best_lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Extended training\n",
    "    num_epochs = 100\n",
    "    print(f\"\\nTraining for {num_epochs} epochs...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        final_model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            predictions = final_model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            optimizer_final.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_final.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: Train MSE = {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_model.eval()\n",
    "    final_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            predictions = final_model(batch_X)\n",
    "            final_test_loss += criterion(predictions, batch_y).item()\n",
    "    \n",
    "    final_avg_loss = final_test_loss / len(test_loader)\n",
    "    print(f\"\\nFinal Test MSE: {final_avg_loss:.4f}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01026d",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Do's\n",
    "\n",
    "✅ **Use descriptive string values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614d34c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds=[(\"xavier_uniform\", \"kaiming_normal\", \"orthogonal\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16744aee",
   "metadata": {},
   "source": [
    "✅ **Explicitly specify var_type for clarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70f86c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_type=[\"num\", \"int\", \"factor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623ec27",
   "metadata": {},
   "source": [
    "✅ **Access results as strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "049cf892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best activation: ReLU (type: str)\n"
     ]
    }
   ],
   "source": [
    "# Example: Accessing factor variable results as strings\n",
    "# (This assumes you've run an optimization with activation as a factor variable)\n",
    "\n",
    "# If you have a result from the previous examples:\n",
    "# best_activation = result.x[3]  # For 4-parameter optimization\n",
    "# Or for simpler cases:\n",
    "# best_activation = result.x[0]  # For single-parameter optimization\n",
    "\n",
    "# Example with inline optimization:\n",
    "from spotoptim import SpotOptim\n",
    "import numpy as np\n",
    "\n",
    "def quick_test(X):\n",
    "    results = []\n",
    "    for params in X:\n",
    "        activation = params[0]\n",
    "        score = {\"ReLU\": 3500, \"Tanh\": 3600}.get(activation, 4000)\n",
    "        results.append(score + np.random.normal(0, 50))\n",
    "    return np.array(results)\n",
    "\n",
    "opt = SpotOptim(\n",
    "    fun=quick_test,\n",
    "    bounds=[(\"ReLU\", \"Tanh\")],\n",
    "    var_type=[\"factor\"],\n",
    "    max_iter=10,\n",
    "    seed=42\n",
    ")\n",
    "result = opt.optimize()\n",
    "\n",
    "# Access as string - this is the correct way\n",
    "best_activation = result.x[0]  # String value like \"ReLU\"\n",
    "print(f\"Best activation: {best_activation} (type: {type(best_activation).__name__})\")\n",
    "\n",
    "# You can use it directly in your model\n",
    "# model = LinearRegressor(activation=best_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f997a70",
   "metadata": {},
   "source": [
    "✅ **Mix factor variables with numeric/integer variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfb98f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds=[(-4, -2), (16, 128), (\"ReLU\", \"Tanh\")]\n",
    "var_type=[\"num\", \"int\", \"factor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9fdc6",
   "metadata": {},
   "source": [
    "### Don'ts\n",
    "\n",
    "❌ **Don't use integers in factor bounds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82d7dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Use strings, not integers\n",
    "bounds=[(0, 1, 2)]  # Wrong!\n",
    "bounds=[(\"ReLU\", \"Sigmoid\", \"Tanh\")]  # Correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5daac6",
   "metadata": {},
   "source": [
    "❌ **Don't expect integers in objective function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a54d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(X):\n",
    "    activation = X[0][2]\n",
    "    # activation is a string, not an integer!\n",
    "    # Don't do: if activation == 0:  # Wrong!\n",
    "    # Do: if activation == \"ReLU\":   # Correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56956d02",
   "metadata": {},
   "source": [
    "❌ **Don't manually convert factor variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bb1e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpotOptim handles conversion automatically\n",
    "# Don't do manual mapping in your objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042b768",
   "metadata": {},
   "source": [
    "❌ **Don't use empty tuples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12c75de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong: Empty tuple\n",
    "bounds=[()]\n",
    "\n",
    "# Correct: At least one string\n",
    "bounds=[(\"ReLU\",)]  # Single choice (will be treated as fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c5dc77",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**Issue**: Objective function receives integers instead of strings\n",
    "\n",
    "**Solution**: Ensure you're using the latest version of SpotOptim with factor variable support. Factor variables are automatically converted before calling the objective function.\n",
    "\n",
    "---\n",
    "\n",
    "**Issue**: `ValueError: could not convert string to float`\n",
    "\n",
    "**Solution**: This occurs if there's a version mismatch. Update SpotOptim to ensure the object array conversion is implemented correctly.\n",
    "\n",
    "---\n",
    "\n",
    "**Issue**: Results show integers instead of strings\n",
    "\n",
    "**Solution**: Check that you're accessing `result.x` (mapped values) instead of internal arrays. The result object automatically maps factor variables to their original strings.\n",
    "\n",
    "---\n",
    "\n",
    "**Issue**: Single-level factor variables cause dimension reduction\n",
    "\n",
    "**Behavior**: If a factor variable has only one choice, e.g., `(\"ReLU\",)`, SpotOptim treats it as a fixed dimension and may reduce the dimensionality. This is expected behavior.\n",
    "\n",
    "**Solution**: Use at least two choices for optimization, or remove single-choice dimensions from bounds.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Factor variables in SpotOptim enable:\n",
    "\n",
    "- ✅ **Categorical optimization**: Optimize over discrete string choices\n",
    "- ✅ **Automatic conversion**: Seamless integer↔string mapping\n",
    "- ✅ **Neural network hyperparameters**: Optimize activation functions, optimizers, etc.\n",
    "- ✅ **Mixed variable types**: Combine with continuous and integer variables\n",
    "- ✅ **Clean interface**: Objective functions work with strings directly\n",
    "- ✅ **String results**: Final results contain original string values\n",
    "\n",
    "Factor variables make categorical hyperparameter optimization as easy as continuous optimization!\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [LinearRegressor Documentation](https://sequential-parameter-optimization.github.io/spotPython/reference/) - Neural network class supporting string-based activation functions\n",
    "- [Diabetes Dataset Utilities](diabetes_dataset.md) - Data loading utilities used in examples\n",
    "- [Variable Types](var_type.md) - Overview of all variable types in SpotOptim\n",
    "- [Save and Load](save_load.md) - Saving and loading optimization results with factor variables\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
