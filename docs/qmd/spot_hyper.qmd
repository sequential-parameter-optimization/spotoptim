---
title: "SpotOptim Hyperparameter Tuning (New Interface)"
format: html
---

```{python}
import warnings
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes

# New SpotOptim Interfaces
from spotoptim.core.data import SpotDataFromArray
from spotoptim.core.experiment import ExperimentControl
from spotoptim.hyperparameters.parameters import ParameterSet
from spotoptim.nn.linear_regressor import LinearRegressor
from spotoptim.function.torch_objective import TorchObjective
from spotoptim.SpotOptim import SpotOptim

warnings.filterwarnings("ignore")
```

# Introduction

This tutorial demonstrates the new, streamlined interface for hyperparameter tuning in `spotoptim`. We will optimize a Neural Network regressor on the Diabetes dataset.

## 1. Data Loading

We use `SpotDataFromArray` to unify data handling. Here we load the standard Diabetes dataset from scikit-learn.

```{python}
# Load data
diabetes = load_diabetes()
X = diabetes.data.astype(np.float32)
y = diabetes.target.reshape(-1, 1).astype(np.float32)

# Create SpotDataSet
data = SpotDataFromArray(x_train=X, y_train=y)
print(f"Data shape: {X.shape} -> {y.shape}")
```

## 2. Define Hyperparameters

The `ParameterSet` class allows for a fluid definition of the search space.

```{python}
params = LinearRegressor.get_default_parameters()
print(params)
```


```{python}
params = ParameterSet() \
    .add_float("lr", 1e-4, 1e-1, transform="log") \
    .add_int("l1", 8, 64) \
    .add_int("num_hidden_layers", 0, 2) \
    .add_factor("activation", ["ReLU", "Tanh", "Sigmoid"]) \
    .add_factor("optimizer", ["Adam", "SGD", "RMSprop"])

print("Variables:", params.var_name)
print(params)
```

## 3. Experiment Configuration

`ExperimentControl` holds all configuration details, replacing the complex dictionary approach.

```{python}
exp = ExperimentControl(
    dataset=data,
    model_class=LinearRegressor,
    hyperparameters=params,
    epochs=15, # Fixed epochs for this simple run
    batch_size=32,
    metrics=["mse"]
)
```

## 4. Objective Function

`TorchObjective` bridges the configuration with the optimization loop, handling model instantiation and training.

```{python}
objective = TorchObjective(exp)
```

## 5. Run Optimization

We initialize `SpotOptim` using the properties from our `ParameterSet`.

```{python}
optimizer = SpotOptim(
    fun=objective,
    bounds=params.bounds,
    var_type=params.var_type,
    var_name=params.var_name,
    max_iter=20,     # Total evaluations
    n_initial=10,    # Random initial points
    seed=42
)

optimizer.optimize()
```

## 6. Results

We can inspect the best result found.

```{python}
optimizer.print_results ()
```

```{python}
print(f"Best MSE Loss: {optimizer.best_y_:.4f}")
print("Best Configuration:")
best_config = objective._get_hyperparameters(optimizer.best_x_)
for k, v in best_config.items():
    print(f"  {k}: {v}")
```

### Visualization

Plotting the optimization progress.

```{python}
#| label: fig-results
#| fig-cap: "Optimization Progress: Loss vs Iterations"
loss = optimizer.y_
iterations = np.arange(1, len(loss) + 1)

plt.figure(figsize=(10, 6))
plt.plot(iterations, loss, label="Validation MSE", color="blue", marker="o")
plt.xlabel("Function Evaluations")
plt.ylabel("MSE Loss")
plt.title("Optimization Progress")
plt.yscale("log")
plt.grid(True, which="both", linestyle="--", linewidth=0.5)
plt.legend()
plt.show()
```
