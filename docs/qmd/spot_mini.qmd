---
title: "SpotOptim Minimal Example"
format: html
---


```{python}
import os
from math import inf
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import minimize

from spotpython.hyperparameters.values import set_hyperparameter

from spotoptim.data.diabetes import DiabetesDataset
from spotoptim.function.mohyperlight import MoHyperLight
from spotoptim.hyperdict.light_hyper_dict import LightHyperDict
from spotoptim.function.mo.myer import fun_myer16a
from spotoptim.plot.mo import plot_mo
from spotoptim.plot.contour import (mo_generate_plot_grid,
                                     contourf_plot)
from spotoptim.utils.eda import print_exp_table, print_res_table
from spotoptim.utils.file import get_experiment_filename
from spotoptim import SpotOptim

from spotoptim.utils.init import (fun_control_init, surrogate_control_init,
                                   design_control_init)
from spotdesirability.utils.desirability import (DOverall, DMax, DCategorical, DMin,
                                                 DTarget, DArb, DBox)
from spotdesirability.plot.ccd import plotCCD
from spotdesirability.functions.rsm import rsm_opt, conversion_pred, activity_pred
warnings.filterwarnings("ignore")
```



```{python}
#| label: des-imports
import os
from math import inf
import warnings
warnings.filterwarnings("ignore")
```

The following process can be used for the hyperparameter tuning of the `Diabetes` data set using the `spotoptim` package. The `Diabetes` data set is a regression data set that contains 10 input features and a single output feature. The goal is to predict the output feature based on the input features. 

After importing the necessary libraries, the `fun_control` dictionary is set up via the `fun_control_init` function.
The `fun_control` dictionary contains

* `PREFIX`: a unique identifier for the experiment
* `fun_evals`: the number of function evaluations
* `max_time`: the maximum run time in minutes
* `data_set`: the data set. Here we use the `Diabetes` data set that is provided by `spotoptim`.
* `core_model_name`: the class name of the neural network model. This neural network model is provided by `spotpython`.
* `hyperdict`: the hyperparameter dictionary. This dictionary is used to define the hyperparameters of the neural network model. It is also provided by `spotoptim`.
* `_L_in`: the number of input features. Since the `Diabetes` data set has 10 features, `_L_in` is set to 10.
* `_L_out`: the number of output features. Since we want to predict a single value, `_L_out` is set to 1.

The `HyperLight` class is used to define the objective function `fun`.
It connects the `PyTorch` and the `spotpython` methods and is provided by `spotpython`.
Details can be found in the hyperparameter tuning cookbook [@bart23iArXiv] or online [https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/](https://sequential-parameter-optimization.github.io/Hyperparameter-Tuning-Cookbook/).


## The Single-Objective Approach

The simplest way for handling multi-objective results is to simply ignore all but the first objective function. This is done by setting the `fun_mo2so` argument in the `fun_control_init` function to `None`. The `fun_mo2so` argument is used to convert the multi-objective function to a single-objective function. If it is set to `None`, the first objective function is used as the single-objective function. Since the `None` is also the default, no argument is needed for the single-objective approach.

```{python}
#| label: des_spotpython_init
#| echo: true
from spotpython.hyperparameters.values import set_hyperparameter
from spotoptim.utils.init import fun_control_init
import math
PREFIX="0000_no_mo"
data_set = DiabetesDataset()
fun_control = fun_control_init(
    # do not run, if a result file exists
    force_run=True,    
    PREFIX=PREFIX,
    fun_evals=inf,
    max_time=1,
    data_set = data_set,
    core_model_name="light.regression.NNLinearRegressor",
    hyperdict=LightHyperDict,
    _L_in=10,
    _L_out=1)
fun = MoHyperLight(fun_control).fun
```

The method `set_hyperparameter` allows the user to modify default hyperparameter settings.
Here we modify some hyperparameters to keep the model small and to decrease the tuning time.

```{python}
set_hyperparameter(fun_control, "optimizer", [ "Adadelta", "Adam", "Adamax"])
set_hyperparameter(fun_control, "l1", [3,4])
set_hyperparameter(fun_control, "epochs", [3,4])
set_hyperparameter(fun_control, "batch_size", [4,11])
set_hyperparameter(fun_control, "dropout_prob", [0.0, 0.025])
set_hyperparameter(fun_control, "patience", [2, 3])

design_control = design_control_init(init_size=10)
print_exp_table(fun_control)
```

Finally, a `SpotOptim` object is created.
Calling the method `run()` starts the hyperparameter tuning process.

```{python}
#| label: des_run
bounds = []
for i, name in enumerate(fun_control["var_name"]):
    str_name = str(name)
    levels = None
    
    # Check top level
    if str_name in fun_control and fun_control[str_name] is not None and "levels" in fun_control[str_name]:
        levels = fun_control[str_name]["levels"]
    
    # Check core_model_hyper_dict
    if levels is None and "core_model_hyper_dict" in fun_control and str_name in fun_control["core_model_hyper_dict"]:
         if "levels" in fun_control["core_model_hyper_dict"][str_name]:
              levels = fun_control["core_model_hyper_dict"][str_name]["levels"]

    if levels is not None:
        n_levels = len(levels)
        bounds.append((0, n_levels - 1))
    else:
        l, u = fun_control["lower"][i], fun_control["upper"][i]
        bounds.append((l, u))
```

```{python}
print(bounds)
```

```{python}
print(fun_control["var_name"])
```

```{python}
print(fun_control["var_type"])
```

```{python}
print(fun)
```


```{python}
#| label: des_spot_run
S = SpotOptim(fun=fun,
              bounds=bounds,
              max_time=1,
              max_iter=math.inf, 
              n_initial=10,
              var_name=fun_control["var_name"],
              var_type=fun_control["var_type"],
              fun_mo2so=None # Default
              )
S.optimize()
```

@fig-plain_results shows the hyperparameter tuning process. The loss and epochs are plotted versus the function evaluations. The x-axis shows the number of function evaluations, and the y-axis shows the loss and epochs. The loss is plotted in blue, and the epochs are plotted in red. The y-axis is set to a logarithmic scale for better visualization.

```{python}
#| label: fig-plain_results
#| fig-cap: "Results of the hyperparameter tuning process. Loss and epochs are plotted versus the function evaluations."
loss = S.y_mo[:, 0]
epochs = S.y_mo[:, 1]
iterations = np.arange(1, len(loss) + 1)  # Iterations (x-axis)
plt.figure(figsize=(10, 6))
plt.plot(iterations, loss, label="Loss", color="blue", marker="o")
plt.plot(iterations, epochs, label="Epochs", color="red", marker="x")
plt.xlabel("Iterations")
plt.ylabel("Values")
plt.title("Loss and Epochs vs. Iterations")
plt.yscale("log")  # Use log scale for better visualization
plt.grid(True, which="both", linestyle="--", linewidth=0.5)
plt.legend()
plt.show()
```


```{python}
_ = S.print_results()
```

## Weighted Multi-Objective Function

The second approach is to use a weighted mean of both objective functions. This is done by setting the `fun_mo2so` argument in the `fun_control_init` function to a custom function that computes the weighted mean of both objective functions. The weights can be adjusted to give more importance to one objective function over the other. Here, we define the function `aggregate` that computes the weighted mean of both objective functions. The first objective function is weighted with 2 and the second objective function is weighted with 0.1. 

```{python}
#| label: des-mohyperlight-0001-agg
#| echo: true
PREFIX="0001_aggregate"

# Weight first objective with 2, second with 1/10
def aggregate(y):
    import numpy as np
    return np.sum(y*np.array([2, 0.1]), axis=1)
fun_control = fun_control_init(
    # do not run, if a result file exists
    force_run=True,
    fun_mo2so=aggregate,
    PREFIX=PREFIX,
    fun_evals=inf,
    max_time=10,
    data_set = data_set,
    core_model_name="light.regression.NNLinearRegressor",
    hyperdict=LightHyperDict,
    _L_in=10,
    _L_out=1)
```

The remaining code is identical to the single-objective approach. The only difference is that the `fun_mo2so` argument is set to the `aggregate` function. 

```{python}
set_hyperparameter(fun_control, "optimizer", [ "Adadelta", "Adam", "Adamax"])
set_hyperparameter(fun_control, "l1", [3,4])
set_hyperparameter(fun_control, "epochs", [3,4  ])
set_hyperparameter(fun_control, "batch_size", [4,11])
set_hyperparameter(fun_control, "dropout_prob", [0.0, 0.025])
set_hyperparameter(fun_control, "patience", [2, 3])

design_control = design_control_init(init_size=10)

bounds = []
for i, name in enumerate(fun_control["var_name"]):
    str_name = str(name)
    levels = None
    
    # Check top level
    if str_name in fun_control and fun_control[str_name] is not None and "levels" in fun_control[str_name]:
        levels = fun_control[str_name]["levels"]
    
    # Check core_model_hyper_dict
    if levels is None and "core_model_hyper_dict" in fun_control and str_name in fun_control["core_model_hyper_dict"]:
         if "levels" in fun_control["core_model_hyper_dict"][str_name]:
              levels = fun_control["core_model_hyper_dict"][str_name]["levels"]

    if levels is not None:
        n_levels = len(levels)
        bounds.append((0, n_levels - 1))
    else:
        l, u = fun_control["lower"][i], fun_control["upper"][i]
        bounds.append((l, u))

S = SpotOptim(fun=fun,
              bounds=bounds,
              max_time=fun_control["max_time"],
              max_iter=fun_control["fun_evals"], 
              n_initial=design_control["init_size"],
              var_name=fun_control["var_name"],
              var_type=fun_control["var_type"],
              fun_mo2so=aggregate
              )
S.optimize()    
```

```{python}
_ = S.print_results()
```

::: {.callout-note}
### Results from the Weighted Multi-Objective Function Approach

* The weighted multi-objective approach reulsted in a validation loss of `5824` and `64` ($=2^{6}$) epochs.
* Although the number of epochs is smaller than in the single-objective approach, the validation loss is larger.
* This is an inherent problem of weighted multi-objective approaches, because the deteriination of "good" weights is non-trivial.

:::



## Multi-Objective Hyperparameter Tuning With Desirability

### Setting Up the Desirability Function

The third approach is to use a desirability function to combine the two objective functions into a single objective function. The desirability function is used to maximize the desirability of the two objective functions. The desirability function is defined in the `fun_control_init` function by setting the `fun_mo2so` argument to a custom function that computes the desirability of both objective functions. The desirability function is defined in the following code segment.

```{python}
PREFIX="0002"
data_set = DiabetesDataset()
# fun is defined later? No, usually in QMD subsequent blocks can use previous defs, 
# but lines 1185-1196 defined fun_control and fun.
# Here we redefine PREFIX and data_set.
# Wait, fun was defined as MoHyperLight().fun.
# We should redefine fun with new fun_control if fun_control changes.
# But here fun_control is defined in next block (lines 1355).
# So line 1323 usage of MoHyperLight().fun is premature if passed to Spot later?
# Wait, line 1323: fun = MoHyperLight().fun.
# Line 1378: S = Spot(fun=fun, fun_control=fun_control...)
# So Spot calls fun(X, fun_control).
# With SpotOptim, we need fun bound to fun_control.
# So we should delay fun creation until fun_control is ready.
# OR creates a placeholder but updated later? No.
# I will REMOVE line 1323 and put it after fun_control_init.

```

```{python}
#| label: des-mohyperlight-0002-desirability
#| echo: true
def desirability(y):
    from spotdesirability.utils.desirability import DOverall, DMin
    lossD = DMin(10, 6000)
    epochsD = DMin(32, 64)
    overallD = DOverall(lossD, epochsD)
    overall_desirability = overallD.predict(y, all=False)
    return 1.0 - overall_desirability
```

```{python}
#| label: fig-des-mohyperlight-0002-lossD
#| fig-cap: "The desirability function for the loss outcome."
lossD = DMin(10, 6000)
lossD.plot(xlabel="loss", ylabel="desirability", figsize=(6,4))
```

```{python}
#| label: fig-des-mohyperlight-0002-epochsD
#| fig-cap: "The desirability function for the epochs outcome."
epochsD = DMin(32, 64)
epochsD.plot(xlabel="epochs", ylabel="desirability", figsize=(6,4))
```

Note: We have chosen simple desirability functions based on `DMin` for the `validation loss` and `number of epochs`. The usage of these functions might result in large plateaus where the optimizer does not find any improvement. Therefore, we will explore more sophisticated desirability functions in the future. These can easily be implemented with the approach shown in {@sec-arbitary}.

```{python}
fun_control = fun_control_init(
    # not run, even if a result file exists
    force_run=True,
    fun_mo2so=desirability,
    device="cpu",
    PREFIX=PREFIX,
    fun_evals=inf,
    max_time=10,
    data_set = data_set,
    core_model_name="light.regression.NNLinearRegressor",
    hyperdict=LightHyperDict,
    _L_in=10,
    _L_out=1)
    
fun = MoHyperLight(fun_control).fun

set_hyperparameter(fun_control, "optimizer", [ "Adadelta", "Adam", "Adamax"])
set_hyperparameter(fun_control, "l1", [3,4])
set_hyperparameter(fun_control, "epochs", [3,4])
set_hyperparameter(fun_control, "batch_size", [4,11])
set_hyperparameter(fun_control, "dropout_prob", [0.0, 0.025])
set_hyperparameter(fun_control, "patience", [2, 3])

design_control = design_control_init(init_size=10)

bounds = []
for i, name in enumerate(fun_control["var_name"]):
    str_name = str(name)
    levels = None
    
    # Check top level
    if str_name in fun_control and fun_control[str_name] is not None and "levels" in fun_control[str_name]:
        levels = fun_control[str_name]["levels"]
    
    # Check core_model_hyper_dict
    if levels is None and "core_model_hyper_dict" in fun_control and str_name in fun_control["core_model_hyper_dict"]:
         if "levels" in fun_control["core_model_hyper_dict"][str_name]:
              levels = fun_control["core_model_hyper_dict"][str_name]["levels"]

    if levels is not None:
        n_levels = len(levels)
        bounds.append((0, n_levels - 1))
    else:
        l, u = fun_control["lower"][i], fun_control["upper"][i]
        bounds.append((l, u))

S = SpotOptim(fun=fun,
              bounds=bounds,
              max_time=fun_control["max_time"],
              max_iter=fun_control["fun_evals"], 
              n_initial=design_control["init_size"],
              var_name=fun_control["var_name"],
              var_type=fun_control["var_type"],
              fun_mo2so=desirability
              )
S.optimize()    
```


```{python}
_ = S.print_results()
```



```{python}
print(f"S.y_mo.shape: {S.y_mo.shape}")
print(f"min loss: {np.nanmin(S.y_mo[:,0]):.2f}")
print(f"min epochs: {np.nanmin(S.y_mo[:,1])}")
# print unique values of S.y_mo[:,1]
print(f"unique epochs values: {np.unique(S.y_mo[:,1])}")

```

::: {.callout-note}
### Results from the Desirability Function Approach

* The desirability  multi-objective approach reulsted in a validation loss of `2960` and `32` ($=2^{5}$) epochs.
* The number of epochs is much smaller than in the single-objective approach, and the validation loss is in a similar range.
* This illustrates the applicability of desirability functions for multi-objective optimization.
:::


### Pareto Front

The following two figures show the Pareto front for the multi-objective optimization problem.
@fig-des-mohyperlight-0002-pareto-2 shows the Pareto front for the multi-objective optimization problem. The y-axis uses a logarithmic scal.
@fig-mohyperlight-0002-pareto-3 shows the Pareto front for the multi-objective optimization problem. The points with loss > 1e5 are removed from the plot (due to this removal, the indices of the pointsin the plot change). The x-axis uses a double-logarithmic scale, whereas the y-axis is set to a logarithmic scale for better visualization.

The best point has the following values:

```{python}
# generate a dataframe with S.y and S.y_mo
df = pd.DataFrame(S.y_mo, columns=["loss", "epochs"])
df["y"] = S.best_y_
# get the row where y is min and show the corresponding loss and epochs
df_min = df.loc[df["y"].idxmin()]
print(f"min y: {df_min['y']}")
print(f"loss: {df_min['loss']}")
print(f"epochs: {df_min['epochs']}")
# put df_min loss and epochs into a 1-d numpy array
best_point = np.array([df_min["loss"], df_min["epochs"]])
print(f"best_point: {best_point}")
```

```{python}
#| label: fig-des-mohyperlight-0002-pareto-2
#| fig-cap: "Pareto front for the multi-objective optimization problem."
y_orig = S.y_mo
df_z = pd.DataFrame(y_orig, columns=["loss", "epochs"])
df_z_sel = df_z.dropna()
target_names = ["loss (log-log)", "epochs"]
combinations=[(0,1)]
plot_mo(y_orig=df_z_sel, target_names=target_names, combinations=combinations, pareto="min", pareto_front_orig=True, title="Pareto front (minimization)", pareto_label=True, x_axis_transformation="loglog")
```


```{python}
#| label: fig-mohyperlight-0002-pareto-3
#| fig-cap: "Pareto front for the multi-objective optimization problem. Points with loss > 1e5 removed. The Pareto points are identical to the points in the previous plot. Their indices changed due to the removal of points."
# remove loss values larger than 1e5 from the y_orig array
y_orig = S.y_mo
df_z = pd.DataFrame(y_orig, columns=["loss", "epochs"])
# remove rows with loss > 1e5
df_z = df_z[df_z["loss"] < 1e5]
df_z_sel = df_z.dropna()
target_names = ["loss", "epochs (log)"]
combinations=[(0,1)]
plot_mo(y_orig=df_z_sel, target_names=target_names, combinations=combinations, pareto="min", pareto_front_orig=True, title="Pareto front (min). Points with loss > 1e5 removed", pareto_label=True, y_axis_transformation="log")
```


# A Simple Example


```{python}
#| label: import-libraries
import copy
import numpy as np
import pandas as pd
import numpy as np
import warnings

from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.kernel_approximation import Nystroem
from sklearn.gaussian_process.kernels import RBF, WhiteKernel
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

import matplotlib.pyplot as plt
import seaborn as sns

from scipy.stats.qmc import LatinHypercube
from scipy.optimize import dual_annealing

from spotdesirability.utils.desirability import DMin, DMax, DOverall


from spotoptim.sampling.mm import mmphi_intensive, mmphi_intensive_update, mm_improvement_contour
from spotoptim.inspection.predictions import plot_actual_vs_predicted
from spotoptim.inspection import (generate_mdi, generate_imp, plot_importances, plot_feature_importances, plot_feature_scatter_matrix)

from spotoptim.utils.boundaries import get_boundaries, map_to_original_scale
from spotoptim.mo.pareto import is_pareto_efficient, plot_mo
from spotoptim.eda import plot_ip_boxplots, plot_ip_histograms
from spotoptim.utils import get_combinations
from spotoptim.sampling.lhs import rlh
from spotoptim.sampling.mm import (bestlh, plot_mmphi_vs_n_lhs, mmphi, mmphi_intensive, mm_improvement, plot_mmphi_vs_points, mmphi_intensive_update)
from spotoptim.mo import mo_mm_desirability_function, mo_mm_desirability_optimizer
from spotoptim.mo.mo_mm import mo_xy_desirability_plot
from spotoptim.function.mo import mo_conv2_max
from spotoptim.mo.pareto import (mo_xy_surface, mo_xy_contour, mo_pareto_optx_plot)
from spotoptim.utils.eval import mo_cv_models, mo_eval_models
warnings.filterwarnings("ignore")
```

## Setup and Data Generation

First, we generate an artificial dataset with two input variables ($X_0$) and two target variables ($y_1, y_2$).

```{python}
#| label: fit-simple2
# Set random seed for reproducibility
np.random.seed(42)

# Generate artificial data
n_samples = 50
n_features = 2

# X0: Random points in [0, 1]^2
X0 = np.random.rand(n_samples, n_features)

# y1: Sphere function (minimize)
y1 = np.sum((X0 - 0.5)**2, axis=1)

# y2: Rosenbrock-like function (minimize)
y2 = 100 * (X0[:, 1] - X0[:, 0]**2)**2 + (1 - X0[:, 0])**2

# Train Random Forest models
model_y1 = RandomForestRegressor(n_estimators=100, random_state=42)
model_y1.fit(X0, y1)
model_y2 = RandomForestRegressor(n_estimators=100, random_state=42)
model_y2.fit(X0, y2)

print("Models trained successfully.")
```

## Desirability Functions

We define desirability functions to map each objective to a [0, 1] scale, where 1 is most desirable.

- **Target 1 ($y_1$)**: Minimize.
- **Target 2 ($y_2$)**: Minimize.
- **Target 3 ($y_{mm}$)**: Minimize negative space-filling improvement.
  - $y_{mm} = \Phi_{base} - \Phi_{new}$
  - Since we want to minimize $\Phi_{new}$ (better space-filling), we want to maximize the difference $\Phi_{base} - \Phi_{new}$.

## Define desirability functions using spotdesirability
```{python}
#| label: desirability-functions-simple
# Target 1 (y1): Minimize
d1 = DMin(y1.min(), y1.max())

# Target 2 (y2): Minimize
d2 = DMin(y2.min(), y2.max())

# Target 3 (y_mm): Maximize space-filling improvement
# y_mm = Phi_base - Phi_new
# We estimate bounds for y_mm
X_base = X0.copy()
phi_base, J_base, d_base = mmphi_intensive(X_base, q=2, p=2)

ymm_min = -0.5 # Allow for some degradation
ymm_max = 0.5  # Expected improvement
d_mm = DMin(ymm_min, ymm_max)

# Combine into overall desirability
# We use geometric mean (default for DOverall)
D_overall = DOverall(d1, d2, d_mm)
```

## The Multi-Objective Function

This function calculates the combined desirability of a candidate point `x`.

```{python}
#| label: multi-objective-function-simple

def multi_objective(x, models, X_base, J_base, d_base, phi_base):
    """
    Calculates the negative combined desirability for a candidate point x.
    
    Args:
        x (np.ndarray): Candidate point (1D array).
        models (list): List of trained models [model_y1, model_y2].
        X_base (np.ndarray): Existing design points.
        J_base (np.ndarray): Multiplicities of distances for X_base.
        d_base (np.ndarray): Unique distances for X_base.
        phi_base (float): Base Morris-Mitchell metric for X_base.
        
    Returns:
        float: Negative geometric mean of desirabilities (for minimization).
    """
    # 1. Predict y1 and y2
    # Reshape x for prediction (1, n_features)
    x_reshaped = x.reshape(1, -1)
    y1_pred = models[0].predict(x_reshaped)[0]
    y2_pred = models[1].predict(x_reshaped)[0]
    
    # 2. Compute y_mm (Space-filling improvement)
    # Use efficient update
    y_mm = -1.0 *  mm_improvement(x, X_base, phi_base, J_base, d_base)    
    
    # 3. Calculate combined desirability
    # We pass the values as a list to the overall desirability object
    D = D_overall.predict([y1_pred, y2_pred, y_mm])
    
    # Ensure D is a scalar
    if isinstance(D, np.ndarray):
        D = D.item()
    
    # Return negative D because optimizers usually minimize
    # Also return the individual values for logging
    return -D, [y1_pred, y2_pred, y_mm]
```

## Optimization Loop (`mo_mm_desirability_optimizer`)

We use a global optimizer (e.g., `dual_annealing` from `scipy`) to find the best `x` that maximizes the combined desirability.

## Running the Optimization

Now we run the optimization to find the next point to add to our design.

```{python}
#| label: mo-mm-desirability-optimizer-simple
# Define bounds for the design variables (0 to 1)
bounds = [(0, 1), (0, 1)]

# Ensure X_base and MM stats are available (in case of partial execution)
if 'X_base' not in locals():
    X_base = X.copy()
    phi_base, J_base, d_base = mmphi_intensive(X_base, q=2, p=2)

# Find the next best point
best_x, best_desirability, callback_values = mo_mm_desirability_optimizer(X_base, [model_y1, model_y2], bounds, multi_objective)

print(f"Best new point: {best_x}")
print(f"Predicted Desirability: {best_desirability:.4f}")

# Verify predictions for the new point
y1_pred = model_y1.predict(best_x.reshape(1, -1))[0]
y2_pred = model_y2.predict(best_x.reshape(1, -1))[0]
y_mm = -1.0 * mm_improvement(best_x, X_base, phi_base, J_base, d_base)

print(f"Predicted y1: {y1_pred:.4f}")
print(f"Predicted y2: {y2_pred:.4f}")
print(f"Space-filling improvement (y_mm): {y_mm:.4f}")
```

```{python}
#| label: mo-mm-desirability-optimizer-plot-simple
# Prepare data for plotting
y_best_vals = np.array([[y1_pred, y2_pred, y_mm]])
target_names = ["y1 (min)", "y2 (min)", "y_mm (min)"]
combinations = [(0, 1), (0, 2), (1, 2)]
plot_mo(
    combinations=combinations,
    pareto="min",
    y_rf=callback_values,
    pareto_front_y_rf=True,
    y_best=y_best_vals,
    title="Optimization Trajectory",
    target_names=target_names,
    y_rf_color="blue",
    y_best_color="red"
)
```


## Comparison of known and best input values

plot_ip_histograms and plot_ip_boxplots are used to compare the known and best input values.

```{python}
#| label: mo-mm-desirability-optimizer-plot-comparison-simple
# Prepare data for comparison
# Convert X_base to DataFrame
df_X = pd.DataFrame(X_base, columns=[f"x{i}" for i in range(n_features)])

# Convert best_x to DataFrame (single row)
df_best = pd.DataFrame(best_x.reshape(1, -1), columns=[f"x{i}" for i in range(n_features)])

# Plot histograms comparing initial design and best point
print("Comparison of Initial Design and Best Point (Histograms)")
plot_ip_histograms(df_X, add_points=df_best)

# Plot boxplots comparing initial design and best point
print("Comparison of Initial Design and Best Point (Boxplots)")
plot_ip_boxplots(df_X, add_points=df_best)
```

