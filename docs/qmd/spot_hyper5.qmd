---
title: "SpotOptim Simple Multi-Objective Example"
format: html
---

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings

# Sklearn for Surrogate Models
from sklearn.ensemble import RandomForestRegressor
from scipy.optimize import dual_annealing

# SpotOptim Components
from spotoptim.sampling.mm import mmphi_intensive, mm_improvement
from spotoptim.mo.pareto import plot_mo
from spotoptim.eda import plot_ip_boxplots, plot_ip_histograms
from spotoptim.mo.mo_mm import mo_mm_desirability_optimizer

# SpotDesirability
from spotdesirability.utils.desirability import DMin, DOverall

warnings.filterwarnings("ignore")
```

# Introduction

This tutorial demonstrates a simple multi-objective optimization workflow using `spotoptim`. We will:
1. Generate synthetic data for two conflicting objectives.
2. Train Random Forest surrogate models.
3. Define Desirability Functions for each objective.
4. Include a space-filling crtierion (Morris-Mitchell).
5. Optimize the combined desirability to find the next best candidate point.

## 1. Data Generation

We generate an artificial dataset with two input variables ($X_0 \in [0, 1]^2$) and two target variables ($y_1, y_2$).

- **$y_1$**: Sphere function (minimize).
- **$y_2$**: Rosenbrock-like function (minimize).

```{python}
#| label: fit-simple2
# Set random seed for reproducibility
np.random.seed(42)

# Generate artificial data
n_samples = 50
n_features = 2

# X0: Random points in [0, 1]^2
X0 = np.random.rand(n_samples, n_features)

# y1: Sphere function
y1 = np.sum((X0 - 0.5)**2, axis=1)

# y2: Rosenbrock-like function
y2 = 100 * (X0[:, 1] - X0[:, 0]**2)**2 + (1 - X0[:, 0])**2

# Train Random Forest models
model_y1 = RandomForestRegressor(n_estimators=100, random_state=42)
model_y1.fit(X0, y1)
model_y2 = RandomForestRegressor(n_estimators=100, random_state=42)
model_y2.fit(X0, y2)

print("Models trained successfully.")
```

## 2. Desirability Functions

We define desirability functions to map each objective to a [0, 1] scale, where 1 is most desirable.

- **Target 1 ($y_1$)**: Minimize.
- **Target 2 ($y_2$)**: Minimize.
- **Target 3 ($y_{mm}$)**: Minimize negative space-filling improvement.
  - $y_{mm} = \Phi_{base} - \Phi_{new}$
  - A positive difference means the new point improves space-filling (lowers energy).
  - Maximizing improvement is equivalent to minimizing negative improvement.

```{python}
#| label: desirability-functions-simple
# Target 1 (y1): Minimize
d1 = DMin(y1.min(), y1.max())

# Target 2 (y2): Minimize
d2 = DMin(y2.min(), y2.max())

# Target 3 (y_mm): Maximize space-filling improvement
# We estimate bounds for y_mm
X_base = X0.copy()
phi_base, J_base, d_base = mmphi_intensive(X_base, q=2, p=2)

ymm_min = -0.5 # Allow for some degradation
ymm_max = 0.5  # Expected improvement
d_mm = DMin(ymm_min, ymm_max)

# Combine into overall desirability
# We use geometric mean (default for DOverall)
D_overall = DOverall(d1, d2, d_mm)
```

## 3. The Multi-Objective Function

This function calculates the combined desirability of a candidate point `x`. It integrates the surrogate model predictions and the space-filling metric.

```{python}
#| label: multi-objective-function-simple

def multi_objective(x, models, X_base, J_base, d_base, phi_base):
    """
    Calculates the negative combined desirability for a candidate point x.
    
    Args:
        x (np.ndarray): Candidate point (1D array).
        models (list): List of trained models [model_y1, model_y2].
        X_base (np.ndarray): Existing design points.
        J_base (np.ndarray): Multiplicities of distances for X_base.
        d_base (np.ndarray): Unique distances for X_base.
        phi_base (float): Base Morris-Mitchell metric for X_base.
        
    Returns:
        float: Negative geometric mean of desirabilities (for minimization).
    """
    # 1. Predict y1 and y2
    # Reshape x for prediction (1, n_features)
    x_reshaped = x.reshape(1, -1)
    y1_pred = models[0].predict(x_reshaped)[0]
    y2_pred = models[1].predict(x_reshaped)[0]
    
    # 2. Compute y_mm (Space-filling improvement)
    # y_mm = Phi_base - Phi_new
    # High y_mm is good.
    y_mm = mm_improvement(x, X_base, phi_base, J_base, d_base)    
    
    # 3. Calculate combined desirability
    # We pass the values as a list to the overall desirability object
    # Note: mm_improvement returns positive improvement if better.
    # d_mm is configured to maximize y_mm (via DMin(min, max)? No DMin minimizes inputs? Wait.)
    # In block above: d_mm = DMin(ymm_min, ymm_max). DMin maps low->1, high->0 ??
    # Actually DMin maps low to 1 (most desirable) and high to 0 (least desirable).
    # If we want to MAXIMIZE improvement, we should use DMax.
    # Let's check user's code: d_mm = DMin(ymm_min, ymm_max).
    # And user comments: "Target 3 (y_mm): Minimize negative space-filling improvement"
    # But code line 607: y_mm = -1.0 * mm_improvement(...)
    # Ah, the user negated mm_improvement in their code to use DMin logic?
    # Let's align with the previous code logic exactly:
    
    # User Code: y_mm = -1.0 * mm_improvement(x, ...). i.e. Negative Improvement.
    # DMin favors LOW values. So DMin favors LOW Negative Improvement -> High Positive Improvement.
    # Correct.
    
    y_mm_neg = -1.0 * y_mm

    D = D_overall.predict([y1_pred, y2_pred, y_mm_neg])
    
    # Ensure D is a scalar
    if isinstance(D, np.ndarray):
        D = D.item()
    
    # Return negative D because optimizers usually minimize
    # Also return the individual values for logging
    return -D, [y1_pred, y2_pred, y_mm_neg]
```

## 4. Run Optimization

We use `mo_mm_desirability_optimizer` (which wraps `dual_annealing`) to find the best point.

```{python}
#| label: mo-mm-desirability-optimizer-simple
# Define bounds for the design variables (0 to 1)
bounds = [(0, 1), (0, 1)]

# Find the next best point
best_x, best_desirability, callback_values = mo_mm_desirability_optimizer(X_base, [model_y1, model_y2], bounds, multi_objective)

print(f"Best new point: {best_x}")
print(f"Predicted Desirability: {best_desirability:.4f}")

# Verify predictions for the new point
y1_pred = model_y1.predict(best_x.reshape(1, -1))[0]
y2_pred = model_y2.predict(best_x.reshape(1, -1))[0]
# Calculating actual improvement for print
y_mm_actual = mm_improvement(best_x, X_base, phi_base, J_base, d_base)

print(f"Predicted y1: {y1_pred:.4f}")
print(f"Predicted y2: {y2_pred:.4f}")
print(f"Space-filling improvement: {y_mm_actual:.4f}")
```

### Visualization

Comparison of the objectives during the optimization search.

```{python}
#| label: mo-mm-desirability-optimizer-plot-simple
# Prepare data for plotting
# The callback values contains [y1, y2, y_mm_neg]
y_best_vals = np.array([[y1_pred, y2_pred, -y_mm_actual]])
target_names = ["y1 (min)", "y2 (min)", "y_mm (neg)"]
combinations = [(0, 1), (0, 2), (1, 2)]

plot_mo(
    combinations=combinations,
    pareto="min",
    y_rf=callback_values,
    pareto_front_y_rf=True,
    y_best=y_best_vals,
    title="Optimization Trajectory",
    target_names=target_names,
    y_rf_color="blue",
    y_best_color="red"
)
```

## 5. Input Space Analysis

We compare the distribution of the initial design points with the chosen optimal point.

```{python}
#| label: mo-mm-desirability-optimizer-plot-comparison-simple
# Convert X_base to DataFrame
df_X = pd.DataFrame(X_base, columns=[f"x{i}" for i in range(n_features)])

# Convert best_x to DataFrame (single row)
df_best = pd.DataFrame(best_x.reshape(1, -1), columns=[f"x{i}" for i in range(n_features)])

# Plot histograms comparing initial design and best point
print("Comparison of Initial Design and Best Point")
plot_ip_histograms(df_X, add_points=df_best)
```
