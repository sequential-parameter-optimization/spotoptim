[
  {
    "objectID": "docs/spotoptim_examples.html",
    "href": "docs/spotoptim_examples.html",
    "title": "SpotOptim: Living Examples",
    "section": "",
    "text": "This document contains fully executable code examples for the SpotOptim class. Every example is a {python} code block and is covered by a corresponding pytest in tests/test_spotoptim_deep.py.\nRun all examples with:"
  },
  {
    "objectID": "docs/spotoptim_examples.html#quick-start-sphere-function",
    "href": "docs/spotoptim_examples.html#quick-start-sphere-function",
    "title": "SpotOptim: Living Examples",
    "section": "Quick Start: Sphere Function",
    "text": "Quick Start: Sphere Function\nThe sphere function \\(f(x) = \\sum_{i=1}^{n} x_i^2\\) has its global minimum at \\(x^* = 0\\) with \\(f(x^*) = 0\\).\n\nimport numpy as np\nfrom spotoptim import SpotOptim\n\ndef sphere(X):\n    X = np.atleast_2d(X)\n    return np.sum(X**2, axis=1)\n\nopt = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=20,\n    n_initial=10,\n    seed=0,\n)\nresult = opt.optimize()\n\nprint(f\"Best x    : {result.x}\")\nprint(f\"Best f(x) : {result.fun:.6f}\")\nprint(f\"Evaluations: {result.nfev}\")\n\nBest x    : [-1.07968718e-04  9.05044427e-05]\nBest f(x) : 0.000000\nEvaluations: 20\n\n\nCorresponding test (test_sphere_2d_converges_near_origin):\n\nassert result.fun &lt; 0.5, f\"Expected convergence near 0, got f={result.fun}\"\nprint(\"Convergence check passed.\")\n\nConvergence check passed."
  },
  {
    "objectID": "docs/spotoptim_examples.html#result-contract",
    "href": "docs/spotoptim_examples.html#result-contract",
    "title": "SpotOptim: Living Examples",
    "section": "Result Contract",
    "text": "Result Contract\nEvery call to optimize() returns a scipy.optimize.OptimizeResult satisfying these invariants:\n\nfrom scipy.optimize import OptimizeResult\n\nassert isinstance(result, OptimizeResult)\n\n# Required fields\nfor field in (\"x\", \"fun\", \"nfev\", \"nit\", \"success\", \"message\", \"X\", \"y\"):\n    assert hasattr(result, field), f\"Missing: {field}\"\n\n# Shape invariants\nassert result.x.ndim == 1            # best point is 1-D\nassert result.X.shape == (20, 2)     # (max_iter, n_dim)\nassert result.y.shape == (20,)       # one y per evaluation\n\n# Optimality\nassert np.isclose(result.fun, np.min(result.y))\nbest_idx = np.argmin(result.y)\nnp.testing.assert_array_almost_equal(result.x, result.X[best_idx])\n\nprint(\"All result contract checks passed.\")\n\nAll result contract checks passed."
  },
  {
    "objectID": "docs/spotoptim_examples.html#acquisition-functions",
    "href": "docs/spotoptim_examples.html#acquisition-functions",
    "title": "SpotOptim: Living Examples",
    "section": "Acquisition Functions",
    "text": "Acquisition Functions\nSpotOptim supports three acquisition functions:\n\n\n\nAcquisition\nDescription\n\n\n\n\n\"y\" (default)\nBest observed value\n\n\n\"ei\"\nExpected Improvement\n\n\n\"pi\"\nProbability of Improvement\n\n\n\n\nresults = {}\nfor acq in [\"y\", \"ei\", \"pi\"]:\n    r = SpotOptim(\n        fun=sphere,\n        bounds=[(-3, 3), (-3, 3)],\n        max_iter=12,\n        n_initial=7,\n        acquisition=acq,\n        seed=0,\n    ).optimize()\n    results[acq] = r\n    print(f\"  acq={acq!r:4s}  f(x*)={r.fun:.4f}  nfev={r.nfev}\")\n\n  acq='y'   f(x*)=0.0000  nfev=12\n  acq='ei'  f(x*)=0.0000  nfev=12\n  acq='pi'  f(x*)=0.0155  nfev=12\n\n\n\nfor acq, r in results.items():\n    assert isinstance(r, OptimizeResult), f\"{acq} did not return OptimizeResult\"\n    assert r.success is True\n    assert np.isfinite(r.fun)\nprint(\"All acquisition checks passed.\")\n\nAll acquisition checks passed."
  },
  {
    "objectID": "docs/spotoptim_examples.html#seed-reproducibility",
    "href": "docs/spotoptim_examples.html#seed-reproducibility",
    "title": "SpotOptim: Living Examples",
    "section": "Seed Reproducibility",
    "text": "Seed Reproducibility\nThe same seed produces byte-identical results across independent runs:\n\ncommon_kwargs = dict(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=12,\n    n_initial=6,\n    seed=42,\n)\nr1 = SpotOptim(**common_kwargs).optimize()\nr2 = SpotOptim(**common_kwargs).optimize()\n\nnp.testing.assert_array_equal(r1.X, r2.X)\nnp.testing.assert_array_equal(r1.y, r2.y)\nassert r1.fun == r2.fun\nprint(f\"Seed 42 result: f(x*)={r1.fun:.6f} — fully reproducible.\")\n\nSeed 42 result: f(x*)=0.000007 — fully reproducible."
  },
  {
    "objectID": "docs/spotoptim_examples.html#integer-variables",
    "href": "docs/spotoptim_examples.html#integer-variables",
    "title": "SpotOptim: Living Examples",
    "section": "Integer Variables",
    "text": "Integer Variables\nUse var_type=[\"int\", ...] to restrict variables to integer domains.\nThe optimum of \\(f(x_1, x_2) = x_1^2 + x_2^2\\) over \\(\\mathbb{Z}^2\\) is at \\(x^* = (0, 0)\\).\n\nopt_int = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=12,\n    n_initial=7,\n    var_type=[\"int\", \"int\"],\n    seed=0,\n)\nresult_int = opt_int.optimize()\n\nprint(f\"Best x    : {result_int.x}  (should be integers)\")\nprint(f\"Best f(x) : {result_int.fun}\")\n\n# All stored X values must be integers\nassert np.allclose(result_int.X, np.round(result_int.X), atol=1e-9)\nassert np.allclose(result_int.x, np.round(result_int.x), atol=1e-9)\nprint(\"Integer domain check passed.\")\n\nBest x    : [-0.  0.]  (should be integers)\nBest f(x) : 0.0\nInteger domain check passed."
  },
  {
    "objectID": "docs/spotoptim_examples.html#log-scaled-variables-var_trans",
    "href": "docs/spotoptim_examples.html#log-scaled-variables-var_trans",
    "title": "SpotOptim: Living Examples",
    "section": "Log-Scaled Variables (var_trans)",
    "text": "Log-Scaled Variables (var_trans)\nFor variables spanning multiple orders of magnitude, use var_trans:\n\ndef log_objective(X):\n    \"\"\"Minimum at x = 10 (log10 scale: minimum at log10(x) = 1).\"\"\"\n    X = np.atleast_2d(X)\n    return (np.log10(X[:, 0]) - 1.0) ** 2\n\nopt_log = SpotOptim(\n    fun=log_objective,\n    bounds=[(1e-3, 1e3)],\n    max_iter=12,\n    n_initial=6,\n    var_trans=[\"log10\"],\n    seed=0,\n)\nresult_log = opt_log.optimize()\n\nprint(f\"Best x      : {result_log.x[0]:.4f}  (expected ~10)\")\nprint(f\"Best f(x)   : {result_log.fun:.6f}\")\n\nassert 1e-3 &lt;= result_log.x[0] &lt;= 1e3, \"Result out of bounds\"\nassert np.isfinite(result_log.fun)\nprint(\"Log-transform checks passed.\")\n\nBest x      : 9.9999  (expected ~10)\nBest f(x)   : 0.000000\nLog-transform checks passed."
  },
  {
    "objectID": "docs/spotoptim_examples.html#factor-categorical-variables",
    "href": "docs/spotoptim_examples.html#factor-categorical-variables",
    "title": "SpotOptim: Living Examples",
    "section": "Factor (Categorical) Variables",
    "text": "Factor (Categorical) Variables\nTuple bounds define categorical levels that are mapped to integers internally:\n\ncall_log = []\n\ndef cat_sphere(X):\n    \"\"\"Records (color, value) pairs; minimises value regardless of color.\"\"\"\n    X = np.atleast_2d(X)\n    call_log.append(X.copy())\n    return X[:, 1].astype(float) ** 2\n\nopt_cat = SpotOptim(\n    fun=cat_sphere,\n    bounds=[(\"red\", \"green\", \"blue\"), (-5.0, 5.0)],\n    max_iter=10,\n    n_initial=6,\n    seed=0,\n)\n\n# Factor dimension mapped to integers 0..2 internally\nassert opt_cat.bounds[0] == (0, 2)\nassert opt_cat._factor_maps[0] == {0: \"red\", 1: \"green\", 2: \"blue\"}\n\nresult_cat = opt_cat.optimize()\nprint(f\"Best value : {result_cat.fun:.4f}\")\nprint(\"Factor variable checks passed.\")\n\nBest value : 0.0000\nFactor variable checks passed."
  },
  {
    "objectID": "docs/spotoptim_examples.html#custom-kriging-surrogate",
    "href": "docs/spotoptim_examples.html#custom-kriging-surrogate",
    "title": "SpotOptim: Living Examples",
    "section": "Custom Kriging Surrogate",
    "text": "Custom Kriging Surrogate\nReplace the default GP surrogate with Kriging:\n\nfrom spotoptim import Kriging\n\nkriging = Kriging(noise=1e-6, min_theta=-3.0, max_theta=2.0, seed=42)\n\nopt_k = SpotOptim(\n    fun=sphere,\n    bounds=[(-3, 3), (-3, 3)],\n    surrogate=kriging,\n    max_iter=12,\n    n_initial=6,\n    seed=0,\n)\nresult_k = opt_k.optimize()\n\nprint(f\"Kriging best f(x) : {result_k.fun:.6f}\")\nassert isinstance(result_k, OptimizeResult)\nassert result_k.success is True\nassert np.isfinite(result_k.fun)\nprint(\"Kriging surrogate checks passed.\")\n\nKriging best f(x) : 0.000665\nKriging surrogate checks passed."
  },
  {
    "objectID": "docs/spotoptim_examples.html#get_best_hyperparameters",
    "href": "docs/spotoptim_examples.html#get_best_hyperparameters",
    "title": "SpotOptim: Living Examples",
    "section": "get_best_hyperparameters",
    "text": "get_best_hyperparameters\nAfter optimization, retrieve the best point as a labelled dict:\n\nopt_named = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=12,\n    n_initial=6,\n    var_name=[\"x0\", \"x1\"],\n    seed=0,\n)\nresult_named = opt_named.optimize()\nbest = opt_named.get_best_hyperparameters(as_dict=True)\n\nprint(f\"result.x           : {result_named.x}\")\nprint(f\"get_best_hyperparams: {best}\")\n\n# Values in the dict must match result.x\nvalues_arr = np.array(list(best.values()), dtype=float)\nnp.testing.assert_array_almost_equal(values_arr, result_named.x)\nprint(\"get_best_hyperparameters check passed.\")\n\nresult.x           : [0.03919328 0.02660746]\nget_best_hyperparams: {'x0': np.float64(0.039193282891963555), 'x1': np.float64(0.02660746414451208)}\nget_best_hyperparameters check passed."
  },
  {
    "objectID": "docs/spotoptim_examples.html#transform_value-roundtrip",
    "href": "docs/spotoptim_examples.html#transform_value-roundtrip",
    "title": "SpotOptim: Living Examples",
    "section": "transform_value Roundtrip",
    "text": "transform_value Roundtrip\nAll supported variable transformations satisfy the roundtrip identity \\(\\text{inverse}(\\text{transform}(x)) = x\\):\n\nopt_t = SpotOptim(fun=sphere, bounds=[(1e-6, 100)])\n\ntransforms = [\"log10\", \"log\", \"ln\", \"sqrt\", \"exp\", \"square\", \"cube\", \"inv\", None]\nx_test = 4.0\n\nfor trans in transforms:\n    tx = opt_t.transform_value(x_test, trans)\n    x_back = opt_t.inverse_transform_value(tx, trans)\n    ok = np.isclose(x_back, x_test, rtol=1e-9)\n    status = \"OK\" if ok else \"FAIL\"\n    print(f\"  trans={str(trans):&lt;10}  {x_test} -&gt; {tx:.4f} -&gt; {x_back:.6f}  [{status}]\")\n    assert ok, f\"Roundtrip failed for {trans!r}\"\n\nprint(\"All transform roundtrip checks passed.\")\n\n  trans=log10       4.0 -&gt; 0.6021 -&gt; 4.000000  [OK]\n  trans=log         4.0 -&gt; 1.3863 -&gt; 4.000000  [OK]\n  trans=ln          4.0 -&gt; 1.3863 -&gt; 4.000000  [OK]\n  trans=sqrt        4.0 -&gt; 2.0000 -&gt; 4.000000  [OK]\n  trans=exp         4.0 -&gt; 54.5982 -&gt; 4.000000  [OK]\n  trans=square      4.0 -&gt; 16.0000 -&gt; 4.000000  [OK]\n  trans=cube        4.0 -&gt; 64.0000 -&gt; 4.000000  [OK]\n  trans=inv         4.0 -&gt; 0.2500 -&gt; 4.000000  [OK]\n  trans=None        4.0 -&gt; 4.0000 -&gt; 4.000000  [OK]\nAll transform roundtrip checks passed."
  },
  {
    "objectID": "docs/spotoptim_examples.html#convergence-on-rosenbrock",
    "href": "docs/spotoptim_examples.html#convergence-on-rosenbrock",
    "title": "SpotOptim: Living Examples",
    "section": "Convergence on Rosenbrock",
    "text": "Convergence on Rosenbrock\nThe Rosenbrock function \\(f(x,y) = (1-x)^2 + 100(y-x^2)^2\\) has its minimum at \\((1, 1)\\) with \\(f(1,1) = 0\\).\n\ndef rosenbrock(X):\n    X = np.atleast_2d(X)\n    x, y = X[:, 0], X[:, 1]\n    return (1.0 - x)**2 + 100.0 * (y - x**2)**2\n\nopt_rb = SpotOptim(\n    fun=rosenbrock,\n    bounds=[(-2, 2), (-2, 2)],\n    max_iter=30,\n    n_initial=10,\n    acquisition=\"ei\",\n    seed=0,\n)\nresult_rb = opt_rb.optimize()\n\nprint(f\"Best x    : {result_rb.x}\")\nprint(f\"Best f(x) : {result_rb.fun:.6f}  (global min = 0 at [1, 1])\")\n\nassert result_rb.fun &lt; 10.0, f\"Rosenbrock did not converge: f={result_rb.fun}\"\nprint(\"Rosenbrock convergence check passed.\")\n\nBest x    : [0.8746919  0.83489777]\nBest f(x) : 0.503071  (global min = 0 at [1, 1])\nRosenbrock convergence check passed."
  },
  {
    "objectID": "docs/reference/utils.stats.vif.html",
    "href": "docs/reference/utils.stats.vif.html",
    "title": "utils.stats.vif",
    "section": "",
    "text": "utils.stats.vif(X, sorted=True)\nCalculates the Variance Inflation Factor (VIF) for each feature in a DataFrame.\nVIF measures the multicollinearity among independent variables within a regression model. High VIF values indicate high multicollinearity, which can cause issues with model interpretation and stability.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npandas.DataFrame\nA DataFrame containing the independent variables.\nrequired\n\n\nsorted\nbool\nWhether to sort the output DataFrame by VIF values.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npandas.DataFrame: A DataFrame with two columns: - “feature”: The name of the feature. - “VIF”: The Variance Inflation Factor for the feature.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotpython.utils.stats import vif\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; vif(data)\n   feature          VIF\n0      x1  1260.000000\n1      x2         0.000000\n2      x3   630.000000",
    "crumbs": [
      "API Reference",
      "Utilities",
      "vif"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.vif.html#parameters",
    "href": "docs/reference/utils.stats.vif.html#parameters",
    "title": "utils.stats.vif",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\npandas.DataFrame\nA DataFrame containing the independent variables.\nrequired\n\n\nsorted\nbool\nWhether to sort the output DataFrame by VIF values.\nTrue",
    "crumbs": [
      "API Reference",
      "Utilities",
      "vif"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.vif.html#returns",
    "href": "docs/reference/utils.stats.vif.html#returns",
    "title": "utils.stats.vif",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npandas.DataFrame: A DataFrame with two columns: - “feature”: The name of the feature. - “VIF”: The Variance Inflation Factor for the feature.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "vif"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.vif.html#examples",
    "href": "docs/reference/utils.stats.vif.html#examples",
    "title": "utils.stats.vif",
    "section": "",
    "text": "&gt;&gt;&gt; from spotpython.utils.stats import vif\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; vif(data)\n   feature          VIF\n0      x1  1260.000000\n1      x2         0.000000\n2      x3   630.000000",
    "crumbs": [
      "API Reference",
      "Utilities",
      "vif"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.html",
    "href": "docs/reference/utils.stats.html",
    "title": "utils.stats",
    "section": "",
    "text": "utils.stats\n\n\n\n\n\nName\nDescription\n\n\n\n\ncalculate_outliers\nCalculate the number of outliers using the IQR method.\n\n\ncompute_coefficients_table\n\n\n\ncompute_standardized_betas\nComputes standardized (beta) coefficients for a fitted statsmodels OLS model.\n\n\ncondition_index\nCalculates the Condition Index for a DataFrame to assess multicollinearity.\n\n\ncov_to_cor\nConvert a covariance matrix to a correlation matrix.\n\n\nfit_all_lm\nFit a linear regression model for all possible combinations of independent variables.\n\n\nget_all_vars_from_formula\nUtility function to extract variables from a formula.\n\n\nget_combinations\nGenerates all possible combinations of two values from a list of values. Order is not important.\n\n\nget_sample_size\nCalculate sample size n for comparing two means.\n\n\nnormalize_X\nNormalize array X to [0, 1] in each dimension.\n\n\npartial_correlation\nCalculate the partial correlation matrix for a given data set.\n\n\npartial_correlation_test\nThe partial correlation coefficient between x and y given z.\n\n\nplot_coeff_vs_pvals\nPlot the coefficient estimates from fit_all_lm against the corresponding p-values.\n\n\nplot_coeff_vs_pvals_by_included\nGenerates a panel of scatter plots with effect estimates of all possible models against p-values.\n\n\npreprocess_df_for_ols\nPreprocesses a df for fiitting an OLS regression model using the specified target column and predictors.\n\n\nvif\nCalculates the Variance Inflation Factor (VIF) for each feature in a DataFrame.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.stats"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.html#functions",
    "href": "docs/reference/utils.stats.html#functions",
    "title": "utils.stats",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncalculate_outliers\nCalculate the number of outliers using the IQR method.\n\n\ncompute_coefficients_table\n\n\n\ncompute_standardized_betas\nComputes standardized (beta) coefficients for a fitted statsmodels OLS model.\n\n\ncondition_index\nCalculates the Condition Index for a DataFrame to assess multicollinearity.\n\n\ncov_to_cor\nConvert a covariance matrix to a correlation matrix.\n\n\nfit_all_lm\nFit a linear regression model for all possible combinations of independent variables.\n\n\nget_all_vars_from_formula\nUtility function to extract variables from a formula.\n\n\nget_combinations\nGenerates all possible combinations of two values from a list of values. Order is not important.\n\n\nget_sample_size\nCalculate sample size n for comparing two means.\n\n\nnormalize_X\nNormalize array X to [0, 1] in each dimension.\n\n\npartial_correlation\nCalculate the partial correlation matrix for a given data set.\n\n\npartial_correlation_test\nThe partial correlation coefficient between x and y given z.\n\n\nplot_coeff_vs_pvals\nPlot the coefficient estimates from fit_all_lm against the corresponding p-values.\n\n\nplot_coeff_vs_pvals_by_included\nGenerates a panel of scatter plots with effect estimates of all possible models against p-values.\n\n\npreprocess_df_for_ols\nPreprocesses a df for fiitting an OLS regression model using the specified target column and predictors.\n\n\nvif\nCalculates the Variance Inflation Factor (VIF) for each feature in a DataFrame.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.stats"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.plot_coeff_vs_pvals_by_included.html",
    "href": "docs/reference/utils.stats.plot_coeff_vs_pvals_by_included.html",
    "title": "utils.stats.plot_coeff_vs_pvals_by_included",
    "section": "",
    "text": "utils.stats.plot_coeff_vs_pvals_by_included(\n    data,\n    xlabels=None,\n    xlim=(0, 1),\n    xlab='P value',\n    ylim=None,\n    ylab=None,\n    yscale_log=False,\n    title=None,\n    grid=True,\n    ncol=2,\n    show=True,\n    y_scaler=1.1,\n)\nGenerates a panel of scatter plots with effect estimates of all possible models against p-values. Uses a dictionry generated by the fit_all_lm function. Each plot includes effect estimates from all models including a specific variable.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\ndict\nA dictionary, generated by the fit_all_lm function, containing the following keys: - estimate (pd.DataFrame): A DataFrame containing the estimates. - xlist (list): A list of variables. - fun (str): The function name. - family (str): The family of the model.\nrequired\n\n\nxlabels\nlist\nA list of x-axis labels.\nNone\n\n\nxlim\ntuple\nThe x-axis limits.\n(0, 1)\n\n\nxlab\nstr\nThe x-axis label.\n'P value'\n\n\nylim\ntuple\nThe y-axis limits.\nNone\n\n\nylab\nstr\nThe y-axis label.\nNone\n\n\nyscale_log\nbool\nWhether to scale y-axis to log10. Default is False.\nFalse\n\n\ntitle\nstr\nThe title of the plot.\nNone\n\n\ngrid\nbool\nWhether to display gridlines. Default is True.\nTrue\n\n\nncol\nint\nNumber of columns in the plot grid. Default is 2.\n2\n\n\nshow\nbool\nWhether to display the plot. Default is True.\nTrue\n\n\ny_scaler\nfloat\nA scaling factor for the y-axis limits. Default is 1.1, i.e., 10% more than the maximum value.\n1.1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\nBased on the R package ‘allestimates’ by Zhiqiang Wang, see https://cran.r-project.org/package=allestimates\n\n\n\n\nWang, Z. (2007). Two Postestimation Commands for Assessing Confounding Effects in Epidemiological Studies. The Stata Journal, 7(2), 183-196. https://doi.org/10.1177/1536867X0700700203\n\n\n\ndata = { “estimate”: pd.DataFrame({ “variables”: [“Crude”, “AL”, “AM”, “AN”, “AO”], “estimate”: [0.5, 0.6, 0.7, 0.8, 0.9], “conf_low”: [0.1, 0.2, 0.3, 0.4, 0.5], “conf_high”: [0.9, 1.0, 1.1, 1.2, 1.3], “p”: [0.01, 0.02, 0.03, 0.04, 0.05], “aic”: [100, 200, 300, 400, 500], “n”: [10, 20, 30, 40, 50] }), “xlist”: [“AL”, “AM”, “AN”, “AO”], “fun”: “all_lm” } plot_coeff_vs_pvals_by_included(data)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_coeff_vs_pvals_by_included"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.plot_coeff_vs_pvals_by_included.html#parameters",
    "href": "docs/reference/utils.stats.plot_coeff_vs_pvals_by_included.html#parameters",
    "title": "utils.stats.plot_coeff_vs_pvals_by_included",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\ndict\nA dictionary, generated by the fit_all_lm function, containing the following keys: - estimate (pd.DataFrame): A DataFrame containing the estimates. - xlist (list): A list of variables. - fun (str): The function name. - family (str): The family of the model.\nrequired\n\n\nxlabels\nlist\nA list of x-axis labels.\nNone\n\n\nxlim\ntuple\nThe x-axis limits.\n(0, 1)\n\n\nxlab\nstr\nThe x-axis label.\n'P value'\n\n\nylim\ntuple\nThe y-axis limits.\nNone\n\n\nylab\nstr\nThe y-axis label.\nNone\n\n\nyscale_log\nbool\nWhether to scale y-axis to log10. Default is False.\nFalse\n\n\ntitle\nstr\nThe title of the plot.\nNone\n\n\ngrid\nbool\nWhether to display gridlines. Default is True.\nTrue\n\n\nncol\nint\nNumber of columns in the plot grid. Default is 2.\n2\n\n\nshow\nbool\nWhether to display the plot. Default is True.\nTrue\n\n\ny_scaler\nfloat\nA scaling factor for the y-axis limits. Default is 1.1, i.e., 10% more than the maximum value.\n1.1",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_coeff_vs_pvals_by_included"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.plot_coeff_vs_pvals_by_included.html#returns",
    "href": "docs/reference/utils.stats.plot_coeff_vs_pvals_by_included.html#returns",
    "title": "utils.stats.plot_coeff_vs_pvals_by_included",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_coeff_vs_pvals_by_included"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.plot_coeff_vs_pvals_by_included.html#notes",
    "href": "docs/reference/utils.stats.plot_coeff_vs_pvals_by_included.html#notes",
    "title": "utils.stats.plot_coeff_vs_pvals_by_included",
    "section": "",
    "text": "Based on the R package ‘allestimates’ by Zhiqiang Wang, see https://cran.r-project.org/package=allestimates",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_coeff_vs_pvals_by_included"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.plot_coeff_vs_pvals_by_included.html#references",
    "href": "docs/reference/utils.stats.plot_coeff_vs_pvals_by_included.html#references",
    "title": "utils.stats.plot_coeff_vs_pvals_by_included",
    "section": "",
    "text": "Wang, Z. (2007). Two Postestimation Commands for Assessing Confounding Effects in Epidemiological Studies. The Stata Journal, 7(2), 183-196. https://doi.org/10.1177/1536867X0700700203",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_coeff_vs_pvals_by_included"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.plot_coeff_vs_pvals_by_included.html#examples",
    "href": "docs/reference/utils.stats.plot_coeff_vs_pvals_by_included.html#examples",
    "title": "utils.stats.plot_coeff_vs_pvals_by_included",
    "section": "",
    "text": "data = { “estimate”: pd.DataFrame({ “variables”: [“Crude”, “AL”, “AM”, “AN”, “AO”], “estimate”: [0.5, 0.6, 0.7, 0.8, 0.9], “conf_low”: [0.1, 0.2, 0.3, 0.4, 0.5], “conf_high”: [0.9, 1.0, 1.1, 1.2, 1.3], “p”: [0.01, 0.02, 0.03, 0.04, 0.05], “aic”: [100, 200, 300, 400, 500], “n”: [10, 20, 30, 40, 50] }), “xlist”: [“AL”, “AM”, “AN”, “AO”], “fun”: “all_lm” } plot_coeff_vs_pvals_by_included(data)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_coeff_vs_pvals_by_included"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.partial_correlation_test.html",
    "href": "docs/reference/utils.stats.partial_correlation_test.html",
    "title": "utils.stats.partial_correlation_test",
    "section": "",
    "text": "utils.stats.partial_correlation_test(x, y, z, method='pearson')\nThe partial correlation coefficient between x and y given z. x and y should be arrays (vectors) of the same length, and z should be a data frame (matrix).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe first variable as a 1-dimensional array or list.\nrequired\n\n\ny\narray - like\nThe second variable as a 1-dimensional array or list.\nrequired\n\n\nz\npandas.DataFrame\nA data frame containing other conditional variables.\nrequired\n\n\nmethod\nstr\nCorrelation method, one of ‘pearson’, ‘kendall’, or ‘spearman’.\n'pearson'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndict\ndict\nA dictionary with the partial correlation estimate, p-value, statistic, sample size (n), number of given parameters (gp), and method used.\n\n\n\n\n\n\n\nKim, S. ppcor: An R package for a fast calculation to semi-partial correlation coefficients. Commun Stat Appl Methods 22, 6 (Nov 2015), 665–674.\n\n\n\n\n&gt;&gt;&gt; from spotpython.utils.stats import pairwise_partial_correlation\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; x = [1, 2, 3]\n&gt;&gt;&gt; y = [4, 5, 6]\n&gt;&gt;&gt; z = pd.DataFrame({'C': [7, 8, 9]})\n&gt;&gt;&gt; pairwise_partial_correlation(x, y, z)\n{'estimate': -1.0, 'p_value': 0.0, 'statistic': -inf, 'n': 3, 'gp': 1, 'method': 'pearson'}",
    "crumbs": [
      "API Reference",
      "Utilities",
      "partial_correlation_test"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.partial_correlation_test.html#parameters",
    "href": "docs/reference/utils.stats.partial_correlation_test.html#parameters",
    "title": "utils.stats.partial_correlation_test",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe first variable as a 1-dimensional array or list.\nrequired\n\n\ny\narray - like\nThe second variable as a 1-dimensional array or list.\nrequired\n\n\nz\npandas.DataFrame\nA data frame containing other conditional variables.\nrequired\n\n\nmethod\nstr\nCorrelation method, one of ‘pearson’, ‘kendall’, or ‘spearman’.\n'pearson'",
    "crumbs": [
      "API Reference",
      "Utilities",
      "partial_correlation_test"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.partial_correlation_test.html#returns",
    "href": "docs/reference/utils.stats.partial_correlation_test.html#returns",
    "title": "utils.stats.partial_correlation_test",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ndict\ndict\nA dictionary with the partial correlation estimate, p-value, statistic, sample size (n), number of given parameters (gp), and method used.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "partial_correlation_test"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.partial_correlation_test.html#references",
    "href": "docs/reference/utils.stats.partial_correlation_test.html#references",
    "title": "utils.stats.partial_correlation_test",
    "section": "",
    "text": "Kim, S. ppcor: An R package for a fast calculation to semi-partial correlation coefficients. Commun Stat Appl Methods 22, 6 (Nov 2015), 665–674.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "partial_correlation_test"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.partial_correlation_test.html#examples",
    "href": "docs/reference/utils.stats.partial_correlation_test.html#examples",
    "title": "utils.stats.partial_correlation_test",
    "section": "",
    "text": "&gt;&gt;&gt; from spotpython.utils.stats import pairwise_partial_correlation\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; x = [1, 2, 3]\n&gt;&gt;&gt; y = [4, 5, 6]\n&gt;&gt;&gt; z = pd.DataFrame({'C': [7, 8, 9]})\n&gt;&gt;&gt; pairwise_partial_correlation(x, y, z)\n{'estimate': -1.0, 'p_value': 0.0, 'statistic': -inf, 'n': 3, 'gp': 1, 'method': 'pearson'}",
    "crumbs": [
      "API Reference",
      "Utilities",
      "partial_correlation_test"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.pairwise_semi_partial_correlation.html",
    "href": "docs/reference/utils.stats.pairwise_semi_partial_correlation.html",
    "title": "utils.stats.pairwise_semi_partial_correlation",
    "section": "",
    "text": "utils.stats.pairwise_semi_partial_correlation\nutils.stats.pairwise_semi_partial_correlation(x, y, z, method='pearson')",
    "crumbs": [
      "API Reference",
      "Utilities",
      "pairwise_semi_partial_correlation"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.get_sample_size.html",
    "href": "docs/reference/utils.stats.get_sample_size.html",
    "title": "utils.stats.get_sample_size",
    "section": "",
    "text": "utils.stats.get_sample_size(alpha, beta, sigma, delta)\nCalculate sample size n for comparing two means.\nFormula: n = 2 * sigma^2 * (z_{1-alpha/2} + z_{1-beta})^2 / delta^2 This corresponds to a two-sided test with equal variance.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nalpha\nfloat\nSignificance level (Type I error probability).\nrequired\n\n\nbeta\nfloat\nType II error probability (1 - Power).\nrequired\n\n\nsigma\nfloat\nStandard deviation of the population (assumed equal for both groups).\nrequired\n\n\ndelta\nfloat\nMinimum detectable difference (effect size to detect).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nfloat\nfloat\nThe required sample size n per group.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.utils.stats import get_sample_size\n&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; beta = 0.2  # Power = 80%\n&gt;&gt;&gt; sigma = 1.0\n&gt;&gt;&gt; delta = 1.0\n&gt;&gt;&gt; n = get_sample_size(alpha, beta, sigma, delta)\n&gt;&gt;&gt; print(f\"{n:.4f}\")\n15.6978",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_sample_size"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.get_sample_size.html#parameters",
    "href": "docs/reference/utils.stats.get_sample_size.html#parameters",
    "title": "utils.stats.get_sample_size",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nalpha\nfloat\nSignificance level (Type I error probability).\nrequired\n\n\nbeta\nfloat\nType II error probability (1 - Power).\nrequired\n\n\nsigma\nfloat\nStandard deviation of the population (assumed equal for both groups).\nrequired\n\n\ndelta\nfloat\nMinimum detectable difference (effect size to detect).\nrequired",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_sample_size"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.get_sample_size.html#returns",
    "href": "docs/reference/utils.stats.get_sample_size.html#returns",
    "title": "utils.stats.get_sample_size",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nfloat\nfloat\nThe required sample size n per group.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_sample_size"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.get_sample_size.html#examples",
    "href": "docs/reference/utils.stats.get_sample_size.html#examples",
    "title": "utils.stats.get_sample_size",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.utils.stats import get_sample_size\n&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; beta = 0.2  # Power = 80%\n&gt;&gt;&gt; sigma = 1.0\n&gt;&gt;&gt; delta = 1.0\n&gt;&gt;&gt; n = get_sample_size(alpha, beta, sigma, delta)\n&gt;&gt;&gt; print(f\"{n:.4f}\")\n15.6978",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_sample_size"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.get_all_vars_from_formula.html",
    "href": "docs/reference/utils.stats.get_all_vars_from_formula.html",
    "title": "utils.stats.get_all_vars_from_formula",
    "section": "",
    "text": "utils.stats.get_all_vars_from_formula(formula)\nUtility function to extract variables from a formula.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nformula\nstr\nA formula.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nlist\nlist\nA list of variables.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotpython.utils.stats import get_all_vars_from_formula\n    get_all_vars_from_formula(\"y ~ x1 + x2\")\n        ['y', 'x1', 'x2']\n    get_all_vars_from_formula(\"y ~ \")\n        ['y']",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_all_vars_from_formula"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.get_all_vars_from_formula.html#parameters",
    "href": "docs/reference/utils.stats.get_all_vars_from_formula.html#parameters",
    "title": "utils.stats.get_all_vars_from_formula",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nformula\nstr\nA formula.\nrequired",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_all_vars_from_formula"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.get_all_vars_from_formula.html#returns",
    "href": "docs/reference/utils.stats.get_all_vars_from_formula.html#returns",
    "title": "utils.stats.get_all_vars_from_formula",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nlist\nlist\nA list of variables.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_all_vars_from_formula"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.get_all_vars_from_formula.html#examples",
    "href": "docs/reference/utils.stats.get_all_vars_from_formula.html#examples",
    "title": "utils.stats.get_all_vars_from_formula",
    "section": "",
    "text": "&gt;&gt;&gt; from spotpython.utils.stats import get_all_vars_from_formula\n    get_all_vars_from_formula(\"y ~ x1 + x2\")\n        ['y', 'x1', 'x2']\n    get_all_vars_from_formula(\"y ~ \")\n        ['y']",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_all_vars_from_formula"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.cov_to_cor.html",
    "href": "docs/reference/utils.stats.cov_to_cor.html",
    "title": "utils.stats.cov_to_cor",
    "section": "",
    "text": "utils.stats.cov_to_cor(covariance_matrix)\nConvert a covariance matrix to a correlation matrix.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncovariance_matrix\nnumpy.ndarray\nA square matrix of covariance values.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnumpy.ndarray: A corresponding square matrix of correlation coefficients.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotpython.utils.stats import cov_to_cor\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; cov_matrix = np.array([[1, 0.8], [0.8, 1]])\n&gt;&gt;&gt; cov_to_cor(cov_matrix)\narray([[1. , 0.8],\n       [0.8, 1. ]])",
    "crumbs": [
      "API Reference",
      "Utilities",
      "cov_to_cor"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.cov_to_cor.html#parameters",
    "href": "docs/reference/utils.stats.cov_to_cor.html#parameters",
    "title": "utils.stats.cov_to_cor",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ncovariance_matrix\nnumpy.ndarray\nA square matrix of covariance values.\nrequired",
    "crumbs": [
      "API Reference",
      "Utilities",
      "cov_to_cor"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.cov_to_cor.html#returns",
    "href": "docs/reference/utils.stats.cov_to_cor.html#returns",
    "title": "utils.stats.cov_to_cor",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnumpy.ndarray: A corresponding square matrix of correlation coefficients.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "cov_to_cor"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.cov_to_cor.html#examples",
    "href": "docs/reference/utils.stats.cov_to_cor.html#examples",
    "title": "utils.stats.cov_to_cor",
    "section": "",
    "text": "&gt;&gt;&gt; from spotpython.utils.stats import cov_to_cor\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; cov_matrix = np.array([[1, 0.8], [0.8, 1]])\n&gt;&gt;&gt; cov_to_cor(cov_matrix)\narray([[1. , 0.8],\n       [0.8, 1. ]])",
    "crumbs": [
      "API Reference",
      "Utilities",
      "cov_to_cor"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.compute_standardized_betas.html",
    "href": "docs/reference/utils.stats.compute_standardized_betas.html",
    "title": "utils.stats.compute_standardized_betas",
    "section": "",
    "text": "utils.stats.compute_standardized_betas(model, X_encoded, y)\nComputes standardized (beta) coefficients for a fitted statsmodels OLS model.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodel\nstatsmodels.regression.linear_model.RegressionResultsWrapper\nThe fitted OLS model.\nrequired\n\n\nX_encoded\npandas.DataFrame\nThe design matrix of independent variables.\nrequired\n\n\ny\npandas.Series\nThe dependent variable.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npandas.DataFrame: A DataFrame containing the standardized beta coefficients.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotpython.utils.stats import compute_standardized_betas\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import statsmodels.api as sm\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; X = sm.add_constant(data)\n&gt;&gt;&gt; model = sm.OLS(y, X).fit()\n&gt;&gt;&gt; compute_standardized_betas(model, data, y)\n   Variable  Standardized Beta\n0     const           0.000000\n1       x1           0.000000\n2       x2           0.000000\n3       x3           0.000000",
    "crumbs": [
      "API Reference",
      "Utilities",
      "compute_standardized_betas"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.compute_standardized_betas.html#parameters",
    "href": "docs/reference/utils.stats.compute_standardized_betas.html#parameters",
    "title": "utils.stats.compute_standardized_betas",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmodel\nstatsmodels.regression.linear_model.RegressionResultsWrapper\nThe fitted OLS model.\nrequired\n\n\nX_encoded\npandas.DataFrame\nThe design matrix of independent variables.\nrequired\n\n\ny\npandas.Series\nThe dependent variable.\nrequired",
    "crumbs": [
      "API Reference",
      "Utilities",
      "compute_standardized_betas"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.compute_standardized_betas.html#returns",
    "href": "docs/reference/utils.stats.compute_standardized_betas.html#returns",
    "title": "utils.stats.compute_standardized_betas",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npandas.DataFrame: A DataFrame containing the standardized beta coefficients.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "compute_standardized_betas"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.compute_standardized_betas.html#examples",
    "href": "docs/reference/utils.stats.compute_standardized_betas.html#examples",
    "title": "utils.stats.compute_standardized_betas",
    "section": "",
    "text": "&gt;&gt;&gt; from spotpython.utils.stats import compute_standardized_betas\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import statsmodels.api as sm\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; X = sm.add_constant(data)\n&gt;&gt;&gt; model = sm.OLS(y, X).fit()\n&gt;&gt;&gt; compute_standardized_betas(model, data, y)\n   Variable  Standardized Beta\n0     const           0.000000\n1       x1           0.000000\n2       x2           0.000000\n3       x3           0.000000",
    "crumbs": [
      "API Reference",
      "Utilities",
      "compute_standardized_betas"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.calculate_outliers.html",
    "href": "docs/reference/utils.stats.calculate_outliers.html",
    "title": "utils.stats.calculate_outliers",
    "section": "",
    "text": "utils.stats.calculate_outliers(series_or_df, irqmultiplier=1.5)\nCalculate the number of outliers using the IQR method.\nAccepts either a pandas Series or a pandas DataFrame. For a DataFrame, counts outliers across all numeric columns and returns the total count.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nseries_or_df\nUnion[pd.Series, pd.DataFrame]\npd.Series or pd.DataFrame containing numeric data.\nrequired\n\n\nirqmultiplier\nfloat\nMultiplier for IQR to define fences. Defaults to 1.5.\n1.5\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nint\nint\nThe number of outliers.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotoptim.utils.stats import calculate_outliers\n&gt;&gt;&gt; s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100])\n&gt;&gt;&gt; calculate_outliers(s)\n1\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'a': [1, 2, 3, 100],\n...     'b': [10, 12, 11, 10]\n... })\n&gt;&gt;&gt; calculate_outliers(df)\n1",
    "crumbs": [
      "API Reference",
      "Utilities",
      "calculate_outliers"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.calculate_outliers.html#parameters",
    "href": "docs/reference/utils.stats.calculate_outliers.html#parameters",
    "title": "utils.stats.calculate_outliers",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nseries_or_df\nUnion[pd.Series, pd.DataFrame]\npd.Series or pd.DataFrame containing numeric data.\nrequired\n\n\nirqmultiplier\nfloat\nMultiplier for IQR to define fences. Defaults to 1.5.\n1.5",
    "crumbs": [
      "API Reference",
      "Utilities",
      "calculate_outliers"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.calculate_outliers.html#returns",
    "href": "docs/reference/utils.stats.calculate_outliers.html#returns",
    "title": "utils.stats.calculate_outliers",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nint\nint\nThe number of outliers.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "calculate_outliers"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.calculate_outliers.html#examples",
    "href": "docs/reference/utils.stats.calculate_outliers.html#examples",
    "title": "utils.stats.calculate_outliers",
    "section": "",
    "text": "&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotoptim.utils.stats import calculate_outliers\n&gt;&gt;&gt; s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100])\n&gt;&gt;&gt; calculate_outliers(s)\n1\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'a': [1, 2, 3, 100],\n...     'b': [10, 12, 11, 10]\n... })\n&gt;&gt;&gt; calculate_outliers(df)\n1",
    "crumbs": [
      "API Reference",
      "Utilities",
      "calculate_outliers"
    ]
  },
  {
    "objectID": "docs/reference/utils.scaler.TorchStandardScaler.html",
    "href": "docs/reference/utils.scaler.TorchStandardScaler.html",
    "title": "utils.scaler.TorchStandardScaler",
    "section": "",
    "text": "utils.scaler.TorchStandardScaler()\nA class for scaling data using standardization with torch tensors. This scaler computes the mean and standard deviation on a dataset so that it can later be used to scale the data using the computed mean and standard deviation.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nmean\ntorch.Tensor\nThe mean value computed over the fitted data.\n\n\nstd\ntorch.Tensor\nThe standard deviation computed over the fitted data.\n\n\n\n\n\n\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.utils.scaler import TorchStandardScaler\n&gt;&gt;&gt; # Create a sample tensor\n&gt;&gt;&gt; tensor = torch.rand((10, 3))  # Random tensor with shape (10, 3)\n&gt;&gt;&gt; scaler = TorchStandardScaler()\n&gt;&gt;&gt; # Fit the scaler to the data\n&gt;&gt;&gt; scaler.fit(tensor)\n&gt;&gt;&gt; # Transform the data using the fitted scaler\n&gt;&gt;&gt; transformed_tensor = scaler.transform(tensor)\n&gt;&gt;&gt; print(transformed_tensor.shape)\ntorch.Size([10, 3])\n&gt;&gt;&gt; # Using fit_transform method to fit and transform in one step\n&gt;&gt;&gt; another_tensor = torch.rand((10, 3))\n&gt;&gt;&gt; scaled_tensor = scaler.fit_transform(another_tensor)\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfit\nCompute the mean and standard deviation of the input tensor.\n\n\nfit_transform\nFit the scaler to the input tensor and then scale the tensor.\n\n\ntransform\nScale the input tensor using the computed mean and standard deviation.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "TorchStandardScaler"
    ]
  },
  {
    "objectID": "docs/reference/utils.scaler.TorchStandardScaler.html#attributes",
    "href": "docs/reference/utils.scaler.TorchStandardScaler.html#attributes",
    "title": "utils.scaler.TorchStandardScaler",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nmean\ntorch.Tensor\nThe mean value computed over the fitted data.\n\n\nstd\ntorch.Tensor\nThe standard deviation computed over the fitted data.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "TorchStandardScaler"
    ]
  },
  {
    "objectID": "docs/reference/utils.scaler.TorchStandardScaler.html#examples",
    "href": "docs/reference/utils.scaler.TorchStandardScaler.html#examples",
    "title": "utils.scaler.TorchStandardScaler",
    "section": "",
    "text": "&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.utils.scaler import TorchStandardScaler\n&gt;&gt;&gt; # Create a sample tensor\n&gt;&gt;&gt; tensor = torch.rand((10, 3))  # Random tensor with shape (10, 3)\n&gt;&gt;&gt; scaler = TorchStandardScaler()\n&gt;&gt;&gt; # Fit the scaler to the data\n&gt;&gt;&gt; scaler.fit(tensor)\n&gt;&gt;&gt; # Transform the data using the fitted scaler\n&gt;&gt;&gt; transformed_tensor = scaler.transform(tensor)\n&gt;&gt;&gt; print(transformed_tensor.shape)\ntorch.Size([10, 3])\n&gt;&gt;&gt; # Using fit_transform method to fit and transform in one step\n&gt;&gt;&gt; another_tensor = torch.rand((10, 3))\n&gt;&gt;&gt; scaled_tensor = scaler.fit_transform(another_tensor)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "TorchStandardScaler"
    ]
  },
  {
    "objectID": "docs/reference/utils.scaler.TorchStandardScaler.html#methods",
    "href": "docs/reference/utils.scaler.TorchStandardScaler.html#methods",
    "title": "utils.scaler.TorchStandardScaler",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfit\nCompute the mean and standard deviation of the input tensor.\n\n\nfit_transform\nFit the scaler to the input tensor and then scale the tensor.\n\n\ntransform\nScale the input tensor using the computed mean and standard deviation.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "TorchStandardScaler"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.html",
    "href": "docs/reference/utils.pca.html",
    "title": "utils.pca",
    "section": "",
    "text": "utils.pca\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_loading_scores\nComputes the loading scores matrix for Principal Component Analysis (PCA).\n\n\nget_pca\nScale the numeric data and perform PCA.\n\n\nget_pca_topk\nIdentify the top k features that have the strongest influence on PC1 and PC2.\n\n\nplot_loading_scores\nCreates a heatmap visualization of PCA loading scores.\n\n\nplot_pca1vs2\nCreate a scatter plot of the first two principal components from PCA.\n\n\nplot_pca_scree\nPlot the scree plot for Principal Component Analysis (PCA).",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.pca"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.html#functions",
    "href": "docs/reference/utils.pca.html#functions",
    "title": "utils.pca",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_loading_scores\nComputes the loading scores matrix for Principal Component Analysis (PCA).\n\n\nget_pca\nScale the numeric data and perform PCA.\n\n\nget_pca_topk\nIdentify the top k features that have the strongest influence on PC1 and PC2.\n\n\nplot_loading_scores\nCreates a heatmap visualization of PCA loading scores.\n\n\nplot_pca1vs2\nCreate a scatter plot of the first two principal components from PCA.\n\n\nplot_pca_scree\nPlot the scree plot for Principal Component Analysis (PCA).",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.pca"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.plot_pca1vs2.html",
    "href": "docs/reference/utils.pca.plot_pca1vs2.html",
    "title": "utils.pca.plot_pca1vs2",
    "section": "",
    "text": "utils.pca.plot_pca1vs2(pca, pca_data, df_name='', figsize=(12, 6))\nCreate a scatter plot of the first two principal components from PCA.\nThis function visualizes the first two principal components (PC1 vs PC2) from a PCA analysis, creating a scatter plot where each point represents a sample in the transformed space. The percentage of variance explained by each component is shown on the axes.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npca\nsklearn.decomposition.PCA\nFitted PCA object containing the explained variance ratios and components.\nrequired\n\n\npca_data\narray - like\nPCA-transformed data, where each row represents a sample and each column represents a principal component.\nrequired\n\n\ndf_name\nstr\nName of the dataset to be displayed in the plot title. Defaults to empty string.\n''\n\n\nfigsize\ntuple\nSize of the figure as (width, height). Defaults to (12, 6).\n(12, 6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nNone\nNone\nThe function creates and displays a matplotlib plot.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.decomposition import PCA\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from spotpython.utils.pca import plot_pca1vs2\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare the iris dataset\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X = iris.data\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit PCA and transform the data\n&gt;&gt;&gt; pca = PCA()\n&gt;&gt;&gt; pca_data = pca.fit_transform(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create PCA scatter plot\n&gt;&gt;&gt; plot_pca1vs2(pca,\n...             pca_data,\n...             df_name=\"Iris Dataset\",\n...             figsize=(10, 5))\n\n\n\n\nThe function assumes that the input data has at least two principal components\nSample names are taken from the index of the created DataFrame\nThe percentage of variance explained is rounded to 1 decimal place",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_pca1vs2"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.plot_pca1vs2.html#parameters",
    "href": "docs/reference/utils.pca.plot_pca1vs2.html#parameters",
    "title": "utils.pca.plot_pca1vs2",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npca\nsklearn.decomposition.PCA\nFitted PCA object containing the explained variance ratios and components.\nrequired\n\n\npca_data\narray - like\nPCA-transformed data, where each row represents a sample and each column represents a principal component.\nrequired\n\n\ndf_name\nstr\nName of the dataset to be displayed in the plot title. Defaults to empty string.\n''\n\n\nfigsize\ntuple\nSize of the figure as (width, height). Defaults to (12, 6).\n(12, 6)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_pca1vs2"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.plot_pca1vs2.html#returns",
    "href": "docs/reference/utils.pca.plot_pca1vs2.html#returns",
    "title": "utils.pca.plot_pca1vs2",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nNone\nNone\nThe function creates and displays a matplotlib plot.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_pca1vs2"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.plot_pca1vs2.html#examples",
    "href": "docs/reference/utils.pca.plot_pca1vs2.html#examples",
    "title": "utils.pca.plot_pca1vs2",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.decomposition import PCA\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from spotpython.utils.pca import plot_pca1vs2\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare the iris dataset\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X = iris.data\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit PCA and transform the data\n&gt;&gt;&gt; pca = PCA()\n&gt;&gt;&gt; pca_data = pca.fit_transform(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create PCA scatter plot\n&gt;&gt;&gt; plot_pca1vs2(pca,\n...             pca_data,\n...             df_name=\"Iris Dataset\",\n...             figsize=(10, 5))",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_pca1vs2"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.plot_pca1vs2.html#note",
    "href": "docs/reference/utils.pca.plot_pca1vs2.html#note",
    "title": "utils.pca.plot_pca1vs2",
    "section": "",
    "text": "The function assumes that the input data has at least two principal components\nSample names are taken from the index of the created DataFrame\nThe percentage of variance explained is rounded to 1 decimal place",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_pca1vs2"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.get_pca_topk.html",
    "href": "docs/reference/utils.pca.get_pca_topk.html",
    "title": "utils.pca.get_pca_topk",
    "section": "",
    "text": "utils.pca.get_pca_topk(pca, feature_names, k=10)\nIdentify the top k features that have the strongest influence on PC1 and PC2.\nThis function analyzes the loading scores (coefficients) of the first two principal components to determine which original features contribute most strongly to these components. The absolute values of the loading scores are used to rank feature importance.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npca\nsklearn.decomposition.PCA\nFitted PCA object containing the components_ attribute with the principal components.\nrequired\n\n\nfeature_names\nlist - like\nNames of the original features, must match the order of features used in PCA fitting.\nrequired\n\n\nk\nint\nNumber of top features to select for each principal component. Defaults to 10.\n10\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\ntuple\nA tuple containing two lists: - list[str]: Names of the k features most influential on PC1 - list[str]: Names of the k features most influential on PC2\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.decomposition import PCA\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from spotpython.utils.pca import get_pca_topk\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare the iris dataset\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X = iris.data\n&gt;&gt;&gt; feature_names = iris.feature_names\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit PCA\n&gt;&gt;&gt; pca = PCA()\n&gt;&gt;&gt; pca.fit(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get top 2 most influential features for PC1 and PC2\n&gt;&gt;&gt; top_pc1, top_pc2 = get_pca_topk(pca,\n...                                 feature_names=feature_names,\n...                                 k=2)\n&gt;&gt;&gt; print(\"Top PC1 features:\", top_pc1)\n&gt;&gt;&gt; print(\"Top PC2 features:\", top_pc2)\n\n\n\n\nThe function assumes that PCA has been fitted on standardized data\nThe length of feature_names must match the number of features in the PCA input\nk should not exceed the total number of features",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_pca_topk"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.get_pca_topk.html#parameters",
    "href": "docs/reference/utils.pca.get_pca_topk.html#parameters",
    "title": "utils.pca.get_pca_topk",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npca\nsklearn.decomposition.PCA\nFitted PCA object containing the components_ attribute with the principal components.\nrequired\n\n\nfeature_names\nlist - like\nNames of the original features, must match the order of features used in PCA fitting.\nrequired\n\n\nk\nint\nNumber of top features to select for each principal component. Defaults to 10.\n10",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_pca_topk"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.get_pca_topk.html#returns",
    "href": "docs/reference/utils.pca.get_pca_topk.html#returns",
    "title": "utils.pca.get_pca_topk",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ntuple\ntuple\nA tuple containing two lists: - list[str]: Names of the k features most influential on PC1 - list[str]: Names of the k features most influential on PC2",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_pca_topk"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.get_pca_topk.html#examples",
    "href": "docs/reference/utils.pca.get_pca_topk.html#examples",
    "title": "utils.pca.get_pca_topk",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.decomposition import PCA\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from spotpython.utils.pca import get_pca_topk\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare the iris dataset\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X = iris.data\n&gt;&gt;&gt; feature_names = iris.feature_names\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit PCA\n&gt;&gt;&gt; pca = PCA()\n&gt;&gt;&gt; pca.fit(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get top 2 most influential features for PC1 and PC2\n&gt;&gt;&gt; top_pc1, top_pc2 = get_pca_topk(pca,\n...                                 feature_names=feature_names,\n...                                 k=2)\n&gt;&gt;&gt; print(\"Top PC1 features:\", top_pc1)\n&gt;&gt;&gt; print(\"Top PC2 features:\", top_pc2)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_pca_topk"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.get_pca_topk.html#note",
    "href": "docs/reference/utils.pca.get_pca_topk.html#note",
    "title": "utils.pca.get_pca_topk",
    "section": "",
    "text": "The function assumes that PCA has been fitted on standardized data\nThe length of feature_names must match the number of features in the PCA input\nk should not exceed the total number of features",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_pca_topk"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.get_loading_scores.html",
    "href": "docs/reference/utils.pca.get_loading_scores.html",
    "title": "utils.pca.get_loading_scores",
    "section": "",
    "text": "utils.pca.get_loading_scores(pca, feature_names)\nComputes the loading scores matrix for Principal Component Analysis (PCA).\nCreates and returns a DataFrame showing how each original feature contributes to each principal component.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npca\nsklearn.decomposition.PCA\nFitted PCA object containing the components_ attribute with the principal components.\nrequired\n\n\nfeature_names\nlist - like\nNames of the original features, must match the order of features used in PCA fitting.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: DataFrame containing the loading scores matrix with features as rows and principal components as columns.\n\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA from sklearn.datasets import load_iris from spotpython.utils.pca import print_loading_scores,\n\niris = load_iris() X = iris.data feature_names = iris.feature_names\n\npca = PCA() pca.fit(X)\n\nscores_df = print_loading_scores(pca, feature_names) print(scores_df)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_loading_scores"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.get_loading_scores.html#parameters",
    "href": "docs/reference/utils.pca.get_loading_scores.html#parameters",
    "title": "utils.pca.get_loading_scores",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npca\nsklearn.decomposition.PCA\nFitted PCA object containing the components_ attribute with the principal components.\nrequired\n\n\nfeature_names\nlist - like\nNames of the original features, must match the order of features used in PCA fitting.\nrequired",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_loading_scores"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.get_loading_scores.html#returns",
    "href": "docs/reference/utils.pca.get_loading_scores.html#returns",
    "title": "utils.pca.get_loading_scores",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: DataFrame containing the loading scores matrix with features as rows and principal components as columns.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_loading_scores"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.get_loading_scores.html#example",
    "href": "docs/reference/utils.pca.get_loading_scores.html#example",
    "title": "utils.pca.get_loading_scores",
    "section": "",
    "text": "from sklearn.decomposition import PCA from sklearn.datasets import load_iris from spotpython.utils.pca import print_loading_scores,\n\niris = load_iris() X = iris.data feature_names = iris.feature_names\n\npca = PCA() pca.fit(X)\n\nscores_df = print_loading_scores(pca, feature_names) print(scores_df)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_loading_scores"
    ]
  },
  {
    "objectID": "docs/reference/utils.mapping.map_lr.html",
    "href": "docs/reference/utils.mapping.map_lr.html",
    "title": "utils.mapping.map_lr",
    "section": "",
    "text": "utils.mapping.map_lr(lr_unified, optimizer_name, use_default_scale=True)\nMap a unified learning rate to an optimizer-specific learning rate.\nThis function provides a unified interface for learning rates across different PyTorch optimizers. Different optimizers operate on vastly different learning rate scales (e.g., SGD typically uses lr ~ 0.01-0.1, while Adam uses lr ~ 0.0001-0.001).\nThe mapping uses the default learning rates from PyTorch as scaling factors, allowing users to work with a normalized learning rate scale where 1.0 represents the optimizer’s default learning rate.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlr_unified\nfloat\nUnified learning rate multiplier. A value of 1.0 corresponds to the optimizer’s default learning rate. Values &lt; 1.0 reduce the learning rate, values &gt; 1.0 increase it. Typical range: [0.001, 100.0].\nrequired\n\n\noptimizer_name\nstr\nName of the PyTorch optimizer. Must be one of: “Adadelta”, “Adagrad”, “Adam”, “AdamW”, “SparseAdam”, “Adamax”, “ASGD”, “LBFGS”, “NAdam”, “RAdam”, “RMSprop”, “Rprop”, “SGD”.\nrequired\n\n\nuse_default_scale\nbool\nWhether to scale by the optimizer’s default learning rate. If True (default), lr_unified is multiplied by the default lr. If False, returns lr_unified directly. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nfloat\nfloat\nThe optimizer-specific learning rate.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf optimizer_name is not supported.\n\n\n\nValueError\nIf lr_unified is not positive.\n\n\n\n\n\n\nUsing unified learning rate with default scaling:\n&gt;&gt;&gt; # Get Adam's default learning rate (0.001)\n&gt;&gt;&gt; lr = map_lr(1.0, \"Adam\")\n&gt;&gt;&gt; print(lr)\n0.001\n&gt;&gt;&gt; # Get half of SGD's default learning rate (0.01 / 2 = 0.005)\n&gt;&gt;&gt; lr = map_lr(0.5, \"SGD\")\n&gt;&gt;&gt; print(lr)\n0.005\n&gt;&gt;&gt; # Get 10x RMSprop's default learning rate (0.01 * 10 = 0.1)\n&gt;&gt;&gt; lr = map_lr(10.0, \"RMSprop\")\n&gt;&gt;&gt; print(lr)\n0.1\nUsing unified learning rate without scaling:\n&gt;&gt;&gt; # Use lr_unified directly (0.01)\n&gt;&gt;&gt; lr = map_lr(0.01, \"Adam\", use_default_scale=False)\n&gt;&gt;&gt; print(lr)\n0.01\nPractical example with model training:\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; from spotoptim.nn.linear_regressor import LinearRegressor\n&gt;&gt;&gt; from spotoptim.utils.mapping import map_lr\n&gt;&gt;&gt;\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use unified learning rate of 0.5 for Adam (gives 0.0005)\n&gt;&gt;&gt; lr_adam = map_lr(0.5, \"Adam\")\n&gt;&gt;&gt; optimizer_adam = model.get_optimizer(\"Adam\", lr=lr_adam)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use same unified learning rate for SGD (gives 0.005)\n&gt;&gt;&gt; lr_sgd = map_lr(0.5, \"SGD\")\n&gt;&gt;&gt; optimizer_sgd = model.get_optimizer(\"SGD\", lr=lr_sgd)\nHyperparameter optimization example:\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; def train_model(X):\n...     results = []\n...     for params in X:\n...         lr_unified = 10 ** params[0]  # Log scale: [-4, 0]\n...         optimizer_name = params[1]     # Factor variable\n...\n...         # Map to optimizer-specific learning rate\n...         lr_actual = map_lr(lr_unified, optimizer_name)\n...\n...         # Train model with this configuration\n...         # ... training code ...\n...         results.append(test_loss)\n...     return np.array(results)\n&gt;&gt;&gt;\n&gt;&gt;&gt; optimizer = SpotOptim(\n...     fun=train_model,\n...     bounds=[(-4, 0), (\"Adam\", \"SGD\", \"RMSprop\")],\n...     var_type=[\"num\", \"factor\"],\n...     max_iter=30\n... )\n\n\n\n\nThe unified learning rate provides a normalized scale across optimizers\nA value of 1.0 always corresponds to the optimizer’s PyTorch default\nThis enables fair comparison when optimizing over different optimizers\nFor log-scale optimization, use lr_unified = 10^x where x ∈ [-4, 2]\nDefault scaling is recommended for most use cases\n\n\n\n\n\nPyTorch Optimizer Documentation: https://pytorch.org/docs/stable/optim.html\nLinearRegressor.get_optimizer(): Convenience method using this mapping",
    "crumbs": [
      "API Reference",
      "Utilities",
      "map_lr"
    ]
  },
  {
    "objectID": "docs/reference/utils.mapping.map_lr.html#parameters",
    "href": "docs/reference/utils.mapping.map_lr.html#parameters",
    "title": "utils.mapping.map_lr",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nlr_unified\nfloat\nUnified learning rate multiplier. A value of 1.0 corresponds to the optimizer’s default learning rate. Values &lt; 1.0 reduce the learning rate, values &gt; 1.0 increase it. Typical range: [0.001, 100.0].\nrequired\n\n\noptimizer_name\nstr\nName of the PyTorch optimizer. Must be one of: “Adadelta”, “Adagrad”, “Adam”, “AdamW”, “SparseAdam”, “Adamax”, “ASGD”, “LBFGS”, “NAdam”, “RAdam”, “RMSprop”, “Rprop”, “SGD”.\nrequired\n\n\nuse_default_scale\nbool\nWhether to scale by the optimizer’s default learning rate. If True (default), lr_unified is multiplied by the default lr. If False, returns lr_unified directly. Defaults to True.\nTrue",
    "crumbs": [
      "API Reference",
      "Utilities",
      "map_lr"
    ]
  },
  {
    "objectID": "docs/reference/utils.mapping.map_lr.html#returns",
    "href": "docs/reference/utils.mapping.map_lr.html#returns",
    "title": "utils.mapping.map_lr",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nfloat\nfloat\nThe optimizer-specific learning rate.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "map_lr"
    ]
  },
  {
    "objectID": "docs/reference/utils.mapping.map_lr.html#raises",
    "href": "docs/reference/utils.mapping.map_lr.html#raises",
    "title": "utils.mapping.map_lr",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf optimizer_name is not supported.\n\n\n\nValueError\nIf lr_unified is not positive.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "map_lr"
    ]
  },
  {
    "objectID": "docs/reference/utils.mapping.map_lr.html#examples",
    "href": "docs/reference/utils.mapping.map_lr.html#examples",
    "title": "utils.mapping.map_lr",
    "section": "",
    "text": "Using unified learning rate with default scaling:\n&gt;&gt;&gt; # Get Adam's default learning rate (0.001)\n&gt;&gt;&gt; lr = map_lr(1.0, \"Adam\")\n&gt;&gt;&gt; print(lr)\n0.001\n&gt;&gt;&gt; # Get half of SGD's default learning rate (0.01 / 2 = 0.005)\n&gt;&gt;&gt; lr = map_lr(0.5, \"SGD\")\n&gt;&gt;&gt; print(lr)\n0.005\n&gt;&gt;&gt; # Get 10x RMSprop's default learning rate (0.01 * 10 = 0.1)\n&gt;&gt;&gt; lr = map_lr(10.0, \"RMSprop\")\n&gt;&gt;&gt; print(lr)\n0.1\nUsing unified learning rate without scaling:\n&gt;&gt;&gt; # Use lr_unified directly (0.01)\n&gt;&gt;&gt; lr = map_lr(0.01, \"Adam\", use_default_scale=False)\n&gt;&gt;&gt; print(lr)\n0.01\nPractical example with model training:\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; from spotoptim.nn.linear_regressor import LinearRegressor\n&gt;&gt;&gt; from spotoptim.utils.mapping import map_lr\n&gt;&gt;&gt;\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use unified learning rate of 0.5 for Adam (gives 0.0005)\n&gt;&gt;&gt; lr_adam = map_lr(0.5, \"Adam\")\n&gt;&gt;&gt; optimizer_adam = model.get_optimizer(\"Adam\", lr=lr_adam)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use same unified learning rate for SGD (gives 0.005)\n&gt;&gt;&gt; lr_sgd = map_lr(0.5, \"SGD\")\n&gt;&gt;&gt; optimizer_sgd = model.get_optimizer(\"SGD\", lr=lr_sgd)\nHyperparameter optimization example:\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; def train_model(X):\n...     results = []\n...     for params in X:\n...         lr_unified = 10 ** params[0]  # Log scale: [-4, 0]\n...         optimizer_name = params[1]     # Factor variable\n...\n...         # Map to optimizer-specific learning rate\n...         lr_actual = map_lr(lr_unified, optimizer_name)\n...\n...         # Train model with this configuration\n...         # ... training code ...\n...         results.append(test_loss)\n...     return np.array(results)\n&gt;&gt;&gt;\n&gt;&gt;&gt; optimizer = SpotOptim(\n...     fun=train_model,\n...     bounds=[(-4, 0), (\"Adam\", \"SGD\", \"RMSprop\")],\n...     var_type=[\"num\", \"factor\"],\n...     max_iter=30\n... )",
    "crumbs": [
      "API Reference",
      "Utilities",
      "map_lr"
    ]
  },
  {
    "objectID": "docs/reference/utils.mapping.map_lr.html#note",
    "href": "docs/reference/utils.mapping.map_lr.html#note",
    "title": "utils.mapping.map_lr",
    "section": "",
    "text": "The unified learning rate provides a normalized scale across optimizers\nA value of 1.0 always corresponds to the optimizer’s PyTorch default\nThis enables fair comparison when optimizing over different optimizers\nFor log-scale optimization, use lr_unified = 10^x where x ∈ [-4, 2]\nDefault scaling is recommended for most use cases",
    "crumbs": [
      "API Reference",
      "Utilities",
      "map_lr"
    ]
  },
  {
    "objectID": "docs/reference/utils.mapping.map_lr.html#see-also",
    "href": "docs/reference/utils.mapping.map_lr.html#see-also",
    "title": "utils.mapping.map_lr",
    "section": "",
    "text": "PyTorch Optimizer Documentation: https://pytorch.org/docs/stable/optim.html\nLinearRegressor.get_optimizer(): Convenience method using this mapping",
    "crumbs": [
      "API Reference",
      "Utilities",
      "map_lr"
    ]
  },
  {
    "objectID": "docs/reference/utils.file.get_experiment_filename.html",
    "href": "docs/reference/utils.file.get_experiment_filename.html",
    "title": "utils.file.get_experiment_filename",
    "section": "",
    "text": "utils.file.get_experiment_filename(\n    PREFIX=None,\n    fun_name=None,\n    dim=None,\n    fun_evals=None,\n    path='experiments',\n    extension='pkl',\n)\nGenerates a standardized filename for experiments.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nPREFIX\nstr\nPrefix/identifier for the experiment\nNone\n\n\nfun_name\nstr\nName of the objective function\nNone\n\n\ndim\nint\nDimensionality of the problem\nNone\n\n\nfun_evals\nint\nNumber of function evaluations\nNone\n\n\npath\nstr\nDirectory path to save the file\n'experiments'\n\n\nextension\nstr\nFile extension (default: “pkl”)\n'pkl'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nstr\nstr\nAbsolute path to the experiment file",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_experiment_filename"
    ]
  },
  {
    "objectID": "docs/reference/utils.file.get_experiment_filename.html#parameters",
    "href": "docs/reference/utils.file.get_experiment_filename.html#parameters",
    "title": "utils.file.get_experiment_filename",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nPREFIX\nstr\nPrefix/identifier for the experiment\nNone\n\n\nfun_name\nstr\nName of the objective function\nNone\n\n\ndim\nint\nDimensionality of the problem\nNone\n\n\nfun_evals\nint\nNumber of function evaluations\nNone\n\n\npath\nstr\nDirectory path to save the file\n'experiments'\n\n\nextension\nstr\nFile extension (default: “pkl”)\n'pkl'",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_experiment_filename"
    ]
  },
  {
    "objectID": "docs/reference/utils.file.get_experiment_filename.html#returns",
    "href": "docs/reference/utils.file.get_experiment_filename.html#returns",
    "title": "utils.file.get_experiment_filename",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nstr\nstr\nAbsolute path to the experiment file",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_experiment_filename"
    ]
  },
  {
    "objectID": "docs/reference/utils.eval.mo_eval_models.html",
    "href": "docs/reference/utils.eval.mo_eval_models.html",
    "title": "utils.eval.mo_eval_models",
    "section": "",
    "text": "utils.eval.mo_eval_models(\n    X_train,\n    y_train,\n    X_test,\n    y_test,\n    model_define_func,\n    scores=None,\n    verbose=False,\n)\nTrains and evaluates separate models for each target in a multi-output regression problem.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX_train\npd.DataFrame or np.ndarray\nTraining feature matrix.\nrequired\n\n\ny_train\npd.DataFrame or np.ndarray\nTraining target matrix with multiple columns (one per target).\nrequired\n\n\nX_test\npd.DataFrame or np.ndarray\nTest feature matrix.\nrequired\n\n\ny_test\npd.DataFrame or np.ndarray\nTest target matrix with multiple columns (one per target).\nrequired\n\n\nmodel_define_func\nCallable\nFunction that returns a fresh model or pipeline instance for training.\nrequired\n\n\nscores\nUnion[Dict[str, Callable], Callable, None]\nScoring metric(s) to evaluate. - If None (default): Uses R2 score. Returns List[float]. - If Callable: Single scoring function (e.g., mean_squared_error). Returns List[float]. - If Dict: Dictionary of {name: scoring_func}. Returns Dict[str, List[float]].\nNone\n\n\nverbose\nbool\nWhether to print verbose output.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nscores\nUnion[List[float], Dict[str, List[float]]]\n- List of scores if scores is None or a single callable. - Dictionary of scores if scores is a dictionary.\n\n\nmodels\nList\nList of trained model instances, one per target.\n\n\npreds_stacked\nnp.ndarray\nArray of stacked predictions for all targets, shape (n_samples, n_targets).\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.utils.eval import mo_eval_models\n&gt;&gt;&gt; # Generate dummy data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; X_train = pd.DataFrame(np.random.rand(100, 5), columns=[f'x{i}' for i in range(5)])\n&gt;&gt;&gt; y_train = pd.DataFrame(np.random.rand(100, 3), columns=[f'y{i}' for i in range(3)])\n&gt;&gt;&gt; X_test = pd.DataFrame(np.random.rand(20, 5), columns=[f'x{i}' for i in range(5)])\n&gt;&gt;&gt; y_test = pd.DataFrame(np.random.rand(20, 3), columns=[f'y{i}' for i in range(3)])\n&gt;&gt;&gt; # Example 1: Default behavior (R2 score)\n&gt;&gt;&gt; def make_model():\n...     from sklearn.linear_model import Ridge\n...     return Ridge()\n&gt;&gt;&gt; r2_scores, models, preds = mo_eval_models(X_train, y_train, X_test, y_test, make_model)\nTraining model for target 1/3...\nTraining model for target 2/3...\nTraining model for target 3/3...\nModel scores: ['-0.10', '-0.13', '-0.19']\nPredictions shape: (20, 3)\n&gt;&gt;&gt; # Example 2: Custom single score (MSE)\n&gt;&gt;&gt; from sklearn.metrics import mean_squared_error\n&gt;&gt;&gt; mse_scores, _, _ = mo_eval_models(X_train, y_train, X_test, y_test, make_model, scores=mean_squared_error)\nTraining model for target 1/3...\nTraining model for target 2/3...\nTraining model for target 3/3...\nModel scores: ['0.07', '0.09', '0.10']\nPredictions shape: (20, 3)\n&gt;&gt;&gt; # Example 3: Multiple custom scores\n&gt;&gt;&gt; from sklearn.metrics import mean_absolute_error, r2_score\n&gt;&gt;&gt; my_scores = {'R2': r2_score, 'MSE': mean_squared_error, 'MAE': mean_absolute_error}\n&gt;&gt;&gt; all_scores, _, _ = mo_eval_models(X_train, y_train, X_test, y_test, make_model, scores=my_scores)\nTraining model for target 1/3...\nTraining model for target 2/3...\nTraining model for target 3/3...\nModel scores:\n  R2: ['-0.10', '-0.13', '-0.19']\n  MSE: ['0.07', '0.09', '0.10']\n  MAE: ['0.21', '0.27', '0.28']\nPredictions shape: (20, 3)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "mo_eval_models"
    ]
  },
  {
    "objectID": "docs/reference/utils.eval.mo_eval_models.html#parameters",
    "href": "docs/reference/utils.eval.mo_eval_models.html#parameters",
    "title": "utils.eval.mo_eval_models",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX_train\npd.DataFrame or np.ndarray\nTraining feature matrix.\nrequired\n\n\ny_train\npd.DataFrame or np.ndarray\nTraining target matrix with multiple columns (one per target).\nrequired\n\n\nX_test\npd.DataFrame or np.ndarray\nTest feature matrix.\nrequired\n\n\ny_test\npd.DataFrame or np.ndarray\nTest target matrix with multiple columns (one per target).\nrequired\n\n\nmodel_define_func\nCallable\nFunction that returns a fresh model or pipeline instance for training.\nrequired\n\n\nscores\nUnion[Dict[str, Callable], Callable, None]\nScoring metric(s) to evaluate. - If None (default): Uses R2 score. Returns List[float]. - If Callable: Single scoring function (e.g., mean_squared_error). Returns List[float]. - If Dict: Dictionary of {name: scoring_func}. Returns Dict[str, List[float]].\nNone\n\n\nverbose\nbool\nWhether to print verbose output.\nFalse",
    "crumbs": [
      "API Reference",
      "Utilities",
      "mo_eval_models"
    ]
  },
  {
    "objectID": "docs/reference/utils.eval.mo_eval_models.html#returns",
    "href": "docs/reference/utils.eval.mo_eval_models.html#returns",
    "title": "utils.eval.mo_eval_models",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nscores\nUnion[List[float], Dict[str, List[float]]]\n- List of scores if scores is None or a single callable. - Dictionary of scores if scores is a dictionary.\n\n\nmodels\nList\nList of trained model instances, one per target.\n\n\npreds_stacked\nnp.ndarray\nArray of stacked predictions for all targets, shape (n_samples, n_targets).",
    "crumbs": [
      "API Reference",
      "Utilities",
      "mo_eval_models"
    ]
  },
  {
    "objectID": "docs/reference/utils.eval.mo_eval_models.html#examples",
    "href": "docs/reference/utils.eval.mo_eval_models.html#examples",
    "title": "utils.eval.mo_eval_models",
    "section": "",
    "text": "&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.utils.eval import mo_eval_models\n&gt;&gt;&gt; # Generate dummy data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; X_train = pd.DataFrame(np.random.rand(100, 5), columns=[f'x{i}' for i in range(5)])\n&gt;&gt;&gt; y_train = pd.DataFrame(np.random.rand(100, 3), columns=[f'y{i}' for i in range(3)])\n&gt;&gt;&gt; X_test = pd.DataFrame(np.random.rand(20, 5), columns=[f'x{i}' for i in range(5)])\n&gt;&gt;&gt; y_test = pd.DataFrame(np.random.rand(20, 3), columns=[f'y{i}' for i in range(3)])\n&gt;&gt;&gt; # Example 1: Default behavior (R2 score)\n&gt;&gt;&gt; def make_model():\n...     from sklearn.linear_model import Ridge\n...     return Ridge()\n&gt;&gt;&gt; r2_scores, models, preds = mo_eval_models(X_train, y_train, X_test, y_test, make_model)\nTraining model for target 1/3...\nTraining model for target 2/3...\nTraining model for target 3/3...\nModel scores: ['-0.10', '-0.13', '-0.19']\nPredictions shape: (20, 3)\n&gt;&gt;&gt; # Example 2: Custom single score (MSE)\n&gt;&gt;&gt; from sklearn.metrics import mean_squared_error\n&gt;&gt;&gt; mse_scores, _, _ = mo_eval_models(X_train, y_train, X_test, y_test, make_model, scores=mean_squared_error)\nTraining model for target 1/3...\nTraining model for target 2/3...\nTraining model for target 3/3...\nModel scores: ['0.07', '0.09', '0.10']\nPredictions shape: (20, 3)\n&gt;&gt;&gt; # Example 3: Multiple custom scores\n&gt;&gt;&gt; from sklearn.metrics import mean_absolute_error, r2_score\n&gt;&gt;&gt; my_scores = {'R2': r2_score, 'MSE': mean_squared_error, 'MAE': mean_absolute_error}\n&gt;&gt;&gt; all_scores, _, _ = mo_eval_models(X_train, y_train, X_test, y_test, make_model, scores=my_scores)\nTraining model for target 1/3...\nTraining model for target 2/3...\nTraining model for target 3/3...\nModel scores:\n  R2: ['-0.10', '-0.13', '-0.19']\n  MSE: ['0.07', '0.09', '0.10']\n  MAE: ['0.21', '0.27', '0.28']\nPredictions shape: (20, 3)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "mo_eval_models"
    ]
  },
  {
    "objectID": "docs/reference/utils.boundaries.html",
    "href": "docs/reference/utils.boundaries.html",
    "title": "utils.boundaries",
    "section": "",
    "text": "utils.boundaries\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_boundaries\nCalculates the minimum and maximum values for each column in a NumPy array.\n\n\nmap_to_original_scale\nMaps the values in X_search from the range [0, 1] to the original scale defined by x_min and x_max.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.boundaries"
    ]
  },
  {
    "objectID": "docs/reference/utils.boundaries.html#functions",
    "href": "docs/reference/utils.boundaries.html#functions",
    "title": "utils.boundaries",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_boundaries\nCalculates the minimum and maximum values for each column in a NumPy array.\n\n\nmap_to_original_scale\nMaps the values in X_search from the range [0, 1] to the original scale defined by x_min and x_max.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.boundaries"
    ]
  },
  {
    "objectID": "docs/reference/utils.boundaries.get_boundaries.html",
    "href": "docs/reference/utils.boundaries.get_boundaries.html",
    "title": "utils.boundaries.get_boundaries",
    "section": "",
    "text": "utils.boundaries.get_boundaries(data)\nCalculates the minimum and maximum values for each column in a NumPy array.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\nnp.ndarray\nA NumPy array of shape (n, k), where n is the number of rows and k is the number of columns.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[np.ndarray, np.ndarray]\ntuple[np.ndarray, np.ndarray]: A tuple containing two NumPy arrays: - The first array contains the minimum values for each column, with shape (k,). - The second array contains the maximum values for each column, with shape (k,).\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the input array has shape (1, 0) (empty array).\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.utils.boundaries import get_boundaries\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; min_values, max_values = get_boundaries(data)\n&gt;&gt;&gt; print(\"Minimum values:\", min_values)\nMinimum values: [1 2 3]\n&gt;&gt;&gt; print(\"Maximum values:\", max_values)\nMaximum values: [7 8 9]",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_boundaries"
    ]
  },
  {
    "objectID": "docs/reference/utils.boundaries.get_boundaries.html#parameters",
    "href": "docs/reference/utils.boundaries.get_boundaries.html#parameters",
    "title": "utils.boundaries.get_boundaries",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\nnp.ndarray\nA NumPy array of shape (n, k), where n is the number of rows and k is the number of columns.\nrequired",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_boundaries"
    ]
  },
  {
    "objectID": "docs/reference/utils.boundaries.get_boundaries.html#returns",
    "href": "docs/reference/utils.boundaries.get_boundaries.html#returns",
    "title": "utils.boundaries.get_boundaries",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\ntuple[np.ndarray, np.ndarray]\ntuple[np.ndarray, np.ndarray]: A tuple containing two NumPy arrays: - The first array contains the minimum values for each column, with shape (k,). - The second array contains the maximum values for each column, with shape (k,).",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_boundaries"
    ]
  },
  {
    "objectID": "docs/reference/utils.boundaries.get_boundaries.html#raises",
    "href": "docs/reference/utils.boundaries.get_boundaries.html#raises",
    "title": "utils.boundaries.get_boundaries",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf the input array has shape (1, 0) (empty array).",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_boundaries"
    ]
  },
  {
    "objectID": "docs/reference/utils.boundaries.get_boundaries.html#examples",
    "href": "docs/reference/utils.boundaries.get_boundaries.html#examples",
    "title": "utils.boundaries.get_boundaries",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.utils.boundaries import get_boundaries\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; min_values, max_values = get_boundaries(data)\n&gt;&gt;&gt; print(\"Minimum values:\", min_values)\nMinimum values: [1 2 3]\n&gt;&gt;&gt; print(\"Maximum values:\", max_values)\nMaximum values: [7 8 9]",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_boundaries"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.tricands_fringe.html",
    "href": "docs/reference/tricands.tricands.tricands_fringe.html",
    "title": "tricands.tricands.tricands_fringe",
    "section": "",
    "text": "tricands.tricands.tricands_fringe(X, p=0.5, lower=0, upper=1)\nGenerate fringe candidates outside the convex hull.\nSubroutine used by tricands wrapper. Assumes a bounding box of [lower, upper]^m.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput design matrix of shape (n_samples, n_features).\nrequired\n\n\np\nfloat\nDistance to the boundary (0 = on hull, 1 = on boundary). Defaults to 0.5.\n0.5\n\n\nlower\nfloat\nLower bound of bounding box for all dimensions. Defaults to 0.\n0\n\n\nupper\nfloat\nUpper bound of bounding box for all dimensions. Defaults to 1.\n1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndict\ndict\nA dictionary containing: - ‘XF’ (np.ndarray): Fringe candidate points. - ‘XB’ (np.ndarray): Boundary points (means of external facets). - ‘qhull’ (scipy.spatial.ConvexHull): The computed convex hull object.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nException\nIf the number of points is less than n_features + 1.",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands_fringe"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.tricands_fringe.html#parameters",
    "href": "docs/reference/tricands.tricands.tricands_fringe.html#parameters",
    "title": "tricands.tricands.tricands_fringe",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput design matrix of shape (n_samples, n_features).\nrequired\n\n\np\nfloat\nDistance to the boundary (0 = on hull, 1 = on boundary). Defaults to 0.5.\n0.5\n\n\nlower\nfloat\nLower bound of bounding box for all dimensions. Defaults to 0.\n0\n\n\nupper\nfloat\nUpper bound of bounding box for all dimensions. Defaults to 1.\n1",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands_fringe"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.tricands_fringe.html#returns",
    "href": "docs/reference/tricands.tricands.tricands_fringe.html#returns",
    "title": "tricands.tricands.tricands_fringe",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ndict\ndict\nA dictionary containing: - ‘XF’ (np.ndarray): Fringe candidate points. - ‘XB’ (np.ndarray): Boundary points (means of external facets). - ‘qhull’ (scipy.spatial.ConvexHull): The computed convex hull object.",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands_fringe"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.tricands_fringe.html#raises",
    "href": "docs/reference/tricands.tricands.tricands_fringe.html#raises",
    "title": "tricands.tricands.tricands_fringe",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nException\nIf the number of points is less than n_features + 1.",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands_fringe"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.html",
    "href": "docs/reference/tricands.tricands.html",
    "title": "tricands.tricands",
    "section": "",
    "text": "tricands.tricands\n\n\n\n\n\nName\nDescription\n\n\n\n\ntricands\nGenerate Triangulation Candidates for Bayesian Optimization.\n\n\ntricands_fringe\nGenerate fringe candidates outside the convex hull.\n\n\ntricands_interior\nGenerate interior candidates using Delaunay triangulation.",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands.tricands"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.html#functions",
    "href": "docs/reference/tricands.tricands.html#functions",
    "title": "tricands.tricands",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ntricands\nGenerate Triangulation Candidates for Bayesian Optimization.\n\n\ntricands_fringe\nGenerate fringe candidates outside the convex hull.\n\n\ntricands_interior\nGenerate interior candidates using Delaunay triangulation.",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands.tricands"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.simple_kriging.html",
    "href": "docs/reference/surrogate.simple_kriging.html",
    "title": "surrogate.simple_kriging",
    "section": "",
    "text": "surrogate.simple_kriging\nSimplified SimpleKriging surrogate model for SpotOptim.\nThis is a streamlined version adapted from spotpython.surrogate.kriging for use with the SpotOptim optimizer.\n\n\n\n\n\nName\nDescription\n\n\n\n\nSimpleKriging\nA simplified Kriging (Gaussian Process) surrogate model for SpotOptim.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate.simple_kriging"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.simple_kriging.html#classes",
    "href": "docs/reference/surrogate.simple_kriging.html#classes",
    "title": "surrogate.simple_kriging",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nSimpleKriging\nA simplified Kriging (Gaussian Process) surrogate model for SpotOptim.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate.simple_kriging"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.html",
    "href": "docs/reference/surrogate.html",
    "title": "surrogate",
    "section": "",
    "text": "surrogate\nSurrogate models for SpotOptim.\nThis module provides two Kriging (Gaussian Process) implementations:\n\nKriging: Full-featured implementation with:\n\nMultiple methods: interpolation, regression, reinterpolation\nMixed variable types: float/num, int, factor\nIsotropic/anisotropic correlation\nLambda (nugget) optimization for regression\nCompatible with SpotOptim’s variable type conventions\n\nSimpleKriging: Lightweight implementation with:\n\nGaussian kernel only\nBasic hyperparameter optimization\nFaster for simple problems\nLimited to continuous variables\n\n\nFor most SpotOptim applications, use Kriging (the default). Use SimpleKriging for quick prototyping or simple continuous problems.\n\n\n&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt; model = Kriging(method='regression', seed=42)\n&gt;&gt;&gt; model.fit(X_train, y_train)\n&gt;&gt;&gt; predictions = model.predict(X_test)\n&gt;&gt;&gt; from spotoptim.surrogate import SimpleKriging\n&gt;&gt;&gt; simple_model = SimpleKriging(noise=1e-10, seed=42)\n&gt;&gt;&gt; simple_model.fit(X_train, y_train)",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.html#examples",
    "href": "docs/reference/surrogate.html#examples",
    "title": "surrogate",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt; model = Kriging(method='regression', seed=42)\n&gt;&gt;&gt; model.fit(X_train, y_train)\n&gt;&gt;&gt; predictions = model.predict(X_test)\n&gt;&gt;&gt; from spotoptim.surrogate import SimpleKriging\n&gt;&gt;&gt; simple_model = SimpleKriging(noise=1e-10, seed=42)\n&gt;&gt;&gt; simple_model.fit(X_train, y_train)",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.pipeline.Pipeline.html",
    "href": "docs/reference/surrogate.pipeline.Pipeline.html",
    "title": "surrogate.pipeline.Pipeline",
    "section": "",
    "text": "surrogate.pipeline.Pipeline(steps)\nPipeline of transforms with a final estimator.\nSequentially apply a list of transforms and a final estimator. This simple implementation assumes that all steps except the last one are transformers (have fit_transform or fit+transform), and the last step is an estimator (has fit and predict).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsteps\nlist\nList of (name, transform) tuples (implementing fit/transform) that are chained, in the order they are chained, with the last object an estimator.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfit\nFit the model.\n\n\npredict\nTransform the data, and apply predict with the final estimator.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.pipeline.Pipeline.html#parameters",
    "href": "docs/reference/surrogate.pipeline.Pipeline.html#parameters",
    "title": "surrogate.pipeline.Pipeline",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsteps\nlist\nList of (name, transform) tuples (implementing fit/transform) that are chained, in the order they are chained, with the last object an estimator.\nrequired",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.pipeline.Pipeline.html#methods",
    "href": "docs/reference/surrogate.pipeline.Pipeline.html#methods",
    "title": "surrogate.pipeline.Pipeline",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfit\nFit the model.\n\n\npredict\nTransform the data, and apply predict with the final estimator.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Pipeline"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.nystroem.Nystroem.html",
    "href": "docs/reference/surrogate.nystroem.Nystroem.html",
    "title": "surrogate.nystroem.Nystroem",
    "section": "",
    "text": "surrogate.nystroem.Nystroem(kernel='rbf', n_components=100, random_state=None)\nApproximate a feature map of a kernel using a subset of data.\nThe Nystroem method approximates a kernel map using a subset of the training data. It constructs an approximate feature map X \\mapsto X' such that X'.dot(X'.T) \\approx K(X, X).\nThis is particularly useful when: * n (samples) is moderate/large: The exact kernel method scales as O(n^3). Nystroem reduces complexity to O(n * n_components^2) for training. * k (features) is large: By setting n_components such that k &lt; n_components &lt;&lt; n, it projects high-dimensional data into a manageable feature space where distance calculations are cheaper (if followed by a linear model).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkernel\nstr or callable or Kernel\nKernel map to be approximated. Can be a string (e.g., ‘rbf’), a callable, or a spotoptim.surrogate.kernels.Kernel instance. Defaults to ‘rbf’.\n'rbf'\n\n\nn_components\nint\nNumber of features to construct. This corresponds to the number of samples used to construct the basis. Determines the dimension of the transformed feature space. Defaults to 100.\n100\n\n\nrandom_state\nint, RandomState instance or None\nPseudo-random number generator to control the uniform sampling without replacement of n_components of the training data. Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfit\nFit estimator to data.\n\n\nfit_transform\nFit to data, then transform it.\n\n\ntransform\nApply feature map to X.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Nystroem"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.nystroem.Nystroem.html#parameters",
    "href": "docs/reference/surrogate.nystroem.Nystroem.html#parameters",
    "title": "surrogate.nystroem.Nystroem",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nkernel\nstr or callable or Kernel\nKernel map to be approximated. Can be a string (e.g., ‘rbf’), a callable, or a spotoptim.surrogate.kernels.Kernel instance. Defaults to ‘rbf’.\n'rbf'\n\n\nn_components\nint\nNumber of features to construct. This corresponds to the number of samples used to construct the basis. Determines the dimension of the transformed feature space. Defaults to 100.\n100\n\n\nrandom_state\nint, RandomState instance or None\nPseudo-random number generator to control the uniform sampling without replacement of n_components of the training data. Defaults to None.\nNone",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Nystroem"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.nystroem.Nystroem.html#methods",
    "href": "docs/reference/surrogate.nystroem.Nystroem.html#methods",
    "title": "surrogate.nystroem.Nystroem",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfit\nFit estimator to data.\n\n\nfit_transform\nFit to data, then transform it.\n\n\ntransform\nApply feature map to X.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Nystroem"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.mlp_surrogate.MLPSurrogate.html",
    "href": "docs/reference/surrogate.mlp_surrogate.MLPSurrogate.html",
    "title": "surrogate.mlp_surrogate.MLPSurrogate",
    "section": "",
    "text": "surrogate.mlp_surrogate.MLPSurrogate(\n    in_channels=None,\n    hidden_channels=None,\n    l1=128,\n    num_hidden_layers=3,\n    dropout=0.0,\n    activation='relu',\n    lr=0.001,\n    epochs=200,\n    batch_size=32,\n    optimizer_name='AdamWScheduleFree',\n    mc_dropout_passes=30,\n    seed=42,\n    var_type=None,\n    name='MLPSurrogate',\n    verbose=False,\n)\nA scikit-learn compatible MLP surrogate model with uncertainty estimation.\nThis class wraps a PyTorch MLP (from spotoptim.nn.mlp) and provides the standard fit/predict interface required by SpotOptim. It uses Monte Carlo (MC) Dropout during prediction to estimate uncertainty (standard deviation) which is crucial for acquisition functions.\nCompatible with SpotOptim’s variable type conventions: - ‘float’: continuous numeric variables - ‘int’: integer variables - ‘factor’: categorical/unordered variables\nNote: All input variables are currently treated as numeric and standardized. For best performance with categorical variables, this simple treatment may be suboptimal compared to embedding processing, but ensures compatibility.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nin_channels\nint\nInput dimension. If None, inferred during fit.\nNone\n\n\nhidden_channels\nList[int]\nExplicit list of hidden layer sizes.\nNone\n\n\nl1\nint\nNeurons per hidden layer (used if hidden_channels is None). Defaults to 64.\n128\n\n\nnum_hidden_layers\nint\nNumber of hidden layers (used if hidden_channels is None). Defaults to 2.\n3\n\n\ndropout\nfloat\nDropout probability. Crucial for uncertainty estimation. Defaults to 0.1.\n0.0\n\n\nlr\nfloat\nLearning rate. Defaults to 1e-3.\n0.001\n\n\nactivation\nstr\nActivation function. Defaults to “relu”.\n'relu'\n\n\nepochs\nint\nNumber of training epochs. Defaults to 200.\n200\n\n\nbatch_size\nint\nTraining batch size. Defaults to 32.\n32\n\n\noptimizer_name\nstr\nName of PyTorch optimizer (“Adam”, “SGD”, etc.) or “AdamWScheduleFree”. Defaults to “AdamWScheduleFree”.\n'AdamWScheduleFree'\n\n\nmc_dropout_passes\nint\nNumber of forward passes for MC Dropout uncertainty estimation. Defaults to 30.\n30\n\n\nseed\nint\nRandom seed for reproducibility. Defaults to 42.\n42\n\n\nvar_type\nList[str]\nVariable types for each dimension. Defaults to None.\nNone\n\n\nname\nstr\nName of the surrogate. Defaults to “MLPSurrogate”.\n'MLPSurrogate'\n\n\nverbose\nbool\nWhether to print training progress. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfit\nFit the MLP model to the training data.\n\n\npredict\nPredict targets for X.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "MLPSurrogate"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.mlp_surrogate.MLPSurrogate.html#parameters",
    "href": "docs/reference/surrogate.mlp_surrogate.MLPSurrogate.html#parameters",
    "title": "surrogate.mlp_surrogate.MLPSurrogate",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nin_channels\nint\nInput dimension. If None, inferred during fit.\nNone\n\n\nhidden_channels\nList[int]\nExplicit list of hidden layer sizes.\nNone\n\n\nl1\nint\nNeurons per hidden layer (used if hidden_channels is None). Defaults to 64.\n128\n\n\nnum_hidden_layers\nint\nNumber of hidden layers (used if hidden_channels is None). Defaults to 2.\n3\n\n\ndropout\nfloat\nDropout probability. Crucial for uncertainty estimation. Defaults to 0.1.\n0.0\n\n\nlr\nfloat\nLearning rate. Defaults to 1e-3.\n0.001\n\n\nactivation\nstr\nActivation function. Defaults to “relu”.\n'relu'\n\n\nepochs\nint\nNumber of training epochs. Defaults to 200.\n200\n\n\nbatch_size\nint\nTraining batch size. Defaults to 32.\n32\n\n\noptimizer_name\nstr\nName of PyTorch optimizer (“Adam”, “SGD”, etc.) or “AdamWScheduleFree”. Defaults to “AdamWScheduleFree”.\n'AdamWScheduleFree'\n\n\nmc_dropout_passes\nint\nNumber of forward passes for MC Dropout uncertainty estimation. Defaults to 30.\n30\n\n\nseed\nint\nRandom seed for reproducibility. Defaults to 42.\n42\n\n\nvar_type\nList[str]\nVariable types for each dimension. Defaults to None.\nNone\n\n\nname\nstr\nName of the surrogate. Defaults to “MLPSurrogate”.\n'MLPSurrogate'\n\n\nverbose\nbool\nWhether to print training progress. Defaults to False.\nFalse",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "MLPSurrogate"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.mlp_surrogate.MLPSurrogate.html#methods",
    "href": "docs/reference/surrogate.mlp_surrogate.MLPSurrogate.html#methods",
    "title": "surrogate.mlp_surrogate.MLPSurrogate",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfit\nFit the MLP model to the training data.\n\n\npredict\nPredict targets for X.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "MLPSurrogate"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kriging.Kriging.html",
    "href": "docs/reference/surrogate.kriging.Kriging.html",
    "title": "surrogate.kriging.Kriging",
    "section": "",
    "text": "surrogate.kriging.Kriging(\n    noise=None,\n    penalty=10000.0,\n    method='regression',\n    var_type=None,\n    name='Kriging',\n    seed=124,\n    model_fun_evals=None,\n    n_theta=None,\n    min_theta=-3.0,\n    max_theta=2.0,\n    theta_init_zero=False,\n    p_val=2.0,\n    n_p=1,\n    optim_p=False,\n    min_Lambda=-9.0,\n    max_Lambda=0.0,\n    metric_factorial='canberra',\n    isotropic=False,\n    theta=None,\n    **kwargs,\n)\nA scikit-learn compatible Kriging model class for regression tasks.\nThis class provides Ordinary Kriging with support for: - Mixed variable types (continuous, integer, factor) - Gaussian/RBF correlation function - Three fitting methods (Forrester (2008), Section 6): - Isotropic or anisotropic length scales\nCompatible with SpotOptim’s variable type conventions: - ‘float’: continuous numeric variables - ‘int’: integer variables - ‘factor’: categorical/unordered variables\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnoise\nfloat\nSmall regularization term for numerical stability (nugget effect). If None, defaults to sqrt(machine epsilon). Only used for “interpolation” method. For “regression” and “reinterpolation”, this is replaced by the Lambda parameter. Defaults to None.\nNone\n\n\npenalty\nfloat\nLarge negative log-likelihood assigned if correlation matrix is not positive-definite. Defaults to 1e4.\n10000.0\n\n\nmethod\nstr\nFitting method (Forrester (2008), Section 6). Options: - “interpolation”: Pure Kriging interpolation (Eq 2.X). Fits exact data points. Uses a small noise (nugget) for numerical stability. - “regression”: Regression Kriging (Section 6.2). Optimizes a regularization parameter Lambda (nugget) along with theta. Suitable for noisy data. - “reinterpolation”: Re-interpolation (Section 6.3). Fits hyperparameters using regression (with Lambda), but predicts using the “noise-free” correlation matrix (removing Lambda). This creates a surrogate that glosses over noise but passes closer to the underlying trend (interpolating the regression model). Defaults to “regression”.\n'regression'\n\n\nvar_type\nList[str]\nVariable types for each dimension. SpotOptim uses: ‘float’ (continuous), ‘int’ (integer), ‘factor’ (categorical). Defaults to [“float”].\nNone\n\n\nname\nstr\nName of the Kriging instance. Defaults to “Kriging”.\n'Kriging'\n\n\nseed\nint\nRandom seed for reproducibility. Defaults to 124.\n124\n\n\nmodel_fun_evals\nint\nMaximum function evaluations for hyperparameter optimization. Defaults to 100.\nNone\n\n\nn_theta\nint\nNumber of theta parameters. If None, set during fit. Defaults to None.\nNone\n\n\nmin_theta\nfloat\nMinimum log10(theta) bound. Defaults to -3.0.\n-3.0\n\n\nmax_theta\nfloat\nMaximum log10(theta) bound. Defaults to 2.0.\n2.0\n\n\ntheta_init_zero\nbool\nInitialize theta to zero. Defaults to False.\nFalse\n\n\np_val\nfloat\nPower parameter for correlation (fixed at 2.0 for Gaussian). Defaults to 2.0.\n2.0\n\n\nn_p\nint\nNumber of p parameters (currently not optimized). Defaults to 1.\n1\n\n\noptim_p\nbool\nOptimize p parameters (currently not supported). Defaults to False.\nFalse\n\n\nmin_Lambda\nfloat\nMinimum log10(Lambda) bound. Defaults to -9.0.\n-9.0\n\n\nmax_Lambda\nfloat\nMaximum log10(Lambda) bound. Defaults to 0.0.\n0.0\n\n\nmetric_factorial\nstr\nDistance metric for factor variables. Defaults to “canberra”.\n'canberra'\n\n\nisotropic\nbool\nUse single theta for all dimensions. Defaults to False.\nFalse\n\n\ntheta\nnp.ndarray\nInitial theta values (log10 scale). Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nX_\nndarray\nTraining data, shape (n_samples, n_features).\n\n\ny_\nndarray\nTraining targets, shape (n_samples,).\n\n\ntheta_\nndarray\nOptimized log10(theta) parameters.\n\n\nLambda_\nfloat or None\nOptimized log10(Lambda) for regression methods.\n\n\nmu_\nfloat\nMean of Kriging predictor.\n\n\nsigma2_\nfloat\nVariance of Kriging predictor.\n\n\nU_\nndarray\nCholesky factor of correlation matrix.\n\n\nPsi_\nndarray\nCorrelation matrix.\n\n\nnegLnLike\nfloat\nNegative log-likelihood value.\n\n\n\n\n\n\nBasic usage with SpotOptim:\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define objective\n&gt;&gt;&gt; def objective(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create Kriging surrogate\n&gt;&gt;&gt; kriging = Kriging(\n...     noise=1e-10,\n...     method='regression',\n...     min_theta=-3.0,\n...     max_theta=2.0,\n...     seed=42\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use with SpotOptim\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     surrogate=kriging,\n...     max_iter=30,\n...     n_initial=10,\n...     seed=42\n... )\n&gt;&gt;&gt; result = opt.optimize()\nDirect usage (scikit-learn compatible):\n&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training data\n&gt;&gt;&gt; X_train = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit model\n&gt;&gt;&gt; model = Kriging(method='regression', seed=42)\n&gt;&gt;&gt; model.fit(X_train, y_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Predict\n&gt;&gt;&gt; X_test = np.array([[0.25, 0.25], [0.75, 0.75]])\n&gt;&gt;&gt; y_pred = model.predict(X_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Predict with uncertainty\n&gt;&gt;&gt; y_pred, std = model.predict(X_test, return_std=True)\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nbuild_correlation_matrix\nBuild correlation matrix from training data.\n\n\nbuild_psi_vector\nBuild correlation vector between x and training points.\n\n\nfit\nFit the Kriging model to training data.\n\n\nget_params\nGet parameters for this estimator (scikit-learn compatibility).\n\n\nlikelihood\nCompute negative concentrated log-likelihood.\n\n\nobjective\nObjective function for hyperparameter optimization.\n\n\npredict\nPredict at new points.\n\n\npredict_single\nPredict at a single point.\n\n\nset_params\nSet parameters (scikit-learn compatibility).",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Kriging"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kriging.Kriging.html#parameters",
    "href": "docs/reference/surrogate.kriging.Kriging.html#parameters",
    "title": "surrogate.kriging.Kriging",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nnoise\nfloat\nSmall regularization term for numerical stability (nugget effect). If None, defaults to sqrt(machine epsilon). Only used for “interpolation” method. For “regression” and “reinterpolation”, this is replaced by the Lambda parameter. Defaults to None.\nNone\n\n\npenalty\nfloat\nLarge negative log-likelihood assigned if correlation matrix is not positive-definite. Defaults to 1e4.\n10000.0\n\n\nmethod\nstr\nFitting method (Forrester (2008), Section 6). Options: - “interpolation”: Pure Kriging interpolation (Eq 2.X). Fits exact data points. Uses a small noise (nugget) for numerical stability. - “regression”: Regression Kriging (Section 6.2). Optimizes a regularization parameter Lambda (nugget) along with theta. Suitable for noisy data. - “reinterpolation”: Re-interpolation (Section 6.3). Fits hyperparameters using regression (with Lambda), but predicts using the “noise-free” correlation matrix (removing Lambda). This creates a surrogate that glosses over noise but passes closer to the underlying trend (interpolating the regression model). Defaults to “regression”.\n'regression'\n\n\nvar_type\nList[str]\nVariable types for each dimension. SpotOptim uses: ‘float’ (continuous), ‘int’ (integer), ‘factor’ (categorical). Defaults to [“float”].\nNone\n\n\nname\nstr\nName of the Kriging instance. Defaults to “Kriging”.\n'Kriging'\n\n\nseed\nint\nRandom seed for reproducibility. Defaults to 124.\n124\n\n\nmodel_fun_evals\nint\nMaximum function evaluations for hyperparameter optimization. Defaults to 100.\nNone\n\n\nn_theta\nint\nNumber of theta parameters. If None, set during fit. Defaults to None.\nNone\n\n\nmin_theta\nfloat\nMinimum log10(theta) bound. Defaults to -3.0.\n-3.0\n\n\nmax_theta\nfloat\nMaximum log10(theta) bound. Defaults to 2.0.\n2.0\n\n\ntheta_init_zero\nbool\nInitialize theta to zero. Defaults to False.\nFalse\n\n\np_val\nfloat\nPower parameter for correlation (fixed at 2.0 for Gaussian). Defaults to 2.0.\n2.0\n\n\nn_p\nint\nNumber of p parameters (currently not optimized). Defaults to 1.\n1\n\n\noptim_p\nbool\nOptimize p parameters (currently not supported). Defaults to False.\nFalse\n\n\nmin_Lambda\nfloat\nMinimum log10(Lambda) bound. Defaults to -9.0.\n-9.0\n\n\nmax_Lambda\nfloat\nMaximum log10(Lambda) bound. Defaults to 0.0.\n0.0\n\n\nmetric_factorial\nstr\nDistance metric for factor variables. Defaults to “canberra”.\n'canberra'\n\n\nisotropic\nbool\nUse single theta for all dimensions. Defaults to False.\nFalse\n\n\ntheta\nnp.ndarray\nInitial theta values (log10 scale). Defaults to None.\nNone",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Kriging"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kriging.Kriging.html#attributes",
    "href": "docs/reference/surrogate.kriging.Kriging.html#attributes",
    "title": "surrogate.kriging.Kriging",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nX_\nndarray\nTraining data, shape (n_samples, n_features).\n\n\ny_\nndarray\nTraining targets, shape (n_samples,).\n\n\ntheta_\nndarray\nOptimized log10(theta) parameters.\n\n\nLambda_\nfloat or None\nOptimized log10(Lambda) for regression methods.\n\n\nmu_\nfloat\nMean of Kriging predictor.\n\n\nsigma2_\nfloat\nVariance of Kriging predictor.\n\n\nU_\nndarray\nCholesky factor of correlation matrix.\n\n\nPsi_\nndarray\nCorrelation matrix.\n\n\nnegLnLike\nfloat\nNegative log-likelihood value.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Kriging"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kriging.Kriging.html#examples",
    "href": "docs/reference/surrogate.kriging.Kriging.html#examples",
    "title": "surrogate.kriging.Kriging",
    "section": "",
    "text": "Basic usage with SpotOptim:\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define objective\n&gt;&gt;&gt; def objective(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create Kriging surrogate\n&gt;&gt;&gt; kriging = Kriging(\n...     noise=1e-10,\n...     method='regression',\n...     min_theta=-3.0,\n...     max_theta=2.0,\n...     seed=42\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use with SpotOptim\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     surrogate=kriging,\n...     max_iter=30,\n...     n_initial=10,\n...     seed=42\n... )\n&gt;&gt;&gt; result = opt.optimize()\nDirect usage (scikit-learn compatible):\n&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training data\n&gt;&gt;&gt; X_train = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit model\n&gt;&gt;&gt; model = Kriging(method='regression', seed=42)\n&gt;&gt;&gt; model.fit(X_train, y_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Predict\n&gt;&gt;&gt; X_test = np.array([[0.25, 0.25], [0.75, 0.75]])\n&gt;&gt;&gt; y_pred = model.predict(X_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Predict with uncertainty\n&gt;&gt;&gt; y_pred, std = model.predict(X_test, return_std=True)",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Kriging"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kriging.Kriging.html#methods",
    "href": "docs/reference/surrogate.kriging.Kriging.html#methods",
    "title": "surrogate.kriging.Kriging",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbuild_correlation_matrix\nBuild correlation matrix from training data.\n\n\nbuild_psi_vector\nBuild correlation vector between x and training points.\n\n\nfit\nFit the Kriging model to training data.\n\n\nget_params\nGet parameters for this estimator (scikit-learn compatibility).\n\n\nlikelihood\nCompute negative concentrated log-likelihood.\n\n\nobjective\nObjective function for hyperparameter optimization.\n\n\npredict\nPredict at new points.\n\n\npredict_single\nPredict at a single point.\n\n\nset_params\nSet parameters (scikit-learn compatibility).",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Kriging"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.WhiteKernel.html",
    "href": "docs/reference/surrogate.kernels.WhiteKernel.html",
    "title": "surrogate.kernels.WhiteKernel",
    "section": "",
    "text": "surrogate.kernels.WhiteKernel(\n    noise_level=1.0,\n    noise_level_bounds=(1e-05, 100000.0),\n)\nWhite kernel.\nThe main use case is capturing noise in the signal: k(x_i, x_j) = noise_level if x_i == x_j else 0\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnoise_level\nfloat\nParameter controlling the noise level (variance). Defaults to 1.0.\n1.0\n\n\nnoise_level_bounds\ntuple\nThe lower and upper bound on noise_level. Defaults to (1e-5, 1e5).\n(1e-05, 100000.0)",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "WhiteKernel"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.WhiteKernel.html#parameters",
    "href": "docs/reference/surrogate.kernels.WhiteKernel.html#parameters",
    "title": "surrogate.kernels.WhiteKernel",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nnoise_level\nfloat\nParameter controlling the noise level (variance). Defaults to 1.0.\n1.0\n\n\nnoise_level_bounds\ntuple\nThe lower and upper bound on noise_level. Defaults to (1e-5, 1e5).\n(1e-05, 100000.0)",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "WhiteKernel"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.SpotOptimKernel.html",
    "href": "docs/reference/surrogate.kernels.SpotOptimKernel.html",
    "title": "surrogate.kernels.SpotOptimKernel",
    "section": "",
    "text": "surrogate.kernels.SpotOptimKernel(\n    theta,\n    var_type,\n    p_val=2.0,\n    metric_factorial='canberra',\n)\nKernel designed for SpotOptim’s Kriging with mixed variable support.\nIt handles continuous (‘float’), integer (‘int’), and categorical (‘factor’) variables similarly to the internal logic of the Kriging class.\nThe correlation function is defined as: Psi = exp(- (D_ordered + D_factor))\nwhere: D_ordered = sum_j theta_j * |x_ij - y_lj|^p (for ordered variables) D_factor = sum_j theta_j * d(x_ij, y_lj) (for factor variables, d is metric like Canberra)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntheta\nnp.ndarray\nThe correlation parameters (weights). Note: In standard Kriging usage, this corresponds to 10^theta_log. This kernel expects the LINEAR scale theta values (weights), not log.\nrequired\n\n\nvar_type\nlist of str\nList of variable types, e.g. [‘float’, ‘int’, ‘factor’].\nrequired\n\n\np_val\nfloat\nPower parameter for ordered distance. Defaults to 2.0.\n2.0\n\n\nmetric_factorial\nstr\nMetric for factor distance (passed to cdist/pdist). Defaults to ‘canberra’.\n'canberra'",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "SpotOptimKernel"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.SpotOptimKernel.html#parameters",
    "href": "docs/reference/surrogate.kernels.SpotOptimKernel.html#parameters",
    "title": "surrogate.kernels.SpotOptimKernel",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntheta\nnp.ndarray\nThe correlation parameters (weights). Note: In standard Kriging usage, this corresponds to 10^theta_log. This kernel expects the LINEAR scale theta values (weights), not log.\nrequired\n\n\nvar_type\nlist of str\nList of variable types, e.g. [‘float’, ‘int’, ‘factor’].\nrequired\n\n\np_val\nfloat\nPower parameter for ordered distance. Defaults to 2.0.\n2.0\n\n\nmetric_factorial\nstr\nMetric for factor distance (passed to cdist/pdist). Defaults to ‘canberra’.\n'canberra'",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "SpotOptimKernel"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.Product.html",
    "href": "docs/reference/surrogate.kernels.Product.html",
    "title": "surrogate.kernels.Product",
    "section": "",
    "text": "surrogate.kernels.Product(k1, k2)\nThe Product kernel k1 * k2.\nThe kernel value is k1(X, Y) * k2(X, Y).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nk1\nKernel\nFirst kernel.\nrequired\n\n\nk2\nKernel\nSecond kernel.\nrequired",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Product"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.Product.html#parameters",
    "href": "docs/reference/surrogate.kernels.Product.html#parameters",
    "title": "surrogate.kernels.Product",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nk1\nKernel\nFirst kernel.\nrequired\n\n\nk2\nKernel\nSecond kernel.\nrequired",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Product"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.ConstantKernel.html",
    "href": "docs/reference/surrogate.kernels.ConstantKernel.html",
    "title": "surrogate.kernels.ConstantKernel",
    "section": "",
    "text": "surrogate.kernels.ConstantKernel(\n    constant_value=1.0,\n    constant_value_bounds=(1e-05, 100000.0),\n)\nConstant kernel.\nCan be used as a scaling factor (e.g. 2.0 * RBF()) or as part of a sum (e.g. RBF() + 1.0).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nconstant_value\nfloat\nThe constant value. Defaults to 1.0.\n1.0\n\n\nconstant_value_bounds\ntuple\nThe lower and upper bound on constant_value. Defaults to (1e-5, 1e5).\n(1e-05, 100000.0)",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "ConstantKernel"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.ConstantKernel.html#parameters",
    "href": "docs/reference/surrogate.kernels.ConstantKernel.html#parameters",
    "title": "surrogate.kernels.ConstantKernel",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nconstant_value\nfloat\nThe constant value. Defaults to 1.0.\n1.0\n\n\nconstant_value_bounds\ntuple\nThe lower and upper bound on constant_value. Defaults to (1e-5, 1e5).\n(1e-05, 100000.0)",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "ConstantKernel"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.subset.html",
    "href": "docs/reference/sampling.mm.subset.html",
    "title": "sampling.mm.subset",
    "section": "",
    "text": "sampling.mm.subset(X, ns)\nReturns a space-filling subset of a given size from a sampling plan, along with the remainder. It repeatedly attempts to substitute each point in the subset with a point from the remainder if doing so improves the Morris-Mitchell metric.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nA 2D array representing the original sampling plan, of shape (n, d).\nrequired\n\n\nns\nint\nThe size of the desired subset.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n(np.ndarray, np.ndarray)\nA tuple (Xs, Xr) where: - Xs is the chosen subset of size ns, with space-filling properties. - Xr is the remainder (X  Xs).\n\n\n\n\n\n\nMany thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”\n\n\n\n&gt;&gt;&gt; from spotoptim.sampling.mm import subset, bestlh\n    X = bestlh(n=5, k=3, population=5, iterations=10)\n    Xs, Xr = subset(X, ns=2)\n    print(Xs)\n    print(Xr)\n        [[0.25 0.   0.5 ]\n        [0.5  0.75 0.  ]]\n        [[1.   0.25 0.25]\n        [0.   1.   0.75]\n        [0.75 0.5  1.  ]]",
    "crumbs": [
      "API Reference",
      "Sampling",
      "subset"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.subset.html#parameters",
    "href": "docs/reference/sampling.mm.subset.html#parameters",
    "title": "sampling.mm.subset",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nA 2D array representing the original sampling plan, of shape (n, d).\nrequired\n\n\nns\nint\nThe size of the desired subset.\nrequired",
    "crumbs": [
      "API Reference",
      "Sampling",
      "subset"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.subset.html#returns",
    "href": "docs/reference/sampling.mm.subset.html#returns",
    "title": "sampling.mm.subset",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n(np.ndarray, np.ndarray)\nA tuple (Xs, Xr) where: - Xs is the chosen subset of size ns, with space-filling properties. - Xr is the remainder (X  Xs).",
    "crumbs": [
      "API Reference",
      "Sampling",
      "subset"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.subset.html#notes",
    "href": "docs/reference/sampling.mm.subset.html#notes",
    "title": "sampling.mm.subset",
    "section": "",
    "text": "Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”",
    "crumbs": [
      "API Reference",
      "Sampling",
      "subset"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.subset.html#examples",
    "href": "docs/reference/sampling.mm.subset.html#examples",
    "title": "sampling.mm.subset",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.sampling.mm import subset, bestlh\n    X = bestlh(n=5, k=3, population=5, iterations=10)\n    Xs, Xr = subset(X, ns=2)\n    print(Xs)\n    print(Xr)\n        [[0.25 0.   0.5 ]\n        [0.5  0.75 0.  ]]\n        [[1.   0.25 0.25]\n        [0.   1.   0.75]\n        [0.75 0.5  1.  ]]",
    "crumbs": [
      "API Reference",
      "Sampling",
      "subset"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.propose_mmphi_intensive_minimizing_point.html",
    "href": "docs/reference/sampling.mm.propose_mmphi_intensive_minimizing_point.html",
    "title": "sampling.mm.propose_mmphi_intensive_minimizing_point",
    "section": "",
    "text": "sampling.mm.propose_mmphi_intensive_minimizing_point(\n    X,\n    n_candidates=1000,\n    q=2.0,\n    p=2.0,\n    seed=None,\n    lower=None,\n    upper=None,\n    normalize_flag=False,\n)\nPropose a new point that, when added to X, minimizes the intensive Morris-Mitchell (mmphi_intensive) criterion.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nExisting points, shape (n_points, n_dim).\nrequired\n\n\nn_candidates\nint\nNumber of random candidates to sample.\n1000\n\n\nq\nfloat\nExponent for mmphi_intensive.\n2.0\n\n\np\nfloat\nDistance norm for mmphi_intensive.\n2.0\n\n\nseed\nint\nRandom seed.\nNone\n\n\nlower\nnp.ndarray\nLower bounds for each dimension (default: 0).\nNone\n\n\nupper\nnp.ndarray\nUpper bounds for each dimension (default: 1).\nNone\n\n\nnormalize_flag\nbool\nIf True, normalizes the X array and candidate points before computing distances. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Proposed new point, shape (1, n_dim).\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n    from spotoptim.sampling.mm import propose_mmphi_intensive_minimizing_point\n    # Existing design with 3 points in 2D\n    X = np.array([[1.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n    # Propose a new point\n    new_point = propose_mmphi_intensive_minimizing_point(X, n_candidates=500, q=2, p=2, seed=42)\n    print(new_point)\n    # plot the existing points and the new proposed point\n    import matplotlib.pyplot as plt\n    plt.scatter(X[:, 0], X[:, 1], color='blue', label='Existing Points')\n    plt.scatter(new_point[0, 0], new_point[0, 1], color='red', label='Proposed Point')\n    plt.legend()\n    # add grid and labels\n    plt.grid()\n    plt.title('MM-PHI Proposed Point')\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.show()",
    "crumbs": [
      "API Reference",
      "Sampling",
      "propose_mmphi_intensive_minimizing_point"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.propose_mmphi_intensive_minimizing_point.html#parameters",
    "href": "docs/reference/sampling.mm.propose_mmphi_intensive_minimizing_point.html#parameters",
    "title": "sampling.mm.propose_mmphi_intensive_minimizing_point",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nExisting points, shape (n_points, n_dim).\nrequired\n\n\nn_candidates\nint\nNumber of random candidates to sample.\n1000\n\n\nq\nfloat\nExponent for mmphi_intensive.\n2.0\n\n\np\nfloat\nDistance norm for mmphi_intensive.\n2.0\n\n\nseed\nint\nRandom seed.\nNone\n\n\nlower\nnp.ndarray\nLower bounds for each dimension (default: 0).\nNone\n\n\nupper\nnp.ndarray\nUpper bounds for each dimension (default: 1).\nNone\n\n\nnormalize_flag\nbool\nIf True, normalizes the X array and candidate points before computing distances. Defaults to False.\nFalse",
    "crumbs": [
      "API Reference",
      "Sampling",
      "propose_mmphi_intensive_minimizing_point"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.propose_mmphi_intensive_minimizing_point.html#returns",
    "href": "docs/reference/sampling.mm.propose_mmphi_intensive_minimizing_point.html#returns",
    "title": "sampling.mm.propose_mmphi_intensive_minimizing_point",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Proposed new point, shape (1, n_dim).",
    "crumbs": [
      "API Reference",
      "Sampling",
      "propose_mmphi_intensive_minimizing_point"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.propose_mmphi_intensive_minimizing_point.html#examples",
    "href": "docs/reference/sampling.mm.propose_mmphi_intensive_minimizing_point.html#examples",
    "title": "sampling.mm.propose_mmphi_intensive_minimizing_point",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n    from spotoptim.sampling.mm import propose_mmphi_intensive_minimizing_point\n    # Existing design with 3 points in 2D\n    X = np.array([[1.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n    # Propose a new point\n    new_point = propose_mmphi_intensive_minimizing_point(X, n_candidates=500, q=2, p=2, seed=42)\n    print(new_point)\n    # plot the existing points and the new proposed point\n    import matplotlib.pyplot as plt\n    plt.scatter(X[:, 0], X[:, 1], color='blue', label='Existing Points')\n    plt.scatter(new_point[0, 0], new_point[0, 1], color='red', label='Proposed Point')\n    plt.legend()\n    # add grid and labels\n    plt.grid()\n    plt.title('MM-PHI Proposed Point')\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.show()",
    "crumbs": [
      "API Reference",
      "Sampling",
      "propose_mmphi_intensive_minimizing_point"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.plot_mmphi_vs_n_lhs.html",
    "href": "docs/reference/sampling.mm.plot_mmphi_vs_n_lhs.html",
    "title": "sampling.mm.plot_mmphi_vs_n_lhs",
    "section": "",
    "text": "sampling.mm.plot_mmphi_vs_n_lhs(\n    k_dim,\n    seed,\n    n_min=10,\n    n_max=100,\n    n_step=5,\n    q_phi=2.0,\n    p_phi=2.0,\n)\nGenerates LHS designs for varying n, calculates mmphi and mmphi_intensive, and plots them against the number of samples (n).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nk_dim\nint\nNumber of dimensions for the LHS design.\nrequired\n\n\nseed\nint\nRandom seed for reproducibility.\nrequired\n\n\nn_min\nint\nMinimum number of samples.\n10\n\n\nn_max\nint\nMaximum number of samples.\n100\n\n\nn_step\nint\nStep size for increasing n.\n5\n\n\nq_phi\nfloat\nExponent q for the Morris-Mitchell criteria.\n2.0\n\n\np_phi\nfloat\nDistance norm p for the Morris-Mitchell criteria.\n2.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nNone\nNone\nDisplays a plot of mmphi and mmphi_intensive vs. number of samples (n).\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.sampling.mm import plot_mmphi_vs_n_lhs\n&gt;&gt;&gt; plot_mmphi_vs_n_lhs(k_dim=3, seed=42, n_min=10, n_max=50, n_step=5, q_phi=2.0, p_phi=2.0)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "plot_mmphi_vs_n_lhs"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.plot_mmphi_vs_n_lhs.html#parameters",
    "href": "docs/reference/sampling.mm.plot_mmphi_vs_n_lhs.html#parameters",
    "title": "sampling.mm.plot_mmphi_vs_n_lhs",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nk_dim\nint\nNumber of dimensions for the LHS design.\nrequired\n\n\nseed\nint\nRandom seed for reproducibility.\nrequired\n\n\nn_min\nint\nMinimum number of samples.\n10\n\n\nn_max\nint\nMaximum number of samples.\n100\n\n\nn_step\nint\nStep size for increasing n.\n5\n\n\nq_phi\nfloat\nExponent q for the Morris-Mitchell criteria.\n2.0\n\n\np_phi\nfloat\nDistance norm p for the Morris-Mitchell criteria.\n2.0",
    "crumbs": [
      "API Reference",
      "Sampling",
      "plot_mmphi_vs_n_lhs"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.plot_mmphi_vs_n_lhs.html#returns",
    "href": "docs/reference/sampling.mm.plot_mmphi_vs_n_lhs.html#returns",
    "title": "sampling.mm.plot_mmphi_vs_n_lhs",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nNone\nNone\nDisplays a plot of mmphi and mmphi_intensive vs. number of samples (n).",
    "crumbs": [
      "API Reference",
      "Sampling",
      "plot_mmphi_vs_n_lhs"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.plot_mmphi_vs_n_lhs.html#examples",
    "href": "docs/reference/sampling.mm.plot_mmphi_vs_n_lhs.html#examples",
    "title": "sampling.mm.plot_mmphi_vs_n_lhs",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.sampling.mm import plot_mmphi_vs_n_lhs\n&gt;&gt;&gt; plot_mmphi_vs_n_lhs(k_dim=3, seed=42, n_min=10, n_max=50, n_step=5, q_phi=2.0, p_phi=2.0)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "plot_mmphi_vs_n_lhs"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.perturb.html",
    "href": "docs/reference/sampling.mm.perturb.html",
    "title": "sampling.mm.perturb",
    "section": "",
    "text": "sampling.mm.perturb(X, PertNum=1)\nPerforms a specified number of random element swaps on a sampling plan. If the plan is a Latin hypercube, the result remains a valid Latin hypercube.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nA 2D array (sampling plan) of shape (n, k), where each row is a point and each column is a dimension.\nrequired\n\n\nPertNum\nint\nThe number of element swaps (perturbations) to perform. Defaults to 1.\n1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: The perturbed sampling plan, identical in shape to the input, with one or more random column swaps executed.\n\n\n\n\n\n\nMany thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import perturb\n&gt;&gt;&gt; # Create a simple 4x2 sampling plan\n&gt;&gt;&gt; X_original = np.array([\n...     [1, 3],\n...     [2, 4],\n...     [3, 1],\n...     [4, 2]\n... ])\n&gt;&gt;&gt; # Perturb it once\n&gt;&gt;&gt; X_perturbed = perturb(X_original, PertNum=1)\n&gt;&gt;&gt; print(X_perturbed)\n# The output may differ due to random swaps, but each column is still a permutation of [1,2,3,4].\n    [[1 3]\n    [2 2]\n    [3 1]\n    [4 4]]",
    "crumbs": [
      "API Reference",
      "Sampling",
      "perturb"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.perturb.html#parameters",
    "href": "docs/reference/sampling.mm.perturb.html#parameters",
    "title": "sampling.mm.perturb",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nA 2D array (sampling plan) of shape (n, k), where each row is a point and each column is a dimension.\nrequired\n\n\nPertNum\nint\nThe number of element swaps (perturbations) to perform. Defaults to 1.\n1",
    "crumbs": [
      "API Reference",
      "Sampling",
      "perturb"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.perturb.html#returns",
    "href": "docs/reference/sampling.mm.perturb.html#returns",
    "title": "sampling.mm.perturb",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: The perturbed sampling plan, identical in shape to the input, with one or more random column swaps executed.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "perturb"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.perturb.html#notes",
    "href": "docs/reference/sampling.mm.perturb.html#notes",
    "title": "sampling.mm.perturb",
    "section": "",
    "text": "Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”",
    "crumbs": [
      "API Reference",
      "Sampling",
      "perturb"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.perturb.html#examples",
    "href": "docs/reference/sampling.mm.perturb.html#examples",
    "title": "sampling.mm.perturb",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import perturb\n&gt;&gt;&gt; # Create a simple 4x2 sampling plan\n&gt;&gt;&gt; X_original = np.array([\n...     [1, 3],\n...     [2, 4],\n...     [3, 1],\n...     [4, 2]\n... ])\n&gt;&gt;&gt; # Perturb it once\n&gt;&gt;&gt; X_perturbed = perturb(X_original, PertNum=1)\n&gt;&gt;&gt; print(X_perturbed)\n# The output may differ due to random swaps, but each column is still a permutation of [1,2,3,4].\n    [[1 3]\n    [2 2]\n    [3 1]\n    [4 4]]",
    "crumbs": [
      "API Reference",
      "Sampling",
      "perturb"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmphi_intensive_update.html",
    "href": "docs/reference/sampling.mm.mmphi_intensive_update.html",
    "title": "sampling.mm.mmphi_intensive_update",
    "section": "",
    "text": "sampling.mm.mmphi_intensive_update(\n    X,\n    new_point,\n    J,\n    d,\n    q=2.0,\n    p=2.0,\n    normalize_flag=False,\n)\nUpdates the Morris-Mitchell intensive criterion for n+1 points by adding a new point to the design. This should be more efficient than recalculating the metric from scratch, because it only needs to compute the distances between the new point and the existing points.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nExisting sampling plan (shape: (n, d)).\nrequired\n\n\nnew_point\nnp.ndarray\nNew point to add (shape: (d,)).\nrequired\n\n\nJ\nnp.ndarray\nMultiplicities of distances for the existing design.\nrequired\n\n\nd\nnp.ndarray\nUnique distances for the existing design.\nrequired\n\n\nq\nfloat\nExponent used in the computation of the Morris-Mitchell metric. Defaults to 2.0.\n2.0\n\n\np\nfloat\nDistance norm to use (e.g., p=1 for Manhattan, p=2 for Euclidean). Defaults to 2.0.\n2.0\n\n\nnormalize_flag\nbool\nIf True, normalizes the X array and the new_point before computing distances. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[float, np.ndarray, np.ndarray]\ntuple[float, np.ndarray, np.ndarray]: Updated intensive_phiq, updated_J, updated_d.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive_update\n&gt;&gt;&gt; # Existing design with 3 points in 2D\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; phiq, J, d = mmphi_intensive(X, q=2, p=2)\n&gt;&gt;&gt; # New point to add\n&gt;&gt;&gt; new_point = np.array([0.1, 0.1])\n&gt;&gt;&gt; # Update the intensive criterion\n&gt;&gt;&gt; updated_phiq, updated_J, updated_d = mmphi_intensive_update(X, new_point, J, d, q=2, p=2)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmphi_intensive_update"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmphi_intensive_update.html#parameters",
    "href": "docs/reference/sampling.mm.mmphi_intensive_update.html#parameters",
    "title": "sampling.mm.mmphi_intensive_update",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nExisting sampling plan (shape: (n, d)).\nrequired\n\n\nnew_point\nnp.ndarray\nNew point to add (shape: (d,)).\nrequired\n\n\nJ\nnp.ndarray\nMultiplicities of distances for the existing design.\nrequired\n\n\nd\nnp.ndarray\nUnique distances for the existing design.\nrequired\n\n\nq\nfloat\nExponent used in the computation of the Morris-Mitchell metric. Defaults to 2.0.\n2.0\n\n\np\nfloat\nDistance norm to use (e.g., p=1 for Manhattan, p=2 for Euclidean). Defaults to 2.0.\n2.0\n\n\nnormalize_flag\nbool\nIf True, normalizes the X array and the new_point before computing distances. Defaults to False.\nFalse",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmphi_intensive_update"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmphi_intensive_update.html#returns",
    "href": "docs/reference/sampling.mm.mmphi_intensive_update.html#returns",
    "title": "sampling.mm.mmphi_intensive_update",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\ntuple[float, np.ndarray, np.ndarray]\ntuple[float, np.ndarray, np.ndarray]: Updated intensive_phiq, updated_J, updated_d.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmphi_intensive_update"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmphi_intensive_update.html#examples",
    "href": "docs/reference/sampling.mm.mmphi_intensive_update.html#examples",
    "title": "sampling.mm.mmphi_intensive_update",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive_update\n&gt;&gt;&gt; # Existing design with 3 points in 2D\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; phiq, J, d = mmphi_intensive(X, q=2, p=2)\n&gt;&gt;&gt; # New point to add\n&gt;&gt;&gt; new_point = np.array([0.1, 0.1])\n&gt;&gt;&gt; # Update the intensive criterion\n&gt;&gt;&gt; updated_phiq, updated_J, updated_d = mmphi_intensive_update(X, new_point, J, d, q=2, p=2)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmphi_intensive_update"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmphi.html",
    "href": "docs/reference/sampling.mm.mmphi.html",
    "title": "sampling.mm.mmphi",
    "section": "",
    "text": "sampling.mm.mmphi(X, q=2.0, p=1.0, verbosity=0)\nCalculates the Morris-Mitchell sampling plan quality criterion.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nA 2D array representing the sampling plan, where each row is a point in d-dimensional space (shape: (n, d)).\nrequired\n\n\nq\nfloat\nExponent used in the computation of the metric. Defaults to 2.0.\n2.0\n\n\np\nfloat\nThe distance norm to use. For example, p=1 is Manhattan (L1), p=2 is Euclidean (L2). Defaults to 1.0.\n1.0\n\n\nverbosity\nint\nIf set to 1, prints additional information about the computation. Defaults to 0 (no additional output).\n0\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nfloat\nfloat\nThe space-fillingness metric Phiq. Larger values typically indicate a more space-filling plan according to the Morris-Mitchell criterion.\n\n\n\n\n\n\nMany thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi\n&gt;&gt;&gt; # Simple 3-point sampling plan in 2D\n&gt;&gt;&gt; X = np.array([\n...     [0.0, 0.0],\n...     [0.5, 0.5],\n...     [1.0, 1.0]\n... ])\n&gt;&gt;&gt; # Calculate the space-fillingness metric with q=2, using Euclidean distances (p=2)\n&gt;&gt;&gt; quality = mmphi(X, q=2, p=2)\n&gt;&gt;&gt; print(quality)\n# This value indicates how well points are spread out, with smaller being better.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmphi"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmphi.html#parameters",
    "href": "docs/reference/sampling.mm.mmphi.html#parameters",
    "title": "sampling.mm.mmphi",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nA 2D array representing the sampling plan, where each row is a point in d-dimensional space (shape: (n, d)).\nrequired\n\n\nq\nfloat\nExponent used in the computation of the metric. Defaults to 2.0.\n2.0\n\n\np\nfloat\nThe distance norm to use. For example, p=1 is Manhattan (L1), p=2 is Euclidean (L2). Defaults to 1.0.\n1.0\n\n\nverbosity\nint\nIf set to 1, prints additional information about the computation. Defaults to 0 (no additional output).\n0",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmphi"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmphi.html#returns",
    "href": "docs/reference/sampling.mm.mmphi.html#returns",
    "title": "sampling.mm.mmphi",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nfloat\nfloat\nThe space-fillingness metric Phiq. Larger values typically indicate a more space-filling plan according to the Morris-Mitchell criterion.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmphi"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmphi.html#notes",
    "href": "docs/reference/sampling.mm.mmphi.html#notes",
    "title": "sampling.mm.mmphi",
    "section": "",
    "text": "Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmphi"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmphi.html#examples",
    "href": "docs/reference/sampling.mm.mmphi.html#examples",
    "title": "sampling.mm.mmphi",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi\n&gt;&gt;&gt; # Simple 3-point sampling plan in 2D\n&gt;&gt;&gt; X = np.array([\n...     [0.0, 0.0],\n...     [0.5, 0.5],\n...     [1.0, 1.0]\n... ])\n&gt;&gt;&gt; # Calculate the space-fillingness metric with q=2, using Euclidean distances (p=2)\n&gt;&gt;&gt; quality = mmphi(X, q=2, p=2)\n&gt;&gt;&gt; print(quality)\n# This value indicates how well points are spread out, with smaller being better.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmphi"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mm_improvement_contour.html",
    "href": "docs/reference/sampling.mm.mm_improvement_contour.html",
    "title": "sampling.mm.mm_improvement_contour",
    "section": "",
    "text": "sampling.mm.mm_improvement_contour(\n    X_base,\n    x1=np.linspace(0, 1, 100),\n    x2=np.linspace(0, 1, 100),\n    q=2,\n    p=2,\n)\nGenerates a contour plot of the Morris-Mitchell improvement over a grid defined by x1 and x2.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX_base\nnp.ndarray\nBase design points.\nrequired\n\n\nx1\nnp.ndarray\nGrid values for the first dimension. Default is np.linspace(0, 1, 100).\nnp.linspace(0, 1, 100)\n\n\nx2\nnp.ndarray\nGrid values for the second dimension. Default is np.linspace(0, 1, 100).\nnp.linspace(0, 1, 100)\n\n\nq\nint\nMorris-Mitchell metric parameter. Default is 2.\n2\n\n\np\nint\nMorris-Mitchell metric parameter. Default is 2.\n2\n\n\n\nReturns: None: Displays a contour plot of the Morris-Mitchell improvement.\n\n\n\n&gt;&gt;&gt; import numpy as np\n    from spotoptim.sampling.mm import mm_improvement_contour\n    X_base = np.array([[0.1, 0.1], [0.2, 0.2], [0.7, 0.7]])\n    mm_improvement_contour(X_base)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mm_improvement_contour"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mm_improvement_contour.html#parameters",
    "href": "docs/reference/sampling.mm.mm_improvement_contour.html#parameters",
    "title": "sampling.mm.mm_improvement_contour",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX_base\nnp.ndarray\nBase design points.\nrequired\n\n\nx1\nnp.ndarray\nGrid values for the first dimension. Default is np.linspace(0, 1, 100).\nnp.linspace(0, 1, 100)\n\n\nx2\nnp.ndarray\nGrid values for the second dimension. Default is np.linspace(0, 1, 100).\nnp.linspace(0, 1, 100)\n\n\nq\nint\nMorris-Mitchell metric parameter. Default is 2.\n2\n\n\np\nint\nMorris-Mitchell metric parameter. Default is 2.\n2\n\n\n\nReturns: None: Displays a contour plot of the Morris-Mitchell improvement.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mm_improvement_contour"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mm_improvement_contour.html#examples",
    "href": "docs/reference/sampling.mm.mm_improvement_contour.html#examples",
    "title": "sampling.mm.mm_improvement_contour",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n    from spotoptim.sampling.mm import mm_improvement_contour\n    X_base = np.array([[0.1, 0.1], [0.2, 0.2], [0.7, 0.7]])\n    mm_improvement_contour(X_base)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mm_improvement_contour"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mm.html",
    "href": "docs/reference/sampling.mm.mm.html",
    "title": "sampling.mm.mm",
    "section": "",
    "text": "sampling.mm.mm(X1, X2, p=1.0)\nDetermines which of two sampling plans has better space-filling properties according to the Morris-Mitchell criterion.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX1\nnp.ndarray\nA 2D array representing the first sampling plan.\nrequired\n\n\nX2\nnp.ndarray\nA 2D array representing the second sampling plan.\nrequired\n\n\np\nfloat\nThe distance metric. p=1 uses Manhattan (L1) distance, while p=2 uses Euclidean (L2). Defaults to 1.0.\n1.0\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nint\nint\n- 0 if both plans are identical or equally space-filling - 1 if X1 is more space-filling - 2 if X2 is more space-filling\n\n\n\n\n\n\nMany thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mm\n&gt;&gt;&gt; # Create two 3-point sampling plans in 2D\n&gt;&gt;&gt; X1 = np.array([[0.0, 0.0],\n...                [0.5, 0.5],\n...                [0.0, 1.0]])\n&gt;&gt;&gt; X2 = np.array([[0.1, 0.1],\n...                [0.4, 0.6],\n...                [0.1, 0.9]])\n&gt;&gt;&gt; # Compare which plan has better space-filling (Morris-Mitchell)\n&gt;&gt;&gt; better = mm(X1, X2, p=2.0)\n&gt;&gt;&gt; print(better)\n# Prints either 0, 1, or 2 depending on which plan is more space-filling.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mm"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mm.html#parameters",
    "href": "docs/reference/sampling.mm.mm.html#parameters",
    "title": "sampling.mm.mm",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX1\nnp.ndarray\nA 2D array representing the first sampling plan.\nrequired\n\n\nX2\nnp.ndarray\nA 2D array representing the second sampling plan.\nrequired\n\n\np\nfloat\nThe distance metric. p=1 uses Manhattan (L1) distance, while p=2 uses Euclidean (L2). Defaults to 1.0.\n1.0",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mm"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mm.html#returns",
    "href": "docs/reference/sampling.mm.mm.html#returns",
    "title": "sampling.mm.mm",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nint\nint\n- 0 if both plans are identical or equally space-filling - 1 if X1 is more space-filling - 2 if X2 is more space-filling",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mm"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mm.html#notes",
    "href": "docs/reference/sampling.mm.mm.html#notes",
    "title": "sampling.mm.mm",
    "section": "",
    "text": "Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mm"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mm.html#examples",
    "href": "docs/reference/sampling.mm.mm.html#examples",
    "title": "sampling.mm.mm",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mm\n&gt;&gt;&gt; # Create two 3-point sampling plans in 2D\n&gt;&gt;&gt; X1 = np.array([[0.0, 0.0],\n...                [0.5, 0.5],\n...                [0.0, 1.0]])\n&gt;&gt;&gt; X2 = np.array([[0.1, 0.1],\n...                [0.4, 0.6],\n...                [0.1, 0.9]])\n&gt;&gt;&gt; # Compare which plan has better space-filling (Morris-Mitchell)\n&gt;&gt;&gt; better = mm(X1, X2, p=2.0)\n&gt;&gt;&gt; print(better)\n# Prints either 0, 1, or 2 depending on which plan is more space-filling.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mm"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.bestlh.html",
    "href": "docs/reference/sampling.mm.bestlh.html",
    "title": "sampling.mm.bestlh",
    "section": "",
    "text": "sampling.mm.bestlh(\n    n,\n    k,\n    population,\n    iterations,\n    p=1,\n    plot=False,\n    verbosity=0,\n    edges=0,\n    q_list=[1, 2, 5, 10, 20, 50, 100],\n)\nGenerates an optimized Latin hypercube by evolving the Morris-Mitchell criterion across multiple exponents (q values) and selecting the best plan.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of points required in the Latin hypercube.\nrequired\n\n\nk\nint\nNumber of design variables (dimensions).\nrequired\n\n\npopulation\nint\nNumber of offspring in each generation of the evolutionary search.\nrequired\n\n\niterations\nint\nNumber of generations for the evolutionary search.\nrequired\n\n\np\nint\nThe distance norm to use. p=1 for Manhattan (L1), p=2 for Euclidean (L2). Defaults to 1 (faster than 2).\n1\n\n\nplot\nbool\nIf True, a scatter plot of the optimized plan in the first two dimensions will be displayed. Only if k&gt;=2. Defaults to False.\nFalse\n\n\nverbosity\nint\nVerbosity level. 0 is silent, 1 prints the best q value found. Defaults to 0.\n0\n\n\nedges\nint\nIf 1, places centers of the extreme bins at the domain edges ([0,1]). Otherwise, bins are fully contained within the domain, i.e. midpoints. Defaults to 0.\n0\n\n\nq_list\nlist\nA list of q values to optimize. Defaults to [1, 2, 5, 10, 20, 50, 100]. These values are used to evaluate the space-fillingness of the Latin hypercube. The best plan is selected based on the lowest mmphi value.\n[1, 2, 5, 10, 20, 50, 100]\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 2D array of shape (n, k) representing an optimized Latin hypercube.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import bestlh\n# Generate a 5-point, 2-dimensional Latin hypercube\n&gt;&gt;&gt; X = bestlh(n=5, k=2, population=5, iterations=10)\n&gt;&gt;&gt; print(X.shape)\n(5, 2)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "bestlh"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.bestlh.html#parameters",
    "href": "docs/reference/sampling.mm.bestlh.html#parameters",
    "title": "sampling.mm.bestlh",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of points required in the Latin hypercube.\nrequired\n\n\nk\nint\nNumber of design variables (dimensions).\nrequired\n\n\npopulation\nint\nNumber of offspring in each generation of the evolutionary search.\nrequired\n\n\niterations\nint\nNumber of generations for the evolutionary search.\nrequired\n\n\np\nint\nThe distance norm to use. p=1 for Manhattan (L1), p=2 for Euclidean (L2). Defaults to 1 (faster than 2).\n1\n\n\nplot\nbool\nIf True, a scatter plot of the optimized plan in the first two dimensions will be displayed. Only if k&gt;=2. Defaults to False.\nFalse\n\n\nverbosity\nint\nVerbosity level. 0 is silent, 1 prints the best q value found. Defaults to 0.\n0\n\n\nedges\nint\nIf 1, places centers of the extreme bins at the domain edges ([0,1]). Otherwise, bins are fully contained within the domain, i.e. midpoints. Defaults to 0.\n0\n\n\nq_list\nlist\nA list of q values to optimize. Defaults to [1, 2, 5, 10, 20, 50, 100]. These values are used to evaluate the space-fillingness of the Latin hypercube. The best plan is selected based on the lowest mmphi value.\n[1, 2, 5, 10, 20, 50, 100]",
    "crumbs": [
      "API Reference",
      "Sampling",
      "bestlh"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.bestlh.html#returns",
    "href": "docs/reference/sampling.mm.bestlh.html#returns",
    "title": "sampling.mm.bestlh",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 2D array of shape (n, k) representing an optimized Latin hypercube.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "bestlh"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.bestlh.html#examples",
    "href": "docs/reference/sampling.mm.bestlh.html#examples",
    "title": "sampling.mm.bestlh",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import bestlh\n# Generate a 5-point, 2-dimensional Latin hypercube\n&gt;&gt;&gt; X = bestlh(n=5, k=2, population=5, iterations=10)\n&gt;&gt;&gt; print(X.shape)\n(5, 2)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "bestlh"
    ]
  },
  {
    "objectID": "docs/reference/sampling.lhs.html",
    "href": "docs/reference/sampling.lhs.html",
    "title": "sampling.lhs",
    "section": "",
    "text": "sampling.lhs\n\n\n\n\n\nName\nDescription\n\n\n\n\nrlh\nGenerates a random Latin hypercube within the [0,1]^k hypercube.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "sampling.lhs"
    ]
  },
  {
    "objectID": "docs/reference/sampling.lhs.html#functions",
    "href": "docs/reference/sampling.lhs.html#functions",
    "title": "sampling.lhs",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nrlh\nGenerates a random Latin hypercube within the [0,1]^k hypercube.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "sampling.lhs"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.screening_print.html",
    "href": "docs/reference/sampling.effects.screening_print.html",
    "title": "sampling.effects.screening_print",
    "section": "",
    "text": "sampling.effects.screening_print(X, fun, xi, p, labels, bounds=None)\nGenerates a DataFrame with elementary effect screening metrics.\nThis function calculates the mean and standard deviation of the elementary effects for a given set of design variables and returns the results as a Pandas DataFrame.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nThe screening plan matrix, typically structured within a [0,1]^k box.\nrequired\n\n\nfun\nobject\nThe objective function to evaluate at each design point in the screening plan.\nrequired\n\n\nxi\nfloat\nThe elementary effect step length factor.\nrequired\n\n\np\nint\nNumber of discrete levels along each dimension.\nrequired\n\n\nlabels\nlist of str\nA list of variable names corresponding to the design variables.\nrequired\n\n\nbounds\nnp.ndarray\nA 2xk matrix where the first row contains lower bounds and the second row contains upper bounds for each variable.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: A DataFrame containing three columns: - ‘varname’: The name of each variable. - ‘mean’: The mean of the elementary effects for each variable. - ‘sd’: The standard deviation of the elementary effects for each variable.\n\n\n\npd.DataFrame\nor None: If print is set to False, a plot of the results is generated instead of returning a DataFrame.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.effects import screening, screeningplan\n    from spotpython.fun.objectivefunctions import Analytical\n    fun = Analytical()\n    k = 10\n    p = 10\n    xi = 1\n    r = 25\n    X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n    # Provide real-world bounds from the wing weight docs (2 x 10).\n    value_range = np.array([\n        [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n        [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n    ])\n    labels = [\n        \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n        \"q\",   \"lambda\", \"tc\", \"N_z\",\n        \"W_dg\", \"W_p\"\n    ]\n    screening(\n        X=X,\n        fun=fun.fun_wingwt,\n        bounds=value_range,\n        xi=xi,\n        p=p,\n        labels=labels,\n        print=False,\n    )",
    "crumbs": [
      "API Reference",
      "Sampling",
      "screening_print"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.screening_print.html#parameters",
    "href": "docs/reference/sampling.effects.screening_print.html#parameters",
    "title": "sampling.effects.screening_print",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nThe screening plan matrix, typically structured within a [0,1]^k box.\nrequired\n\n\nfun\nobject\nThe objective function to evaluate at each design point in the screening plan.\nrequired\n\n\nxi\nfloat\nThe elementary effect step length factor.\nrequired\n\n\np\nint\nNumber of discrete levels along each dimension.\nrequired\n\n\nlabels\nlist of str\nA list of variable names corresponding to the design variables.\nrequired\n\n\nbounds\nnp.ndarray\nA 2xk matrix where the first row contains lower bounds and the second row contains upper bounds for each variable.\nNone",
    "crumbs": [
      "API Reference",
      "Sampling",
      "screening_print"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.screening_print.html#returns",
    "href": "docs/reference/sampling.effects.screening_print.html#returns",
    "title": "sampling.effects.screening_print",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: A DataFrame containing three columns: - ‘varname’: The name of each variable. - ‘mean’: The mean of the elementary effects for each variable. - ‘sd’: The standard deviation of the elementary effects for each variable.\n\n\n\npd.DataFrame\nor None: If print is set to False, a plot of the results is generated instead of returning a DataFrame.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "screening_print"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.screening_print.html#examples",
    "href": "docs/reference/sampling.effects.screening_print.html#examples",
    "title": "sampling.effects.screening_print",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.effects import screening, screeningplan\n    from spotpython.fun.objectivefunctions import Analytical\n    fun = Analytical()\n    k = 10\n    p = 10\n    xi = 1\n    r = 25\n    X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n    # Provide real-world bounds from the wing weight docs (2 x 10).\n    value_range = np.array([\n        [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n        [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n    ])\n    labels = [\n        \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n        \"q\",   \"lambda\", \"tc\", \"N_z\",\n        \"W_dg\", \"W_p\"\n    ]\n    screening(\n        X=X,\n        fun=fun.fun_wingwt,\n        bounds=value_range,\n        xi=xi,\n        p=p,\n        labels=labels,\n        print=False,\n    )",
    "crumbs": [
      "API Reference",
      "Sampling",
      "screening_print"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.randorient.html",
    "href": "docs/reference/sampling.effects.randorient.html",
    "title": "sampling.effects.randorient",
    "section": "",
    "text": "sampling.effects.randorient(k, p, xi, seed=None)\nGenerates a random orientation of a sampling matrix. This function creates a random sampling matrix for a given number of dimensions (k), number of levels (p), and step length (xi). The resulting matrix is used for screening designs in the context of experimental design.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nk\nint\nNumber of dimensions.\nrequired\n\n\np\nint\nNumber of levels.\nrequired\n\n\nxi\nfloat\nStep length.\nrequired\n\n\nseed\nint\nSeed for the random number generator. Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A random sampling matrix of shape (k+1, k).\n\n\n\n\n\n\n\n\n\nrandorient(k=2, p=3, xi=0.5) array([[0. , 0. ], [0.5, 0.5], [1. , 1. ]])",
    "crumbs": [
      "API Reference",
      "Sampling",
      "randorient"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.randorient.html#parameters",
    "href": "docs/reference/sampling.effects.randorient.html#parameters",
    "title": "sampling.effects.randorient",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nk\nint\nNumber of dimensions.\nrequired\n\n\np\nint\nNumber of levels.\nrequired\n\n\nxi\nfloat\nStep length.\nrequired\n\n\nseed\nint\nSeed for the random number generator. Defaults to None.\nNone",
    "crumbs": [
      "API Reference",
      "Sampling",
      "randorient"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.randorient.html#returns",
    "href": "docs/reference/sampling.effects.randorient.html#returns",
    "title": "sampling.effects.randorient",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A random sampling matrix of shape (k+1, k).",
    "crumbs": [
      "API Reference",
      "Sampling",
      "randorient"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.randorient.html#example",
    "href": "docs/reference/sampling.effects.randorient.html#example",
    "title": "sampling.effects.randorient",
    "section": "",
    "text": "randorient(k=2, p=3, xi=0.5) array([[0. , 0. ], [0.5, 0.5], [1. , 1. ]])",
    "crumbs": [
      "API Reference",
      "Sampling",
      "randorient"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.plot_all_partial_dependence.html",
    "href": "docs/reference/sampling.effects.plot_all_partial_dependence.html",
    "title": "sampling.effects.plot_all_partial_dependence",
    "section": "",
    "text": "sampling.effects.plot_all_partial_dependence(\n    df,\n    df_target,\n    model='GradientBoostingRegressor',\n    nrows=5,\n    ncols=6,\n    figsize=(20, 15),\n    title='',\n)\nGenerates Partial Dependence Plots (PDPs) for every feature in a DataFrame against a target variable, arranged in a grid.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nDataFrame containing the features.\nrequired\n\n\ndf_target\npd.Series\nSeries containing the target variable.\nrequired\n\n\nmodel\nstr\nName of the model class to use (e.g., “GradientBoostingRegressor”). Defaults to “GradientBoostingRegressor”.\n'GradientBoostingRegressor'\n\n\nnrows\nint\nNumber of rows in the grid of subplots. Defaults to 5.\n5\n\n\nncols\nint\nNumber of columns in the grid of subplots. Defaults to 6.\n6\n\n\nfigsize\ntuple\nFigure size (width, height) in inches. Defaults to (20, 15).\n(20, 15)\n\n\ntitle\nstr\nTitle for the subplots. Defaults to ““.\n''\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; form spotpython.utils.effects import plot_all_partial_dependence\n&gt;&gt;&gt; from sklearn.datasets import load_boston\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = load_boston()\n&gt;&gt;&gt; df = pd.DataFrame(data.data, columns=data.feature_names)\n&gt;&gt;&gt; df_target = pd.Series(data.target, name=\"target\")\n&gt;&gt;&gt; plot_all_partial_dependence(df, df_target, model=\"GradientBoostingRegressor\", nrows=5, ncols=6, figsize=(20, 15))",
    "crumbs": [
      "API Reference",
      "Sampling",
      "plot_all_partial_dependence"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.plot_all_partial_dependence.html#parameters",
    "href": "docs/reference/sampling.effects.plot_all_partial_dependence.html#parameters",
    "title": "sampling.effects.plot_all_partial_dependence",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nDataFrame containing the features.\nrequired\n\n\ndf_target\npd.Series\nSeries containing the target variable.\nrequired\n\n\nmodel\nstr\nName of the model class to use (e.g., “GradientBoostingRegressor”). Defaults to “GradientBoostingRegressor”.\n'GradientBoostingRegressor'\n\n\nnrows\nint\nNumber of rows in the grid of subplots. Defaults to 5.\n5\n\n\nncols\nint\nNumber of columns in the grid of subplots. Defaults to 6.\n6\n\n\nfigsize\ntuple\nFigure size (width, height) in inches. Defaults to (20, 15).\n(20, 15)\n\n\ntitle\nstr\nTitle for the subplots. Defaults to ““.\n''",
    "crumbs": [
      "API Reference",
      "Sampling",
      "plot_all_partial_dependence"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.plot_all_partial_dependence.html#returns",
    "href": "docs/reference/sampling.effects.plot_all_partial_dependence.html#returns",
    "title": "sampling.effects.plot_all_partial_dependence",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "API Reference",
      "Sampling",
      "plot_all_partial_dependence"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.plot_all_partial_dependence.html#examples",
    "href": "docs/reference/sampling.effects.plot_all_partial_dependence.html#examples",
    "title": "sampling.effects.plot_all_partial_dependence",
    "section": "",
    "text": "&gt;&gt;&gt; form spotpython.utils.effects import plot_all_partial_dependence\n&gt;&gt;&gt; from sklearn.datasets import load_boston\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = load_boston()\n&gt;&gt;&gt; df = pd.DataFrame(data.data, columns=data.feature_names)\n&gt;&gt;&gt; df_target = pd.Series(data.target, name=\"target\")\n&gt;&gt;&gt; plot_all_partial_dependence(df, df_target, model=\"GradientBoostingRegressor\", nrows=5, ncols=6, figsize=(20, 15))",
    "crumbs": [
      "API Reference",
      "Sampling",
      "plot_all_partial_dependence"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_uniform_design.html",
    "href": "docs/reference/sampling.design.generate_uniform_design.html",
    "title": "sampling.design.generate_uniform_design",
    "section": "",
    "text": "sampling.design.generate_uniform_design(bounds, n_design, seed=None)\nGenerate a uniform random experimental design.\nGenerates n_design points uniformly distributed within the specified bounds. This function is compatible with SpotOptim’s random number handling.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbounds\nUnion[List[Tuple[float, float]], np.ndarray]\nDesign space bounds. List of (lower, upper) tuples for each dimension.\nrequired\n\n\nn_design\nint\nNumber of design points to generate.\nrequired\n\n\nseed\nOptional[Union[int, Generator]]\nRandom seed or generator. Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Generated design points of shape (n_design, n_dim).\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_uniform_design\n&gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n&gt;&gt;&gt; X = generate_uniform_design(bounds, n_design=5, seed=42)\n&gt;&gt;&gt; X.shape\n(5, 2)\n&gt;&gt;&gt; np.all((X &gt;= [-5, 0]) & (X &lt;= [5, 10]))\nTrue",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_uniform_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_uniform_design.html#parameters",
    "href": "docs/reference/sampling.design.generate_uniform_design.html#parameters",
    "title": "sampling.design.generate_uniform_design",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbounds\nUnion[List[Tuple[float, float]], np.ndarray]\nDesign space bounds. List of (lower, upper) tuples for each dimension.\nrequired\n\n\nn_design\nint\nNumber of design points to generate.\nrequired\n\n\nseed\nOptional[Union[int, Generator]]\nRandom seed or generator. Defaults to None.\nNone",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_uniform_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_uniform_design.html#returns",
    "href": "docs/reference/sampling.design.generate_uniform_design.html#returns",
    "title": "sampling.design.generate_uniform_design",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Generated design points of shape (n_design, n_dim).",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_uniform_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_uniform_design.html#examples",
    "href": "docs/reference/sampling.design.generate_uniform_design.html#examples",
    "title": "sampling.design.generate_uniform_design",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_uniform_design\n&gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n&gt;&gt;&gt; X = generate_uniform_design(bounds, n_design=5, seed=42)\n&gt;&gt;&gt; X.shape\n(5, 2)\n&gt;&gt;&gt; np.all((X &gt;= [-5, 0]) & (X &lt;= [5, 10]))\nTrue",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_uniform_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_qmc_lhs_design.html",
    "href": "docs/reference/sampling.design.generate_qmc_lhs_design.html",
    "title": "sampling.design.generate_qmc_lhs_design",
    "section": "",
    "text": "sampling.design.generate_qmc_lhs_design(bounds, n_design, seed=None)\nGenerates a Latin Hypercube Sampling design using QMC.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbounds\nUnion[List[Tuple[float, float]], np.ndarray]\nDesign space bounds.\nrequired\n\n\nn_design\nint\nThe number of points to generate.\nrequired\n\n\nseed\nOptional[Union[int, Generator]]\nRandom seed or generator.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: An array of shape (n_design, n_dim) containing the generated LHS points.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_qmc_lhs_design\n&gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n&gt;&gt;&gt; X = generate_qmc_lhs_design(bounds, n_design=5, seed=42)\n&gt;&gt;&gt; X.shape\n(5, 2)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_qmc_lhs_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_qmc_lhs_design.html#parameters",
    "href": "docs/reference/sampling.design.generate_qmc_lhs_design.html#parameters",
    "title": "sampling.design.generate_qmc_lhs_design",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbounds\nUnion[List[Tuple[float, float]], np.ndarray]\nDesign space bounds.\nrequired\n\n\nn_design\nint\nThe number of points to generate.\nrequired\n\n\nseed\nOptional[Union[int, Generator]]\nRandom seed or generator.\nNone",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_qmc_lhs_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_qmc_lhs_design.html#returns",
    "href": "docs/reference/sampling.design.generate_qmc_lhs_design.html#returns",
    "title": "sampling.design.generate_qmc_lhs_design",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: An array of shape (n_design, n_dim) containing the generated LHS points.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_qmc_lhs_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_qmc_lhs_design.html#examples",
    "href": "docs/reference/sampling.design.generate_qmc_lhs_design.html#examples",
    "title": "sampling.design.generate_qmc_lhs_design",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_qmc_lhs_design\n&gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n&gt;&gt;&gt; X = generate_qmc_lhs_design(bounds, n_design=5, seed=42)\n&gt;&gt;&gt; X.shape\n(5, 2)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_qmc_lhs_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_collinear_design.html",
    "href": "docs/reference/sampling.design.generate_collinear_design.html",
    "title": "sampling.design.generate_collinear_design",
    "section": "",
    "text": "sampling.design.generate_collinear_design(\n    bounds,\n    n_design,\n    sigma=0.01,\n    seed=None,\n)\nGenerates a collinear design (poorly projected).\nCurrently implemented for 2D designs only. Generates points along a line with some Gaussian noise. The points are scaled to the provided bounds.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbounds\nUnion[List[Tuple[float, float]], np.ndarray]\nDesign space bounds.\nrequired\n\n\nn_design\nint\nThe number of points to generate.\nrequired\n\n\nsigma\nfloat\nThe standard deviation of the noise added to the y-coordinate (relative to unit scale). Defaults to 0.01.\n0.01\n\n\nseed\nOptional[Union[int, Generator]]\nRandom seed or generator.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 2D array of shape (n_design, n_dim) with collinear points.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf dimension is not 2.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_collinear_design\n&gt;&gt;&gt; bounds = [(0, 1), (0, 1)]\n&gt;&gt;&gt; X = generate_collinear_design(bounds, n_design=10, seed=42)\n&gt;&gt;&gt; X.shape\n(10, 2)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_collinear_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_collinear_design.html#parameters",
    "href": "docs/reference/sampling.design.generate_collinear_design.html#parameters",
    "title": "sampling.design.generate_collinear_design",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbounds\nUnion[List[Tuple[float, float]], np.ndarray]\nDesign space bounds.\nrequired\n\n\nn_design\nint\nThe number of points to generate.\nrequired\n\n\nsigma\nfloat\nThe standard deviation of the noise added to the y-coordinate (relative to unit scale). Defaults to 0.01.\n0.01\n\n\nseed\nOptional[Union[int, Generator]]\nRandom seed or generator.\nNone",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_collinear_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_collinear_design.html#returns",
    "href": "docs/reference/sampling.design.generate_collinear_design.html#returns",
    "title": "sampling.design.generate_collinear_design",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 2D array of shape (n_design, n_dim) with collinear points.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_collinear_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_collinear_design.html#raises",
    "href": "docs/reference/sampling.design.generate_collinear_design.html#raises",
    "title": "sampling.design.generate_collinear_design",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf dimension is not 2.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_collinear_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_collinear_design.html#examples",
    "href": "docs/reference/sampling.design.generate_collinear_design.html#examples",
    "title": "sampling.design.generate_collinear_design",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_collinear_design\n&gt;&gt;&gt; bounds = [(0, 1), (0, 1)]\n&gt;&gt;&gt; X = generate_collinear_design(bounds, n_design=10, seed=42)\n&gt;&gt;&gt; X.shape\n(10, 2)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_collinear_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.fullfactorial.html",
    "href": "docs/reference/sampling.design.fullfactorial.html",
    "title": "sampling.design.fullfactorial",
    "section": "",
    "text": "sampling.design.fullfactorial(q, Edges=1)\nGenerates a full factorial sampling plan in the unit cube.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nq\nlist or np.ndarray\nA list or array containing the number of points along each dimension (k-vector).\nrequired\n\n\nEdges\nint\nDetermines spacing of points. If Edges=1, points are equally spaced from edge to edge (default). Otherwise, points will be in the centers of n = q[0]q[1]…*q[k-1] bins filling the unit cube.\n1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nFull factorial sampling plan as an array of shape (n, k), where n is the total number of points and k is the number of dimensions.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf any dimension in q is less than 2.\n\n\n\n\n\n\nMany thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”\n\n\n\n&gt;&gt;&gt; from spotpython.utils.sampling import fullfactorial\n&gt;&gt;&gt; q = [3, 2]\n&gt;&gt;&gt; X = fullfactorial(q, Edges=0)\n&gt;&gt;&gt; print(X)\n        [[0.         0.        ]\n        [0.         0.75      ]\n        [0.41666667 0.        ]\n        [0.41666667 0.75      ]\n        [0.83333333 0.        ]\n        [0.83333333 0.75      ]]\n&gt;&gt;&gt; X = fullfactorial(q, Edges=1)\n&gt;&gt;&gt; print(X)\n        [[0.  0. ]\n        [0.  1. ]\n        [0.5 0. ]\n        [0.5 1. ]\n        [1.  0. ]\n        [1.  1. ]]",
    "crumbs": [
      "API Reference",
      "Sampling",
      "fullfactorial"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.fullfactorial.html#parameters",
    "href": "docs/reference/sampling.design.fullfactorial.html#parameters",
    "title": "sampling.design.fullfactorial",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nq\nlist or np.ndarray\nA list or array containing the number of points along each dimension (k-vector).\nrequired\n\n\nEdges\nint\nDetermines spacing of points. If Edges=1, points are equally spaced from edge to edge (default). Otherwise, points will be in the centers of n = q[0]q[1]…*q[k-1] bins filling the unit cube.\n1",
    "crumbs": [
      "API Reference",
      "Sampling",
      "fullfactorial"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.fullfactorial.html#returns",
    "href": "docs/reference/sampling.design.fullfactorial.html#returns",
    "title": "sampling.design.fullfactorial",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nFull factorial sampling plan as an array of shape (n, k), where n is the total number of points and k is the number of dimensions.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "fullfactorial"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.fullfactorial.html#raises",
    "href": "docs/reference/sampling.design.fullfactorial.html#raises",
    "title": "sampling.design.fullfactorial",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf any dimension in q is less than 2.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "fullfactorial"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.fullfactorial.html#notes",
    "href": "docs/reference/sampling.design.fullfactorial.html#notes",
    "title": "sampling.design.fullfactorial",
    "section": "",
    "text": "Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”",
    "crumbs": [
      "API Reference",
      "Sampling",
      "fullfactorial"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.fullfactorial.html#examples",
    "href": "docs/reference/sampling.design.fullfactorial.html#examples",
    "title": "sampling.design.fullfactorial",
    "section": "",
    "text": "&gt;&gt;&gt; from spotpython.utils.sampling import fullfactorial\n&gt;&gt;&gt; q = [3, 2]\n&gt;&gt;&gt; X = fullfactorial(q, Edges=0)\n&gt;&gt;&gt; print(X)\n        [[0.         0.        ]\n        [0.         0.75      ]\n        [0.41666667 0.        ]\n        [0.41666667 0.75      ]\n        [0.83333333 0.        ]\n        [0.83333333 0.75      ]]\n&gt;&gt;&gt; X = fullfactorial(q, Edges=1)\n&gt;&gt;&gt; print(X)\n        [[0.  0. ]\n        [0.  1. ]\n        [0.5 0. ]\n        [0.5 1. ]\n        [1.  0. ]\n        [1.  1. ]]",
    "crumbs": [
      "API Reference",
      "Sampling",
      "fullfactorial"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.plot_surrogate.html",
    "href": "docs/reference/plot.visualization.plot_surrogate.html",
    "title": "plot.visualization.plot_surrogate",
    "section": "",
    "text": "plot.visualization.plot_surrogate(\n    optimizer,\n    i=0,\n    j=1,\n    show=True,\n    alpha=0.8,\n    var_name=None,\n    cmap='jet',\n    num=100,\n    vmin=None,\n    vmax=None,\n    add_points=True,\n    grid_visible=True,\n    contour_levels=30,\n    figsize=(12, 10),\n)\nPlot the surrogate model for two dimensions.\nCreates a 2x2 plot showing: - Top left: 3D surface of predictions - Top right: 3D surface of prediction uncertainty - Bottom left: Contour plot of predictions with evaluated points - Bottom right: Contour plot of prediction uncertainty\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noptimizer\nobject\nSpotOptim instance containing optimization data and surrogate model.\nrequired\n\n\ni\nint\nIndex of the first dimension to plot. Defaults to 0.\n0\n\n\nj\nint\nIndex of the second dimension to plot. Defaults to 1.\n1\n\n\nshow\nbool\nIf True, displays the plot immediately. Defaults to True.\nTrue\n\n\nalpha\nfloat\nTransparency of the 3D surface plots (0=transparent, 1=opaque). Defaults to 0.8.\n0.8\n\n\nvar_name\nlist of str\nNames for each dimension. If None, uses instance var_name. Defaults to None.\nNone\n\n\ncmap\nstr\nMatplotlib colormap name. Defaults to ‘jet’.\n'jet'\n\n\nnum\nint\nNumber of grid points per dimension for mesh grid. Defaults to 100.\n100\n\n\nvmin\nfloat\nMinimum value for color scale. If None, determined from data. Defaults to None.\nNone\n\n\nvmax\nfloat\nMaximum value for color scale. If None, determined from data. Defaults to None.\nNone\n\n\nadd_points\nbool\nIf True, overlay evaluated points on contour plots. Defaults to True.\nTrue\n\n\ngrid_visible\nbool\nIf True, show grid lines on contour plots. Defaults to True.\nTrue\n\n\ncontour_levels\nint\nNumber of contour levels. Defaults to 30.\n30\n\n\nfigsize\ntuple of int\nFigure size in inches (width, height). Defaults to (12, 10).\n(12, 10)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf optimization hasn’t been run yet, or if i, j are invalid.\n\n\n\nImportError\nIf matplotlib is not installed.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; from spotoptim.plot.visualization import plot_surrogate\n&gt;&gt;&gt;\n&gt;&gt;&gt; def sphere(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize and run optimizer\n&gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5), (-5, 5)],\n...                 max_iter=20, n_initial=10, var_name=['x1', 'x2'])\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Plot surrogate model for dimensions 0 and 1\n&gt;&gt;&gt; plot_surrogate(opt, i=0, j=1, show=False)",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_surrogate"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.plot_surrogate.html#parameters",
    "href": "docs/reference/plot.visualization.plot_surrogate.html#parameters",
    "title": "plot.visualization.plot_surrogate",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\noptimizer\nobject\nSpotOptim instance containing optimization data and surrogate model.\nrequired\n\n\ni\nint\nIndex of the first dimension to plot. Defaults to 0.\n0\n\n\nj\nint\nIndex of the second dimension to plot. Defaults to 1.\n1\n\n\nshow\nbool\nIf True, displays the plot immediately. Defaults to True.\nTrue\n\n\nalpha\nfloat\nTransparency of the 3D surface plots (0=transparent, 1=opaque). Defaults to 0.8.\n0.8\n\n\nvar_name\nlist of str\nNames for each dimension. If None, uses instance var_name. Defaults to None.\nNone\n\n\ncmap\nstr\nMatplotlib colormap name. Defaults to ‘jet’.\n'jet'\n\n\nnum\nint\nNumber of grid points per dimension for mesh grid. Defaults to 100.\n100\n\n\nvmin\nfloat\nMinimum value for color scale. If None, determined from data. Defaults to None.\nNone\n\n\nvmax\nfloat\nMaximum value for color scale. If None, determined from data. Defaults to None.\nNone\n\n\nadd_points\nbool\nIf True, overlay evaluated points on contour plots. Defaults to True.\nTrue\n\n\ngrid_visible\nbool\nIf True, show grid lines on contour plots. Defaults to True.\nTrue\n\n\ncontour_levels\nint\nNumber of contour levels. Defaults to 30.\n30\n\n\nfigsize\ntuple of int\nFigure size in inches (width, height). Defaults to (12, 10).\n(12, 10)",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_surrogate"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.plot_surrogate.html#raises",
    "href": "docs/reference/plot.visualization.plot_surrogate.html#raises",
    "title": "plot.visualization.plot_surrogate",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf optimization hasn’t been run yet, or if i, j are invalid.\n\n\n\nImportError\nIf matplotlib is not installed.",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_surrogate"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.plot_surrogate.html#examples",
    "href": "docs/reference/plot.visualization.plot_surrogate.html#examples",
    "title": "plot.visualization.plot_surrogate",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; from spotoptim.plot.visualization import plot_surrogate\n&gt;&gt;&gt;\n&gt;&gt;&gt; def sphere(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize and run optimizer\n&gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5), (-5, 5)],\n...                 max_iter=20, n_initial=10, var_name=['x1', 'x2'])\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Plot surrogate model for dimensions 0 and 1\n&gt;&gt;&gt; plot_surrogate(opt, i=0, j=1, show=False)",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_surrogate"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.plot_important_hyperparameter_contour.html",
    "href": "docs/reference/plot.visualization.plot_important_hyperparameter_contour.html",
    "title": "plot.visualization.plot_important_hyperparameter_contour",
    "section": "",
    "text": "plot.visualization.plot_important_hyperparameter_contour(\n    optimizer,\n    max_imp=3,\n    show=True,\n    alpha=0.8,\n    cmap='jet',\n    num=100,\n    add_points=True,\n    grid_visible=True,\n    contour_levels=30,\n    figsize=(12, 10),\n)\nPlot surrogate contours for all combinations of the top max_imp important parameters.\nThis method identifies the most important parameters using importance scores, then generates surrogate contour plots for all pairwise combinations of these parameters. Factor (categorical) variables are handled by creating discrete grids and displaying factor level names on the axes.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noptimizer\nobject\nSpotOptim instance containing optimization data.\nrequired\n\n\nmax_imp\nint\nNumber of most important parameters to visualize. Defaults to 3. For max_imp=3, creates 3 plots: (0,1), (0,2), (1,2).\n3\n\n\nshow\nbool\nIf True, displays plots immediately. Defaults to True.\nTrue\n\n\nalpha\nfloat\nTransparency of 3D surface plots (0=transparent, 1=opaque). Defaults to 0.8.\n0.8\n\n\ncmap\nstr\nMatplotlib colormap name. Defaults to ‘jet’.\n'jet'\n\n\nnum\nint\nNumber of grid points per dimension. Defaults to 100. For factor variables, uses the number of unique levels instead.\n100\n\n\nadd_points\nbool\nIf True, overlay evaluated points on contour plots. Defaults to True.\nTrue\n\n\ngrid_visible\nbool\nIf True, show grid lines. Defaults to True.\nTrue\n\n\ncontour_levels\nint\nNumber of contour levels. Defaults to 30.\n30\n\n\nfigsize\ntuple of int\nFigure size in inches (width, height). Defaults to (12, 10).\n(12, 10)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf optimization hasn’t been run yet or max_imp is invalid.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; from spotoptim.plot.visualization import plot_important_hyperparameter_contour\n&gt;&gt;&gt;\n&gt;&gt;&gt; def sphere(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize and run optimizer with enough dimensions\n&gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5)]*4,\n...                 max_iter=20, n_initial=10,\n...                 var_name=['x1', 'x2', 'x3', 'x4'])\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Plot contours for top 3 important hyperparameters\n&gt;&gt;&gt; plot_important_hyperparameter_contour(opt, max_imp=3, show=False)",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_important_hyperparameter_contour"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.plot_important_hyperparameter_contour.html#parameters",
    "href": "docs/reference/plot.visualization.plot_important_hyperparameter_contour.html#parameters",
    "title": "plot.visualization.plot_important_hyperparameter_contour",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\noptimizer\nobject\nSpotOptim instance containing optimization data.\nrequired\n\n\nmax_imp\nint\nNumber of most important parameters to visualize. Defaults to 3. For max_imp=3, creates 3 plots: (0,1), (0,2), (1,2).\n3\n\n\nshow\nbool\nIf True, displays plots immediately. Defaults to True.\nTrue\n\n\nalpha\nfloat\nTransparency of 3D surface plots (0=transparent, 1=opaque). Defaults to 0.8.\n0.8\n\n\ncmap\nstr\nMatplotlib colormap name. Defaults to ‘jet’.\n'jet'\n\n\nnum\nint\nNumber of grid points per dimension. Defaults to 100. For factor variables, uses the number of unique levels instead.\n100\n\n\nadd_points\nbool\nIf True, overlay evaluated points on contour plots. Defaults to True.\nTrue\n\n\ngrid_visible\nbool\nIf True, show grid lines. Defaults to True.\nTrue\n\n\ncontour_levels\nint\nNumber of contour levels. Defaults to 30.\n30\n\n\nfigsize\ntuple of int\nFigure size in inches (width, height). Defaults to (12, 10).\n(12, 10)",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_important_hyperparameter_contour"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.plot_important_hyperparameter_contour.html#raises",
    "href": "docs/reference/plot.visualization.plot_important_hyperparameter_contour.html#raises",
    "title": "plot.visualization.plot_important_hyperparameter_contour",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf optimization hasn’t been run yet or max_imp is invalid.",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_important_hyperparameter_contour"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.plot_important_hyperparameter_contour.html#examples",
    "href": "docs/reference/plot.visualization.plot_important_hyperparameter_contour.html#examples",
    "title": "plot.visualization.plot_important_hyperparameter_contour",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; from spotoptim.plot.visualization import plot_important_hyperparameter_contour\n&gt;&gt;&gt;\n&gt;&gt;&gt; def sphere(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize and run optimizer with enough dimensions\n&gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5)]*4,\n...                 max_iter=20, n_initial=10,\n...                 var_name=['x1', 'x2', 'x3', 'x4'])\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Plot contours for top 3 important hyperparameters\n&gt;&gt;&gt; plot_important_hyperparameter_contour(opt, max_imp=3, show=False)",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_important_hyperparameter_contour"
    ]
  },
  {
    "objectID": "docs/reference/plot.mo.html",
    "href": "docs/reference/plot.mo.html",
    "title": "plot.mo",
    "section": "",
    "text": "plot.mo\n\n\n\n\n\nName\nDescription\n\n\n\n\nplot_mo\nGenerates scatter plots for each combination of two targets from a multi-output prediction while highlighting Pareto optimal points.",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot.mo"
    ]
  },
  {
    "objectID": "docs/reference/plot.mo.html#functions",
    "href": "docs/reference/plot.mo.html#functions",
    "title": "plot.mo",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nplot_mo\nGenerates scatter plots for each combination of two targets from a multi-output prediction while highlighting Pareto optimal points.",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot.mo"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.simple_contour.html",
    "href": "docs/reference/plot.contour.simple_contour.html",
    "title": "plot.contour.simple_contour",
    "section": "",
    "text": "plot.contour.simple_contour(\n    fun,\n    min_x=-1,\n    max_x=1,\n    min_y=-1,\n    max_y=1,\n    min_z=None,\n    max_z=None,\n    n_samples=100,\n    n_levels=30,\n)\nSimple contour plot\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfun\ntype\ndescription\nrequired\n\n\nmin_x\nint\ndescription. Defaults to -1.\n-1\n\n\nmax_x\nint\ndescription. Defaults to 1.\n1\n\n\nmin_y\nint\ndescription. Defaults to -1.\n-1\n\n\nmax_y\nint\ndescription. Defaults to 1.\n1\n\n\nmin_z\nint\ndescription. Defaults to 0.\nNone\n\n\nmax_z\nint\ndescription. Defaults to 1.\nNone\n\n\nn_samples\nint\ndescription. Defaults to 100.\n100\n\n\nn_levels\nint\ndescription. Defaults to 5.\n30\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n    import numpy as np\n    # from spotpython.fun.objectivefunctions import analytical\n    # fun = analytical().fun_branin\n    # simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15)",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "simple_contour"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.simple_contour.html#parameters",
    "href": "docs/reference/plot.contour.simple_contour.html#parameters",
    "title": "plot.contour.simple_contour",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfun\ntype\ndescription\nrequired\n\n\nmin_x\nint\ndescription. Defaults to -1.\n-1\n\n\nmax_x\nint\ndescription. Defaults to 1.\n1\n\n\nmin_y\nint\ndescription. Defaults to -1.\n-1\n\n\nmax_y\nint\ndescription. Defaults to 1.\n1\n\n\nmin_z\nint\ndescription. Defaults to 0.\nNone\n\n\nmax_z\nint\ndescription. Defaults to 1.\nNone\n\n\nn_samples\nint\ndescription. Defaults to 100.\n100\n\n\nn_levels\nint\ndescription. Defaults to 5.\n30",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "simple_contour"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.simple_contour.html#returns",
    "href": "docs/reference/plot.contour.simple_contour.html#returns",
    "title": "plot.contour.simple_contour",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "simple_contour"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.simple_contour.html#examples",
    "href": "docs/reference/plot.contour.simple_contour.html#examples",
    "title": "plot.contour.simple_contour",
    "section": "",
    "text": "&gt;&gt;&gt; import matplotlib.pyplot as plt\n    import numpy as np\n    # from spotpython.fun.objectivefunctions import analytical\n    # fun = analytical().fun_branin\n    # simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15)",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "simple_contour"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.plotModel.html",
    "href": "docs/reference/plot.contour.plotModel.html",
    "title": "plot.contour.plotModel",
    "section": "",
    "text": "plot.contour.plotModel(\n    model,\n    lower,\n    upper,\n    i=0,\n    j=1,\n    min_z=None,\n    max_z=None,\n    var_type=None,\n    var_name=None,\n    show=True,\n    filename=None,\n    n_grid=50,\n    contour_levels=10,\n    dpi=200,\n    title='',\n    figsize=(12, 6),\n    use_min=False,\n    use_max=False,\n    aspect_equal=True,\n    legend_fontsize=12,\n    cmap='viridis',\n    X_points=None,\n    y_points=None,\n    plot_points=True,\n    points_color='white',\n    points_size=30,\n    point_color_below='blue',\n    point_color_above='red',\n    atol=1e-06,\n    plot_3d=False,\n)\nGenerate 2D contour and optionally 3D surface plots for a model’s predictions.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodel\nobject\nA model with a predict method.\nrequired\n\n\nlower\narray_like\nLower bounds for each dimension.\nrequired\n\n\nupper\narray_like\nUpper bounds for each dimension.\nrequired\n\n\ni\nint\nIndex for the x-axis dimension.\n0\n\n\nj\nint\nIndex for the y-axis dimension.\n1\n\n\nmin_z\nfloat\nMin value for color scaling. Defaults to None.\nNone\n\n\nmax_z\nfloat\nMax value for color scaling. Defaults to None.\nNone\n\n\nvar_type\nlist\nVariable types for each dimension. Defaults to None.\nNone\n\n\nvar_name\nlist\nVariable names for labeling axes. Defaults to None.\nNone\n\n\nshow\nbool\nWhether to display the plot. Defaults to True.\nTrue\n\n\nfilename\nstr\nFile path to save the figure. Defaults to None.\nNone\n\n\nn_grid\nint\nResolution for each axis. Defaults to 50.\n50\n\n\ncontour_levels\nint\nNumber of contour levels. Defaults to 10.\n10\n\n\ndpi\nint\nDPI for saving. Defaults to 200.\n200\n\n\ntitle\nstr\nTitle for the figure. Defaults to ““.\n''\n\n\nfigsize\ntuple\nFigure size. Defaults to (12, 6).\n(12, 6)\n\n\nuse_min\nbool\nIf True, leftover dims are set to lower bounds.\nFalse\n\n\nuse_max\nbool\nIf True, leftover dims are set to upper bounds.\nFalse\n\n\naspect_equal\nbool\nWhether axes have equal scaling. Defaults to True.\nTrue\n\n\nlegend_fontsize\nint\nFont size for labels and legends. Defaults to 12.\n12\n\n\ncmap\nstr\nColormap. Defaults to “viridis”.\n'viridis'\n\n\nX_points\nndarray\nOriginal data points. Shape: (N, D).\nNone\n\n\ny_points\nndarray\nOriginal target values. Shape: (N,).\nNone\n\n\nplot_points\nbool\nWhether to plot X_points. Defaults to True.\nTrue\n\n\npoints_color\nstr\nFallback color for data points. Defaults to “white”.\n'white'\n\n\npoints_size\nint\nMarker size for data points. Defaults to 30.\n30\n\n\npoint_color_below\nstr\nColor if actual z &lt; predicted z. Defaults to “blue”.\n'blue'\n\n\npoint_color_above\nstr\nColor if actual z &gt;= predicted z. Defaults to “red”.\n'red'\n\n\natol\nfloat\nAbsolute tolerance for comparing actual and predicted z-values. Defaults to 1e-6.\n1e-06\n\n\nplot_3d\nbool\nWhether to plot the 3D surface plot. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n(fig, (ax_contour, ax_surface))\nFigure and axes for the contour and surface plots.",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plotModel"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.plotModel.html#parameters",
    "href": "docs/reference/plot.contour.plotModel.html#parameters",
    "title": "plot.contour.plotModel",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmodel\nobject\nA model with a predict method.\nrequired\n\n\nlower\narray_like\nLower bounds for each dimension.\nrequired\n\n\nupper\narray_like\nUpper bounds for each dimension.\nrequired\n\n\ni\nint\nIndex for the x-axis dimension.\n0\n\n\nj\nint\nIndex for the y-axis dimension.\n1\n\n\nmin_z\nfloat\nMin value for color scaling. Defaults to None.\nNone\n\n\nmax_z\nfloat\nMax value for color scaling. Defaults to None.\nNone\n\n\nvar_type\nlist\nVariable types for each dimension. Defaults to None.\nNone\n\n\nvar_name\nlist\nVariable names for labeling axes. Defaults to None.\nNone\n\n\nshow\nbool\nWhether to display the plot. Defaults to True.\nTrue\n\n\nfilename\nstr\nFile path to save the figure. Defaults to None.\nNone\n\n\nn_grid\nint\nResolution for each axis. Defaults to 50.\n50\n\n\ncontour_levels\nint\nNumber of contour levels. Defaults to 10.\n10\n\n\ndpi\nint\nDPI for saving. Defaults to 200.\n200\n\n\ntitle\nstr\nTitle for the figure. Defaults to ““.\n''\n\n\nfigsize\ntuple\nFigure size. Defaults to (12, 6).\n(12, 6)\n\n\nuse_min\nbool\nIf True, leftover dims are set to lower bounds.\nFalse\n\n\nuse_max\nbool\nIf True, leftover dims are set to upper bounds.\nFalse\n\n\naspect_equal\nbool\nWhether axes have equal scaling. Defaults to True.\nTrue\n\n\nlegend_fontsize\nint\nFont size for labels and legends. Defaults to 12.\n12\n\n\ncmap\nstr\nColormap. Defaults to “viridis”.\n'viridis'\n\n\nX_points\nndarray\nOriginal data points. Shape: (N, D).\nNone\n\n\ny_points\nndarray\nOriginal target values. Shape: (N,).\nNone\n\n\nplot_points\nbool\nWhether to plot X_points. Defaults to True.\nTrue\n\n\npoints_color\nstr\nFallback color for data points. Defaults to “white”.\n'white'\n\n\npoints_size\nint\nMarker size for data points. Defaults to 30.\n30\n\n\npoint_color_below\nstr\nColor if actual z &lt; predicted z. Defaults to “blue”.\n'blue'\n\n\npoint_color_above\nstr\nColor if actual z &gt;= predicted z. Defaults to “red”.\n'red'\n\n\natol\nfloat\nAbsolute tolerance for comparing actual and predicted z-values. Defaults to 1e-6.\n1e-06\n\n\nplot_3d\nbool\nWhether to plot the 3D surface plot. Defaults to False.\nFalse",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plotModel"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.plotModel.html#returns",
    "href": "docs/reference/plot.contour.plotModel.html#returns",
    "title": "plot.contour.plotModel",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n(fig, (ax_contour, ax_surface))\nFigure and axes for the contour and surface plots.",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plotModel"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.contourf_plot.html",
    "href": "docs/reference/plot.contour.contourf_plot.html",
    "title": "plot.contour.contourf_plot",
    "section": "",
    "text": "plot.contour.contourf_plot(\n    data,\n    x_col,\n    y_col,\n    z_col,\n    facet_col=None,\n    aspect=1,\n    as_table=True,\n    figsize=(4, 4),\n    levels=10,\n    cmap='viridis',\n    show_contour_lines=True,\n    contour_line_color='black',\n    contour_line_width=0.5,\n    colorbar_orientation='vertical',\n    wspace=0.4,\n    hspace=0.4,\n    highlight_point=None,\n    highlight_color='red',\n    highlight_marker='x',\n    highlight_size=100,\n)\nCreates contour plots (single or faceted) using matplotlib.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nData for plotting.\nrequired\n\n\nx_col\nstr\nColumn name for x-axis.\nrequired\n\n\ny_col\nstr\nColumn name for y-axis.\nrequired\n\n\nz_col\nstr\nColumn name for z-axis (values).\nrequired\n\n\nfacet_col\nstr\nColumn name for faceting.\nNone\n\n\naspect\nfloat\nAspect ratio.\n1\n\n\nas_table\nbool\nIf True, arranges facets in a table.\nTrue\n\n\nfigsize\ntuple\nFigure size per plot key (if not faceted) or base.\n(4, 4)\n\n\nlevels\nint\nNumber of contour levels.\n10\n\n\ncmap\nstr\nColormap.\n'viridis'\n\n\nshow_contour_lines\nbool\nWhether to show contour lines.\nTrue\n\n\ncontour_line_color\nstr\nColor of contour lines.\n'black'\n\n\ncontour_line_width\nfloat\nWidth of contour lines.\n0.5\n\n\ncolorbar_orientation\nstr\n‘vertical’ or ‘horizontal’.\n'vertical'\n\n\nwspace\nfloat\nWidth reserved for space between subplots.\n0.4\n\n\nhspace\nfloat\nHeight reserved for space between subplots.\n0.4\n\n\nhighlight_point\ndict - like\nPoint to highlight. Must contain x_col, y_col keys. If faceted, must contain facet_col key.\nNone\n\n\nhighlight_color\nstr\nColor of the highlight point.\n'red'\n\n\nhighlight_marker\nstr\nMarker style for the highlight point.\n'x'\n\n\nhighlight_size\nint\nSize of the highlight point.\n100",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "contourf_plot"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.contourf_plot.html#parameters",
    "href": "docs/reference/plot.contour.contourf_plot.html#parameters",
    "title": "plot.contour.contourf_plot",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\npd.DataFrame\nData for plotting.\nrequired\n\n\nx_col\nstr\nColumn name for x-axis.\nrequired\n\n\ny_col\nstr\nColumn name for y-axis.\nrequired\n\n\nz_col\nstr\nColumn name for z-axis (values).\nrequired\n\n\nfacet_col\nstr\nColumn name for faceting.\nNone\n\n\naspect\nfloat\nAspect ratio.\n1\n\n\nas_table\nbool\nIf True, arranges facets in a table.\nTrue\n\n\nfigsize\ntuple\nFigure size per plot key (if not faceted) or base.\n(4, 4)\n\n\nlevels\nint\nNumber of contour levels.\n10\n\n\ncmap\nstr\nColormap.\n'viridis'\n\n\nshow_contour_lines\nbool\nWhether to show contour lines.\nTrue\n\n\ncontour_line_color\nstr\nColor of contour lines.\n'black'\n\n\ncontour_line_width\nfloat\nWidth of contour lines.\n0.5\n\n\ncolorbar_orientation\nstr\n‘vertical’ or ‘horizontal’.\n'vertical'\n\n\nwspace\nfloat\nWidth reserved for space between subplots.\n0.4\n\n\nhspace\nfloat\nHeight reserved for space between subplots.\n0.4\n\n\nhighlight_point\ndict - like\nPoint to highlight. Must contain x_col, y_col keys. If faceted, must contain facet_col key.\nNone\n\n\nhighlight_color\nstr\nColor of the highlight point.\n'red'\n\n\nhighlight_marker\nstr\nMarker style for the highlight point.\n'x'\n\n\nhighlight_size\nint\nSize of the highlight point.\n100",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "contourf_plot"
    ]
  },
  {
    "objectID": "docs/reference/optimizer.schedule_free.AdamWScheduleFree.html",
    "href": "docs/reference/optimizer.schedule_free.AdamWScheduleFree.html",
    "title": "optimizer.schedule_free.AdamWScheduleFree",
    "section": "",
    "text": "optimizer.schedule_free.AdamWScheduleFree(\n    params,\n    lr=0.0025,\n    betas=(0.9, 0.999),\n    eps=1e-08,\n    weight_decay=0,\n    warmup_steps=0,\n    r=0.0,\n    weight_lr_power=2.0,\n)\nSchedule-Free AdamW in PyTorch.\nAs the name suggests, no scheduler is needed with this optimizer. To add warmup, rather than using a learning rate schedule you can just set the warmup_steps parameter.\nThis optimizer requires that .train() and .eval() be called on the optimizer before the beginning of training and evaluation respectively.\nReference: https://github.com/facebookresearch/schedule_free\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\neval\nSwitch model parameters to evaluation mode (using averaged parameters ‘x’).\n\n\nstep\nPerforms a single optimization step.\n\n\ntrain\nSwitch model parameters to training mode (using current iterate ‘y’).",
    "crumbs": [
      "API Reference",
      "Optimization",
      "AdamWScheduleFree"
    ]
  },
  {
    "objectID": "docs/reference/optimizer.schedule_free.AdamWScheduleFree.html#methods",
    "href": "docs/reference/optimizer.schedule_free.AdamWScheduleFree.html#methods",
    "title": "optimizer.schedule_free.AdamWScheduleFree",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\neval\nSwitch model parameters to evaluation mode (using averaged parameters ‘x’).\n\n\nstep\nPerforms a single optimization step.\n\n\ntrain\nSwitch model parameters to training mode (using current iterate ‘y’).",
    "crumbs": [
      "API Reference",
      "Optimization",
      "AdamWScheduleFree"
    ]
  },
  {
    "objectID": "docs/reference/nn.html",
    "href": "docs/reference/nn.html",
    "title": "nn",
    "section": "",
    "text": "nn\nnn\nNeural network models for spotoptim.",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "nn"
    ]
  },
  {
    "objectID": "docs/reference/nn.mlp.MLP.html",
    "href": "docs/reference/nn.mlp.MLP.html",
    "title": "nn.mlp.MLP",
    "section": "",
    "text": "nn.mlp.MLP(\n    in_channels,\n    hidden_channels=None,\n    norm_layer=None,\n    activation_layer=torch.nn.ReLU,\n    inplace=None,\n    bias=True,\n    dropout=0.0,\n    lr=1.0,\n    l1=64,\n    num_hidden_layers=2,\n    output_dim=1,\n)\nThis block implements the multi-layer perceptron (MLP) module.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nin_channels\nint\nNumber of channels of the input\nrequired\n\n\nhidden_channels\nList[int]\nList of the hidden channel dimensions. Note that the last element of this list is the output dimension of the network.\nNone\n\n\nnorm_layer\nCallable[…, torch.nn.Module]\nNorm layer that will be stacked on top of the linear layer. If None this layer won’t be used. Default: None\nNone\n\n\nactivation_layer\nCallable[…, torch.nn.Module]\nActivation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If None this layer won’t be used. Default: torch.nn.ReLU\ntorch.nn.ReLU\n\n\ninplace\nbool\nParameter for the activation layer, which can optionally do the operation in-place. Default is None, which uses the respective default values of the activation_layer and Dropout layer.\nNone\n\n\nbias\nbool\nWhether to use bias in the linear layer. Default True\nTrue\n\n\ndropout\nfloat\nThe probability for the dropout layer. Default: 0.0\n0.0\n\n\nlr\nfloat\nUnified learning rate multiplier. This value is automatically scaled to optimizer-specific learning rates using the map_lr() function. A value of 1.0 corresponds to the optimizer’s default learning rate. Default: 1.0.\n1.0\n\n\nl1\nint\nNumber of neurons in each hidden layer. Will only be used if hidden_channels is None. Default: 64\n64\n\n\nnum_hidden_layers\nint\nNumber of hidden layers. Will only be used if hidden_channels is None. Default: 2\n2\n\n\n\n\n\n\nParameter Definitions:\n\nhidden_channels: This defines the explicit architecture of the MLP. It is a list where each element is the size of a layer. The last element is the output dimension. Example: [32, 32, 1] means two hidden layers of size 32 and an output layer of size 1.\nl1 and num_hidden_layers: These are helper parameters often used in hyperparameter optimization (see get_default_parameters()). They will only be used if hidden_channels is None.\n\nl1: The number of neurons in each hidden layer.\nnum_hidden_layers: The number of hidden layers before the output layer.\n\nThey describe the architecture in a more compact way but are less flexible than hidden_channels. Relationship: To convert l1 and num_hidden_layers to hidden_channels for a given output_dim: hidden_channels = [l1] * num_hidden_layers + [output_dim]\n\n\n\n\nBasic usage:\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.nn.mlp import MLP\n&gt;&gt;&gt; # Input: 10 features. Output (is considered a hidden layer): 30 features. Hidden layer: 20 neurons.\n&gt;&gt;&gt; mlp = MLP(in_channels=10, hidden_channels=[20, 30])\n&gt;&gt;&gt; x = torch.randn(5, 10)\n&gt;&gt;&gt; output = mlp(x)\n&gt;&gt;&gt; print(output.shape)\ntorch.Size([5, 30])\nUsing get_optimizer:\n&gt;&gt;&gt; model = MLP(in_channels=10, hidden_channels=[32, 1], lr=0.5)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\")  # Uses 0.5 * 0.001\n&gt;&gt;&gt; print(optimizer)\nAdam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0005\n    maximize: False\n    weight_decay: 0\n)\nUsing l1 and num_hidden_layers parameters: This example shows how to use the hyperparameters suggested by get_default_parameters() to construct the hidden_channels list.\n&gt;&gt;&gt; input_dim = 10\n&gt;&gt;&gt; output_dim = 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Hyperparameters (e.g., from spotoptim tuning)\n&gt;&gt;&gt; l1 = 64\n&gt;&gt;&gt; num_hidden_layers = 2\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Construct hidden_channels list\n&gt;&gt;&gt; # [64, 64, 1] -&gt; 2 hidden layers of 64, output layer of 1\n&gt;&gt;&gt; # Relationship: To convert l1 and num_hidden_layers to hidden_channels for a given output_dim:\n&gt;&gt;&gt; # hidden_channels = [l1] * num_hidden_layers + [output_dim]\n&gt;&gt;&gt; # but we can pass l1 and num_hidden_layers directly to the constructor\n&gt;&gt;&gt; model = MLP(in_channels=input_dim, l1=l1, num_hidden_layers=num_hidden_layers, output_dim=output_dim)\n&gt;&gt;&gt; print(model)\nMLP(\n  (0): Linear(in_features=10, out_features=64, bias=True)\n  (1): ReLU()\n  (2): Dropout(p=0.0, inplace=False)\n  (3): Linear(in_features=64, out_features=64, bias=True)\n  (4): ReLU()\n  (5): Dropout(p=0.0, inplace=False)\n  (6): Linear(in_features=64, out_features=1, bias=True)\n  (7): Dropout(p=0.0, inplace=False)\n)\nGetting default parameters for tuning:\n&gt;&gt;&gt; params = MLP.get_default_parameters()\n&gt;&gt;&gt; print(params.names())\n['l1', 'num_hidden_layers', 'activation', 'lr', 'optimizer']\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_default_parameters\nReturns a ParameterSet populated with default hyperparameters for this model.\n\n\nget_optimizer\nGet a PyTorch optimizer configured for this model.",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "MLP"
    ]
  },
  {
    "objectID": "docs/reference/nn.mlp.MLP.html#parameters",
    "href": "docs/reference/nn.mlp.MLP.html#parameters",
    "title": "nn.mlp.MLP",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nin_channels\nint\nNumber of channels of the input\nrequired\n\n\nhidden_channels\nList[int]\nList of the hidden channel dimensions. Note that the last element of this list is the output dimension of the network.\nNone\n\n\nnorm_layer\nCallable[…, torch.nn.Module]\nNorm layer that will be stacked on top of the linear layer. If None this layer won’t be used. Default: None\nNone\n\n\nactivation_layer\nCallable[…, torch.nn.Module]\nActivation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If None this layer won’t be used. Default: torch.nn.ReLU\ntorch.nn.ReLU\n\n\ninplace\nbool\nParameter for the activation layer, which can optionally do the operation in-place. Default is None, which uses the respective default values of the activation_layer and Dropout layer.\nNone\n\n\nbias\nbool\nWhether to use bias in the linear layer. Default True\nTrue\n\n\ndropout\nfloat\nThe probability for the dropout layer. Default: 0.0\n0.0\n\n\nlr\nfloat\nUnified learning rate multiplier. This value is automatically scaled to optimizer-specific learning rates using the map_lr() function. A value of 1.0 corresponds to the optimizer’s default learning rate. Default: 1.0.\n1.0\n\n\nl1\nint\nNumber of neurons in each hidden layer. Will only be used if hidden_channels is None. Default: 64\n64\n\n\nnum_hidden_layers\nint\nNumber of hidden layers. Will only be used if hidden_channels is None. Default: 2\n2",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "MLP"
    ]
  },
  {
    "objectID": "docs/reference/nn.mlp.MLP.html#note",
    "href": "docs/reference/nn.mlp.MLP.html#note",
    "title": "nn.mlp.MLP",
    "section": "",
    "text": "Parameter Definitions:\n\nhidden_channels: This defines the explicit architecture of the MLP. It is a list where each element is the size of a layer. The last element is the output dimension. Example: [32, 32, 1] means two hidden layers of size 32 and an output layer of size 1.\nl1 and num_hidden_layers: These are helper parameters often used in hyperparameter optimization (see get_default_parameters()). They will only be used if hidden_channels is None.\n\nl1: The number of neurons in each hidden layer.\nnum_hidden_layers: The number of hidden layers before the output layer.\n\nThey describe the architecture in a more compact way but are less flexible than hidden_channels. Relationship: To convert l1 and num_hidden_layers to hidden_channels for a given output_dim: hidden_channels = [l1] * num_hidden_layers + [output_dim]",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "MLP"
    ]
  },
  {
    "objectID": "docs/reference/nn.mlp.MLP.html#examples",
    "href": "docs/reference/nn.mlp.MLP.html#examples",
    "title": "nn.mlp.MLP",
    "section": "",
    "text": "Basic usage:\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.nn.mlp import MLP\n&gt;&gt;&gt; # Input: 10 features. Output (is considered a hidden layer): 30 features. Hidden layer: 20 neurons.\n&gt;&gt;&gt; mlp = MLP(in_channels=10, hidden_channels=[20, 30])\n&gt;&gt;&gt; x = torch.randn(5, 10)\n&gt;&gt;&gt; output = mlp(x)\n&gt;&gt;&gt; print(output.shape)\ntorch.Size([5, 30])\nUsing get_optimizer:\n&gt;&gt;&gt; model = MLP(in_channels=10, hidden_channels=[32, 1], lr=0.5)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\")  # Uses 0.5 * 0.001\n&gt;&gt;&gt; print(optimizer)\nAdam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0005\n    maximize: False\n    weight_decay: 0\n)\nUsing l1 and num_hidden_layers parameters: This example shows how to use the hyperparameters suggested by get_default_parameters() to construct the hidden_channels list.\n&gt;&gt;&gt; input_dim = 10\n&gt;&gt;&gt; output_dim = 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Hyperparameters (e.g., from spotoptim tuning)\n&gt;&gt;&gt; l1 = 64\n&gt;&gt;&gt; num_hidden_layers = 2\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Construct hidden_channels list\n&gt;&gt;&gt; # [64, 64, 1] -&gt; 2 hidden layers of 64, output layer of 1\n&gt;&gt;&gt; # Relationship: To convert l1 and num_hidden_layers to hidden_channels for a given output_dim:\n&gt;&gt;&gt; # hidden_channels = [l1] * num_hidden_layers + [output_dim]\n&gt;&gt;&gt; # but we can pass l1 and num_hidden_layers directly to the constructor\n&gt;&gt;&gt; model = MLP(in_channels=input_dim, l1=l1, num_hidden_layers=num_hidden_layers, output_dim=output_dim)\n&gt;&gt;&gt; print(model)\nMLP(\n  (0): Linear(in_features=10, out_features=64, bias=True)\n  (1): ReLU()\n  (2): Dropout(p=0.0, inplace=False)\n  (3): Linear(in_features=64, out_features=64, bias=True)\n  (4): ReLU()\n  (5): Dropout(p=0.0, inplace=False)\n  (6): Linear(in_features=64, out_features=1, bias=True)\n  (7): Dropout(p=0.0, inplace=False)\n)\nGetting default parameters for tuning:\n&gt;&gt;&gt; params = MLP.get_default_parameters()\n&gt;&gt;&gt; print(params.names())\n['l1', 'num_hidden_layers', 'activation', 'lr', 'optimizer']",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "MLP"
    ]
  },
  {
    "objectID": "docs/reference/nn.mlp.MLP.html#methods",
    "href": "docs/reference/nn.mlp.MLP.html#methods",
    "title": "nn.mlp.MLP",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_default_parameters\nReturns a ParameterSet populated with default hyperparameters for this model.\n\n\nget_optimizer\nGet a PyTorch optimizer configured for this model.",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "MLP"
    ]
  },
  {
    "objectID": "docs/reference/nn.linear_regressor.LinearRegressor.html",
    "href": "docs/reference/nn.linear_regressor.LinearRegressor.html",
    "title": "nn.linear_regressor.LinearRegressor",
    "section": "",
    "text": "nn.linear_regressor.LinearRegressor(\n    input_dim,\n    output_dim,\n    l1=64,\n    num_hidden_layers=0,\n    activation='ReLU',\n    lr=1.0,\n)\nPyTorch neural network for regression with configurable architecture.\nA flexible regression model that supports: - Pure linear regression (no hidden layers) - Deep neural networks with multiple hidden layers - Various activation functions (ReLU, Tanh, Sigmoid, etc.) - Easy optimizer selection (Adam, SGD, RMSprop, etc.)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninput_dim\nint\nNumber of input features.\nrequired\n\n\noutput_dim\nint\nNumber of output features/targets.\nrequired\n\n\nl1\nint\nNumber of neurons in each hidden layer. Defaults to 64.\n64\n\n\nnum_hidden_layers\nint\nNumber of hidden layers. Set to 0 for pure linear regression. Defaults to 0.\n0\n\n\nactivation\nstr\nName of activation function from torch.nn to use between layers. Common options: “ReLU”, “Sigmoid”, “Tanh”, “LeakyReLU”, “ELU”, “SELU”, “GELU”, “Softplus”, “Softsign”, “Mish”. Defaults to “ReLU”.\n'ReLU'\n\n\nlr\nfloat\nUnified learning rate multiplier. This value is automatically scaled to optimizer-specific learning rates using the map_lr() function. A value of 1.0 corresponds to the optimizer’s default learning rate. For example, lr=1.0 gives 0.001 for Adam and 0.01 for SGD. Typical range: [0.001, 100.0]. Defaults to 1.0.\n1.0\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_dim\nint\nNumber of input features.\n\n\noutput_dim\nint\nNumber of output features.\n\n\nl1\nint\nNumber of neurons per hidden layer.\n\n\nnum_hidden_layers\nint\nNumber of hidden layers in the network.\n\n\nactivation_name\nstr\nName of the activation function.\n\n\nactivation\nnn.Module\nInstance of the activation function.\n\n\nlr\nfloat\nUnified learning rate multiplier.\n\n\nnetwork\nnn.Sequential\nThe complete neural network architecture.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the specified activation function is not found in torch.nn.\n\n\n\n\n\n\nBasic usage with pure linear regression:\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.nn.linear_regressor import LinearRegressor\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Pure linear regression (no hidden layers)\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1)\n&gt;&gt;&gt; x = torch.randn(32, 10)  # Batch of 32 samples\n&gt;&gt;&gt; y_pred = model(x)\n&gt;&gt;&gt; print(y_pred.shape)\ntorch.Size([32, 1])\nSingle hidden layer with custom neurons:\n&gt;&gt;&gt; # Single hidden layer with 64 neurons and ReLU activation\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=64, num_hidden_layers=1)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.001)\nDeep network with custom activation:\n&gt;&gt;&gt; # Three hidden layers with 128 neurons each and Tanh activation\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=128,\n...                         num_hidden_layers=3, activation=\"Tanh\")\nComplete example using diabetes dataset:\n&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Split and scale data\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.2, random_state=42\n... )\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n&gt;&gt;&gt; X_test = scaler.transform(X_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Convert to PyTorch tensors\n&gt;&gt;&gt; X_train = torch.FloatTensor(X_train)\n&gt;&gt;&gt; y_train = torch.FloatTensor(y_train)\n&gt;&gt;&gt; X_test = torch.FloatTensor(X_test)\n&gt;&gt;&gt; y_test = torch.FloatTensor(y_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model with 2 hidden layers\n&gt;&gt;&gt; model = LinearRegressor(\n...     input_dim=10,  # diabetes dataset has 10 features\n...     output_dim=1,\n...     l1=32,\n...     num_hidden_layers=2,\n...     activation=\"ReLU\"\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get optimizer and loss function\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n&gt;&gt;&gt; criterion = nn.MSELoss()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training loop\n&gt;&gt;&gt; for epoch in range(100):\n...     # Forward pass\n...     y_pred = model(X_train)\n...     loss = criterion(y_pred, y_train)\n...\n...     # Backward pass and optimization\n...     optimizer.zero_grad()\n...     loss.backward()\n...     optimizer.step()\n...\n...     if (epoch + 1) % 20 == 0:\n...         print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Evaluate on test set\n&gt;&gt;&gt; model.eval()\n&gt;&gt;&gt; with torch.no_grad():\n...     y_pred = model(X_test)\n...     test_loss = criterion(y_pred, y_test)\n...     print(f'Test Loss: {test_loss.item():.4f}')\nUsing PyTorch Dataset and DataLoader (recommended for larger datasets):\n&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch.utils.data import Dataset, DataLoader\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Custom Dataset class for diabetes data\n&gt;&gt;&gt; class DiabetesDataset(Dataset):\n...     def __init__(self, X, y):\n...         self.X = torch.FloatTensor(X)\n...         self.y = torch.FloatTensor(y)\n...\n...     def __len__(self):\n...         return len(self.X)\n...\n...     def __getitem__(self, idx):\n...         return self.X[idx], self.y[idx]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.2, random_state=42\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Scale data\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n&gt;&gt;&gt; X_test = scaler.transform(X_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create Dataset and DataLoader\n&gt;&gt;&gt; train_dataset = DiabetesDataset(X_train, y_train)\n&gt;&gt;&gt; test_dataset = DiabetesDataset(X_test, y_test)\n&gt;&gt;&gt; train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n&gt;&gt;&gt; test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=32,\n...                         num_hidden_layers=2, activation=\"ReLU\")\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n&gt;&gt;&gt; criterion = nn.MSELoss()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training loop with DataLoader\n&gt;&gt;&gt; for epoch in range(100):\n...     model.train()\n...     for batch_X, batch_y in train_loader:\n...         # Forward pass\n...         predictions = model(batch_X)\n...         loss = criterion(predictions, batch_y)\n...\n...         # Backward pass\n...         optimizer.zero_grad()\n...         loss.backward()\n...         optimizer.step()\n...\n...     # Validation\n...     if (epoch + 1) % 20 == 0:\n...         model.eval()\n...         val_loss = 0.0\n...         with torch.no_grad():\n...             for batch_X, batch_y in test_loader:\n...                 predictions = model(batch_X)\n...                 val_loss += criterion(predictions, batch_y).item()\n...         val_loss /= len(test_loader)\n...         print(f'Epoch [{epoch+1}/100], Val Loss: {val_loss:.4f}')\n\n\n\n\nWhen num_hidden_layers=0, the model performs pure linear regression\nActivation functions are only applied between hidden layers, not on output\nUse get_optimizer() method for convenient optimizer instantiation\nFor large datasets, use PyTorch Dataset and DataLoader for efficient batch processing\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nforward\nForward pass through the network.\n\n\nget_default_parameters\nReturns a ParameterSet populated with default hyperparameters for this model.\n\n\nget_optimizer\nGet a PyTorch optimizer configured for this model.",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "LinearRegressor"
    ]
  },
  {
    "objectID": "docs/reference/nn.linear_regressor.LinearRegressor.html#parameters",
    "href": "docs/reference/nn.linear_regressor.LinearRegressor.html#parameters",
    "title": "nn.linear_regressor.LinearRegressor",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ninput_dim\nint\nNumber of input features.\nrequired\n\n\noutput_dim\nint\nNumber of output features/targets.\nrequired\n\n\nl1\nint\nNumber of neurons in each hidden layer. Defaults to 64.\n64\n\n\nnum_hidden_layers\nint\nNumber of hidden layers. Set to 0 for pure linear regression. Defaults to 0.\n0\n\n\nactivation\nstr\nName of activation function from torch.nn to use between layers. Common options: “ReLU”, “Sigmoid”, “Tanh”, “LeakyReLU”, “ELU”, “SELU”, “GELU”, “Softplus”, “Softsign”, “Mish”. Defaults to “ReLU”.\n'ReLU'\n\n\nlr\nfloat\nUnified learning rate multiplier. This value is automatically scaled to optimizer-specific learning rates using the map_lr() function. A value of 1.0 corresponds to the optimizer’s default learning rate. For example, lr=1.0 gives 0.001 for Adam and 0.01 for SGD. Typical range: [0.001, 100.0]. Defaults to 1.0.\n1.0",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "LinearRegressor"
    ]
  },
  {
    "objectID": "docs/reference/nn.linear_regressor.LinearRegressor.html#attributes",
    "href": "docs/reference/nn.linear_regressor.LinearRegressor.html#attributes",
    "title": "nn.linear_regressor.LinearRegressor",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ninput_dim\nint\nNumber of input features.\n\n\noutput_dim\nint\nNumber of output features.\n\n\nl1\nint\nNumber of neurons per hidden layer.\n\n\nnum_hidden_layers\nint\nNumber of hidden layers in the network.\n\n\nactivation_name\nstr\nName of the activation function.\n\n\nactivation\nnn.Module\nInstance of the activation function.\n\n\nlr\nfloat\nUnified learning rate multiplier.\n\n\nnetwork\nnn.Sequential\nThe complete neural network architecture.",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "LinearRegressor"
    ]
  },
  {
    "objectID": "docs/reference/nn.linear_regressor.LinearRegressor.html#raises",
    "href": "docs/reference/nn.linear_regressor.LinearRegressor.html#raises",
    "title": "nn.linear_regressor.LinearRegressor",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf the specified activation function is not found in torch.nn.",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "LinearRegressor"
    ]
  },
  {
    "objectID": "docs/reference/nn.linear_regressor.LinearRegressor.html#examples",
    "href": "docs/reference/nn.linear_regressor.LinearRegressor.html#examples",
    "title": "nn.linear_regressor.LinearRegressor",
    "section": "",
    "text": "Basic usage with pure linear regression:\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.nn.linear_regressor import LinearRegressor\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Pure linear regression (no hidden layers)\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1)\n&gt;&gt;&gt; x = torch.randn(32, 10)  # Batch of 32 samples\n&gt;&gt;&gt; y_pred = model(x)\n&gt;&gt;&gt; print(y_pred.shape)\ntorch.Size([32, 1])\nSingle hidden layer with custom neurons:\n&gt;&gt;&gt; # Single hidden layer with 64 neurons and ReLU activation\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=64, num_hidden_layers=1)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.001)\nDeep network with custom activation:\n&gt;&gt;&gt; # Three hidden layers with 128 neurons each and Tanh activation\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=128,\n...                         num_hidden_layers=3, activation=\"Tanh\")\nComplete example using diabetes dataset:\n&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Split and scale data\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.2, random_state=42\n... )\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n&gt;&gt;&gt; X_test = scaler.transform(X_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Convert to PyTorch tensors\n&gt;&gt;&gt; X_train = torch.FloatTensor(X_train)\n&gt;&gt;&gt; y_train = torch.FloatTensor(y_train)\n&gt;&gt;&gt; X_test = torch.FloatTensor(X_test)\n&gt;&gt;&gt; y_test = torch.FloatTensor(y_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model with 2 hidden layers\n&gt;&gt;&gt; model = LinearRegressor(\n...     input_dim=10,  # diabetes dataset has 10 features\n...     output_dim=1,\n...     l1=32,\n...     num_hidden_layers=2,\n...     activation=\"ReLU\"\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get optimizer and loss function\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n&gt;&gt;&gt; criterion = nn.MSELoss()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training loop\n&gt;&gt;&gt; for epoch in range(100):\n...     # Forward pass\n...     y_pred = model(X_train)\n...     loss = criterion(y_pred, y_train)\n...\n...     # Backward pass and optimization\n...     optimizer.zero_grad()\n...     loss.backward()\n...     optimizer.step()\n...\n...     if (epoch + 1) % 20 == 0:\n...         print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Evaluate on test set\n&gt;&gt;&gt; model.eval()\n&gt;&gt;&gt; with torch.no_grad():\n...     y_pred = model(X_test)\n...     test_loss = criterion(y_pred, y_test)\n...     print(f'Test Loss: {test_loss.item():.4f}')\nUsing PyTorch Dataset and DataLoader (recommended for larger datasets):\n&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch.utils.data import Dataset, DataLoader\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Custom Dataset class for diabetes data\n&gt;&gt;&gt; class DiabetesDataset(Dataset):\n...     def __init__(self, X, y):\n...         self.X = torch.FloatTensor(X)\n...         self.y = torch.FloatTensor(y)\n...\n...     def __len__(self):\n...         return len(self.X)\n...\n...     def __getitem__(self, idx):\n...         return self.X[idx], self.y[idx]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.2, random_state=42\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Scale data\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n&gt;&gt;&gt; X_test = scaler.transform(X_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create Dataset and DataLoader\n&gt;&gt;&gt; train_dataset = DiabetesDataset(X_train, y_train)\n&gt;&gt;&gt; test_dataset = DiabetesDataset(X_test, y_test)\n&gt;&gt;&gt; train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n&gt;&gt;&gt; test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=32,\n...                         num_hidden_layers=2, activation=\"ReLU\")\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n&gt;&gt;&gt; criterion = nn.MSELoss()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training loop with DataLoader\n&gt;&gt;&gt; for epoch in range(100):\n...     model.train()\n...     for batch_X, batch_y in train_loader:\n...         # Forward pass\n...         predictions = model(batch_X)\n...         loss = criterion(predictions, batch_y)\n...\n...         # Backward pass\n...         optimizer.zero_grad()\n...         loss.backward()\n...         optimizer.step()\n...\n...     # Validation\n...     if (epoch + 1) % 20 == 0:\n...         model.eval()\n...         val_loss = 0.0\n...         with torch.no_grad():\n...             for batch_X, batch_y in test_loader:\n...                 predictions = model(batch_X)\n...                 val_loss += criterion(predictions, batch_y).item()\n...         val_loss /= len(test_loader)\n...         print(f'Epoch [{epoch+1}/100], Val Loss: {val_loss:.4f}')",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "LinearRegressor"
    ]
  },
  {
    "objectID": "docs/reference/nn.linear_regressor.LinearRegressor.html#note",
    "href": "docs/reference/nn.linear_regressor.LinearRegressor.html#note",
    "title": "nn.linear_regressor.LinearRegressor",
    "section": "",
    "text": "When num_hidden_layers=0, the model performs pure linear regression\nActivation functions are only applied between hidden layers, not on output\nUse get_optimizer() method for convenient optimizer instantiation\nFor large datasets, use PyTorch Dataset and DataLoader for efficient batch processing",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "LinearRegressor"
    ]
  },
  {
    "objectID": "docs/reference/nn.linear_regressor.LinearRegressor.html#methods",
    "href": "docs/reference/nn.linear_regressor.LinearRegressor.html#methods",
    "title": "nn.linear_regressor.LinearRegressor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nforward\nForward pass through the network.\n\n\nget_default_parameters\nReturns a ParameterSet populated with default hyperparameters for this model.\n\n\nget_optimizer\nGet a PyTorch optimizer configured for this model.",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "LinearRegressor"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.html",
    "href": "docs/reference/mo.pareto.html",
    "title": "mo.pareto",
    "section": "",
    "text": "mo.pareto\n\n\n\n\n\nName\nDescription\n\n\n\n\nis_pareto_efficient\nFind the Pareto-efficient points from a set of points.\n\n\nmo_pareto_optx_plot\nVisualizes the Pareto-optimal points in the input space for each pair of inputs\n\n\nmo_xy_contour\nGenerates contour plots of every combination of two input variables x_i and x_j\n\n\nmo_xy_surface\nGenerates surface plots of every combination of two input variables x_i and x_j",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo.pareto"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.html#functions",
    "href": "docs/reference/mo.pareto.html#functions",
    "title": "mo.pareto",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nis_pareto_efficient\nFind the Pareto-efficient points from a set of points.\n\n\nmo_pareto_optx_plot\nVisualizes the Pareto-optimal points in the input space for each pair of inputs\n\n\nmo_xy_contour\nGenerates contour plots of every combination of two input variables x_i and x_j\n\n\nmo_xy_surface\nGenerates surface plots of every combination of two input variables x_i and x_j",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo.pareto"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.mo_xy_contour.html",
    "href": "docs/reference/mo.pareto.mo_xy_contour.html",
    "title": "mo.pareto.mo_xy_contour",
    "section": "",
    "text": "mo.pareto.mo_xy_contour(\n    models,\n    bounds,\n    target_names=None,\n    feature_names=None,\n    resolution=50,\n    feature_pairs=None,\n    **kwargs,\n)\nGenerates contour plots of every combination of two input variables x_i and x_j (where i &lt; j) and for each of the multiple objectives f_k.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodels\nlist\nList of trained models (one per objective).\nrequired\n\n\nbounds\nlist\nList of tuples (min, max) for each input variable.\nrequired\n\n\ntarget_names\nlist\nList of names for the objectives. Defaults to None.\nNone\n\n\nfeature_names\nlist\nList of names for the input variables. Defaults to None.\nNone\n\n\nresolution\nint\nGrid resolution for the contour plot. Defaults to 50.\n50\n\n\nfeature_pairs\nlist\nList of tuples (i, j) specifying which feature pairs to plot. If None, all combinations are plotted. Defaults to None.\nNone\n\n\n**kwargs\nAny\nAdditional keyword arguments passed to plt.subplots (e.g., figsize).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotoptim.mo.pareto import mo_xy_contour\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Train dummy models\n&gt;&gt;&gt; X = np.random.rand(10, 2)\n&gt;&gt;&gt; y1 = X[:, 0] + X[:, 1]\n&gt;&gt;&gt; y2 = X[:, 0] * X[:, 1]\n&gt;&gt;&gt; m1 = RandomForestRegressor().fit(X, y1)\n&gt;&gt;&gt; m2 = RandomForestRegressor().fit(X, y2)\n&gt;&gt;&gt; # Plot\n&gt;&gt;&gt; mo_xy_contour([m1, m2], bounds=[(0, 1), (0, 1)], target_names=[\"Sum\", \"Prod\"])",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_xy_contour"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.mo_xy_contour.html#parameters",
    "href": "docs/reference/mo.pareto.mo_xy_contour.html#parameters",
    "title": "mo.pareto.mo_xy_contour",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmodels\nlist\nList of trained models (one per objective).\nrequired\n\n\nbounds\nlist\nList of tuples (min, max) for each input variable.\nrequired\n\n\ntarget_names\nlist\nList of names for the objectives. Defaults to None.\nNone\n\n\nfeature_names\nlist\nList of names for the input variables. Defaults to None.\nNone\n\n\nresolution\nint\nGrid resolution for the contour plot. Defaults to 50.\n50\n\n\nfeature_pairs\nlist\nList of tuples (i, j) specifying which feature pairs to plot. If None, all combinations are plotted. Defaults to None.\nNone\n\n\n**kwargs\nAny\nAdditional keyword arguments passed to plt.subplots (e.g., figsize).\n{}",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_xy_contour"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.mo_xy_contour.html#returns",
    "href": "docs/reference/mo.pareto.mo_xy_contour.html#returns",
    "title": "mo.pareto.mo_xy_contour",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_xy_contour"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.mo_xy_contour.html#examples",
    "href": "docs/reference/mo.pareto.mo_xy_contour.html#examples",
    "title": "mo.pareto.mo_xy_contour",
    "section": "",
    "text": "&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotoptim.mo.pareto import mo_xy_contour\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Train dummy models\n&gt;&gt;&gt; X = np.random.rand(10, 2)\n&gt;&gt;&gt; y1 = X[:, 0] + X[:, 1]\n&gt;&gt;&gt; y2 = X[:, 0] * X[:, 1]\n&gt;&gt;&gt; m1 = RandomForestRegressor().fit(X, y1)\n&gt;&gt;&gt; m2 = RandomForestRegressor().fit(X, y2)\n&gt;&gt;&gt; # Plot\n&gt;&gt;&gt; mo_xy_contour([m1, m2], bounds=[(0, 1), (0, 1)], target_names=[\"Sum\", \"Prod\"])",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_xy_contour"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.is_pareto_efficient.html",
    "href": "docs/reference/mo.pareto.is_pareto_efficient.html",
    "title": "mo.pareto.is_pareto_efficient",
    "section": "",
    "text": "mo.pareto.is_pareto_efficient(costs, minimize=True)\nFind the Pareto-efficient points from a set of points.\nA point is Pareto-efficient if no other point exists that is better in all objectives. This function assumes that lower values are preferred for each objective when minimize=True, and higher values are preferred when minimize=False.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncosts\nnp.ndarray\nAn (N,M) array-like object of points, where N is the number of points and M is the number of objectives.\nrequired\n\n\nminimize\nbool\nIf True, the function finds Pareto-efficient points assuming lower values are better. If False, it assumes higher values are better. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A boolean mask of length N, where True indicates that the corresponding point is Pareto-efficient.\n\n\n\nExamples:\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.mo.pareto import is_pareto_efficient\n&gt;&gt;&gt; points = np.array([[1, 2], [2, 1], [1.5, 1.5], [3, 3]])\n&gt;&gt;&gt; pareto_mask = is_pareto_efficient(points, minimize=True)\n&gt;&gt;&gt; print(pareto_mask)\n[ True  True  True False]",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "is_pareto_efficient"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.is_pareto_efficient.html#parameters",
    "href": "docs/reference/mo.pareto.is_pareto_efficient.html#parameters",
    "title": "mo.pareto.is_pareto_efficient",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ncosts\nnp.ndarray\nAn (N,M) array-like object of points, where N is the number of points and M is the number of objectives.\nrequired\n\n\nminimize\nbool\nIf True, the function finds Pareto-efficient points assuming lower values are better. If False, it assumes higher values are better. Defaults to True.\nTrue",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "is_pareto_efficient"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.is_pareto_efficient.html#returns",
    "href": "docs/reference/mo.pareto.is_pareto_efficient.html#returns",
    "title": "mo.pareto.is_pareto_efficient",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A boolean mask of length N, where True indicates that the corresponding point is Pareto-efficient.\n\n\n\nExamples:\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.mo.pareto import is_pareto_efficient\n&gt;&gt;&gt; points = np.array([[1, 2], [2, 1], [1.5, 1.5], [3, 3]])\n&gt;&gt;&gt; pareto_mask = is_pareto_efficient(points, minimize=True)\n&gt;&gt;&gt; print(pareto_mask)\n[ True  True  True False]",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "is_pareto_efficient"
    ]
  },
  {
    "objectID": "docs/reference/mo.mo_mm.mo_xy_desirability_plot.html",
    "href": "docs/reference/mo.mo_mm.mo_xy_desirability_plot.html",
    "title": "mo.mo_mm.mo_xy_desirability_plot",
    "section": "",
    "text": "mo.mo_mm.mo_xy_desirability_plot(\n    models,\n    X_base,\n    J_base,\n    d_base,\n    phi_base,\n    D_overall,\n    bounds=None,\n    mm_objective=True,\n    resolution=50,\n    feature_names=None,\n    **kwargs,\n)\nGenerates a plot of the desirability landscape. Plots the 2-dim X values as points in the plane and colors them according to their desirability values. For each pair of inputs, x_i and x_j (with i &lt; j), one plot is generated.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodels\nlist\nList of trained models (one per objective).\nrequired\n\n\nX_base\nnp.ndarray\nExisting design points.\nrequired\n\n\nJ_base\nnp.ndarray\nMultiplicities of distances for X_base.\nrequired\n\n\nd_base\nnp.ndarray\nUnique distances for X_base.\nrequired\n\n\nphi_base\nfloat\nBase Morris-Mitchell metric.\nrequired\n\n\nD_overall\nDOverall\nThe overall desirability function.\nrequired\n\n\nbounds\nlist\nList of tuples (min, max) for each dimension. If None, derived from X_base.\nNone\n\n\nmm_objective\nbool\nWhether to include space-filling improvement. Defaults to True.\nTrue\n\n\nresolution\nint\nGrid resolution for the plot. Defaults to 50.\n50\n\n\nfeature_names\nlist\nList of names for the input variables. Defaults to None.\nNone\n\n\n**kwargs\nAny\nAdditional arguments for plt.subplots (e.g., figsize).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.mo.mo_mm import mo_xy_desirability_plot\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_max\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive\n&gt;&gt;&gt; # X_base in the range [0,1]\n&gt;&gt;&gt; X_base = np.random.rand(500, 2)\n&gt;&gt;&gt; y = mo_conv2_max(X_base)\n&gt;&gt;&gt; models = []\n&gt;&gt;&gt; for i in range(y.shape[1]):\n...     model = RandomForestRegressor(n_estimators=100, random_state=42)\n...     model.fit(X_base, y[:, i])\n...     models.append(model)\n&gt;&gt;&gt; # calculate base Morris-Mitchell stats\n&gt;&gt;&gt; phi_base, J_base, d_base = mmphi_intensive(X_base, q=2, p=2)\n&gt;&gt;&gt; d_funcs = []\n&gt;&gt;&gt; for i in range(y.shape[1]):\n...     d_func = DMax(low=np.min(y[:, i]), high=np.max(y[:, i]))\n...     d_funcs.append(d_func)\n&gt;&gt;&gt; D_overall = DOverall(*d_funcs)\n&gt;&gt;&gt; mo_xy_desirability_plot(models, X_base, J_base, d_base, phi_base, D_overall)",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_xy_desirability_plot"
    ]
  },
  {
    "objectID": "docs/reference/mo.mo_mm.mo_xy_desirability_plot.html#parameters",
    "href": "docs/reference/mo.mo_mm.mo_xy_desirability_plot.html#parameters",
    "title": "mo.mo_mm.mo_xy_desirability_plot",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmodels\nlist\nList of trained models (one per objective).\nrequired\n\n\nX_base\nnp.ndarray\nExisting design points.\nrequired\n\n\nJ_base\nnp.ndarray\nMultiplicities of distances for X_base.\nrequired\n\n\nd_base\nnp.ndarray\nUnique distances for X_base.\nrequired\n\n\nphi_base\nfloat\nBase Morris-Mitchell metric.\nrequired\n\n\nD_overall\nDOverall\nThe overall desirability function.\nrequired\n\n\nbounds\nlist\nList of tuples (min, max) for each dimension. If None, derived from X_base.\nNone\n\n\nmm_objective\nbool\nWhether to include space-filling improvement. Defaults to True.\nTrue\n\n\nresolution\nint\nGrid resolution for the plot. Defaults to 50.\n50\n\n\nfeature_names\nlist\nList of names for the input variables. Defaults to None.\nNone\n\n\n**kwargs\nAny\nAdditional arguments for plt.subplots (e.g., figsize).\n{}",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_xy_desirability_plot"
    ]
  },
  {
    "objectID": "docs/reference/mo.mo_mm.mo_xy_desirability_plot.html#returns",
    "href": "docs/reference/mo.mo_mm.mo_xy_desirability_plot.html#returns",
    "title": "mo.mo_mm.mo_xy_desirability_plot",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_xy_desirability_plot"
    ]
  },
  {
    "objectID": "docs/reference/mo.mo_mm.mo_xy_desirability_plot.html#examples",
    "href": "docs/reference/mo.mo_mm.mo_xy_desirability_plot.html#examples",
    "title": "mo.mo_mm.mo_xy_desirability_plot",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.mo.mo_mm import mo_xy_desirability_plot\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_max\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive\n&gt;&gt;&gt; # X_base in the range [0,1]\n&gt;&gt;&gt; X_base = np.random.rand(500, 2)\n&gt;&gt;&gt; y = mo_conv2_max(X_base)\n&gt;&gt;&gt; models = []\n&gt;&gt;&gt; for i in range(y.shape[1]):\n...     model = RandomForestRegressor(n_estimators=100, random_state=42)\n...     model.fit(X_base, y[:, i])\n...     models.append(model)\n&gt;&gt;&gt; # calculate base Morris-Mitchell stats\n&gt;&gt;&gt; phi_base, J_base, d_base = mmphi_intensive(X_base, q=2, p=2)\n&gt;&gt;&gt; d_funcs = []\n&gt;&gt;&gt; for i in range(y.shape[1]):\n...     d_func = DMax(low=np.min(y[:, i]), high=np.max(y[:, i]))\n...     d_funcs.append(d_func)\n&gt;&gt;&gt; D_overall = DOverall(*d_funcs)\n&gt;&gt;&gt; mo_xy_desirability_plot(models, X_base, J_base, d_base, phi_base, D_overall)",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_xy_desirability_plot"
    ]
  },
  {
    "objectID": "docs/reference/mo.mo_mm.mo_mm_desirability_function.html",
    "href": "docs/reference/mo.mo_mm.mo_mm_desirability_function.html",
    "title": "mo.mo_mm.mo_mm_desirability_function",
    "section": "",
    "text": "mo.mo_mm.mo_mm_desirability_function(\n    x,\n    models,\n    X_base,\n    J_base,\n    d_base,\n    phi_base,\n    D_overall,\n    mm_objective=True,\n    verbose=False,\n)\nCalculates the negative combined desirability for a candidate point x. Can be used by the mo_mm_desirability_optimizer. For each objective, a model is used to predict the objective value at x. If mm_objective is True, the Morris-Mitchell improvement is also calculated and included as an additional objective. The combined desirability, which uses the predictions from the models and optionally the Morris-Mitchell improvement, is then computed using the provided DOverall object.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nnp.ndarray\nCandidate point (1D array).\nrequired\n\n\nmodels\nlist\nList of trained models. One model per objective.\nrequired\n\n\nX_base\nnp.ndarray\nExisting design points. Used for computing Morris-Mitchell improvement.\nrequired\n\n\nJ_base\nnp.ndarray\nMultiplicities of distances for X_base. Used for Morris-Mitchell improvement.\nrequired\n\n\nd_base\nnp.ndarray\nUnique distances for X_base. Used for Morris-Mitchell improvement.\nrequired\n\n\nphi_base\nfloat\nBase Morris-Mitchell metric for X_base. Used for Morris-Mitchell improvement.\nrequired\n\n\nD_overall\nDOverall\nThe overall desirability function. Must include desirability functions for each objective and optionally for Morris-Mitchell.\nrequired\n\n\nmm_objective\nbool\nWhether to include space-filling improvement as an objective. Defaults to True.\nTrue\n\n\nverbose\nbool\nWhether to print Morris-Mitchell improvement values. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[float, List[float]]\nTuple[float, List[float]]: A tuple containing: - Negative geometric mean of desirabilities (for minimization). - List of individual objective values.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.mo import mo_mm_desirability_function\n&gt;&gt;&gt; from spotdesirability import DOverall, DMax\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_max\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive\n&gt;&gt;&gt; # X_base in the range [0,1]\n&gt;&gt;&gt; X_base = np.random.rand(500, 2)\n&gt;&gt;&gt; y = mo_conv2_max(X_base)\n&gt;&gt;&gt; models = []\n&gt;&gt;&gt; for i in range(y.shape[1]):\n...     model = RandomForestRegressor(n_estimators=100, random_state=42)\n...     model.fit(X_base, y[:, i])\n...     models.append(model)\n&gt;&gt;&gt; # calculate base Morris-Mitchell stats\n&gt;&gt;&gt; phi_base, J_base, d_base = mmphi_intensive(X_base, q=2, p=2)\n&gt;&gt;&gt; d_funcs = []\n&gt;&gt;&gt; for i in range(y.shape[1]):\n...     d_func = DMax(low=np.min(y[:, i]), high=np.max(y[:, i]))\n...     d_funcs.append(d_func)\n&gt;&gt;&gt; D_overall = DOverall(*d_funcs)\n&gt;&gt;&gt; x_test = np.random.rand(2)  # Example test point\n&gt;&gt;&gt; neg_D, objectives = mo_mm_desirability_function(x_test, models, X_base, J_base, d_base, phi_base, D_overall, mm_objective=False)\n&gt;&gt;&gt; print(f\"Negative Desirability: {neg_D}\")\nNegative Desirability: ...\n&gt;&gt;&gt; print(f\"Objectives: {objectives}\")\nObjectives: ...",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_mm_desirability_function"
    ]
  },
  {
    "objectID": "docs/reference/mo.mo_mm.mo_mm_desirability_function.html#parameters",
    "href": "docs/reference/mo.mo_mm.mo_mm_desirability_function.html#parameters",
    "title": "mo.mo_mm.mo_mm_desirability_function",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\nnp.ndarray\nCandidate point (1D array).\nrequired\n\n\nmodels\nlist\nList of trained models. One model per objective.\nrequired\n\n\nX_base\nnp.ndarray\nExisting design points. Used for computing Morris-Mitchell improvement.\nrequired\n\n\nJ_base\nnp.ndarray\nMultiplicities of distances for X_base. Used for Morris-Mitchell improvement.\nrequired\n\n\nd_base\nnp.ndarray\nUnique distances for X_base. Used for Morris-Mitchell improvement.\nrequired\n\n\nphi_base\nfloat\nBase Morris-Mitchell metric for X_base. Used for Morris-Mitchell improvement.\nrequired\n\n\nD_overall\nDOverall\nThe overall desirability function. Must include desirability functions for each objective and optionally for Morris-Mitchell.\nrequired\n\n\nmm_objective\nbool\nWhether to include space-filling improvement as an objective. Defaults to True.\nTrue\n\n\nverbose\nbool\nWhether to print Morris-Mitchell improvement values. Defaults to False.\nFalse",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_mm_desirability_function"
    ]
  },
  {
    "objectID": "docs/reference/mo.mo_mm.mo_mm_desirability_function.html#returns",
    "href": "docs/reference/mo.mo_mm.mo_mm_desirability_function.html#returns",
    "title": "mo.mo_mm.mo_mm_desirability_function",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nTuple[float, List[float]]\nTuple[float, List[float]]: A tuple containing: - Negative geometric mean of desirabilities (for minimization). - List of individual objective values.",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_mm_desirability_function"
    ]
  },
  {
    "objectID": "docs/reference/mo.mo_mm.mo_mm_desirability_function.html#examples",
    "href": "docs/reference/mo.mo_mm.mo_mm_desirability_function.html#examples",
    "title": "mo.mo_mm.mo_mm_desirability_function",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.mo import mo_mm_desirability_function\n&gt;&gt;&gt; from spotdesirability import DOverall, DMax\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_max\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive\n&gt;&gt;&gt; # X_base in the range [0,1]\n&gt;&gt;&gt; X_base = np.random.rand(500, 2)\n&gt;&gt;&gt; y = mo_conv2_max(X_base)\n&gt;&gt;&gt; models = []\n&gt;&gt;&gt; for i in range(y.shape[1]):\n...     model = RandomForestRegressor(n_estimators=100, random_state=42)\n...     model.fit(X_base, y[:, i])\n...     models.append(model)\n&gt;&gt;&gt; # calculate base Morris-Mitchell stats\n&gt;&gt;&gt; phi_base, J_base, d_base = mmphi_intensive(X_base, q=2, p=2)\n&gt;&gt;&gt; d_funcs = []\n&gt;&gt;&gt; for i in range(y.shape[1]):\n...     d_func = DMax(low=np.min(y[:, i]), high=np.max(y[:, i]))\n...     d_funcs.append(d_func)\n&gt;&gt;&gt; D_overall = DOverall(*d_funcs)\n&gt;&gt;&gt; x_test = np.random.rand(2)  # Example test point\n&gt;&gt;&gt; neg_D, objectives = mo_mm_desirability_function(x_test, models, X_base, J_base, d_base, phi_base, D_overall, mm_objective=False)\n&gt;&gt;&gt; print(f\"Negative Desirability: {neg_D}\")\nNegative Desirability: ...\n&gt;&gt;&gt; print(f\"Objectives: {objectives}\")\nObjectives: ...",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_mm_desirability_function"
    ]
  },
  {
    "objectID": "docs/reference/inspection.predictions.html",
    "href": "docs/reference/inspection.predictions.html",
    "title": "inspection.predictions",
    "section": "",
    "text": "inspection.predictions\n\n\n\n\n\nName\nDescription\n\n\n\n\nplot_actual_vs_predicted\nPlot actual vs. predicted values.",
    "crumbs": [
      "API Reference",
      "Inspection",
      "inspection.predictions"
    ]
  },
  {
    "objectID": "docs/reference/inspection.predictions.html#functions",
    "href": "docs/reference/inspection.predictions.html#functions",
    "title": "inspection.predictions",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nplot_actual_vs_predicted\nPlot actual vs. predicted values.",
    "crumbs": [
      "API Reference",
      "Inspection",
      "inspection.predictions"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.html",
    "href": "docs/reference/inspection.importance.html",
    "title": "inspection.importance",
    "section": "",
    "text": "inspection.importance\n\n\n\n\n\nName\nDescription\n\n\n\n\ngenerate_imp\nGenerates permutation importances from a RandomForestRegressor.\n\n\ngenerate_mdi\nGenerates a DataFrame with Gini importances from a RandomForestRegressor.\n\n\nplot_feature_importances\nGenerate and plot feature importances using MDI and permutation importance.\n\n\nplot_feature_scatter_matrix\nGenerate scatter plot matrix for the most important features.\n\n\nplot_importances\nPlots the impurity-based and permutation-based feature importances for a given classifier.",
    "crumbs": [
      "API Reference",
      "Inspection",
      "inspection.importance"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.html#functions",
    "href": "docs/reference/inspection.importance.html#functions",
    "title": "inspection.importance",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ngenerate_imp\nGenerates permutation importances from a RandomForestRegressor.\n\n\ngenerate_mdi\nGenerates a DataFrame with Gini importances from a RandomForestRegressor.\n\n\nplot_feature_importances\nGenerate and plot feature importances using MDI and permutation importance.\n\n\nplot_feature_scatter_matrix\nGenerate scatter plot matrix for the most important features.\n\n\nplot_importances\nPlots the impurity-based and permutation-based feature importances for a given classifier.",
    "crumbs": [
      "API Reference",
      "Inspection",
      "inspection.importance"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.plot_feature_scatter_matrix.html",
    "href": "docs/reference/inspection.importance.plot_feature_scatter_matrix.html",
    "title": "inspection.importance.plot_feature_scatter_matrix",
    "section": "",
    "text": "inspection.importance.plot_feature_scatter_matrix(\n    X,\n    y,\n    feature_names,\n    target_names,\n    top_features,\n    target_index,\n    figsize=(6, 6),\n)\nGenerate scatter plot matrix for the most important features.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput features array\nrequired\n\n\ny\nnp.ndarray\nTarget array\nrequired\n\n\nfeature_names\nlist\nList of feature names\nrequired\n\n\ntarget_names\nlist\nList of target names\nrequired\n\n\ntop_features\nlist\nList of top feature names to include\nrequired\n\n\ntarget_index\nint\nIndex of target variable to analyze\nrequired\n\n\nfigsize\ntuple\nSize of the figure\n(6, 6)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.sensitivity import plot_feature_scatter_matrix\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n&gt;&gt;&gt; target_names = [\"target\"]\n&gt;&gt;&gt; top_features = [\"feature_0\", \"feature_1\", \"feature_2\"]\n&gt;&gt;&gt; plot_feature_scatter_matrix(X, y, feature_names, target_names, top_features, target_index=0)",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_feature_scatter_matrix"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.plot_feature_scatter_matrix.html#parameters",
    "href": "docs/reference/inspection.importance.plot_feature_scatter_matrix.html#parameters",
    "title": "inspection.importance.plot_feature_scatter_matrix",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput features array\nrequired\n\n\ny\nnp.ndarray\nTarget array\nrequired\n\n\nfeature_names\nlist\nList of feature names\nrequired\n\n\ntarget_names\nlist\nList of target names\nrequired\n\n\ntop_features\nlist\nList of top feature names to include\nrequired\n\n\ntarget_index\nint\nIndex of target variable to analyze\nrequired\n\n\nfigsize\ntuple\nSize of the figure\n(6, 6)",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_feature_scatter_matrix"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.plot_feature_scatter_matrix.html#returns",
    "href": "docs/reference/inspection.importance.plot_feature_scatter_matrix.html#returns",
    "title": "inspection.importance.plot_feature_scatter_matrix",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_feature_scatter_matrix"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.plot_feature_scatter_matrix.html#examples",
    "href": "docs/reference/inspection.importance.plot_feature_scatter_matrix.html#examples",
    "title": "inspection.importance.plot_feature_scatter_matrix",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.sensitivity import plot_feature_scatter_matrix\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n&gt;&gt;&gt; target_names = [\"target\"]\n&gt;&gt;&gt; top_features = [\"feature_0\", \"feature_1\", \"feature_2\"]\n&gt;&gt;&gt; plot_feature_scatter_matrix(X, y, feature_names, target_names, top_features, target_index=0)",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_feature_scatter_matrix"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.generate_mdi.html",
    "href": "docs/reference/inspection.importance.generate_mdi.html",
    "title": "inspection.importance.generate_mdi",
    "section": "",
    "text": "inspection.importance.generate_mdi(X, y, feature_names=None, random_state=42)\nGenerates a DataFrame with Gini importances from a RandomForestRegressor.\n\n\nThere are two limitations of impurity-based feature importances: - impurity-based importances are biased towards high cardinality features; - impurity-based importances are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set. Permutation importances can mitigate the last limitation, because ti can be computed on the test set.\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame or np.ndarray\nThe feature set.\nrequired\n\n\ny\npd.Series or np.ndarray\nThe target variable.\nrequired\n\n\nfeature_names\nlist\nList of feature names for labeling. Defaults to None.\nNone\n\n\nrandom_state\nint\nRandom state for the RandomForestRegressor. Defaults to 42.\n42\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: DataFrame with ‘Feature’ and ‘Importance’ columns.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.sensitivity.importance import generate_mdi\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; X_df = pd.DataFrame(X)\n&gt;&gt;&gt; y_series = pd.Series(y)\n&gt;&gt;&gt; result = generate_mdi(X_df, y_series)\n&gt;&gt;&gt; print(result)",
    "crumbs": [
      "API Reference",
      "Inspection",
      "generate_mdi"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.generate_mdi.html#notes",
    "href": "docs/reference/inspection.importance.generate_mdi.html#notes",
    "title": "inspection.importance.generate_mdi",
    "section": "",
    "text": "There are two limitations of impurity-based feature importances: - impurity-based importances are biased towards high cardinality features; - impurity-based importances are computed on training set statistics and therefore do not reflect the ability of feature to be useful to make predictions that generalize to the test set. Permutation importances can mitigate the last limitation, because ti can be computed on the test set.",
    "crumbs": [
      "API Reference",
      "Inspection",
      "generate_mdi"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.generate_mdi.html#parameters",
    "href": "docs/reference/inspection.importance.generate_mdi.html#parameters",
    "title": "inspection.importance.generate_mdi",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame or np.ndarray\nThe feature set.\nrequired\n\n\ny\npd.Series or np.ndarray\nThe target variable.\nrequired\n\n\nfeature_names\nlist\nList of feature names for labeling. Defaults to None.\nNone\n\n\nrandom_state\nint\nRandom state for the RandomForestRegressor. Defaults to 42.\n42",
    "crumbs": [
      "API Reference",
      "Inspection",
      "generate_mdi"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.generate_mdi.html#returns",
    "href": "docs/reference/inspection.importance.generate_mdi.html#returns",
    "title": "inspection.importance.generate_mdi",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: DataFrame with ‘Feature’ and ‘Importance’ columns.",
    "crumbs": [
      "API Reference",
      "Inspection",
      "generate_mdi"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.generate_mdi.html#examples",
    "href": "docs/reference/inspection.importance.generate_mdi.html#examples",
    "title": "inspection.importance.generate_mdi",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.sensitivity.importance import generate_mdi\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; X_df = pd.DataFrame(X)\n&gt;&gt;&gt; y_series = pd.Series(y)\n&gt;&gt;&gt; result = generate_mdi(X_df, y_series)\n&gt;&gt;&gt; print(result)",
    "crumbs": [
      "API Reference",
      "Inspection",
      "generate_mdi"
    ]
  },
  {
    "objectID": "docs/reference/index.html",
    "href": "docs/reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "SpotOptim\n\n\n\nSpotOptim.SpotOptim\nSPOT optimizer compatible with scipy.optimize interface.\n\n\nSpotOptim.SpotOptimConfig\nConfiguration parameters for SpotOptim.\n\n\nSpotOptim.SpotOptimState\nMutable state of the optimization process.\n\n\ncore\n\n\n\ncore.data\n\n\n\ncore.data.SpotDataFromArray\nData handler for numpy arrays or torch tensors.\n\n\ncore.data.SpotDataFromTorchDataset\nData handler for PyTorch Datasets.\n\n\ncore.data.SpotDataSet\nAbstract base class for data handling in SpotOptim.\n\n\ncore.experiment\n\n\n\ncore.experiment.ExperimentControl\nControls the experiment configuration, replacing the legacy fun_control dictionary.\n\n\n\n\n\n\n\n\n\nsurrogate\nSurrogate models for SpotOptim.\n\n\nsurrogate.kriging\nKriging (Gaussian Process) surrogate model for SpotOptim.\n\n\nsurrogate.kriging.Kriging\nA scikit-learn compatible Kriging model class for regression tasks.\n\n\nsurrogate.simple_kriging\nSimplified SimpleKriging surrogate model for SpotOptim.\n\n\nsurrogate.simple_kriging.SimpleKriging\nA simplified Kriging (Gaussian Process) surrogate model for SpotOptim.\n\n\nsurrogate.mlp_surrogate\nMLP Surrogate model for SpotOptim.\n\n\nsurrogate.mlp_surrogate.MLPSurrogate\nA scikit-learn compatible MLP surrogate model with uncertainty estimation.\n\n\nsurrogate.nystroem\n\n\n\nsurrogate.nystroem.Nystroem\nApproximate a feature map of a kernel using a subset of data.\n\n\nsurrogate.kernels\n\n\n\nsurrogate.kernels.ConstantKernel\nConstant kernel.\n\n\nsurrogate.kernels.Kernel\nBase class for Kernels.\n\n\nsurrogate.kernels.Product\nThe Product kernel k1 * k2.\n\n\nsurrogate.kernels.RBF\nRadial Basis Function (RBF) kernel.\n\n\nsurrogate.kernels.SpotOptimKernel\nKernel designed for SpotOptim’s Kriging with mixed variable support.\n\n\nsurrogate.kernels.Sum\nThe Sum kernel k1 + k2.\n\n\nsurrogate.kernels.WhiteKernel\nWhite kernel.\n\n\nsurrogate.pipeline\n\n\n\nsurrogate.pipeline.Pipeline\nPipeline of transforms with a final estimator.\n\n\n\n\n\n\n\n\n\nsampling\nSampling methods for design of experiments.\n\n\nsampling.design\n\n\n\nsampling.design.fullfactorial\nGenerates a full factorial sampling plan in the unit cube.\n\n\nsampling.design.generate_clustered_design\nGenerates a clustered design.\n\n\nsampling.design.generate_collinear_design\nGenerates a collinear design (poorly projected).\n\n\nsampling.design.generate_grid_design\nGenerates a regular grid design.\n\n\nsampling.design.generate_qmc_lhs_design\nGenerates a Latin Hypercube Sampling design using QMC.\n\n\nsampling.design.generate_sobol_design\nGenerates a Sobol sequence design.\n\n\nsampling.design.generate_uniform_design\nGenerate a uniform random experimental design.\n\n\nsampling.effects\n\n\n\nsampling.effects.plot_all_partial_dependence\nGenerates Partial Dependence Plots (PDPs) for every feature in a DataFrame against a target variable,\n\n\nsampling.effects.randorient\nGenerates a random orientation of a sampling matrix.\n\n\nsampling.effects.screening_plot\nGenerates a plot with elementary effect screening metrics.\n\n\nsampling.effects.screening_print\nGenerates a DataFrame with elementary effect screening metrics.\n\n\nsampling.effects.screeningplan\n\n\n\nsampling.lhs\n\n\n\nsampling.lhs.rlh\nGenerates a random Latin hypercube within the [0,1]^k hypercube.\n\n\nsampling.mm\n\n\n\nsampling.mm.bestlh\nGenerates an optimized Latin hypercube by evolving the Morris-Mitchell\n\n\nsampling.mm.jd\nComputes and counts the distinct p-norm distances between all pairs of points in X.\n\n\nsampling.mm.mm\nDetermines which of two sampling plans has better space-filling properties\n\n\nsampling.mm.mm_improvement\nCalculates the Morris-Mitchell improvement for a candidate point x.\n\n\nsampling.mm.mm_improvement_contour\nGenerates a contour plot of the Morris-Mitchell improvement over a grid defined by x1 and x2.\n\n\nsampling.mm.mmlhs\nPerforms an evolutionary search (using perturbations) to find a Morris-Mitchell\n\n\nsampling.mm.mmphi\nCalculates the Morris-Mitchell sampling plan quality criterion.\n\n\nsampling.mm.mmphi_intensive\nCalculates a size-invariant Morris-Mitchell criterion.\n\n\nsampling.mm.mmphi_intensive_update\nUpdates the Morris-Mitchell intensive criterion for n+1 points by adding a new point to the design.\n\n\nsampling.mm.mmsort\nRanks multiple sampling plans stored in a 3D array according to the\n\n\nsampling.mm.perturb\nPerforms a specified number of random element swaps on a sampling plan.\n\n\nsampling.mm.phisort\nRanks multiple sampling plans stored in a 3D array by the Morris-Mitchell\n\n\nsampling.mm.plot_mmphi_vs_n_lhs\nGenerates LHS designs for varying n, calculates mmphi and mmphi_intensive,\n\n\nsampling.mm.plot_mmphi_vs_points\nPlot the Morris-Mitchell criterion versus the number of added points.\n\n\nsampling.mm.propose_mmphi_intensive_minimizing_point\nPropose a new point that, when added to X, minimizes the intensive Morris-Mitchell (mmphi_intensive) criterion.\n\n\nsampling.mm.subset\nReturns a space-filling subset of a given size from a sampling plan, along with\n\n\n\n\n\n\n\n\n\noptimizer\n\n\n\noptimizer.schedule_free\n\n\n\noptimizer.schedule_free.AdamWScheduleFree\nSchedule-Free AdamW in PyTorch.\n\n\n\n\n\n\n\n\n\nhyperparameters\n\n\n\nhyperparameters.parameters\n\n\n\nhyperparameters.parameters.ParameterSet\nUser-friendly interface for defining hyperparameters.\n\n\nhyperparameters.repr_helpers\n\n\n\nhyperparameters.repr_helpers.Bounds\n\n\n\nhyperparameters.repr_helpers.Parameter\n\n\n\n\n\n\n\n\n\n\nnn\nNeural network models for spotoptim.\n\n\nnn.linear_regressor\n\n\n\nnn.linear_regressor.LinearRegressor\nPyTorch neural network for regression with configurable architecture.\n\n\nnn.mlp\n\n\n\nnn.mlp.MLP\nThis block implements the multi-layer perceptron (MLP) module.\n\n\n\n\n\n\n\n\n\nplot\n\n\n\nplot.contour\n\n\n\nplot.contour.contourf_plot\nCreates contour plots (single or faceted) using matplotlib.\n\n\nplot.contour.mo_generate_plot_grid\nGenerate a grid of input variables and apply objective functions.\n\n\nplot.contour.plotModel\nGenerate 2D contour and optionally 3D surface plots for a model’s predictions.\n\n\nplot.contour.simple_contour\nSimple contour plot\n\n\nplot.mo\n\n\n\nplot.mo.plot_mo\nGenerates scatter plots for each combination of two targets from a multi-output prediction while highlighting Pareto optimal points.\n\n\nplot.visualization\n\n\n\nplot.visualization.plot_important_hyperparameter_contour\nPlot surrogate contours for all combinations of the top max_imp important parameters.\n\n\nplot.visualization.plot_progress\nPlot optimization progress showing all evaluations and best-so-far curve.\n\n\nplot.visualization.plot_surrogate\nPlot the surrogate model for two dimensions.\n\n\n\n\n\n\n\n\n\nutils\nUtility functions for spotoptim.\n\n\nutils.boundaries\n\n\n\nutils.boundaries.get_boundaries\nCalculates the minimum and maximum values for each column in a NumPy array.\n\n\nutils.boundaries.map_to_original_scale\nMaps the values in X_search from the range [0, 1] to the original scale defined by x_min and x_max.\n\n\nutils.eval\n\n\n\nutils.eval.mo_cv_models\nPerforms cross-validation for separate models for each target in a multi-output problem.\n\n\nutils.eval.mo_eval_models\nTrains and evaluates separate models for each target in a multi-output regression problem.\n\n\nutils.file\n\n\n\nutils.file.get_experiment_filename\nGenerates a standardized filename for experiments.\n\n\nutils.mapping\nLearning rate mapping functions for unified optimizer interface.\n\n\nutils.mapping.map_lr\nMap a unified learning rate to an optimizer-specific learning rate.\n\n\nutils.pca\n\n\n\nutils.pca.get_loading_scores\nComputes the loading scores matrix for Principal Component Analysis (PCA).\n\n\nutils.pca.get_pca\nScale the numeric data and perform PCA.\n\n\nutils.pca.get_pca_topk\nIdentify the top k features that have the strongest influence on PC1 and PC2.\n\n\nutils.pca.plot_loading_scores\nCreates a heatmap visualization of PCA loading scores.\n\n\nutils.pca.plot_pca1vs2\nCreate a scatter plot of the first two principal components from PCA.\n\n\nutils.pca.plot_pca_scree\nPlot the scree plot for Principal Component Analysis (PCA).\n\n\nutils.scaler\n\n\n\nutils.scaler.TorchStandardScaler\nA class for scaling data using standardization with torch tensors.\n\n\nutils.stats\n\n\n\nutils.stats.calculate_outliers\nCalculate the number of outliers using the IQR method.\n\n\nutils.stats.compute_coefficients_table\n\n\n\nutils.stats.compute_standardized_betas\nComputes standardized (beta) coefficients for a fitted statsmodels OLS model.\n\n\nutils.stats.condition_index\nCalculates the Condition Index for a DataFrame to assess multicollinearity.\n\n\nutils.stats.cov_to_cor\nConvert a covariance matrix to a correlation matrix.\n\n\nutils.stats.fit_all_lm\nFit a linear regression model for all possible combinations of independent variables.\n\n\nutils.stats.get_all_vars_from_formula\nUtility function to extract variables from a formula.\n\n\nutils.stats.get_combinations\nGenerates all possible combinations of two values from a list of values. Order is not important.\n\n\nutils.stats.get_sample_size\nCalculate sample size n for comparing two means.\n\n\nutils.stats.normalize_X\nNormalize array X to [0, 1] in each dimension.\n\n\nutils.stats.pairwise_semi_partial_correlation\n\n\n\nutils.stats.partial_correlation\nCalculate the partial correlation matrix for a given data set.\n\n\nutils.stats.partial_correlation_test\nThe partial correlation coefficient between x and y given z.\n\n\nutils.stats.plot_coeff_vs_pvals\nPlot the coefficient estimates from fit_all_lm against the corresponding p-values.\n\n\nutils.stats.plot_coeff_vs_pvals_by_included\nGenerates a panel of scatter plots with effect estimates of all possible models against p-values.\n\n\nutils.stats.preprocess_df_for_ols\nPreprocesses a df for fiitting an OLS regression model using the specified target column and predictors.\n\n\nutils.stats.semi_partial_correlation\n\n\n\nutils.stats.vif\nCalculates the Variance Inflation Factor (VIF) for each feature in a DataFrame.\n\n\n\n\n\n\n\n\n\ndata\nData utilities for spotoptim package.\n\n\ndata.base\n\n\n\ndata.base.Config\nBase class for all configurations.\n\n\ndata.base.FileConfig\nBase class for configurations that are stored in a local file.\n\n\ndata.diabetes\n\n\n\ndata.diabetes.DiabetesDataset\nDiabetes dataset wrapping sklearn’s diabetes dataset or custom data.\n\n\ndata.diabetes.get_diabetes_dataloaders\nReturns train and test dataloaders for the Diabetes dataset.\n\n\n\n\n\n\n\n\n\neda\nExploratory Data Analysis (EDA) module for spotoptim.\n\n\neda.plots\n\n\n\neda.plots.plot_ip_boxplots\nGenerate infill-point boxplots (ip-boxplots). A separate ip-boxplot is generated for each numerical column in a DataFrame, arranged in a grid.\n\n\neda.plots.plot_ip_histograms\nGenerate infill-point histograms (ip-histograms) for each numerical column in the DataFrame within a single figure.\n\n\n\n\n\n\n\n\n\nmo\n\n\n\nmo.mo_mm\n\n\n\nmo.mo_mm.mo_mm_desirability_function\nCalculates the negative combined desirability for a candidate point x. Can be used by the mo_mm_desirability_optimizer.\n\n\nmo.mo_mm.mo_mm_desirability_optimizer\nOptimizes the multi-objective function to find the next best point.\n\n\nmo.mo_mm.mo_xy_desirability_plot\nGenerates a plot of the desirability landscape.\n\n\nmo.pareto\n\n\n\nmo.pareto.is_pareto_efficient\nFind the Pareto-efficient points from a set of points.\n\n\nmo.pareto.mo_pareto_optx_plot\nVisualizes the Pareto-optimal points in the input space for each pair of inputs\n\n\nmo.pareto.mo_xy_contour\nGenerates contour plots of every combination of two input variables x_i and x_j\n\n\nmo.pareto.mo_xy_surface\nGenerates surface plots of every combination of two input variables x_i and x_j\n\n\n\n\n\n\n\n\n\ninspection\nInspection (sensitivity analysis) module for spotoptim.\n\n\ninspection.importance\n\n\n\ninspection.importance.generate_imp\nGenerates permutation importances from a RandomForestRegressor.\n\n\ninspection.importance.generate_mdi\nGenerates a DataFrame with Gini importances from a RandomForestRegressor.\n\n\ninspection.importance.plot_feature_importances\nGenerate and plot feature importances using MDI and permutation importance.\n\n\ninspection.importance.plot_feature_scatter_matrix\nGenerate scatter plot matrix for the most important features.\n\n\ninspection.importance.plot_importances\nPlots the impurity-based and permutation-based feature importances for a given classifier.\n\n\ninspection.predictions\n\n\n\ninspection.predictions.plot_actual_vs_predicted\nPlot actual vs. predicted values.\n\n\n\n\n\n\n\n\n\nfactor_analyzer\nThis module performs exploratory and confirmatory factor analyses.\n\n\nfactor_analyzer.confirmatory_factor_analyzer\nConfirmatory factor analysis using machine learning methods.\n\n\nfactor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer\nFit a confirmatory factor analysis model using maximum likelihood.\n\n\nfactor_analyzer.confirmatory_factor_analyzer.ModelSpecification\nEncapsulate the model specification for CFA.\n\n\nfactor_analyzer.confirmatory_factor_analyzer.ModelSpecificationParser\nGenerate the model specification for CFA.\n\n\nfactor_analyzer.factor_analyzer\nFactor analysis using MINRES or ML, with optional rotation using Varimax or Promax.\n\n\nfactor_analyzer.factor_analyzer.FactorAnalyzer\nThe main exploratory factor analysis class.\n\n\nfactor_analyzer.factor_analyzer.calculate_bartlett_sphericity\nCompute the Bartlett sphericity test.\n\n\nfactor_analyzer.factor_analyzer.calculate_kmo\nCalculate the Kaiser-Meyer-Olkin criterion for items and overall.\n\n\nfactor_analyzer.factor_analyzer_rotator\nClass to perform various rotations of factor loading matrices.\n\n\nfactor_analyzer.factor_analyzer_rotator.Rotator\nPerform rotations on an unrotated factor loading matrix.\n\n\nfactor_analyzer.factor_analyzer_utils\nUtility functions, used primarily by the confirmatory factor analysis module.\n\n\nfactor_analyzer.factor_analyzer_utils.apply_impute_nan\nApply a function to impute np.nan values with the mean or the median.\n\n\nfactor_analyzer.factor_analyzer_utils.commutation_matrix\nCalculate the commutation matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.corr\nCalculate the correlation matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.cov\nCalculate the covariance matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.covariance_to_correlation\nCompute cross-correlations from the given covariance matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.duplication_matrix\nCalculate the duplication matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post\nTransform given input symmetric matrix using pre-post duplication.\n\n\nfactor_analyzer.factor_analyzer_utils.fill_lower_diag\nFill the lower diagonal of a square matrix, given a 1-D input array.\n\n\nfactor_analyzer.factor_analyzer_utils.get_first_idxs_from_values\nGet the indexes for a given value.\n\n\nfactor_analyzer.factor_analyzer_utils.get_free_parameter_idxs\nGet the free parameter indices from the flattened matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.get_symmetric_lower_idxs\nGet the indices for the lower triangle of a symmetric matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.get_symmetric_upper_idxs\nGet the indices for the upper triangle of a symmetric matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.impute_values\nImpute np.nan values with the mean or median, or drop the containing rows.\n\n\nfactor_analyzer.factor_analyzer_utils.inv_chol\nCalculate matrix inverse using Cholesky decomposition.\n\n\nfactor_analyzer.factor_analyzer_utils.merge_variance_covariance\nMerge variances and covariances into a single variance-covariance matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.partial_correlations\nCompute partial correlations between variable pairs.\n\n\nfactor_analyzer.factor_analyzer_utils.smc\nCalculate the squared multiple correlations.\n\n\nfactor_analyzer.factor_analyzer_utils.unique_elements\nGet first unique instance of every list element, while maintaining order.\n\n\n\n\n\n\n\n\n\nfunction\nAnalytical test functions for optimization.\n\n\nfunction.forr08a\n\n\n\nfunction.forr08a.aerofoilcd\nComputes the drag coefficient (cd) of an aerofoil based on the shape parameter X.\n\n\nfunction.forr08a.branin\nBranin’s test function that takes a 2D input vector x in the range [0, 1] for each dimension\n\n\nfunction.forr08a.onevar\nOne-variable test function that takes a scalar or 1D array input x in the range [0, 1]\n\n\nfunction.mo\nAnalytical multi-objective test functions for optimization benchmarking.\n\n\nfunction.mo.activity_pred\nCompute activity predictions for each row in the input array.\n\n\nfunction.mo.conversion_pred\nCompute conversion predictions for each row in the input array.\n\n\nfunction.mo.dtlz1\nDTLZ1 multi-objective test function (scalable objectives).\n\n\nfunction.mo.dtlz2\nDTLZ2 multi-objective test function (scalable objectives).\n\n\nfunction.mo.fonseca_fleming\nFonseca-Fleming multi-objective test function (2 objectives).\n\n\nfunction.mo.fun_myer16a\nCompute both conversion and activity predictions for each row in the input array.\n\n\nfunction.mo.kursawe\nKursawe multi-objective test function (2 objectives, minimization).\n\n\nfunction.mo.mo_conv2_max\nConvex bi-objective maximization test function (2 objectives).\n\n\nfunction.mo.mo_conv2_min\nConvex bi-objective minimization test function (2 objectives).\n\n\nfunction.mo.schaffer_n1\nSchaffer N1 multi-objective test function (2 objectives).\n\n\nfunction.mo.zdt1\nZDT1 multi-objective test function (2 objectives).\n\n\nfunction.mo.zdt2\nZDT2 multi-objective test function (2 objectives).\n\n\nfunction.mo.zdt3\nZDT3 multi-objective test function (2 objectives).\n\n\nfunction.mo.zdt4\nZDT4 multi-objective test function (2 objectives).\n\n\nfunction.mo.zdt6\nZDT6 multi-objective test function (2 objectives).\n\n\nfunction.remote\n\n\n\nfunction.remote.objective_remote\nEvaluates an objective function remotely via an HTTP POST request.\n\n\nfunction.so\nAnalytical single-objective test functions for optimization benchmarking.\n\n\nfunction.so.ackley\nN-dimensional Ackley function.\n\n\nfunction.so.lennard_jones\nLennard-Jones Atomic Cluster Potential Energy.\n\n\nfunction.so.michalewicz\nN-dimensional Michalewicz function.\n\n\nfunction.so.robot_arm_hard\n10-Link Robot Arm with Maze-Like Hard Constraints.\n\n\nfunction.so.robot_arm_obstacle\n10-Link Planar Robot Arm Inverse Kinematics with Obstacle Avoidance.\n\n\nfunction.so.rosenbrock\nN-dimensional Rosenbrock function.\n\n\nfunction.so.wingwt\nAircraft Wing Weight function.\n\n\nfunction.torch_objective\n\n\n\nfunction.torch_objective.TorchObjective\nA callable objective function for SpotOptim that trains and evaluates a PyTorch model.\n\n\n\n\n\n\n\n\n\ntricands\n\n\n\ntricands.tricands\n\n\n\ntricands.tricands.tricands\nGenerate Triangulation Candidates for Bayesian Optimization.\n\n\ntricands.tricands.tricands_fringe\nGenerate fringe candidates outside the convex hull.\n\n\ntricands.tricands.tricands_interior\nGenerate interior candidates using Delaunay triangulation.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#core",
    "href": "docs/reference/index.html#core",
    "title": "Function reference",
    "section": "",
    "text": "SpotOptim\n\n\n\nSpotOptim.SpotOptim\nSPOT optimizer compatible with scipy.optimize interface.\n\n\nSpotOptim.SpotOptimConfig\nConfiguration parameters for SpotOptim.\n\n\nSpotOptim.SpotOptimState\nMutable state of the optimization process.\n\n\ncore\n\n\n\ncore.data\n\n\n\ncore.data.SpotDataFromArray\nData handler for numpy arrays or torch tensors.\n\n\ncore.data.SpotDataFromTorchDataset\nData handler for PyTorch Datasets.\n\n\ncore.data.SpotDataSet\nAbstract base class for data handling in SpotOptim.\n\n\ncore.experiment\n\n\n\ncore.experiment.ExperimentControl\nControls the experiment configuration, replacing the legacy fun_control dictionary.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#surrogate-models",
    "href": "docs/reference/index.html#surrogate-models",
    "title": "Function reference",
    "section": "",
    "text": "surrogate\nSurrogate models for SpotOptim.\n\n\nsurrogate.kriging\nKriging (Gaussian Process) surrogate model for SpotOptim.\n\n\nsurrogate.kriging.Kriging\nA scikit-learn compatible Kriging model class for regression tasks.\n\n\nsurrogate.simple_kriging\nSimplified SimpleKriging surrogate model for SpotOptim.\n\n\nsurrogate.simple_kriging.SimpleKriging\nA simplified Kriging (Gaussian Process) surrogate model for SpotOptim.\n\n\nsurrogate.mlp_surrogate\nMLP Surrogate model for SpotOptim.\n\n\nsurrogate.mlp_surrogate.MLPSurrogate\nA scikit-learn compatible MLP surrogate model with uncertainty estimation.\n\n\nsurrogate.nystroem\n\n\n\nsurrogate.nystroem.Nystroem\nApproximate a feature map of a kernel using a subset of data.\n\n\nsurrogate.kernels\n\n\n\nsurrogate.kernels.ConstantKernel\nConstant kernel.\n\n\nsurrogate.kernels.Kernel\nBase class for Kernels.\n\n\nsurrogate.kernels.Product\nThe Product kernel k1 * k2.\n\n\nsurrogate.kernels.RBF\nRadial Basis Function (RBF) kernel.\n\n\nsurrogate.kernels.SpotOptimKernel\nKernel designed for SpotOptim’s Kriging with mixed variable support.\n\n\nsurrogate.kernels.Sum\nThe Sum kernel k1 + k2.\n\n\nsurrogate.kernels.WhiteKernel\nWhite kernel.\n\n\nsurrogate.pipeline\n\n\n\nsurrogate.pipeline.Pipeline\nPipeline of transforms with a final estimator.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#sampling",
    "href": "docs/reference/index.html#sampling",
    "title": "Function reference",
    "section": "",
    "text": "sampling\nSampling methods for design of experiments.\n\n\nsampling.design\n\n\n\nsampling.design.fullfactorial\nGenerates a full factorial sampling plan in the unit cube.\n\n\nsampling.design.generate_clustered_design\nGenerates a clustered design.\n\n\nsampling.design.generate_collinear_design\nGenerates a collinear design (poorly projected).\n\n\nsampling.design.generate_grid_design\nGenerates a regular grid design.\n\n\nsampling.design.generate_qmc_lhs_design\nGenerates a Latin Hypercube Sampling design using QMC.\n\n\nsampling.design.generate_sobol_design\nGenerates a Sobol sequence design.\n\n\nsampling.design.generate_uniform_design\nGenerate a uniform random experimental design.\n\n\nsampling.effects\n\n\n\nsampling.effects.plot_all_partial_dependence\nGenerates Partial Dependence Plots (PDPs) for every feature in a DataFrame against a target variable,\n\n\nsampling.effects.randorient\nGenerates a random orientation of a sampling matrix.\n\n\nsampling.effects.screening_plot\nGenerates a plot with elementary effect screening metrics.\n\n\nsampling.effects.screening_print\nGenerates a DataFrame with elementary effect screening metrics.\n\n\nsampling.effects.screeningplan\n\n\n\nsampling.lhs\n\n\n\nsampling.lhs.rlh\nGenerates a random Latin hypercube within the [0,1]^k hypercube.\n\n\nsampling.mm\n\n\n\nsampling.mm.bestlh\nGenerates an optimized Latin hypercube by evolving the Morris-Mitchell\n\n\nsampling.mm.jd\nComputes and counts the distinct p-norm distances between all pairs of points in X.\n\n\nsampling.mm.mm\nDetermines which of two sampling plans has better space-filling properties\n\n\nsampling.mm.mm_improvement\nCalculates the Morris-Mitchell improvement for a candidate point x.\n\n\nsampling.mm.mm_improvement_contour\nGenerates a contour plot of the Morris-Mitchell improvement over a grid defined by x1 and x2.\n\n\nsampling.mm.mmlhs\nPerforms an evolutionary search (using perturbations) to find a Morris-Mitchell\n\n\nsampling.mm.mmphi\nCalculates the Morris-Mitchell sampling plan quality criterion.\n\n\nsampling.mm.mmphi_intensive\nCalculates a size-invariant Morris-Mitchell criterion.\n\n\nsampling.mm.mmphi_intensive_update\nUpdates the Morris-Mitchell intensive criterion for n+1 points by adding a new point to the design.\n\n\nsampling.mm.mmsort\nRanks multiple sampling plans stored in a 3D array according to the\n\n\nsampling.mm.perturb\nPerforms a specified number of random element swaps on a sampling plan.\n\n\nsampling.mm.phisort\nRanks multiple sampling plans stored in a 3D array by the Morris-Mitchell\n\n\nsampling.mm.plot_mmphi_vs_n_lhs\nGenerates LHS designs for varying n, calculates mmphi and mmphi_intensive,\n\n\nsampling.mm.plot_mmphi_vs_points\nPlot the Morris-Mitchell criterion versus the number of added points.\n\n\nsampling.mm.propose_mmphi_intensive_minimizing_point\nPropose a new point that, when added to X, minimizes the intensive Morris-Mitchell (mmphi_intensive) criterion.\n\n\nsampling.mm.subset\nReturns a space-filling subset of a given size from a sampling plan, along with",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#optimization",
    "href": "docs/reference/index.html#optimization",
    "title": "Function reference",
    "section": "",
    "text": "optimizer\n\n\n\noptimizer.schedule_free\n\n\n\noptimizer.schedule_free.AdamWScheduleFree\nSchedule-Free AdamW in PyTorch.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#hyperparameters",
    "href": "docs/reference/index.html#hyperparameters",
    "title": "Function reference",
    "section": "",
    "text": "hyperparameters\n\n\n\nhyperparameters.parameters\n\n\n\nhyperparameters.parameters.ParameterSet\nUser-friendly interface for defining hyperparameters.\n\n\nhyperparameters.repr_helpers\n\n\n\nhyperparameters.repr_helpers.Bounds\n\n\n\nhyperparameters.repr_helpers.Parameter",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#neural-networks",
    "href": "docs/reference/index.html#neural-networks",
    "title": "Function reference",
    "section": "",
    "text": "nn\nNeural network models for spotoptim.\n\n\nnn.linear_regressor\n\n\n\nnn.linear_regressor.LinearRegressor\nPyTorch neural network for regression with configurable architecture.\n\n\nnn.mlp\n\n\n\nnn.mlp.MLP\nThis block implements the multi-layer perceptron (MLP) module.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#plotting-visualization",
    "href": "docs/reference/index.html#plotting-visualization",
    "title": "Function reference",
    "section": "",
    "text": "plot\n\n\n\nplot.contour\n\n\n\nplot.contour.contourf_plot\nCreates contour plots (single or faceted) using matplotlib.\n\n\nplot.contour.mo_generate_plot_grid\nGenerate a grid of input variables and apply objective functions.\n\n\nplot.contour.plotModel\nGenerate 2D contour and optionally 3D surface plots for a model’s predictions.\n\n\nplot.contour.simple_contour\nSimple contour plot\n\n\nplot.mo\n\n\n\nplot.mo.plot_mo\nGenerates scatter plots for each combination of two targets from a multi-output prediction while highlighting Pareto optimal points.\n\n\nplot.visualization\n\n\n\nplot.visualization.plot_important_hyperparameter_contour\nPlot surrogate contours for all combinations of the top max_imp important parameters.\n\n\nplot.visualization.plot_progress\nPlot optimization progress showing all evaluations and best-so-far curve.\n\n\nplot.visualization.plot_surrogate\nPlot the surrogate model for two dimensions.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#utilities",
    "href": "docs/reference/index.html#utilities",
    "title": "Function reference",
    "section": "",
    "text": "utils\nUtility functions for spotoptim.\n\n\nutils.boundaries\n\n\n\nutils.boundaries.get_boundaries\nCalculates the minimum and maximum values for each column in a NumPy array.\n\n\nutils.boundaries.map_to_original_scale\nMaps the values in X_search from the range [0, 1] to the original scale defined by x_min and x_max.\n\n\nutils.eval\n\n\n\nutils.eval.mo_cv_models\nPerforms cross-validation for separate models for each target in a multi-output problem.\n\n\nutils.eval.mo_eval_models\nTrains and evaluates separate models for each target in a multi-output regression problem.\n\n\nutils.file\n\n\n\nutils.file.get_experiment_filename\nGenerates a standardized filename for experiments.\n\n\nutils.mapping\nLearning rate mapping functions for unified optimizer interface.\n\n\nutils.mapping.map_lr\nMap a unified learning rate to an optimizer-specific learning rate.\n\n\nutils.pca\n\n\n\nutils.pca.get_loading_scores\nComputes the loading scores matrix for Principal Component Analysis (PCA).\n\n\nutils.pca.get_pca\nScale the numeric data and perform PCA.\n\n\nutils.pca.get_pca_topk\nIdentify the top k features that have the strongest influence on PC1 and PC2.\n\n\nutils.pca.plot_loading_scores\nCreates a heatmap visualization of PCA loading scores.\n\n\nutils.pca.plot_pca1vs2\nCreate a scatter plot of the first two principal components from PCA.\n\n\nutils.pca.plot_pca_scree\nPlot the scree plot for Principal Component Analysis (PCA).\n\n\nutils.scaler\n\n\n\nutils.scaler.TorchStandardScaler\nA class for scaling data using standardization with torch tensors.\n\n\nutils.stats\n\n\n\nutils.stats.calculate_outliers\nCalculate the number of outliers using the IQR method.\n\n\nutils.stats.compute_coefficients_table\n\n\n\nutils.stats.compute_standardized_betas\nComputes standardized (beta) coefficients for a fitted statsmodels OLS model.\n\n\nutils.stats.condition_index\nCalculates the Condition Index for a DataFrame to assess multicollinearity.\n\n\nutils.stats.cov_to_cor\nConvert a covariance matrix to a correlation matrix.\n\n\nutils.stats.fit_all_lm\nFit a linear regression model for all possible combinations of independent variables.\n\n\nutils.stats.get_all_vars_from_formula\nUtility function to extract variables from a formula.\n\n\nutils.stats.get_combinations\nGenerates all possible combinations of two values from a list of values. Order is not important.\n\n\nutils.stats.get_sample_size\nCalculate sample size n for comparing two means.\n\n\nutils.stats.normalize_X\nNormalize array X to [0, 1] in each dimension.\n\n\nutils.stats.pairwise_semi_partial_correlation\n\n\n\nutils.stats.partial_correlation\nCalculate the partial correlation matrix for a given data set.\n\n\nutils.stats.partial_correlation_test\nThe partial correlation coefficient between x and y given z.\n\n\nutils.stats.plot_coeff_vs_pvals\nPlot the coefficient estimates from fit_all_lm against the corresponding p-values.\n\n\nutils.stats.plot_coeff_vs_pvals_by_included\nGenerates a panel of scatter plots with effect estimates of all possible models against p-values.\n\n\nutils.stats.preprocess_df_for_ols\nPreprocesses a df for fiitting an OLS regression model using the specified target column and predictors.\n\n\nutils.stats.semi_partial_correlation\n\n\n\nutils.stats.vif\nCalculates the Variance Inflation Factor (VIF) for each feature in a DataFrame.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#data",
    "href": "docs/reference/index.html#data",
    "title": "Function reference",
    "section": "",
    "text": "data\nData utilities for spotoptim package.\n\n\ndata.base\n\n\n\ndata.base.Config\nBase class for all configurations.\n\n\ndata.base.FileConfig\nBase class for configurations that are stored in a local file.\n\n\ndata.diabetes\n\n\n\ndata.diabetes.DiabetesDataset\nDiabetes dataset wrapping sklearn’s diabetes dataset or custom data.\n\n\ndata.diabetes.get_diabetes_dataloaders\nReturns train and test dataloaders for the Diabetes dataset.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#exploratory-data-analysis",
    "href": "docs/reference/index.html#exploratory-data-analysis",
    "title": "Function reference",
    "section": "",
    "text": "eda\nExploratory Data Analysis (EDA) module for spotoptim.\n\n\neda.plots\n\n\n\neda.plots.plot_ip_boxplots\nGenerate infill-point boxplots (ip-boxplots). A separate ip-boxplot is generated for each numerical column in a DataFrame, arranged in a grid.\n\n\neda.plots.plot_ip_histograms\nGenerate infill-point histograms (ip-histograms) for each numerical column in the DataFrame within a single figure.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#multi-objective",
    "href": "docs/reference/index.html#multi-objective",
    "title": "Function reference",
    "section": "",
    "text": "mo\n\n\n\nmo.mo_mm\n\n\n\nmo.mo_mm.mo_mm_desirability_function\nCalculates the negative combined desirability for a candidate point x. Can be used by the mo_mm_desirability_optimizer.\n\n\nmo.mo_mm.mo_mm_desirability_optimizer\nOptimizes the multi-objective function to find the next best point.\n\n\nmo.mo_mm.mo_xy_desirability_plot\nGenerates a plot of the desirability landscape.\n\n\nmo.pareto\n\n\n\nmo.pareto.is_pareto_efficient\nFind the Pareto-efficient points from a set of points.\n\n\nmo.pareto.mo_pareto_optx_plot\nVisualizes the Pareto-optimal points in the input space for each pair of inputs\n\n\nmo.pareto.mo_xy_contour\nGenerates contour plots of every combination of two input variables x_i and x_j\n\n\nmo.pareto.mo_xy_surface\nGenerates surface plots of every combination of two input variables x_i and x_j",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#inspection",
    "href": "docs/reference/index.html#inspection",
    "title": "Function reference",
    "section": "",
    "text": "inspection\nInspection (sensitivity analysis) module for spotoptim.\n\n\ninspection.importance\n\n\n\ninspection.importance.generate_imp\nGenerates permutation importances from a RandomForestRegressor.\n\n\ninspection.importance.generate_mdi\nGenerates a DataFrame with Gini importances from a RandomForestRegressor.\n\n\ninspection.importance.plot_feature_importances\nGenerate and plot feature importances using MDI and permutation importance.\n\n\ninspection.importance.plot_feature_scatter_matrix\nGenerate scatter plot matrix for the most important features.\n\n\ninspection.importance.plot_importances\nPlots the impurity-based and permutation-based feature importances for a given classifier.\n\n\ninspection.predictions\n\n\n\ninspection.predictions.plot_actual_vs_predicted\nPlot actual vs. predicted values.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#factor-analyzer",
    "href": "docs/reference/index.html#factor-analyzer",
    "title": "Function reference",
    "section": "",
    "text": "factor_analyzer\nThis module performs exploratory and confirmatory factor analyses.\n\n\nfactor_analyzer.confirmatory_factor_analyzer\nConfirmatory factor analysis using machine learning methods.\n\n\nfactor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer\nFit a confirmatory factor analysis model using maximum likelihood.\n\n\nfactor_analyzer.confirmatory_factor_analyzer.ModelSpecification\nEncapsulate the model specification for CFA.\n\n\nfactor_analyzer.confirmatory_factor_analyzer.ModelSpecificationParser\nGenerate the model specification for CFA.\n\n\nfactor_analyzer.factor_analyzer\nFactor analysis using MINRES or ML, with optional rotation using Varimax or Promax.\n\n\nfactor_analyzer.factor_analyzer.FactorAnalyzer\nThe main exploratory factor analysis class.\n\n\nfactor_analyzer.factor_analyzer.calculate_bartlett_sphericity\nCompute the Bartlett sphericity test.\n\n\nfactor_analyzer.factor_analyzer.calculate_kmo\nCalculate the Kaiser-Meyer-Olkin criterion for items and overall.\n\n\nfactor_analyzer.factor_analyzer_rotator\nClass to perform various rotations of factor loading matrices.\n\n\nfactor_analyzer.factor_analyzer_rotator.Rotator\nPerform rotations on an unrotated factor loading matrix.\n\n\nfactor_analyzer.factor_analyzer_utils\nUtility functions, used primarily by the confirmatory factor analysis module.\n\n\nfactor_analyzer.factor_analyzer_utils.apply_impute_nan\nApply a function to impute np.nan values with the mean or the median.\n\n\nfactor_analyzer.factor_analyzer_utils.commutation_matrix\nCalculate the commutation matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.corr\nCalculate the correlation matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.cov\nCalculate the covariance matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.covariance_to_correlation\nCompute cross-correlations from the given covariance matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.duplication_matrix\nCalculate the duplication matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post\nTransform given input symmetric matrix using pre-post duplication.\n\n\nfactor_analyzer.factor_analyzer_utils.fill_lower_diag\nFill the lower diagonal of a square matrix, given a 1-D input array.\n\n\nfactor_analyzer.factor_analyzer_utils.get_first_idxs_from_values\nGet the indexes for a given value.\n\n\nfactor_analyzer.factor_analyzer_utils.get_free_parameter_idxs\nGet the free parameter indices from the flattened matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.get_symmetric_lower_idxs\nGet the indices for the lower triangle of a symmetric matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.get_symmetric_upper_idxs\nGet the indices for the upper triangle of a symmetric matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.impute_values\nImpute np.nan values with the mean or median, or drop the containing rows.\n\n\nfactor_analyzer.factor_analyzer_utils.inv_chol\nCalculate matrix inverse using Cholesky decomposition.\n\n\nfactor_analyzer.factor_analyzer_utils.merge_variance_covariance\nMerge variances and covariances into a single variance-covariance matrix.\n\n\nfactor_analyzer.factor_analyzer_utils.partial_correlations\nCompute partial correlations between variable pairs.\n\n\nfactor_analyzer.factor_analyzer_utils.smc\nCalculate the squared multiple correlations.\n\n\nfactor_analyzer.factor_analyzer_utils.unique_elements\nGet first unique instance of every list element, while maintaining order.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#functions",
    "href": "docs/reference/index.html#functions",
    "title": "Function reference",
    "section": "",
    "text": "function\nAnalytical test functions for optimization.\n\n\nfunction.forr08a\n\n\n\nfunction.forr08a.aerofoilcd\nComputes the drag coefficient (cd) of an aerofoil based on the shape parameter X.\n\n\nfunction.forr08a.branin\nBranin’s test function that takes a 2D input vector x in the range [0, 1] for each dimension\n\n\nfunction.forr08a.onevar\nOne-variable test function that takes a scalar or 1D array input x in the range [0, 1]\n\n\nfunction.mo\nAnalytical multi-objective test functions for optimization benchmarking.\n\n\nfunction.mo.activity_pred\nCompute activity predictions for each row in the input array.\n\n\nfunction.mo.conversion_pred\nCompute conversion predictions for each row in the input array.\n\n\nfunction.mo.dtlz1\nDTLZ1 multi-objective test function (scalable objectives).\n\n\nfunction.mo.dtlz2\nDTLZ2 multi-objective test function (scalable objectives).\n\n\nfunction.mo.fonseca_fleming\nFonseca-Fleming multi-objective test function (2 objectives).\n\n\nfunction.mo.fun_myer16a\nCompute both conversion and activity predictions for each row in the input array.\n\n\nfunction.mo.kursawe\nKursawe multi-objective test function (2 objectives, minimization).\n\n\nfunction.mo.mo_conv2_max\nConvex bi-objective maximization test function (2 objectives).\n\n\nfunction.mo.mo_conv2_min\nConvex bi-objective minimization test function (2 objectives).\n\n\nfunction.mo.schaffer_n1\nSchaffer N1 multi-objective test function (2 objectives).\n\n\nfunction.mo.zdt1\nZDT1 multi-objective test function (2 objectives).\n\n\nfunction.mo.zdt2\nZDT2 multi-objective test function (2 objectives).\n\n\nfunction.mo.zdt3\nZDT3 multi-objective test function (2 objectives).\n\n\nfunction.mo.zdt4\nZDT4 multi-objective test function (2 objectives).\n\n\nfunction.mo.zdt6\nZDT6 multi-objective test function (2 objectives).\n\n\nfunction.remote\n\n\n\nfunction.remote.objective_remote\nEvaluates an objective function remotely via an HTTP POST request.\n\n\nfunction.so\nAnalytical single-objective test functions for optimization benchmarking.\n\n\nfunction.so.ackley\nN-dimensional Ackley function.\n\n\nfunction.so.lennard_jones\nLennard-Jones Atomic Cluster Potential Energy.\n\n\nfunction.so.michalewicz\nN-dimensional Michalewicz function.\n\n\nfunction.so.robot_arm_hard\n10-Link Robot Arm with Maze-Like Hard Constraints.\n\n\nfunction.so.robot_arm_obstacle\n10-Link Planar Robot Arm Inverse Kinematics with Obstacle Avoidance.\n\n\nfunction.so.rosenbrock\nN-dimensional Rosenbrock function.\n\n\nfunction.so.wingwt\nAircraft Wing Weight function.\n\n\nfunction.torch_objective\n\n\n\nfunction.torch_objective.TorchObjective\nA callable objective function for SpotOptim that trains and evaluates a PyTorch model.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/index.html#tricands",
    "href": "docs/reference/index.html#tricands",
    "title": "Function reference",
    "section": "",
    "text": "tricands\n\n\n\ntricands.tricands\n\n\n\ntricands.tricands.tricands\nGenerate Triangulation Candidates for Bayesian Optimization.\n\n\ntricands.tricands.tricands_fringe\nGenerate fringe candidates outside the convex hull.\n\n\ntricands.tricands.tricands_interior\nGenerate interior candidates using Delaunay triangulation.",
    "crumbs": [
      "API Reference",
      "Overview"
    ]
  },
  {
    "objectID": "docs/reference/hyperparameters.repr_helpers.Parameter.html",
    "href": "docs/reference/hyperparameters.repr_helpers.Parameter.html",
    "title": "hyperparameters.repr_helpers.Parameter",
    "section": "",
    "text": "hyperparameters.repr_helpers.Parameter\nhyperparameters.repr_helpers.Parameter(\n    name,\n    var_name,\n    bounds,\n    default,\n    transform,\n    type,\n)",
    "crumbs": [
      "API Reference",
      "Hyperparameters",
      "Parameter"
    ]
  },
  {
    "objectID": "docs/reference/hyperparameters.html",
    "href": "docs/reference/hyperparameters.html",
    "title": "hyperparameters",
    "section": "",
    "text": "hyperparameters\nhyperparameters",
    "crumbs": [
      "API Reference",
      "Hyperparameters",
      "hyperparameters"
    ]
  },
  {
    "objectID": "docs/reference/hyperparameters.parameters.ParameterSet.html",
    "href": "docs/reference/hyperparameters.parameters.ParameterSet.html",
    "title": "hyperparameters.parameters.ParameterSet",
    "section": "",
    "text": "hyperparameters.parameters.ParameterSet()\nUser-friendly interface for defining hyperparameters.\nThis class allows for the definition of a set of hyperparameters including their types, bounds, default values, and transformations. It supports float, integer, and categorical parameters and provides a fluent interface for chaining parameter definitions.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n_parameters\nList[Dict]\nList of parameter definitions.\n\n\n_var_names\nList[str]\nList of parameter names.\n\n\n_var_types\nList[str]\nList of parameter types (‘float’, ‘int’, ‘factor’).\n\n\n_bounds\nList[Union[Tuple, List]]\nList of bounds for each parameter.\n\n\n_defaults\nDict[str, Any]\nDictionary of default values.\n\n\n_var_trans\nList[Optional[str]]\nList of variable transformations.\n\n\n\n\n\n\n&gt;&gt;&gt; ps = ParameterSet()\n&gt;&gt;&gt; ps.add_float(\"max_depth\", 1, 10, default=3)\nParameterSet(\n    max_depth=Parameter(name='max_depth', type='float', check_on_set=True, bounds=(1, 10), default=3),\n)\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nadd_factor\nAdd a factor (categorical) hyperparameter.\n\n\nadd_float\nAdd a float hyperparameter.\n\n\nadd_int\nAdd an integer hyperparameter.\n\n\nnames\nReturns a list of parameter names.\n\n\nsample_default\nReturns the default configuration.",
    "crumbs": [
      "API Reference",
      "Hyperparameters",
      "ParameterSet"
    ]
  },
  {
    "objectID": "docs/reference/hyperparameters.parameters.ParameterSet.html#attributes",
    "href": "docs/reference/hyperparameters.parameters.ParameterSet.html#attributes",
    "title": "hyperparameters.parameters.ParameterSet",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n_parameters\nList[Dict]\nList of parameter definitions.\n\n\n_var_names\nList[str]\nList of parameter names.\n\n\n_var_types\nList[str]\nList of parameter types (‘float’, ‘int’, ‘factor’).\n\n\n_bounds\nList[Union[Tuple, List]]\nList of bounds for each parameter.\n\n\n_defaults\nDict[str, Any]\nDictionary of default values.\n\n\n_var_trans\nList[Optional[str]]\nList of variable transformations.",
    "crumbs": [
      "API Reference",
      "Hyperparameters",
      "ParameterSet"
    ]
  },
  {
    "objectID": "docs/reference/hyperparameters.parameters.ParameterSet.html#examples",
    "href": "docs/reference/hyperparameters.parameters.ParameterSet.html#examples",
    "title": "hyperparameters.parameters.ParameterSet",
    "section": "",
    "text": "&gt;&gt;&gt; ps = ParameterSet()\n&gt;&gt;&gt; ps.add_float(\"max_depth\", 1, 10, default=3)\nParameterSet(\n    max_depth=Parameter(name='max_depth', type='float', check_on_set=True, bounds=(1, 10), default=3),\n)",
    "crumbs": [
      "API Reference",
      "Hyperparameters",
      "ParameterSet"
    ]
  },
  {
    "objectID": "docs/reference/hyperparameters.parameters.ParameterSet.html#methods",
    "href": "docs/reference/hyperparameters.parameters.ParameterSet.html#methods",
    "title": "hyperparameters.parameters.ParameterSet",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nadd_factor\nAdd a factor (categorical) hyperparameter.\n\n\nadd_float\nAdd a float hyperparameter.\n\n\nadd_int\nAdd an integer hyperparameter.\n\n\nnames\nReturns a list of parameter names.\n\n\nsample_default\nReturns the default configuration.",
    "crumbs": [
      "API Reference",
      "Hyperparameters",
      "ParameterSet"
    ]
  },
  {
    "objectID": "docs/reference/function.torch_objective.TorchObjective.html",
    "href": "docs/reference/function.torch_objective.TorchObjective.html",
    "title": "function.torch_objective.TorchObjective",
    "section": "",
    "text": "function.torch_objective.TorchObjective(experiment, seed=None, use_scaler=False)\nA callable objective function for SpotOptim that trains and evaluates a PyTorch model.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nbounds\nReturns the bounds of the hyperparameters.\n\n\nobjective_names\nReturns the names of the objectives.\n\n\nvar_name\nReturns the names of the hyperparameters.\n\n\nvar_trans\nReturns the transformations of the hyperparameters.\n\n\nvar_type\nReturns the types of the hyperparameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ntrain_model\nTrains the model and returns a dictionary of metrics.",
    "crumbs": [
      "API Reference",
      "Functions",
      "TorchObjective"
    ]
  },
  {
    "objectID": "docs/reference/function.torch_objective.TorchObjective.html#attributes",
    "href": "docs/reference/function.torch_objective.TorchObjective.html#attributes",
    "title": "function.torch_objective.TorchObjective",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbounds\nReturns the bounds of the hyperparameters.\n\n\nobjective_names\nReturns the names of the objectives.\n\n\nvar_name\nReturns the names of the hyperparameters.\n\n\nvar_trans\nReturns the transformations of the hyperparameters.\n\n\nvar_type\nReturns the types of the hyperparameters.",
    "crumbs": [
      "API Reference",
      "Functions",
      "TorchObjective"
    ]
  },
  {
    "objectID": "docs/reference/function.torch_objective.TorchObjective.html#methods",
    "href": "docs/reference/function.torch_objective.TorchObjective.html#methods",
    "title": "function.torch_objective.TorchObjective",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ntrain_model\nTrains the model and returns a dictionary of metrics.",
    "crumbs": [
      "API Reference",
      "Functions",
      "TorchObjective"
    ]
  },
  {
    "objectID": "docs/reference/function.so.rosenbrock.html",
    "href": "docs/reference/function.so.rosenbrock.html",
    "title": "function.so.rosenbrock",
    "section": "",
    "text": "function.so.rosenbrock(X)\nN-dimensional Rosenbrock function.\nThe Rosenbrock function is a classic test function for optimization algorithms. It is characterized by a long, narrow, parabolic-shaped valley. The global minimum is inside the valley and is hard to find for many algorithms.\n\n\nf(x, y) = (1 - x)^2 + 100 * (y - x2)2\n\n\n\nf(X) = sum_{i=1}^{N-1} [100 * (x_{i+1} - x_i2)2 + (1 - x_i)^2]\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Function values at the input points with shape (n_samples,).\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.\n\n\n\n\n\n\n\nGlobal minimum: f(1, 1, …, 1) = 0\nTypical search domain: [-5, 10]^N or [-2, 2]^N\nCharacteristics: Non-convex, unimodal\n\n\n\n\nSingle point evaluation:\n&gt;&gt;&gt; from spotoptim.function import rosenbrock\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([1.0, 1.0])\n&gt;&gt;&gt; rosenbrock(X)\narray([0.])\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [1.0, 1.0], [0.5, 0.5]])\n&gt;&gt;&gt; rosenbrock(X)\narray([1.00e+00, 0.00e+00, 3.06e+01])\n\n\n\nRosenbrock, H.H. (1960). “An automatic method for finding the greatest or least value of a function”. The Computer Journal. 3 (3): 175–184.",
    "crumbs": [
      "API Reference",
      "Functions",
      "rosenbrock"
    ]
  },
  {
    "objectID": "docs/reference/function.so.rosenbrock.html#for-the-2d-case",
    "href": "docs/reference/function.so.rosenbrock.html#for-the-2d-case",
    "title": "function.so.rosenbrock",
    "section": "",
    "text": "f(x, y) = (1 - x)^2 + 100 * (y - x2)2",
    "crumbs": [
      "API Reference",
      "Functions",
      "rosenbrock"
    ]
  },
  {
    "objectID": "docs/reference/function.so.rosenbrock.html#the-generalized-form-for-n-dimensions",
    "href": "docs/reference/function.so.rosenbrock.html#the-generalized-form-for-n-dimensions",
    "title": "function.so.rosenbrock",
    "section": "",
    "text": "f(X) = sum_{i=1}^{N-1} [100 * (x_{i+1} - x_i2)2 + (1 - x_i)^2]",
    "crumbs": [
      "API Reference",
      "Functions",
      "rosenbrock"
    ]
  },
  {
    "objectID": "docs/reference/function.so.rosenbrock.html#parameters",
    "href": "docs/reference/function.so.rosenbrock.html#parameters",
    "title": "function.so.rosenbrock",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "rosenbrock"
    ]
  },
  {
    "objectID": "docs/reference/function.so.rosenbrock.html#returns",
    "href": "docs/reference/function.so.rosenbrock.html#returns",
    "title": "function.so.rosenbrock",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Function values at the input points with shape (n_samples,).",
    "crumbs": [
      "API Reference",
      "Functions",
      "rosenbrock"
    ]
  },
  {
    "objectID": "docs/reference/function.so.rosenbrock.html#raises",
    "href": "docs/reference/function.so.rosenbrock.html#raises",
    "title": "function.so.rosenbrock",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.",
    "crumbs": [
      "API Reference",
      "Functions",
      "rosenbrock"
    ]
  },
  {
    "objectID": "docs/reference/function.so.rosenbrock.html#note",
    "href": "docs/reference/function.so.rosenbrock.html#note",
    "title": "function.so.rosenbrock",
    "section": "",
    "text": "Global minimum: f(1, 1, …, 1) = 0\nTypical search domain: [-5, 10]^N or [-2, 2]^N\nCharacteristics: Non-convex, unimodal",
    "crumbs": [
      "API Reference",
      "Functions",
      "rosenbrock"
    ]
  },
  {
    "objectID": "docs/reference/function.so.rosenbrock.html#examples",
    "href": "docs/reference/function.so.rosenbrock.html#examples",
    "title": "function.so.rosenbrock",
    "section": "",
    "text": "Single point evaluation:\n&gt;&gt;&gt; from spotoptim.function import rosenbrock\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([1.0, 1.0])\n&gt;&gt;&gt; rosenbrock(X)\narray([0.])\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [1.0, 1.0], [0.5, 0.5]])\n&gt;&gt;&gt; rosenbrock(X)\narray([1.00e+00, 0.00e+00, 3.06e+01])",
    "crumbs": [
      "API Reference",
      "Functions",
      "rosenbrock"
    ]
  },
  {
    "objectID": "docs/reference/function.so.rosenbrock.html#references",
    "href": "docs/reference/function.so.rosenbrock.html#references",
    "title": "function.so.rosenbrock",
    "section": "",
    "text": "Rosenbrock, H.H. (1960). “An automatic method for finding the greatest or least value of a function”. The Computer Journal. 3 (3): 175–184.",
    "crumbs": [
      "API Reference",
      "Functions",
      "rosenbrock"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_hard.html",
    "href": "docs/reference/function.so.robot_arm_hard.html",
    "title": "function.so.robot_arm_hard",
    "section": "",
    "text": "function.so.robot_arm_hard(X)\n10-Link Robot Arm with Maze-Like Hard Constraints.\nA challenging constrained optimization problem where a 10-link planar robot arm must reach a target point (5.0, 5.0) while avoiding multiple obstacles forming a maze-like environment. This function tests an optimizer’s ability to handle hard constraints and navigate complex feasible regions.\nThe problem features three main difficulty factors: 1. ‘The Great Wall’: A vertical barrier at x=2.5 blocking direct paths 2. ‘The Ceiling’: A horizontal bar at y=8.5 preventing high loop strategies 3. ‘The Target Trap’: Obstacles surrounding the target, requiring precise approach\n\n\nf(X) = distance_cost + constraint_penalty + energy_regularization\n\n\n\n\ndistance_cost = (x_end - 5.0)^2 + (y_end - 5.0)^2\nconstraint_penalty = 10,000 * sum(max(0, r - d + 0.05)^2) for all joints and obstacles\nenergy_regularization = 0.01 * sum(angles^2)\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput points with shape (n_samples, 10) or (10,). Each sample contains 10 joint angles normalized to [0, 1], which are internally mapped to [-1.2π, 1.2π] to allow looping strategies. Can be a 1D array for a single point or 2D array for multiple points.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Function values at the input points with shape (n_samples,). Lower values indicate better solutions (closer to target with fewer constraint violations).\n\n\n\n\n\n\n\nDimension: 10 (one angle per link)\nLink length: L = 1.0 for all links\nTarget position: (5.0, 5.0)\nSearch domain: [0, 1]^10 (mapped internally to [-1.2π, 1.2π]^10)\nCharacteristics: Highly constrained, non-convex, multimodal\nConstraint penalty: 10,000 per violation (effectively hard constraints)\nNumber of obstacles: ~30 forming walls and traps\nFeasible region: Very small relative to search space\n\n\n\n\nSingle point evaluation with random configuration:\n&gt;&gt;&gt; from spotoptim.function import robot_arm_hard\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.random.rand(10) * 0.5  # Conservative random angles\n&gt;&gt;&gt; result = robot_arm_hard(X)\n&gt;&gt;&gt; result.shape\n(1,)\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n...               [0.3, 0.4, 0.5, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]])\n&gt;&gt;&gt; robot_arm_hard(X)\narray([...])  # Returns costs for both configurations\nEvaluating a straight configuration (all angles = 0.5, mapped to 0 radians):\n&gt;&gt;&gt; X_straight = np.full(10, 0.5)\n&gt;&gt;&gt; cost_straight = robot_arm_hard(X_straight)\n&gt;&gt;&gt; cost_straight[0] &gt; 1000  # High cost due to obstacles\nTrue\n\n\n\nThis function is inspired by robot motion planning problems with obstacles, commonly studied in:\n\nLaValle, S. M. (2006). “Planning Algorithms”. Cambridge University Press.\nChoset, H., et al. (2005). “Principles of Robot Motion: Theory, Algorithms, and Implementations”. MIT Press.",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_hard"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_hard.html#mathematical-formulation",
    "href": "docs/reference/function.so.robot_arm_hard.html#mathematical-formulation",
    "title": "function.so.robot_arm_hard",
    "section": "",
    "text": "f(X) = distance_cost + constraint_penalty + energy_regularization",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_hard"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_hard.html#where",
    "href": "docs/reference/function.so.robot_arm_hard.html#where",
    "title": "function.so.robot_arm_hard",
    "section": "",
    "text": "distance_cost = (x_end - 5.0)^2 + (y_end - 5.0)^2\nconstraint_penalty = 10,000 * sum(max(0, r - d + 0.05)^2) for all joints and obstacles\nenergy_regularization = 0.01 * sum(angles^2)",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_hard"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_hard.html#parameters",
    "href": "docs/reference/function.so.robot_arm_hard.html#parameters",
    "title": "function.so.robot_arm_hard",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput points with shape (n_samples, 10) or (10,). Each sample contains 10 joint angles normalized to [0, 1], which are internally mapped to [-1.2π, 1.2π] to allow looping strategies. Can be a 1D array for a single point or 2D array for multiple points.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_hard"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_hard.html#returns",
    "href": "docs/reference/function.so.robot_arm_hard.html#returns",
    "title": "function.so.robot_arm_hard",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Function values at the input points with shape (n_samples,). Lower values indicate better solutions (closer to target with fewer constraint violations).",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_hard"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_hard.html#note",
    "href": "docs/reference/function.so.robot_arm_hard.html#note",
    "title": "function.so.robot_arm_hard",
    "section": "",
    "text": "Dimension: 10 (one angle per link)\nLink length: L = 1.0 for all links\nTarget position: (5.0, 5.0)\nSearch domain: [0, 1]^10 (mapped internally to [-1.2π, 1.2π]^10)\nCharacteristics: Highly constrained, non-convex, multimodal\nConstraint penalty: 10,000 per violation (effectively hard constraints)\nNumber of obstacles: ~30 forming walls and traps\nFeasible region: Very small relative to search space",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_hard"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_hard.html#examples",
    "href": "docs/reference/function.so.robot_arm_hard.html#examples",
    "title": "function.so.robot_arm_hard",
    "section": "",
    "text": "Single point evaluation with random configuration:\n&gt;&gt;&gt; from spotoptim.function import robot_arm_hard\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.random.rand(10) * 0.5  # Conservative random angles\n&gt;&gt;&gt; result = robot_arm_hard(X)\n&gt;&gt;&gt; result.shape\n(1,)\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n...               [0.3, 0.4, 0.5, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]])\n&gt;&gt;&gt; robot_arm_hard(X)\narray([...])  # Returns costs for both configurations\nEvaluating a straight configuration (all angles = 0.5, mapped to 0 radians):\n&gt;&gt;&gt; X_straight = np.full(10, 0.5)\n&gt;&gt;&gt; cost_straight = robot_arm_hard(X_straight)\n&gt;&gt;&gt; cost_straight[0] &gt; 1000  # High cost due to obstacles\nTrue",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_hard"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_hard.html#references",
    "href": "docs/reference/function.so.robot_arm_hard.html#references",
    "title": "function.so.robot_arm_hard",
    "section": "",
    "text": "This function is inspired by robot motion planning problems with obstacles, commonly studied in:\n\nLaValle, S. M. (2006). “Planning Algorithms”. Cambridge University Press.\nChoset, H., et al. (2005). “Principles of Robot Motion: Theory, Algorithms, and Implementations”. MIT Press.",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_hard"
    ]
  },
  {
    "objectID": "docs/reference/function.so.michalewicz.html",
    "href": "docs/reference/function.so.michalewicz.html",
    "title": "function.so.michalewicz",
    "section": "",
    "text": "function.so.michalewicz(X, m=10)\nN-dimensional Michalewicz function.\nThe Michalewicz function is a multimodal test function with steep ridges and valleys. The parameter m defines the steepness of the valleys and ridges. Larger values of m result in more difficult search problems. The number of local minima increases exponentially with the dimension.\n\n\nf(X) = -sum_{i=1}^{n} sin(x_i) * [sin(i * x_i^2 / π)]^(2m)\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.\nrequired\n\n\nm\nint\nSteepness parameter. Higher values make the function more difficult to optimize. Defaults to 10.\n10\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Function values at the input points with shape (n_samples,).\n\n\n\n\n\n\n\nGlobal minimum depends on dimension:\n\n2D: f(2.20, 1.57) ≈ -1.8013\n5D: f ≈ -4.687658\n10D: f ≈ -9.66015\n\nTypical search domain: [0, π]^N\nCharacteristics: Non-convex, multimodal, non-separable\n\n\n\n\nSingle point evaluation:\n&gt;&gt;&gt; from spotoptim.function import michalewicz\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([2.20, 1.57])\n&gt;&gt;&gt; result = michalewicz(X)\n&gt;&gt;&gt; result[0]  # Should be close to -1.8013\n-1.801303...\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[2.20, 1.57], [1.0, 1.0]])\n&gt;&gt;&gt; michalewicz(X)\narray([-1.8013..., -1.4508...])\nUsing different steepness parameter:\n&gt;&gt;&gt; X = np.array([2.20, 1.57])\n&gt;&gt;&gt; michalewicz(X, m=5)\narray([-1.6862...])\n\n\n\nMichalewicz, Z. (1996). “Genetic Algorithms + Data Structures = Evolution Programs”. Springer-Verlag.",
    "crumbs": [
      "API Reference",
      "Functions",
      "michalewicz"
    ]
  },
  {
    "objectID": "docs/reference/function.so.michalewicz.html#mathematical-formula",
    "href": "docs/reference/function.so.michalewicz.html#mathematical-formula",
    "title": "function.so.michalewicz",
    "section": "",
    "text": "f(X) = -sum_{i=1}^{n} sin(x_i) * [sin(i * x_i^2 / π)]^(2m)",
    "crumbs": [
      "API Reference",
      "Functions",
      "michalewicz"
    ]
  },
  {
    "objectID": "docs/reference/function.so.michalewicz.html#parameters",
    "href": "docs/reference/function.so.michalewicz.html#parameters",
    "title": "function.so.michalewicz",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.\nrequired\n\n\nm\nint\nSteepness parameter. Higher values make the function more difficult to optimize. Defaults to 10.\n10",
    "crumbs": [
      "API Reference",
      "Functions",
      "michalewicz"
    ]
  },
  {
    "objectID": "docs/reference/function.so.michalewicz.html#returns",
    "href": "docs/reference/function.so.michalewicz.html#returns",
    "title": "function.so.michalewicz",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Function values at the input points with shape (n_samples,).",
    "crumbs": [
      "API Reference",
      "Functions",
      "michalewicz"
    ]
  },
  {
    "objectID": "docs/reference/function.so.michalewicz.html#note",
    "href": "docs/reference/function.so.michalewicz.html#note",
    "title": "function.so.michalewicz",
    "section": "",
    "text": "Global minimum depends on dimension:\n\n2D: f(2.20, 1.57) ≈ -1.8013\n5D: f ≈ -4.687658\n10D: f ≈ -9.66015\n\nTypical search domain: [0, π]^N\nCharacteristics: Non-convex, multimodal, non-separable",
    "crumbs": [
      "API Reference",
      "Functions",
      "michalewicz"
    ]
  },
  {
    "objectID": "docs/reference/function.so.michalewicz.html#examples",
    "href": "docs/reference/function.so.michalewicz.html#examples",
    "title": "function.so.michalewicz",
    "section": "",
    "text": "Single point evaluation:\n&gt;&gt;&gt; from spotoptim.function import michalewicz\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([2.20, 1.57])\n&gt;&gt;&gt; result = michalewicz(X)\n&gt;&gt;&gt; result[0]  # Should be close to -1.8013\n-1.801303...\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[2.20, 1.57], [1.0, 1.0]])\n&gt;&gt;&gt; michalewicz(X)\narray([-1.8013..., -1.4508...])\nUsing different steepness parameter:\n&gt;&gt;&gt; X = np.array([2.20, 1.57])\n&gt;&gt;&gt; michalewicz(X, m=5)\narray([-1.6862...])",
    "crumbs": [
      "API Reference",
      "Functions",
      "michalewicz"
    ]
  },
  {
    "objectID": "docs/reference/function.so.michalewicz.html#references",
    "href": "docs/reference/function.so.michalewicz.html#references",
    "title": "function.so.michalewicz",
    "section": "",
    "text": "Michalewicz, Z. (1996). “Genetic Algorithms + Data Structures = Evolution Programs”. Springer-Verlag.",
    "crumbs": [
      "API Reference",
      "Functions",
      "michalewicz"
    ]
  },
  {
    "objectID": "docs/reference/function.so.ackley.html",
    "href": "docs/reference/function.so.ackley.html",
    "title": "function.so.ackley",
    "section": "",
    "text": "function.so.ackley(X)\nN-dimensional Ackley function.\nThe Ackley function is a widely used test function for optimization algorithms. It is characterized by a nearly flat outer region and a large hole at the center. The function is multimodal with many local minima but only one global minimum.\n\n\nf(X) = -a * exp(-b * sqrt(sum(x_i^2) / n)) - exp(sum(cos(c * x_i)) / n) + a + e\n\n\n\n\na = 20 (default)\nb = 0.2 (default)\nc = 2π (default)\ne = exp(1) ≈ 2.71828\nn = number of dimensions\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Function values at the input points with shape (n_samples,).\n\n\n\n\n\n\n\nGlobal minimum: f(0, 0, …, 0) = 0\nTypical search domain: [-32.768, 32.768]^N\nCharacteristics: Non-convex, multimodal, separable\n\n\n\n\nSingle point evaluation at global minimum:\n&gt;&gt;&gt; from spotoptim.function import ackley\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; ackley(X)\narray([0.])\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [1.0, 1.0], [-1.0, 1.0]])\n&gt;&gt;&gt; result = ackley(X)\n&gt;&gt;&gt; result[0]  # Should be close to 0\n0.0\n&gt;&gt;&gt; result[1] &gt; 0  # Should be positive\nTrue\n\n\n\nAckley, D. H. (1987). “A connectionist machine for genetic hillclimbing”. Kluwer Academic Publishers.",
    "crumbs": [
      "API Reference",
      "Functions",
      "ackley"
    ]
  },
  {
    "objectID": "docs/reference/function.so.ackley.html#mathematical-formula",
    "href": "docs/reference/function.so.ackley.html#mathematical-formula",
    "title": "function.so.ackley",
    "section": "",
    "text": "f(X) = -a * exp(-b * sqrt(sum(x_i^2) / n)) - exp(sum(cos(c * x_i)) / n) + a + e",
    "crumbs": [
      "API Reference",
      "Functions",
      "ackley"
    ]
  },
  {
    "objectID": "docs/reference/function.so.ackley.html#where",
    "href": "docs/reference/function.so.ackley.html#where",
    "title": "function.so.ackley",
    "section": "",
    "text": "a = 20 (default)\nb = 0.2 (default)\nc = 2π (default)\ne = exp(1) ≈ 2.71828\nn = number of dimensions",
    "crumbs": [
      "API Reference",
      "Functions",
      "ackley"
    ]
  },
  {
    "objectID": "docs/reference/function.so.ackley.html#parameters",
    "href": "docs/reference/function.so.ackley.html#parameters",
    "title": "function.so.ackley",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "ackley"
    ]
  },
  {
    "objectID": "docs/reference/function.so.ackley.html#returns",
    "href": "docs/reference/function.so.ackley.html#returns",
    "title": "function.so.ackley",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Function values at the input points with shape (n_samples,).",
    "crumbs": [
      "API Reference",
      "Functions",
      "ackley"
    ]
  },
  {
    "objectID": "docs/reference/function.so.ackley.html#note",
    "href": "docs/reference/function.so.ackley.html#note",
    "title": "function.so.ackley",
    "section": "",
    "text": "Global minimum: f(0, 0, …, 0) = 0\nTypical search domain: [-32.768, 32.768]^N\nCharacteristics: Non-convex, multimodal, separable",
    "crumbs": [
      "API Reference",
      "Functions",
      "ackley"
    ]
  },
  {
    "objectID": "docs/reference/function.so.ackley.html#examples",
    "href": "docs/reference/function.so.ackley.html#examples",
    "title": "function.so.ackley",
    "section": "",
    "text": "Single point evaluation at global minimum:\n&gt;&gt;&gt; from spotoptim.function import ackley\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; ackley(X)\narray([0.])\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [1.0, 1.0], [-1.0, 1.0]])\n&gt;&gt;&gt; result = ackley(X)\n&gt;&gt;&gt; result[0]  # Should be close to 0\n0.0\n&gt;&gt;&gt; result[1] &gt; 0  # Should be positive\nTrue",
    "crumbs": [
      "API Reference",
      "Functions",
      "ackley"
    ]
  },
  {
    "objectID": "docs/reference/function.so.ackley.html#references",
    "href": "docs/reference/function.so.ackley.html#references",
    "title": "function.so.ackley",
    "section": "",
    "text": "Ackley, D. H. (1987). “A connectionist machine for genetic hillclimbing”. Kluwer Academic Publishers.",
    "crumbs": [
      "API Reference",
      "Functions",
      "ackley"
    ]
  },
  {
    "objectID": "docs/reference/function.remote.objective_remote.html",
    "href": "docs/reference/function.remote.objective_remote.html",
    "title": "function.remote.objective_remote",
    "section": "",
    "text": "function.remote.objective_remote(X, url=DEFAULT_SERVER_URL, **kwargs)\nEvaluates an objective function remotely via an HTTP POST request.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput data of shape (n_samples, n_features).\nrequired\n\n\nurl\nstr\nThe URL of the remote computation server. Defaults to “http://139.6.66.164:8000/compute/”.\nDEFAULT_SERVER_URL\n\n\n**kwargs\nAny\nAdditional arguments to include in the request payload (optional).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: The computed objective values of shape (n_samples,).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nrequests.exceptions.RequestException\nIf the remote request fails.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.remote import objective_remote\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; y = objective_remote(X)\n&gt;&gt;&gt; print(y)",
    "crumbs": [
      "API Reference",
      "Functions",
      "objective_remote"
    ]
  },
  {
    "objectID": "docs/reference/function.remote.objective_remote.html#parameters",
    "href": "docs/reference/function.remote.objective_remote.html#parameters",
    "title": "function.remote.objective_remote",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput data of shape (n_samples, n_features).\nrequired\n\n\nurl\nstr\nThe URL of the remote computation server. Defaults to “http://139.6.66.164:8000/compute/”.\nDEFAULT_SERVER_URL\n\n\n**kwargs\nAny\nAdditional arguments to include in the request payload (optional).\n{}",
    "crumbs": [
      "API Reference",
      "Functions",
      "objective_remote"
    ]
  },
  {
    "objectID": "docs/reference/function.remote.objective_remote.html#returns",
    "href": "docs/reference/function.remote.objective_remote.html#returns",
    "title": "function.remote.objective_remote",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: The computed objective values of shape (n_samples,).",
    "crumbs": [
      "API Reference",
      "Functions",
      "objective_remote"
    ]
  },
  {
    "objectID": "docs/reference/function.remote.objective_remote.html#raises",
    "href": "docs/reference/function.remote.objective_remote.html#raises",
    "title": "function.remote.objective_remote",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nrequests.exceptions.RequestException\nIf the remote request fails.",
    "crumbs": [
      "API Reference",
      "Functions",
      "objective_remote"
    ]
  },
  {
    "objectID": "docs/reference/function.remote.objective_remote.html#examples",
    "href": "docs/reference/function.remote.objective_remote.html#examples",
    "title": "function.remote.objective_remote",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.remote import objective_remote\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; y = objective_remote(X)\n&gt;&gt;&gt; print(y)",
    "crumbs": [
      "API Reference",
      "Functions",
      "objective_remote"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt6.html",
    "href": "docs/reference/function.mo.zdt6.html",
    "title": "function.mo.zdt6",
    "section": "",
    "text": "function.mo.zdt6(X)\nZDT6 multi-objective test function (2 objectives).\nZDT6 has a non-uniform search space with a non-convex Pareto front and low density of solutions near the Pareto front.\n\n\nf1(X) = 1 - exp(-4 * x1) * sin^6(6 * π * x1) f2(X) = g(X) * [1 - (f1 / g(X))^2] g(X) = 1 + 9 * [sum(x_i for i=2 to n) / (n - 1)]^0.25\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.\n\n\n\n\n\n\n\nNumber of objectives: 2\nTypical number of variables: 10\nSearch domain: [0, 1]^n\nPareto front: Non-convex, non-uniform density\nCharacteristics: Non-uniform, biased search space\n\n\n\n\n&gt;&gt;&gt; from spotoptim.function.mo import zdt6\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.5, 0.5, 0.5])\n&gt;&gt;&gt; result = zdt6(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n\n\n\nZitzler, E., Deb, K., & Thiele, L. (2000). “Comparison of multiobjective evolutionary algorithms: Empirical results.” Evolutionary computation, 8(2), 173-195.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt6"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt6.html#mathematical-formulation",
    "href": "docs/reference/function.mo.zdt6.html#mathematical-formulation",
    "title": "function.mo.zdt6",
    "section": "",
    "text": "f1(X) = 1 - exp(-4 * x1) * sin^6(6 * π * x1) f2(X) = g(X) * [1 - (f1 / g(X))^2] g(X) = 1 + 9 * [sum(x_i for i=2 to n) / (n - 1)]^0.25",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt6"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt6.html#parameters",
    "href": "docs/reference/function.mo.zdt6.html#parameters",
    "title": "function.mo.zdt6",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt6"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt6.html#returns",
    "href": "docs/reference/function.mo.zdt6.html#returns",
    "title": "function.mo.zdt6",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt6"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt6.html#raises",
    "href": "docs/reference/function.mo.zdt6.html#raises",
    "title": "function.mo.zdt6",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt6"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt6.html#note",
    "href": "docs/reference/function.mo.zdt6.html#note",
    "title": "function.mo.zdt6",
    "section": "",
    "text": "Number of objectives: 2\nTypical number of variables: 10\nSearch domain: [0, 1]^n\nPareto front: Non-convex, non-uniform density\nCharacteristics: Non-uniform, biased search space",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt6"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt6.html#examples",
    "href": "docs/reference/function.mo.zdt6.html#examples",
    "title": "function.mo.zdt6",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.function.mo import zdt6\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.5, 0.5, 0.5])\n&gt;&gt;&gt; result = zdt6(X)\n&gt;&gt;&gt; result.shape\n(1, 2)",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt6"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt6.html#references",
    "href": "docs/reference/function.mo.zdt6.html#references",
    "title": "function.mo.zdt6",
    "section": "",
    "text": "Zitzler, E., Deb, K., & Thiele, L. (2000). “Comparison of multiobjective evolutionary algorithms: Empirical results.” Evolutionary computation, 8(2), 173-195.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt6"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt3.html",
    "href": "docs/reference/function.mo.zdt3.html",
    "title": "function.mo.zdt3",
    "section": "",
    "text": "function.mo.zdt3(X)\nZDT3 multi-objective test function (2 objectives).\nZDT3 has a disconnected (discontinuous) Pareto front, making it more challenging.\n\n\nf1(X) = x1 f2(X) = g(X) * [1 - sqrt(x1 / g(X)) - (x1 / g(X)) * sin(10 * π * x1)] g(X) = 1 + 9 * sum(x_i for i=2 to n) / (n - 1)\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.\n\n\n\n\n\n\n\nNumber of objectives: 2\nTypical number of variables: 30\nSearch domain: [0, 1]^n\nPareto front: Disconnected (5 separate regions)\nCharacteristics: Discontinuous, multimodal\n\n\n\n\n&gt;&gt;&gt; from spotoptim.function.mo import zdt3\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; result = zdt3(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n\n\n\nZitzler, E., Deb, K., & Thiele, L. (2000). “Comparison of multiobjective evolutionary algorithms: Empirical results.” Evolutionary computation, 8(2), 173-195.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt3"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt3.html#mathematical-formulation",
    "href": "docs/reference/function.mo.zdt3.html#mathematical-formulation",
    "title": "function.mo.zdt3",
    "section": "",
    "text": "f1(X) = x1 f2(X) = g(X) * [1 - sqrt(x1 / g(X)) - (x1 / g(X)) * sin(10 * π * x1)] g(X) = 1 + 9 * sum(x_i for i=2 to n) / (n - 1)",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt3"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt3.html#parameters",
    "href": "docs/reference/function.mo.zdt3.html#parameters",
    "title": "function.mo.zdt3",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt3"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt3.html#returns",
    "href": "docs/reference/function.mo.zdt3.html#returns",
    "title": "function.mo.zdt3",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt3"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt3.html#raises",
    "href": "docs/reference/function.mo.zdt3.html#raises",
    "title": "function.mo.zdt3",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt3"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt3.html#note",
    "href": "docs/reference/function.mo.zdt3.html#note",
    "title": "function.mo.zdt3",
    "section": "",
    "text": "Number of objectives: 2\nTypical number of variables: 30\nSearch domain: [0, 1]^n\nPareto front: Disconnected (5 separate regions)\nCharacteristics: Discontinuous, multimodal",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt3"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt3.html#examples",
    "href": "docs/reference/function.mo.zdt3.html#examples",
    "title": "function.mo.zdt3",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.function.mo import zdt3\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; result = zdt3(X)\n&gt;&gt;&gt; result.shape\n(1, 2)",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt3"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt3.html#references",
    "href": "docs/reference/function.mo.zdt3.html#references",
    "title": "function.mo.zdt3",
    "section": "",
    "text": "Zitzler, E., Deb, K., & Thiele, L. (2000). “Comparison of multiobjective evolutionary algorithms: Empirical results.” Evolutionary computation, 8(2), 173-195.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt3"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt1.html",
    "href": "docs/reference/function.mo.zdt1.html",
    "title": "function.mo.zdt1",
    "section": "",
    "text": "function.mo.zdt1(X)\nZDT1 multi-objective test function (2 objectives).\nZDT1 is a classical bi-objective test problem with a convex Pareto front. It is one of the most widely used benchmark functions for multi-objective optimization.\n\n\nf1(X) = x1 f2(X) = g(X) * [1 - sqrt(x1 / g(X))] g(X) = 1 + 9 * sum(x_i for i=2 to n) / (n - 1)\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.\n\n\n\n\n\n\n\nNumber of objectives: 2\nTypical number of variables: 30\nSearch domain: [0, 1]^n\nPareto front: Convex, f1 ∈ [0, 1], f2 = 1 - sqrt(f1)\nCharacteristics: Convex, unimodal\n\n\n\n\nSingle point evaluation:\n&gt;&gt;&gt; from spotoptim.function.mo import zdt1\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; result = zdt1(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; result[0, 0]  # f1\n0.0\n&gt;&gt;&gt; result[0, 1]  # f2\n1.0\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; result = zdt1(X)\n&gt;&gt;&gt; result.shape\n(3, 2)\n\n\n\nZitzler, E., Deb, K., & Thiele, L. (2000). “Comparison of multiobjective evolutionary algorithms: Empirical results.” Evolutionary computation, 8(2), 173-195.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt1.html#mathematical-formulation",
    "href": "docs/reference/function.mo.zdt1.html#mathematical-formulation",
    "title": "function.mo.zdt1",
    "section": "",
    "text": "f1(X) = x1 f2(X) = g(X) * [1 - sqrt(x1 / g(X))] g(X) = 1 + 9 * sum(x_i for i=2 to n) / (n - 1)",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt1.html#parameters",
    "href": "docs/reference/function.mo.zdt1.html#parameters",
    "title": "function.mo.zdt1",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt1.html#returns",
    "href": "docs/reference/function.mo.zdt1.html#returns",
    "title": "function.mo.zdt1",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt1.html#raises",
    "href": "docs/reference/function.mo.zdt1.html#raises",
    "title": "function.mo.zdt1",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt1.html#note",
    "href": "docs/reference/function.mo.zdt1.html#note",
    "title": "function.mo.zdt1",
    "section": "",
    "text": "Number of objectives: 2\nTypical number of variables: 30\nSearch domain: [0, 1]^n\nPareto front: Convex, f1 ∈ [0, 1], f2 = 1 - sqrt(f1)\nCharacteristics: Convex, unimodal",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt1.html#examples",
    "href": "docs/reference/function.mo.zdt1.html#examples",
    "title": "function.mo.zdt1",
    "section": "",
    "text": "Single point evaluation:\n&gt;&gt;&gt; from spotoptim.function.mo import zdt1\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; result = zdt1(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; result[0, 0]  # f1\n0.0\n&gt;&gt;&gt; result[0, 1]  # f2\n1.0\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; result = zdt1(X)\n&gt;&gt;&gt; result.shape\n(3, 2)",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt1.html#references",
    "href": "docs/reference/function.mo.zdt1.html#references",
    "title": "function.mo.zdt1",
    "section": "",
    "text": "Zitzler, E., Deb, K., & Thiele, L. (2000). “Comparison of multiobjective evolutionary algorithms: Empirical results.” Evolutionary computation, 8(2), 173-195.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.html",
    "href": "docs/reference/function.mo.html",
    "title": "function.mo",
    "section": "",
    "text": "function.mo\nAnalytical multi-objective test functions for optimization benchmarking.\nThis module provides well-known multi-objective analytical test functions commonly used for evaluating and benchmarking multiobjective optimization algorithms.\n\n\n\n\n\nName\nDescription\n\n\n\n\nactivity_pred\nCompute activity predictions for each row in the input array.\n\n\nconversion_pred\nCompute conversion predictions for each row in the input array.\n\n\ndtlz1\nDTLZ1 multi-objective test function (scalable objectives).\n\n\ndtlz2\nDTLZ2 multi-objective test function (scalable objectives).\n\n\nfonseca_fleming\nFonseca-Fleming multi-objective test function (2 objectives).\n\n\nfun_myer16a\nCompute both conversion and activity predictions for each row in the input array.\n\n\nkursawe\nKursawe multi-objective test function (2 objectives, minimization).\n\n\nmo_conv2_max\nConvex bi-objective maximization test function (2 objectives).\n\n\nmo_conv2_min\nConvex bi-objective minimization test function (2 objectives).\n\n\nschaffer_n1\nSchaffer N1 multi-objective test function (2 objectives).\n\n\nzdt1\nZDT1 multi-objective test function (2 objectives).\n\n\nzdt2\nZDT2 multi-objective test function (2 objectives).\n\n\nzdt3\nZDT3 multi-objective test function (2 objectives).\n\n\nzdt4\nZDT4 multi-objective test function (2 objectives).\n\n\nzdt6\nZDT6 multi-objective test function (2 objectives).",
    "crumbs": [
      "API Reference",
      "Functions",
      "function.mo"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.html#functions",
    "href": "docs/reference/function.mo.html#functions",
    "title": "function.mo",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nactivity_pred\nCompute activity predictions for each row in the input array.\n\n\nconversion_pred\nCompute conversion predictions for each row in the input array.\n\n\ndtlz1\nDTLZ1 multi-objective test function (scalable objectives).\n\n\ndtlz2\nDTLZ2 multi-objective test function (scalable objectives).\n\n\nfonseca_fleming\nFonseca-Fleming multi-objective test function (2 objectives).\n\n\nfun_myer16a\nCompute both conversion and activity predictions for each row in the input array.\n\n\nkursawe\nKursawe multi-objective test function (2 objectives, minimization).\n\n\nmo_conv2_max\nConvex bi-objective maximization test function (2 objectives).\n\n\nmo_conv2_min\nConvex bi-objective minimization test function (2 objectives).\n\n\nschaffer_n1\nSchaffer N1 multi-objective test function (2 objectives).\n\n\nzdt1\nZDT1 multi-objective test function (2 objectives).\n\n\nzdt2\nZDT2 multi-objective test function (2 objectives).\n\n\nzdt3\nZDT3 multi-objective test function (2 objectives).\n\n\nzdt4\nZDT4 multi-objective test function (2 objectives).\n\n\nzdt6\nZDT6 multi-objective test function (2 objectives).",
    "crumbs": [
      "API Reference",
      "Functions",
      "function.mo"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_max.html",
    "href": "docs/reference/function.mo.mo_conv2_max.html",
    "title": "function.mo.mo_conv2_max",
    "section": "",
    "text": "function.mo.mo_conv2_max(X)\nConvex bi-objective maximization test function (2 objectives).\nA smooth, convex two-objective maximization problem on [0, 1]^2 using flipped versions of the minimization objectives: f1(x, y) = 2 - (x^2 + y^2) f2(x, y) = 2 - ((1 - x)^2 + (1 - y)^2)\n\n\n\nDomain: [0, 1]^2\nObjectives: maximize both f1 and f2\nIdeal points: (0, 0) for f1 (gives f1=2); (1, 1) for f2 (gives f2=2)\nPareto set: line x = y in [0, 1]\nPareto front: convex quadratic trade-off f1 = 2 - 2t^2, f2 = 2 - 2(1 - t)^2, t ∈ [0, 1]\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have exactly 2 dimensions.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values (to be maximized) - Column 1: f2 values (to be maximized)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf X does not have exactly 2 dimensions.\n\n\n\n\n\n\n\nNumber of objectives: 2\nNumber of variables: 2\nSearch domain: [0, 1]^2\nIdeal points: (1, 1) for f1, (0, 0) for f2\nPareto front: Convex, quadratic\nProblem type: Maximization\nCharacteristics: Convex, smooth, bounded\n\n\n\n\nSingle point evaluation:\n&gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_max\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0])\n&gt;&gt;&gt; result = mo_conv2_max(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; result[0]  # f1 maximum\narray([0., 2.])\n&gt;&gt;&gt; X = np.array([1.0, 1.0])\n&gt;&gt;&gt; result = mo_conv2_max(X)\n&gt;&gt;&gt; result[0]  # f2 maximum\narray([2., 0.])\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; result = mo_conv2_min(X)\n&gt;&gt;&gt; result.shape\n(3, 2)\n&gt;&gt;&gt; result[1]  # Pareto front\narray([0.5, 0.5])",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_max"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_max.html#properties",
    "href": "docs/reference/function.mo.mo_conv2_max.html#properties",
    "title": "function.mo.mo_conv2_max",
    "section": "",
    "text": "Domain: [0, 1]^2\nObjectives: maximize both f1 and f2\nIdeal points: (0, 0) for f1 (gives f1=2); (1, 1) for f2 (gives f2=2)\nPareto set: line x = y in [0, 1]\nPareto front: convex quadratic trade-off f1 = 2 - 2t^2, f2 = 2 - 2(1 - t)^2, t ∈ [0, 1]",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_max"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_max.html#parameters",
    "href": "docs/reference/function.mo.mo_conv2_max.html#parameters",
    "title": "function.mo.mo_conv2_max",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have exactly 2 dimensions.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_max"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_max.html#returns",
    "href": "docs/reference/function.mo.mo_conv2_max.html#returns",
    "title": "function.mo.mo_conv2_max",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values (to be maximized) - Column 1: f2 values (to be maximized)",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_max"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_max.html#raises",
    "href": "docs/reference/function.mo.mo_conv2_max.html#raises",
    "title": "function.mo.mo_conv2_max",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf X does not have exactly 2 dimensions.",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_max"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_max.html#note",
    "href": "docs/reference/function.mo.mo_conv2_max.html#note",
    "title": "function.mo.mo_conv2_max",
    "section": "",
    "text": "Number of objectives: 2\nNumber of variables: 2\nSearch domain: [0, 1]^2\nIdeal points: (1, 1) for f1, (0, 0) for f2\nPareto front: Convex, quadratic\nProblem type: Maximization\nCharacteristics: Convex, smooth, bounded",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_max"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_max.html#examples",
    "href": "docs/reference/function.mo.mo_conv2_max.html#examples",
    "title": "function.mo.mo_conv2_max",
    "section": "",
    "text": "Single point evaluation:\n&gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_max\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0])\n&gt;&gt;&gt; result = mo_conv2_max(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; result[0]  # f1 maximum\narray([0., 2.])\n&gt;&gt;&gt; X = np.array([1.0, 1.0])\n&gt;&gt;&gt; result = mo_conv2_max(X)\n&gt;&gt;&gt; result[0]  # f2 maximum\narray([2., 0.])\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; result = mo_conv2_min(X)\n&gt;&gt;&gt; result.shape\n(3, 2)\n&gt;&gt;&gt; result[1]  # Pareto front\narray([0.5, 0.5])",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_max"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.fun_myer16a.html",
    "href": "docs/reference/function.mo.fun_myer16a.html",
    "title": "function.mo.fun_myer16a",
    "section": "",
    "text": "function.mo.fun_myer16a(X, fun_control=None)\nCompute both conversion and activity predictions for each row in the input array.\n\n\nImplements a response surface experiment described by Myers, Montgomery, and Anderson-Cook (2016). The function computes two objectives: conversion and activity.\n\n\n\n\nMyers, R. H., Montgomery, D. C., and Anderson-Cook, C. M. Response surface methodology: process and product optimization using designed experiments. John Wiley & Sons, 2016.\nKuhn, M. desirability: Function optimization and ranking via desirability functions. Tech. rep., 9 2016.\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\n2D array where each row is a configuration.\nrequired\n\n\nfun_control\ndict\nAdditional control parameters (not used here).\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: 2D array where each row contains [conversion_pred, activity_pred].\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import fun_myer16a\n&gt;&gt;&gt; # Example input data\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun_myer16a(X)\narray([[  3.5,   1.5],\n       [ 19.5,  10.5]])",
    "crumbs": [
      "API Reference",
      "Functions",
      "fun_myer16a"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.fun_myer16a.html#notes",
    "href": "docs/reference/function.mo.fun_myer16a.html#notes",
    "title": "function.mo.fun_myer16a",
    "section": "",
    "text": "Implements a response surface experiment described by Myers, Montgomery, and Anderson-Cook (2016). The function computes two objectives: conversion and activity.",
    "crumbs": [
      "API Reference",
      "Functions",
      "fun_myer16a"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.fun_myer16a.html#references",
    "href": "docs/reference/function.mo.fun_myer16a.html#references",
    "title": "function.mo.fun_myer16a",
    "section": "",
    "text": "Myers, R. H., Montgomery, D. C., and Anderson-Cook, C. M. Response surface methodology: process and product optimization using designed experiments. John Wiley & Sons, 2016.\nKuhn, M. desirability: Function optimization and ranking via desirability functions. Tech. rep., 9 2016.",
    "crumbs": [
      "API Reference",
      "Functions",
      "fun_myer16a"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.fun_myer16a.html#parameters",
    "href": "docs/reference/function.mo.fun_myer16a.html#parameters",
    "title": "function.mo.fun_myer16a",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\n2D array where each row is a configuration.\nrequired\n\n\nfun_control\ndict\nAdditional control parameters (not used here).\nNone",
    "crumbs": [
      "API Reference",
      "Functions",
      "fun_myer16a"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.fun_myer16a.html#returns",
    "href": "docs/reference/function.mo.fun_myer16a.html#returns",
    "title": "function.mo.fun_myer16a",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: 2D array where each row contains [conversion_pred, activity_pred].",
    "crumbs": [
      "API Reference",
      "Functions",
      "fun_myer16a"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.fun_myer16a.html#examples",
    "href": "docs/reference/function.mo.fun_myer16a.html#examples",
    "title": "function.mo.fun_myer16a",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import fun_myer16a\n&gt;&gt;&gt; # Example input data\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun_myer16a(X)\narray([[  3.5,   1.5],\n       [ 19.5,  10.5]])",
    "crumbs": [
      "API Reference",
      "Functions",
      "fun_myer16a"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz2.html",
    "href": "docs/reference/function.mo.dtlz2.html",
    "title": "function.mo.dtlz2",
    "section": "",
    "text": "function.mo.dtlz2(X, n_obj=3)\nDTLZ2 multi-objective test function (scalable objectives).\nDTLZ2 is a scalable test problem with a concave Pareto front (a unit sphere).\n\n\nf_i(X) = [1 + g(X)] * cos(x1 * π/2) * … * cos(x_{M-i} * π/2) * sin(x_{M-i+1} * π/2) f_M(X) = [1 + g(X)] * sin(x1 * π/2) g(X) = sum((x_i - 0.5)^2 for i in X_M)\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.\nrequired\n\n\nn_obj\nint\nNumber of objectives. Defaults to 3. Must be at least 2 and at most n_features.\n3\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, n_obj).\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf n_obj is invalid or X has insufficient dimensions.\n\n\n\n\n\n\n\nNumber of objectives: Scalable (typically 3)\nTypical number of variables: n_obj + k - 1 (often k = 10, so n = 12 for 3 objectives)\nSearch domain: [0, 1]^n\nPareto front: Concave (sphere: sum(f_i^2) = 1)\nCharacteristics: Concave, unimodal\n\n\n\n\n&gt;&gt;&gt; from spotoptim.function.mo import dtlz2\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n&gt;&gt;&gt; result = dtlz2(X, n_obj=3)\n&gt;&gt;&gt; result.shape\n(1, 3)\n\n\n\nDeb, K., Thiele, L., Laumanns, M., & Zitzler, E. (2005). “Scalable test problems for evolutionary multiobjective optimization.” In Evolutionary multiobjective optimization (pp. 105-145). Springer.",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz2.html#mathematical-formulation",
    "href": "docs/reference/function.mo.dtlz2.html#mathematical-formulation",
    "title": "function.mo.dtlz2",
    "section": "",
    "text": "f_i(X) = [1 + g(X)] * cos(x1 * π/2) * … * cos(x_{M-i} * π/2) * sin(x_{M-i+1} * π/2) f_M(X) = [1 + g(X)] * sin(x1 * π/2) g(X) = sum((x_i - 0.5)^2 for i in X_M)",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz2.html#parameters",
    "href": "docs/reference/function.mo.dtlz2.html#parameters",
    "title": "function.mo.dtlz2",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.\nrequired\n\n\nn_obj\nint\nNumber of objectives. Defaults to 3. Must be at least 2 and at most n_features.\n3",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz2.html#returns",
    "href": "docs/reference/function.mo.dtlz2.html#returns",
    "title": "function.mo.dtlz2",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, n_obj).",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz2.html#raises",
    "href": "docs/reference/function.mo.dtlz2.html#raises",
    "title": "function.mo.dtlz2",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf n_obj is invalid or X has insufficient dimensions.",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz2.html#note",
    "href": "docs/reference/function.mo.dtlz2.html#note",
    "title": "function.mo.dtlz2",
    "section": "",
    "text": "Number of objectives: Scalable (typically 3)\nTypical number of variables: n_obj + k - 1 (often k = 10, so n = 12 for 3 objectives)\nSearch domain: [0, 1]^n\nPareto front: Concave (sphere: sum(f_i^2) = 1)\nCharacteristics: Concave, unimodal",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz2.html#examples",
    "href": "docs/reference/function.mo.dtlz2.html#examples",
    "title": "function.mo.dtlz2",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.function.mo import dtlz2\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n&gt;&gt;&gt; result = dtlz2(X, n_obj=3)\n&gt;&gt;&gt; result.shape\n(1, 3)",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz2.html#references",
    "href": "docs/reference/function.mo.dtlz2.html#references",
    "title": "function.mo.dtlz2",
    "section": "",
    "text": "Deb, K., Thiele, L., Laumanns, M., & Zitzler, E. (2005). “Scalable test problems for evolutionary multiobjective optimization.” In Evolutionary multiobjective optimization (pp. 105-145). Springer.",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.conversion_pred.html",
    "href": "docs/reference/function.mo.conversion_pred.html",
    "title": "function.mo.conversion_pred",
    "section": "",
    "text": "function.mo.conversion_pred(X)\nCompute conversion predictions for each row in the input array.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\n2D array where each row is a configuration.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: 1D array of conversion predictions.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import conversion_pred\n&gt;&gt;&gt; # Example input data\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; conversion_pred(X)\narray([  3.5,  19.5])",
    "crumbs": [
      "API Reference",
      "Functions",
      "conversion_pred"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.conversion_pred.html#parameters",
    "href": "docs/reference/function.mo.conversion_pred.html#parameters",
    "title": "function.mo.conversion_pred",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\n2D array where each row is a configuration.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "conversion_pred"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.conversion_pred.html#returns",
    "href": "docs/reference/function.mo.conversion_pred.html#returns",
    "title": "function.mo.conversion_pred",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: 1D array of conversion predictions.",
    "crumbs": [
      "API Reference",
      "Functions",
      "conversion_pred"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.conversion_pred.html#examples",
    "href": "docs/reference/function.mo.conversion_pred.html#examples",
    "title": "function.mo.conversion_pred",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import conversion_pred\n&gt;&gt;&gt; # Example input data\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; conversion_pred(X)\narray([  3.5,  19.5])",
    "crumbs": [
      "API Reference",
      "Functions",
      "conversion_pred"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.html",
    "href": "docs/reference/function.forr08a.html",
    "title": "function.forr08a",
    "section": "",
    "text": "function.forr08a\n\n\n\n\n\nName\nDescription\n\n\n\n\naerofoilcd\nComputes the drag coefficient (cd) of an aerofoil based on the shape parameter X.\n\n\nbranin\nBranin’s test function that takes a 2D input vector x in the range [0, 1] for each dimension\n\n\nonevar\nOne-variable test function that takes a scalar or 1D array input x in the range [0, 1]",
    "crumbs": [
      "API Reference",
      "Functions",
      "function.forr08a"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.html#functions",
    "href": "docs/reference/function.forr08a.html#functions",
    "title": "function.forr08a",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\naerofoilcd\nComputes the drag coefficient (cd) of an aerofoil based on the shape parameter X.\n\n\nbranin\nBranin’s test function that takes a 2D input vector x in the range [0, 1] for each dimension\n\n\nonevar\nOne-variable test function that takes a scalar or 1D array input x in the range [0, 1]",
    "crumbs": [
      "API Reference",
      "Functions",
      "function.forr08a"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.branin.html",
    "href": "docs/reference/function.forr08a.branin.html",
    "title": "function.forr08a.branin",
    "section": "",
    "text": "function.forr08a.branin(x)\nBranin’s test function that takes a 2D input vector x in the range [0, 1] for each dimension and returns the corresponding scalar function value. The function is vectorized to handle multiple inputs.\n\n\nf(x) = a * (X2 - b * X1^2 + c * X1 - d)^2 + e * (1 - ff) * cos(X1) + e + 5 * x1\nwhere: X1 = 15 * x1 - 5 X2 = 15 * x2\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nnp.ndarray\nA 2D NumPy array of shape (n_samples, 2) where each row is a 2D input vector.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: The calculated function values for the input x.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf x does not have exactly 2 columns or if any value in x is outside the range [0, 1].\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import branin\n&gt;&gt;&gt; # Single input\n&gt;&gt;&gt; print(branin(np.array([[0.5, 0.5]])))\n[26.63]\n&gt;&gt;&gt; # Multiple inputs\n&gt;&gt;&gt; x = np.array([[0.0, 0.0], [0.25, 0.25], [0.5, 0.5], [0.75, 0.75], [1.0, 1.0]])\n&gt;&gt;&gt; print(branin(x))\n[308.1291, 34.0028, 26.63, 126.3879, 150.8722]",
    "crumbs": [
      "API Reference",
      "Functions",
      "branin"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.branin.html#the-function-is-defined-as",
    "href": "docs/reference/function.forr08a.branin.html#the-function-is-defined-as",
    "title": "function.forr08a.branin",
    "section": "",
    "text": "f(x) = a * (X2 - b * X1^2 + c * X1 - d)^2 + e * (1 - ff) * cos(X1) + e + 5 * x1\nwhere: X1 = 15 * x1 - 5 X2 = 15 * x2",
    "crumbs": [
      "API Reference",
      "Functions",
      "branin"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.branin.html#parameters",
    "href": "docs/reference/function.forr08a.branin.html#parameters",
    "title": "function.forr08a.branin",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\nnp.ndarray\nA 2D NumPy array of shape (n_samples, 2) where each row is a 2D input vector.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "branin"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.branin.html#returns",
    "href": "docs/reference/function.forr08a.branin.html#returns",
    "title": "function.forr08a.branin",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: The calculated function values for the input x.",
    "crumbs": [
      "API Reference",
      "Functions",
      "branin"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.branin.html#raises",
    "href": "docs/reference/function.forr08a.branin.html#raises",
    "title": "function.forr08a.branin",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf x does not have exactly 2 columns or if any value in x is outside the range [0, 1].",
    "crumbs": [
      "API Reference",
      "Functions",
      "branin"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.branin.html#examples",
    "href": "docs/reference/function.forr08a.branin.html#examples",
    "title": "function.forr08a.branin",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import branin\n&gt;&gt;&gt; # Single input\n&gt;&gt;&gt; print(branin(np.array([[0.5, 0.5]])))\n[26.63]\n&gt;&gt;&gt; # Multiple inputs\n&gt;&gt;&gt; x = np.array([[0.0, 0.0], [0.25, 0.25], [0.5, 0.5], [0.75, 0.75], [1.0, 1.0]])\n&gt;&gt;&gt; print(branin(x))\n[308.1291, 34.0028, 26.63, 126.3879, 150.8722]",
    "crumbs": [
      "API Reference",
      "Functions",
      "branin"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.html",
    "href": "docs/reference/factor_analyzer.html",
    "title": "factor_analyzer",
    "section": "",
    "text": "factor_analyzer\nfactor_analyzer\nThis module performs exploratory and confirmatory factor analyses.\n:author: Jeremy Biggs (jeremy.m.biggs@gmail.com) :author: Nitin Madnani (nmadnani@ets.org) :organization: Educational Testing Service :date: 2022-09-05\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "factor_analyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.smc.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.smc.html",
    "title": "factor_analyzer.factor_analyzer_utils.smc",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.smc(corr_mtx, sort=False)\nCalculate the squared multiple correlations.\nThis is equivalent to regressing each variable on all others and calculating the r-squared values.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncorr_mtx\narray - like\nThe correlation matrix used to calculate SMC.\nrequired\n\n\nsort\nbool\nWhether to sort the values for SMC before returning. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nsmc\nnumpy.ndarray\nThe squared multiple correlations matrix.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "smc"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.smc.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.smc.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.smc",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ncorr_mtx\narray - like\nThe correlation matrix used to calculate SMC.\nrequired\n\n\nsort\nbool\nWhether to sort the values for SMC before returning. Defaults to False.\nFalse",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "smc"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.smc.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.smc.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.smc",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nsmc\nnumpy.ndarray\nThe squared multiple correlations matrix.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "smc"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.partial_correlations.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.partial_correlations.html",
    "title": "factor_analyzer.factor_analyzer_utils.partial_correlations",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.partial_correlations(x)\nCompute partial correlations between variable pairs.\nThis is a python port of the pcor() function implemented in the ppcor R package, which computes partial correlations for each pair of variables in the given array, excluding all other variables.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nAn array containing the feature values.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npcor\n:obj:numpy.ndarray\nAn array containing the partial correlations of of each pair of variables in the given array, excluding all other variables.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "partial_correlations"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.partial_correlations.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.partial_correlations.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.partial_correlations",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nAn array containing the feature values.\nrequired",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "partial_correlations"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.partial_correlations.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.partial_correlations.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.partial_correlations",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\npcor\n:obj:numpy.ndarray\nAn array containing the partial correlations of of each pair of variables in the given array, excluding all other variables.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "partial_correlations"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.inv_chol.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.inv_chol.html",
    "title": "factor_analyzer.factor_analyzer_utils.inv_chol",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.inv_chol(x, logdet=False)\nCalculate matrix inverse using Cholesky decomposition.\nOptionally, calculate the log determinant of the Cholesky.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe matrix to invert.\nrequired\n\n\nlogdet\nbool\nWhether to calculate the log determinant, instead of the inverse. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\nTuple[np.ndarray, Optional[float]]\n- chol_inv (array-like): The inverted matrix. - chol_logdet (array-like or None): The log determinant, if logdet was True, otherwise None.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "inv_chol"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.inv_chol.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.inv_chol.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.inv_chol",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe matrix to invert.\nrequired\n\n\nlogdet\nbool\nWhether to calculate the log determinant, instead of the inverse. Defaults to False.\nFalse",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "inv_chol"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.inv_chol.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.inv_chol.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.inv_chol",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ntuple\nTuple[np.ndarray, Optional[float]]\n- chol_inv (array-like): The inverted matrix. - chol_logdet (array-like or None): The log determinant, if logdet was True, otherwise None.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "inv_chol"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.get_symmetric_upper_idxs.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.get_symmetric_upper_idxs.html",
    "title": "factor_analyzer.factor_analyzer_utils.get_symmetric_upper_idxs",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.get_symmetric_upper_idxs(n=1, diag=True)\nGet the indices for the upper triangle of a symmetric matrix.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nThe dimension of the n x n symmetric matrix. Defaults to 1.\n1\n\n\ndiag\nbool\nWhether to include the diagonal.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nindices\n:obj:numpy.ndarray\nThe indices for the upper triangle.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "get_symmetric_upper_idxs"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.get_symmetric_upper_idxs.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.get_symmetric_upper_idxs.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.get_symmetric_upper_idxs",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nn\nint\nThe dimension of the n x n symmetric matrix. Defaults to 1.\n1\n\n\ndiag\nbool\nWhether to include the diagonal.\nTrue",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "get_symmetric_upper_idxs"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.get_symmetric_upper_idxs.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.get_symmetric_upper_idxs.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.get_symmetric_upper_idxs",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nindices\n:obj:numpy.ndarray\nThe indices for the upper triangle.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "get_symmetric_upper_idxs"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.get_free_parameter_idxs.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.get_free_parameter_idxs.html",
    "title": "factor_analyzer.factor_analyzer_utils.get_free_parameter_idxs",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.get_free_parameter_idxs(x, eq=1)\nGet the free parameter indices from the flattened matrix.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe input matrix.\nrequired\n\n\neq\nstr or int\nThe value that free parameters should be equal to. np.nan fields will be populated with this value. Defaults to 1.\n1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nidx\n:obj:numpy.ndarray\nThe free parameter indexes.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "get_free_parameter_idxs"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.get_free_parameter_idxs.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.get_free_parameter_idxs.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.get_free_parameter_idxs",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe input matrix.\nrequired\n\n\neq\nstr or int\nThe value that free parameters should be equal to. np.nan fields will be populated with this value. Defaults to 1.\n1",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "get_free_parameter_idxs"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.get_free_parameter_idxs.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.get_free_parameter_idxs.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.get_free_parameter_idxs",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nidx\n:obj:numpy.ndarray\nThe free parameter indexes.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "get_free_parameter_idxs"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.fill_lower_diag.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.fill_lower_diag.html",
    "title": "factor_analyzer.factor_analyzer_utils.fill_lower_diag",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.fill_lower_diag(x)\nFill the lower diagonal of a square matrix, given a 1-D input array.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe flattened input matrix that will be used to fill the lower diagonal of the square matrix.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nout\n:obj:numpy.ndarray\nThe output square matrix, with the lower diagonal filled by x.\n\n\n\n\n\n\n[1] https://stackoverflow.com/questions/51439271/ convert-1d-array-to-lower-triangular-matrix",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "fill_lower_diag"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.fill_lower_diag.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.fill_lower_diag.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.fill_lower_diag",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe flattened input matrix that will be used to fill the lower diagonal of the square matrix.\nrequired",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "fill_lower_diag"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.fill_lower_diag.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.fill_lower_diag.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.fill_lower_diag",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nout\n:obj:numpy.ndarray\nThe output square matrix, with the lower diagonal filled by x.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "fill_lower_diag"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.fill_lower_diag.html#references",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.fill_lower_diag.html#references",
    "title": "factor_analyzer.factor_analyzer_utils.fill_lower_diag",
    "section": "",
    "text": "[1] https://stackoverflow.com/questions/51439271/ convert-1d-array-to-lower-triangular-matrix",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "fill_lower_diag"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix.html",
    "title": "factor_analyzer.factor_analyzer_utils.duplication_matrix",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.duplication_matrix(n=1)\nCalculate the duplication matrix.\nA function to create the duplication matrix (Dn), which is the unique n2 × n(n+1)/2 matrix which, for any n × n symmetric matrix A, transforms vech(A) into vec(A), as in Dn vech(A) = vec(A).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nThe dimension of the n x n symmetric matrix. Defaults to 1.\n1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nduplication_matrix\n:obj:numpy.ndarray\nThe duplication matrix.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf n is not a positive integer greater than 1.\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Duplication_and_elimination_matrices",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "duplication_matrix"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.duplication_matrix",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nn\nint\nThe dimension of the n x n symmetric matrix. Defaults to 1.\n1",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "duplication_matrix"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.duplication_matrix",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nduplication_matrix\n:obj:numpy.ndarray\nThe duplication matrix.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "duplication_matrix"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix.html#raises",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix.html#raises",
    "title": "factor_analyzer.factor_analyzer_utils.duplication_matrix",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf n is not a positive integer greater than 1.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "duplication_matrix"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix.html#references",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix.html#references",
    "title": "factor_analyzer.factor_analyzer_utils.duplication_matrix",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Duplication_and_elimination_matrices",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "duplication_matrix"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.cov.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.cov.html",
    "title": "factor_analyzer.factor_analyzer_utils.cov",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.cov(x, ddof=0)\nCalculate the covariance matrix.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nA 1-D or 2-D array containing multiple variables and observations. Each column of x represents a variable, and each row a single observation of all those variables.\nrequired\n\n\nddof\nint\nMeans Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. Defaults to 0.\n0\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nr\nnumpy.ndarray\nThe covariance matrix of the variables.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "cov"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.cov.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.cov.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.cov",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nA 1-D or 2-D array containing multiple variables and observations. Each column of x represents a variable, and each row a single observation of all those variables.\nrequired\n\n\nddof\nint\nMeans Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. Defaults to 0.\n0",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "cov"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.cov.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.cov.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.cov",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nr\nnumpy.ndarray\nThe covariance matrix of the variables.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "cov"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.commutation_matrix.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.commutation_matrix.html",
    "title": "factor_analyzer.factor_analyzer_utils.commutation_matrix",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.commutation_matrix(p, q)\nCalculate the commutation matrix.\nThis matrix transforms the vectorized form of the matrix into the vectorized form of its transpose.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\np\nint\nThe number of rows.\nrequired\n\n\nq\nint\nThe number of columns.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ncommutation_matrix\n:obj:numpy.ndarray\nThe commutation matrix\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Commutation_matrix",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "commutation_matrix"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.commutation_matrix.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.commutation_matrix.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.commutation_matrix",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\np\nint\nThe number of rows.\nrequired\n\n\nq\nint\nThe number of columns.\nrequired",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "commutation_matrix"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.commutation_matrix.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.commutation_matrix.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.commutation_matrix",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ncommutation_matrix\n:obj:numpy.ndarray\nThe commutation matrix",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "commutation_matrix"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.commutation_matrix.html#references",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.commutation_matrix.html#references",
    "title": "factor_analyzer.factor_analyzer_utils.commutation_matrix",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Commutation_matrix",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "commutation_matrix"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_rotator.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_rotator.html",
    "title": "factor_analyzer.factor_analyzer_rotator",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_rotator\nClass to perform various rotations of factor loading matrices.\nConfirmatory factor analysis using machine learning methods. Re-implementation of the factor-analyzer package.\nSee https://factor-analyzer.readthedocs.io/en/latest/introduction.html for more details.\nAuthors of the original implementation: * Jeremy Biggs (jeremy.m.biggs@gmail.com) * Nitin Madnani (nmadnani@ets.org) Organization: Educational Testing Service Date: 2022-09-05\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\n\n\n\n\nName\nDescription\n\n\n\n\nRotator\nPerform rotations on an unrotated factor loading matrix.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "factor_analyzer.factor_analyzer_rotator"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_rotator.html#classes",
    "href": "docs/reference/factor_analyzer.factor_analyzer_rotator.html#classes",
    "title": "factor_analyzer.factor_analyzer_rotator",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nRotator\nPerform rotations on an unrotated factor loading matrix.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "factor_analyzer.factor_analyzer_rotator"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer.html",
    "title": "factor_analyzer.factor_analyzer",
    "section": "",
    "text": "factor_analyzer.factor_analyzer\nFactor analysis using MINRES or ML, with optional rotation using Varimax or Promax.\nConfirmatory factor analysis using machine learning methods. Re-implementation of the factor-analyzer package.\nSee https://factor-analyzer.readthedocs.io/en/latest/introduction.html for more details.\nAuthors of the original implementation: * Jeremy Biggs (jeremy.m.biggs@gmail.com) * Nitin Madnani (nmadnani@ets.org) Organization: Educational Testing Service Date: 2022-09-05\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\n\n\n\n\nName\nDescription\n\n\n\n\nFactorAnalyzer\nThe main exploratory factor analysis class.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncalculate_bartlett_sphericity\nCompute the Bartlett sphericity test.\n\n\ncalculate_kmo\nCalculate the Kaiser-Meyer-Olkin criterion for items and overall.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "factor_analyzer.factor_analyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.html#classes",
    "href": "docs/reference/factor_analyzer.factor_analyzer.html#classes",
    "title": "factor_analyzer.factor_analyzer",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nFactorAnalyzer\nThe main exploratory factor analysis class.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "factor_analyzer.factor_analyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.html#functions",
    "href": "docs/reference/factor_analyzer.factor_analyzer.html#functions",
    "title": "factor_analyzer.factor_analyzer",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncalculate_bartlett_sphericity\nCompute the Bartlett sphericity test.\n\n\ncalculate_kmo\nCalculate the Kaiser-Meyer-Olkin criterion for items and overall.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "factor_analyzer.factor_analyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.calculate_bartlett_sphericity.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer.calculate_bartlett_sphericity.html",
    "title": "factor_analyzer.factor_analyzer.calculate_bartlett_sphericity",
    "section": "",
    "text": "factor_analyzer.factor_analyzer.calculate_bartlett_sphericity(x)\nCompute the Bartlett sphericity test.\nH0: The matrix of population correlations is equal to I. H1: The matrix of population correlations is not equal to I.\nThe formula for Bartlett’s Sphericity test is:\n.. math:: -1 * (n - 1 - ((2p + 5) / 6)) * ln(det(R))\nWhere R det(R) is the determinant of the correlation matrix, and p is the number of variables.\n\n\nx : array-like The array for which to calculate sphericity.\n\n\n\nstatistic : float The chi-square value. p_value : float The associated p-value for the test.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "calculate_bartlett_sphericity"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.calculate_bartlett_sphericity.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer.calculate_bartlett_sphericity.html#parameters",
    "title": "factor_analyzer.factor_analyzer.calculate_bartlett_sphericity",
    "section": "",
    "text": "x : array-like The array for which to calculate sphericity.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "calculate_bartlett_sphericity"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.calculate_bartlett_sphericity.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer.calculate_bartlett_sphericity.html#returns",
    "title": "factor_analyzer.factor_analyzer.calculate_bartlett_sphericity",
    "section": "",
    "text": "statistic : float The chi-square value. p_value : float The associated p-value for the test.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "calculate_bartlett_sphericity"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.html",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.html",
    "title": "factor_analyzer.confirmatory_factor_analyzer",
    "section": "",
    "text": "factor_analyzer.confirmatory_factor_analyzer\nConfirmatory factor analysis using machine learning methods. Re-implementation of the factor-analyzer package.\nSee https://factor-analyzer.readthedocs.io/en/latest/introduction.html for more details.\nAuthors of the original implementation: * Jeremy Biggs (jeremy.m.biggs@gmail.com) * Nitin Madnani (nmadnani@ets.org) Organization: Educational Testing Service Date: 2022-09-05\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\n\n\n\n\nName\nDescription\n\n\n\n\nConfirmatoryFactorAnalyzer\nFit a confirmatory factor analysis model using maximum likelihood.\n\n\nModelSpecification\nEncapsulate the model specification for CFA.\n\n\nModelSpecificationParser\nGenerate the model specification for CFA.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "factor_analyzer.confirmatory_factor_analyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.html#classes",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.html#classes",
    "title": "factor_analyzer.confirmatory_factor_analyzer",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nConfirmatoryFactorAnalyzer\nFit a confirmatory factor analysis model using maximum likelihood.\n\n\nModelSpecification\nEncapsulate the model specification for CFA.\n\n\nModelSpecificationParser\nGenerate the model specification for CFA.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "factor_analyzer.confirmatory_factor_analyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.html",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.html",
    "title": "factor_analyzer.confirmatory_factor_analyzer.ModelSpecification",
    "section": "",
    "text": "factor_analyzer.confirmatory_factor_analyzer.ModelSpecification(\n    loadings,\n    n_factors,\n    n_variables,\n    factor_names=None,\n    variable_names=None,\n)\nEncapsulate the model specification for CFA.\nThis class contains a number of specification properties that are used in the CFA procedure.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nloadings\narray - like\nThe factor loadings specification.\nrequired\n\n\nn_factors\nint\nThe number of factors.\nrequired\n\n\nn_variables\nint\nThe number of variables.\nrequired\n\n\nfactor_names\nlist of str\nA list of factor names, if available. Defaults to None.\nNone\n\n\nvariable_names\nlist of str\nA list of variable names, if available. Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nerror_vars\nGet the error variance specification.\n\n\nerror_vars_free\nGet the indices of “free” error variance parameters.\n\n\nfactor_covs\nGet the factor covariance specification.\n\n\nfactor_covs_free\nGet the indices of “free” factor covariance parameters.\n\n\nfactor_names\nGet list of factor names, if available.\n\n\nloadings\nGet the factor loadings specification.\n\n\nloadings_free\nGet the indices of “free” factor loading parameters.\n\n\nn_factors\nGet the number of factors.\n\n\nn_lower_diag\nGet the lower diagonal of the factor covariance matrix.\n\n\nn_variables\nGet the number of variables.\n\n\nvariable_names\nGet list of variable names, if available.\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncopy\nReturn a copy of the model specification.\n\n\nget_model_specification_as_dict\nGet the model specification as a dictionary.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "ModelSpecification"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.html#parameters",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.html#parameters",
    "title": "factor_analyzer.confirmatory_factor_analyzer.ModelSpecification",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nloadings\narray - like\nThe factor loadings specification.\nrequired\n\n\nn_factors\nint\nThe number of factors.\nrequired\n\n\nn_variables\nint\nThe number of variables.\nrequired\n\n\nfactor_names\nlist of str\nA list of factor names, if available. Defaults to None.\nNone\n\n\nvariable_names\nlist of str\nA list of variable names, if available. Defaults to None.\nNone",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "ModelSpecification"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.html#attributes",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.html#attributes",
    "title": "factor_analyzer.confirmatory_factor_analyzer.ModelSpecification",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nerror_vars\nGet the error variance specification.\n\n\nerror_vars_free\nGet the indices of “free” error variance parameters.\n\n\nfactor_covs\nGet the factor covariance specification.\n\n\nfactor_covs_free\nGet the indices of “free” factor covariance parameters.\n\n\nfactor_names\nGet list of factor names, if available.\n\n\nloadings\nGet the factor loadings specification.\n\n\nloadings_free\nGet the indices of “free” factor loading parameters.\n\n\nn_factors\nGet the number of factors.\n\n\nn_lower_diag\nGet the lower diagonal of the factor covariance matrix.\n\n\nn_variables\nGet the number of variables.\n\n\nvariable_names\nGet list of variable names, if available.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "ModelSpecification"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.html#methods",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.html#methods",
    "title": "factor_analyzer.confirmatory_factor_analyzer.ModelSpecification",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncopy\nReturn a copy of the model specification.\n\n\nget_model_specification_as_dict\nGet the model specification as a dictionary.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "ModelSpecification"
    ]
  },
  {
    "objectID": "docs/reference/eda.html",
    "href": "docs/reference/eda.html",
    "title": "eda",
    "section": "",
    "text": "eda\neda\nExploratory Data Analysis (EDA) module for spotoptim.\nThis module provides visualization and analysis tools for exploring optimization results and data distributions.",
    "crumbs": [
      "API Reference",
      "Exploratory Data Analysis",
      "eda"
    ]
  },
  {
    "objectID": "docs/reference/eda.plots.plot_ip_histograms.html",
    "href": "docs/reference/eda.plots.plot_ip_histograms.html",
    "title": "eda.plots.plot_ip_histograms",
    "section": "",
    "text": "eda.plots.plot_ip_histograms(\n    df,\n    bins=10,\n    num_cols=2,\n    figwidth=10,\n    thrs_unique=5,\n    add_points=None,\n    add_points_col=['red'],\n)\nGenerate infill-point histograms (ip-histograms) for each numerical column in the DataFrame within a single figure. The title of each histogram shows the total, unique values count, outliers, and standard deviation. If there are fewer unique values than the threshold thrs_unique, the ip-histogram is colored differently. Additional points can be added and highlighted in red.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nDataFrame containing the data to plot.\nrequired\n\n\nbins\nint\nNumber of bins for the histograms. Defaults to 10.\n10\n\n\nnum_cols\nint\nNumber of columns in the subplot grid. Defaults to 2.\n2\n\n\nfigwidth\nint\nWidth of the entire figure. Defaults to 10.\n10\n\n\nthrs_unique\nint\nThreshold for unique values to change histogram color. Defaults to 5.\n5\n\n\nadd_points\npd.DataFrame\nDataFrame containing additional points to highlight. Defaults to None.\nNone\n\n\nadd_points_col\nlist\nList of colors for the additional points. Defaults to [“red”].\n['red']\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotoptim.eda.plots import plot_ip_histograms\n&gt;&gt;&gt; data = {'A': [1, 2, 2, 3, 4, 5, 100], 'B': [10, 10, 10, 10, 10, 10, 10]}\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; plot_ip_histograms(df, bins=5, num_cols=1, thrs_unique=3)\n&gt;&gt;&gt; # Example with multiple added points and colors\n&gt;&gt;&gt; add_points = pd.DataFrame({'A': [1.5, 3.5], 'B': [10, 10]})\n&gt;&gt;&gt; plot_ip_histograms(df, add_points=add_points, add_points_col=[\"red\", \"blue\"])",
    "crumbs": [
      "API Reference",
      "Exploratory Data Analysis",
      "plot_ip_histograms"
    ]
  },
  {
    "objectID": "docs/reference/eda.plots.plot_ip_histograms.html#parameters",
    "href": "docs/reference/eda.plots.plot_ip_histograms.html#parameters",
    "title": "eda.plots.plot_ip_histograms",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nDataFrame containing the data to plot.\nrequired\n\n\nbins\nint\nNumber of bins for the histograms. Defaults to 10.\n10\n\n\nnum_cols\nint\nNumber of columns in the subplot grid. Defaults to 2.\n2\n\n\nfigwidth\nint\nWidth of the entire figure. Defaults to 10.\n10\n\n\nthrs_unique\nint\nThreshold for unique values to change histogram color. Defaults to 5.\n5\n\n\nadd_points\npd.DataFrame\nDataFrame containing additional points to highlight. Defaults to None.\nNone\n\n\nadd_points_col\nlist\nList of colors for the additional points. Defaults to [“red”].\n['red']",
    "crumbs": [
      "API Reference",
      "Exploratory Data Analysis",
      "plot_ip_histograms"
    ]
  },
  {
    "objectID": "docs/reference/eda.plots.plot_ip_histograms.html#returns",
    "href": "docs/reference/eda.plots.plot_ip_histograms.html#returns",
    "title": "eda.plots.plot_ip_histograms",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "API Reference",
      "Exploratory Data Analysis",
      "plot_ip_histograms"
    ]
  },
  {
    "objectID": "docs/reference/eda.plots.plot_ip_histograms.html#examples",
    "href": "docs/reference/eda.plots.plot_ip_histograms.html#examples",
    "title": "eda.plots.plot_ip_histograms",
    "section": "",
    "text": "&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotoptim.eda.plots import plot_ip_histograms\n&gt;&gt;&gt; data = {'A': [1, 2, 2, 3, 4, 5, 100], 'B': [10, 10, 10, 10, 10, 10, 10]}\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; plot_ip_histograms(df, bins=5, num_cols=1, thrs_unique=3)\n&gt;&gt;&gt; # Example with multiple added points and colors\n&gt;&gt;&gt; add_points = pd.DataFrame({'A': [1.5, 3.5], 'B': [10, 10]})\n&gt;&gt;&gt; plot_ip_histograms(df, add_points=add_points, add_points_col=[\"red\", \"blue\"])",
    "crumbs": [
      "API Reference",
      "Exploratory Data Analysis",
      "plot_ip_histograms"
    ]
  },
  {
    "objectID": "docs/reference/data.html",
    "href": "docs/reference/data.html",
    "title": "data",
    "section": "",
    "text": "data\ndata\nData utilities for spotoptim package.\nThis module provides ready-to-use datasets and data loaders for common machine learning tasks.",
    "crumbs": [
      "API Reference",
      "Data",
      "data"
    ]
  },
  {
    "objectID": "docs/reference/data.diabetes.get_diabetes_dataloaders.html",
    "href": "docs/reference/data.diabetes.get_diabetes_dataloaders.html",
    "title": "data.diabetes.get_diabetes_dataloaders",
    "section": "",
    "text": "data.diabetes.get_diabetes_dataloaders(\n    test_size=0.2,\n    batch_size=32,\n    scale_features=True,\n    shuffle_train=True,\n    shuffle_test=False,\n    random_state=42,\n    num_workers=0,\n    pin_memory=False,\n)\nReturns train and test dataloaders for the Diabetes dataset.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntest_size\nfloat\nFraction of data to use for testing.\n0.2\n\n\nbatch_size\nint\nBatch size.\n32\n\n\nscale_features\nbool\nWhether to standardize features using StandardScaler.\nTrue\n\n\nshuffle_train\nbool\nWhether to shuffle the training data.\nTrue\n\n\nshuffle_test\nbool\nWhether to shuffle the test data.\nFalse\n\n\nrandom_state\nint\nRandom seed for splitting.\n42\n\n\nnum_workers\nint\nNumber of subprocesses to use for data loading.\n0\n\n\npin_memory\nbool\nIf True, the data loader will copy Tensors into CUDA pinned memory before returning them.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\nTuple[DataLoader, DataLoader, Optional[StandardScaler]]\n(train_loader, test_loader, scaler) scaler is the StandardScaler implementation if scale_features=True, else None.",
    "crumbs": [
      "API Reference",
      "Data",
      "get_diabetes_dataloaders"
    ]
  },
  {
    "objectID": "docs/reference/data.diabetes.get_diabetes_dataloaders.html#parameters",
    "href": "docs/reference/data.diabetes.get_diabetes_dataloaders.html#parameters",
    "title": "data.diabetes.get_diabetes_dataloaders",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ntest_size\nfloat\nFraction of data to use for testing.\n0.2\n\n\nbatch_size\nint\nBatch size.\n32\n\n\nscale_features\nbool\nWhether to standardize features using StandardScaler.\nTrue\n\n\nshuffle_train\nbool\nWhether to shuffle the training data.\nTrue\n\n\nshuffle_test\nbool\nWhether to shuffle the test data.\nFalse\n\n\nrandom_state\nint\nRandom seed for splitting.\n42\n\n\nnum_workers\nint\nNumber of subprocesses to use for data loading.\n0\n\n\npin_memory\nbool\nIf True, the data loader will copy Tensors into CUDA pinned memory before returning them.\nFalse",
    "crumbs": [
      "API Reference",
      "Data",
      "get_diabetes_dataloaders"
    ]
  },
  {
    "objectID": "docs/reference/data.diabetes.get_diabetes_dataloaders.html#returns",
    "href": "docs/reference/data.diabetes.get_diabetes_dataloaders.html#returns",
    "title": "data.diabetes.get_diabetes_dataloaders",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ntuple\nTuple[DataLoader, DataLoader, Optional[StandardScaler]]\n(train_loader, test_loader, scaler) scaler is the StandardScaler implementation if scale_features=True, else None.",
    "crumbs": [
      "API Reference",
      "Data",
      "get_diabetes_dataloaders"
    ]
  },
  {
    "objectID": "docs/reference/data.base.html",
    "href": "docs/reference/data.base.html",
    "title": "data.base",
    "section": "",
    "text": "data.base\n\n\n\n\n\nName\nDescription\n\n\n\n\nConfig\nBase class for all configurations.\n\n\nFileConfig\nBase class for configurations that are stored in a local file.",
    "crumbs": [
      "API Reference",
      "Data",
      "data.base"
    ]
  },
  {
    "objectID": "docs/reference/data.base.html#classes",
    "href": "docs/reference/data.base.html#classes",
    "title": "data.base",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nConfig\nBase class for all configurations.\n\n\nFileConfig\nBase class for configurations that are stored in a local file.",
    "crumbs": [
      "API Reference",
      "Data",
      "data.base"
    ]
  },
  {
    "objectID": "docs/reference/data.base.Config.html",
    "href": "docs/reference/data.base.Config.html",
    "title": "data.base.Config",
    "section": "",
    "text": "data.base.Config()\nBase class for all configurations.\nAll configurations inherit from this class, be they stored in a file or generated on the fly.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndesc\nstr\nThe description from the docstring.\n\n\n_repr_content\ndict\nThe items that are displayed in the repr method.",
    "crumbs": [
      "API Reference",
      "Data",
      "Config"
    ]
  },
  {
    "objectID": "docs/reference/data.base.Config.html#attributes",
    "href": "docs/reference/data.base.Config.html#attributes",
    "title": "data.base.Config",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ndesc\nstr\nThe description from the docstring.\n\n\n_repr_content\ndict\nThe items that are displayed in the repr method.",
    "crumbs": [
      "API Reference",
      "Data",
      "Config"
    ]
  },
  {
    "objectID": "docs/reference/core.experiment.html",
    "href": "docs/reference/core.experiment.html",
    "title": "core.experiment",
    "section": "",
    "text": "core.experiment\n\n\n\n\n\nName\nDescription\n\n\n\n\nExperimentControl\nControls the experiment configuration, replacing the legacy fun_control dictionary.",
    "crumbs": [
      "API Reference",
      "Core",
      "core.experiment"
    ]
  },
  {
    "objectID": "docs/reference/core.experiment.html#classes",
    "href": "docs/reference/core.experiment.html#classes",
    "title": "core.experiment",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nExperimentControl\nControls the experiment configuration, replacing the legacy fun_control dictionary.",
    "crumbs": [
      "API Reference",
      "Core",
      "core.experiment"
    ]
  },
  {
    "objectID": "docs/reference/core.data.html",
    "href": "docs/reference/core.data.html",
    "title": "core.data",
    "section": "",
    "text": "core.data\n\n\n\n\n\nName\nDescription\n\n\n\n\nSpotDataFromArray\nData handler for numpy arrays or torch tensors.\n\n\nSpotDataFromTorchDataset\nData handler for PyTorch Datasets.\n\n\nSpotDataSet\nAbstract base class for data handling in SpotOptim.",
    "crumbs": [
      "API Reference",
      "Core",
      "core.data"
    ]
  },
  {
    "objectID": "docs/reference/core.data.html#classes",
    "href": "docs/reference/core.data.html#classes",
    "title": "core.data",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nSpotDataFromArray\nData handler for numpy arrays or torch tensors.\n\n\nSpotDataFromTorchDataset\nData handler for PyTorch Datasets.\n\n\nSpotDataSet\nAbstract base class for data handling in SpotOptim.",
    "crumbs": [
      "API Reference",
      "Core",
      "core.data"
    ]
  },
  {
    "objectID": "docs/reference/core.data.SpotDataFromTorchDataset.html",
    "href": "docs/reference/core.data.SpotDataFromTorchDataset.html",
    "title": "core.data.SpotDataFromTorchDataset",
    "section": "",
    "text": "core.data.SpotDataFromTorchDataset\ncore.data.SpotDataFromTorchDataset(\n    train_dataset,\n    input_dim,\n    output_dim,\n    val_dataset=None,\n    test_dataset=None,\n    target_column=None,\n)\nData handler for PyTorch Datasets.",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotDataFromTorchDataset"
    ]
  },
  {
    "objectID": "docs/reference/SpotOptim.html",
    "href": "docs/reference/SpotOptim.html",
    "title": "SpotOptim",
    "section": "",
    "text": "SpotOptim\n\n\n\n\n\nName\nDescription\n\n\n\n\nSpotOptim\nSPOT optimizer compatible with scipy.optimize interface.\n\n\nSpotOptimConfig\nConfiguration parameters for SpotOptim.\n\n\nSpotOptimState\nMutable state of the optimization process.",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotOptim"
    ]
  },
  {
    "objectID": "docs/reference/SpotOptim.html#classes",
    "href": "docs/reference/SpotOptim.html#classes",
    "title": "SpotOptim",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nSpotOptim\nSPOT optimizer compatible with scipy.optimize interface.\n\n\nSpotOptimConfig\nConfiguration parameters for SpotOptim.\n\n\nSpotOptimState\nMutable state of the optimization process.",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotOptim"
    ]
  },
  {
    "objectID": "docs/reference/SpotOptim.SpotOptimConfig.html",
    "href": "docs/reference/SpotOptim.SpotOptimConfig.html",
    "title": "SpotOptim.SpotOptimConfig",
    "section": "",
    "text": "SpotOptim.SpotOptimConfig\nSpotOptim.SpotOptimConfig(\n    bounds=None,\n    max_iter=20,\n    n_initial=10,\n    surrogate=None,\n    acquisition='y',\n    var_type=None,\n    var_name=None,\n    var_trans=None,\n    tolerance_x=None,\n    max_time=np.inf,\n    repeats_initial=1,\n    repeats_surrogate=1,\n    ocba_delta=0,\n    tensorboard_log=False,\n    tensorboard_path=None,\n    tensorboard_clean=False,\n    fun_mo2so=None,\n    seed=None,\n    verbose=False,\n    warnings_filter='ignore',\n    n_infill_points=1,\n    max_surrogate_points=None,\n    selection_method='distant',\n    acquisition_failure_strategy='random',\n    penalty=False,\n    penalty_val=None,\n    acquisition_fun_return_size=3,\n    acquisition_optimizer='differential_evolution',\n    restart_after_n=100,\n    restart_inject_best=True,\n    x0=None,\n    de_x0_prob=0.1,\n    tricands_fringe=False,\n    prob_de_tricands=0.8,\n    window_size=None,\n    min_tol_metric='chebyshev',\n    prob_surrogate=None,\n    n_jobs=1,\n    acquisition_optimizer_kwargs=None,\n    args=(),\n    kwargs=None,\n)\nConfiguration parameters for SpotOptim.",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotOptimConfig"
    ]
  },
  {
    "objectID": "docs/examples.html",
    "href": "docs/examples.html",
    "title": "SPOT Examples",
    "section": "",
    "text": "The following example demonstrates how to use SpotOptim with the Sphere function.\n\nimport numpy as np\nfrom spotoptim import SpotOptim\n\ndef sphere(X):\n    X = np.atleast_2d(X)\n    return np.sum(X**2, axis=1)\n\nopt = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=20,\n    n_initial=10,\n    seed=0,\n)\nresult = opt.optimize()\n\nprint(f\"Best x    : {result.x}\")\nprint(f\"Best f(x) : {result.fun:.6f}\")\nprint(f\"Evaluations: {result.nfev}\")\n\nBest x    : [-1.07968718e-04  9.05044427e-05]\nBest f(x) : 0.000000\nEvaluations: 20",
    "crumbs": [
      "Examples"
    ]
  },
  {
    "objectID": "docs/examples.html#simple-spotoptim-run",
    "href": "docs/examples.html#simple-spotoptim-run",
    "title": "SPOT Examples",
    "section": "",
    "text": "The following example demonstrates how to use SpotOptim with the Sphere function.\n\nimport numpy as np\nfrom spotoptim import SpotOptim\n\ndef sphere(X):\n    X = np.atleast_2d(X)\n    return np.sum(X**2, axis=1)\n\nopt = SpotOptim(\n    fun=sphere,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=20,\n    n_initial=10,\n    seed=0,\n)\nresult = opt.optimize()\n\nprint(f\"Best x    : {result.x}\")\nprint(f\"Best f(x) : {result.fun:.6f}\")\nprint(f\"Evaluations: {result.nfev}\")\n\nBest x    : [-1.07968718e-04  9.05044427e-05]\nBest f(x) : 0.000000\nEvaluations: 20",
    "crumbs": [
      "Examples"
    ]
  },
  {
    "objectID": "docs/examples.html#convergence-plot",
    "href": "docs/examples.html#convergence-plot",
    "title": "SPOT Examples",
    "section": "Convergence Plot",
    "text": "Convergence Plot\n\nimport matplotlib.pyplot as plt\nplt.plot(result.y)\nplt.xlabel(\"Evaluation Number\")\nplt.ylabel(\"Objective Value\")\nplt.title(\"Optimization Progress\")\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "Examples"
    ]
  },
  {
    "objectID": "docs/examples.html#further-examples",
    "href": "docs/examples.html#further-examples",
    "title": "SPOT Examples",
    "section": "Further Examples",
    "text": "Further Examples\nMore detailed examples showing acquisition functions, integer variables, and categorical variables can be found in the Living Examples document.",
    "crumbs": [
      "Examples"
    ]
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "spotoptim",
    "section": "",
    "text": "Sequential Parameter Optimization Toolbox",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/index.html#about-spotoptim",
    "href": "docs/index.html#about-spotoptim",
    "title": "spotoptim",
    "section": "About spotoptim",
    "text": "About spotoptim\nspotoptim is a Python toolbox for Sequential Parameter Optimization (SPO), designed for robust and efficient optimization of expensive-to-evaluate functions.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/index.html#surrogate-model-based-optimization",
    "href": "docs/index.html#surrogate-model-based-optimization",
    "title": "spotoptim",
    "section": "Surrogate Model Based Optimization",
    "text": "Surrogate Model Based Optimization\n\nDocumentation for spotoptim see Sequential Parameter Optimization Cookbook.\nNews and updates related to SPOT see SPOTSeven",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/download.html",
    "href": "docs/download.html",
    "title": "Install spotoptim",
    "section": "",
    "text": "pip install spotoptim\n\n\n\nuv pip install spotoptim",
    "crumbs": [
      "Download"
    ]
  },
  {
    "objectID": "docs/download.html#using-pip",
    "href": "docs/download.html#using-pip",
    "title": "Install spotoptim",
    "section": "",
    "text": "pip install spotoptim",
    "crumbs": [
      "Download"
    ]
  },
  {
    "objectID": "docs/download.html#using-uv",
    "href": "docs/download.html#using-uv",
    "title": "Install spotoptim",
    "section": "",
    "text": "uv pip install spotoptim",
    "crumbs": [
      "Download"
    ]
  },
  {
    "objectID": "docs/reference/SpotOptim.SpotOptim.html",
    "href": "docs/reference/SpotOptim.SpotOptim.html",
    "title": "SpotOptim.SpotOptim",
    "section": "",
    "text": "SpotOptim.SpotOptim(\n    fun,\n    bounds=None,\n    max_iter=20,\n    n_initial=10,\n    surrogate=None,\n    acquisition='y',\n    var_type=None,\n    var_name=None,\n    var_trans=None,\n    tolerance_x=None,\n    max_time=np.inf,\n    repeats_initial=1,\n    repeats_surrogate=1,\n    ocba_delta=0,\n    tensorboard_log=False,\n    tensorboard_path=None,\n    tensorboard_clean=False,\n    fun_mo2so=None,\n    seed=None,\n    verbose=False,\n    warnings_filter='ignore',\n    n_infill_points=1,\n    max_surrogate_points=None,\n    selection_method='distant',\n    acquisition_failure_strategy='random',\n    penalty=False,\n    penalty_val=None,\n    acquisition_fun_return_size=3,\n    acquisition_optimizer='differential_evolution',\n    restart_after_n=100,\n    restart_inject_best=True,\n    x0=None,\n    de_x0_prob=0.1,\n    tricands_fringe=False,\n    prob_de_tricands=0.8,\n    window_size=None,\n    min_tol_metric='chebyshev',\n    prob_surrogate=None,\n    n_jobs=1,\n    acquisition_optimizer_kwargs=None,\n    args=(),\n    kwargs=None,\n)\nSPOT optimizer compatible with scipy.optimize interface.",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotOptim"
    ]
  },
  {
    "objectID": "docs/reference/SpotOptim.SpotOptim.html#parameters",
    "href": "docs/reference/SpotOptim.SpotOptim.html#parameters",
    "title": "SpotOptim.SpotOptim",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfun\ncallable\nObjective function to minimize. Should accept array of shape (n_samples, n_features).\nrequired\n\n\nbounds\nlist of tuple\nBounds for each dimension as [(low, high), …].\nNone\n\n\nmax_iter\nint\nMaximum number of total function evaluations (including initial design). For example, max_iter=30 with n_initial=10 will perform 10 initial evaluations plus 20 sequential optimization iterations. Defaults to 20.\n20\n\n\nn_initial\nint\nNumber of initial design points. Defaults to 10.\n10\n\n\nsurrogate\nobject\nSurrogate model with scikit-learn interface (fit/predict methods). If None, uses a Gaussian Process Regressor with Matern kernel. Default configuration:: from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import Matern, ConstantKernel kernel = ConstantKernel(1.0, (1e-2, 1e12)) * Matern(length_scale=1.0, length_scale_bounds=(1e-4, 1e2), nu=2.5) surrogate = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=100) surrogate = GaussianProcessRegressor( kernel=kernel, n_restarts_optimizer=10, normalize_y=True, random_state=self.seed, ) Alternative surrogates can be provided, including SpotOptim’s Kriging model, Random Forests, or any scikit-learn compatible regressor. See Examples section. Defaults to None (uses default Gaussian Process configuration).\nNone\n\n\nacquisition\nstr\nAcquisition function (‘ei’, ‘y’, ‘pi’). Defaults to ‘y’.\n'y'\n\n\nvar_type\nlist of str\nVariable types for each dimension. Supported types: - ‘float’: Python floats, continuous optimization (no rounding) - ‘int’: Python int, float values will be rounded to integers - ‘factor’: Unordered categorical data, internally mapped to int values (e.g., “red”-&gt;0, “green”-&gt;1, etc.) Defaults to None (which sets all dimensions to ‘float’).\nNone\n\n\nvar_name\nlist of str\nVariable names for each dimension. If None, uses default names [‘x0’, ‘x1’, ‘x2’, …]. Defaults to None.\nNone\n\n\ntolerance_x\nfloat\nMinimum distance between points. Defaults to np.sqrt(np.spacing(1))\nNone\n\n\nvar_trans\nlist of str\nVariable transformations for each dimension. Supported: - ‘log’: Logarithmic transformation, e.g. “log10” for base-10 log - ‘sqrt’: Square root transformation, “sqrt” for square root - None or ‘id’ or ‘None’: No transformation Defaults to None (no transformations).\nNone\n\n\nmax_time\nfloat\nMaximum runtime in minutes. If np.inf (default), no time limit. The optimization terminates when either max_iter evaluations are reached OR max_time minutes have elapsed, whichever comes first. Defaults to np.inf.\nnp.inf\n\n\nrepeats_initial\nint\nNumber of times to evaluate each initial design point. Useful for noisy objective functions. If &gt; 1, noise handling is activated and statistics (mean, variance) are tracked. Defaults to 1.\n1\n\n\nrepeats_surrogate\nint\nNumber of times to evaluate each surrogate-suggested point. Useful for noisy objective functions. If &gt; 1, noise handling is activated and statistics (mean, variance) are tracked. Defaults to 1.\n1\n\n\nocba_delta\nint\nNumber of additional evaluations to allocate using Optimal Computing Budget Allocation (OCBA) when noise handling is active. OCBA determines which existing design points should be re-evaluated to best distinguish between alternatives. Only used when noise=True (repeats &gt; 1) and ocba_delta &gt; 0. Requires at least 3 design points with variance information. Defaults to 0 (no OCBA).\n0\n\n\ntensorboard_log\nbool\nEnable TensorBoard logging. If True, optimization metrics and hyperparameters are logged to TensorBoard. View logs by running: tensorboard --logdir=&lt;tensorboard_path&gt; in a separate terminal. Defaults to False.\nFalse\n\n\ntensorboard_path\nstr\nPath for TensorBoard log files. If None and tensorboard_log is True, creates a default path: runs/spotoptim_YYYYMMDD_HHMMSS. Defaults to None.\nNone\n\n\ntensorboard_clean\nbool\nIf True, removes all old TensorBoard log directories from the ‘runs’ folder before starting optimization. Use with caution as this permanently deletes all subdirectories in ‘runs’. Defaults to False.\nFalse\n\n\nfun_mo2so\ncallable\nFunction to convert multi-objective values to single-objective. Takes an array of shape (n_samples, n_objectives) and returns array of shape (n_samples,). If None and objective function returns multi-objective values, uses first objective. Defaults to None.\nNone\n\n\nseed\nint\nRandom seed for reproducibility. Defaults to None.\nNone\n\n\nverbose\nbool\nPrint progress information. Defaults to False.\nFalse\n\n\nwarnings_filter\nstr\nFilter for warnings. One of “error”, “ignore”, “always”, “all”, “default”, “module”, or “once”. Defaults to “ignore”.\n'ignore'\n\n\nn_infill_points\nint\nNumber of infill points to suggest at each iteration. Defaults to 1. If &gt; 1, multiple distinct points are proposed using the optimizer and fallback strategies.\n1\n\n\nmax_surrogate_points\nint\nMaximum number of points to use for surrogate model fitting. If None, all points are used. If the number of evaluated points exceeds this limit, a subset is selected using the selection method. Defaults to None.\nNone\n\n\nselection_method\nstr\nMethod for selecting points when max_surrogate_points is exceeded. Options: ‘distant’ (Select points that are distant from each other via K-means clustering) or ‘best’ (Select all points from the cluster with the best mean objective value). Defaults to ‘distant’.\n'distant'\n\n\nacquisition_failure_strategy\nstr\nStrategy for handling acquisition function failures. Options: ‘random’ (space-filling design via Latin Hypercube Sampling) Defaults to ‘random’.\n'random'\n\n\npenalty\nbool\nWhether to use penalty for handling NaN/inf values in objective function evaluations. Defaults to False.\nFalse\n\n\npenalty_val\nfloat\nPenalty value to replace NaN/inf values in objective function evaluations. When the objective function returns NaN or inf, these values are replaced with penalty plus a small random noise (sampled from N(0, 0.1)) to avoid identical penalty values. This allows optimization to continue despite occasional function evaluation failures. Defaults to None.\nNone\n\n\nacquisition_fun_return_size\nint\nNumber of top candidates to return from acquisition function optimization. Defaults to 3.\n3\n\n\nacquisition_optimizer\nstr or callable\nOptimizer to use for maximizing acquisition function. Can be “differential_evolution” (default) or any method name supported by scipy.optimize.minimize (e.g., “Nelder-Mead”, “L-BFGS-B”). Can also be a callable with signature compatible with scipy.optimize.minimize (fun, x0, bounds, …). A specific version is “de_tricands”, which combines DE with Tricands. It can be parameterized with “prob_de_tricands” (probability of using DE). Defaults to “differential_evolution”.\n'differential_evolution'\n\n\nacquisition_optimizer_kwargs\ndict\nKwargs passed to the acquisition function optimizer and GPR surrogate optimizer. Defaults to {‘maxiter’: 10000, ‘gtol’: 1e-9}.\nNone\n\n\nrestart_after_n\nint\nNumber of consecutive iterations with zero success rate before triggering a restart. Defaults to 100.\n100\n\n\nrestart_inject_best\nbool\nWhether to inject the best solution found so far as a starting point for the next restart. Defaults to True.\nTrue\n\n\nx0\narray - like\nStarting point for optimization, shape (n_features,). If provided, this point will be evaluated first and included in the initial design. The point should be within the bounds and will be validated before use. Defaults to None (no starting point, uses only LHS design).\nNone\n\n\nde_x0_prob\nfloat\nProbability of using the best point as starting point for differential evolution. Defaults to 0.1.\n0.1\n\n\ntricands_fringe\nbool\nWhether to use the fringe of the design space for the initial design. Defaults to False.\nFalse\n\n\nprob_de_tricands\nfloat\nProbability of using differential evolution as an optimizer on the surrogate model. 1 - prob_de_tricands is the probability of using tricands. Defaults to 0.8.\n0.8\n\n\nwindow_size\nint\nWindow size for success rate calculation.\nNone\n\n\nmin_tol_metric\nstr\nDistance metric used when checking tolerance_x for duplicate detection. Default is “chebyshev”. Supports all metrics from scipy.spatial.distance.cdist, including: - “chebyshev”: L-infinity distance (hypercube). Default. Matches previous behavior. - “euclidean”: L2 distance (hypersphere). - “minkowski”: Lp distance (default p=2). - “cityblock”: Manhattan/L1 distance. - “cosine”: Cosine distance. - “correlation”: Correlation distance. - “canberra”, “braycurtis”, “sqeuclidean”, etc.\n'chebyshev'",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotOptim"
    ]
  },
  {
    "objectID": "docs/reference/SpotOptim.SpotOptim.html#attributes",
    "href": "docs/reference/SpotOptim.SpotOptim.html#attributes",
    "title": "SpotOptim.SpotOptim",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nType\nDescription\n\n\n\n\nX_\nndarray\nAll evaluated points, shape (n_samples, n_features).\n\n\ny_\nndarray\nFunction values at X_, shape (n_samples,). For multi-objective problems, these are the converted single-objective values.\n\n\ny_mo\nndarray or None\nMulti-objective function values, shape (n_samples, n_objectives). None for single-objective problems.\n\n\nbest_x_\nndarray\nBest point found, shape (n_features,).\n\n\nbest_y_\nfloat\nBest function value found.\n\n\nn_iter_\nint\nNumber of iterations performed. This is not the same as counter. Provided for compatibility with scipy.optimize routines.\n\n\ncounter\nint\nTotal number of function evaluations.\n\n\nsuccess_rate\nfloat\nRolling success rate over the last window_size evaluations. A success is counted when a new evaluation improves upon the best value found so far.\n\n\nwarnings_filter\nstr\nFilter for warnings during optimization.\n\n\nmax_surrogate_points\nint or None\nMaximum number of points for surrogate fitting.\n\n\nselection_method\nstr\nPoint selection method.\n\n\nacquisition_failure_strategy\nstr\nStrategy for handling acquisition failures (‘random’).\n\n\nnoise\nbool\nTrue if noise handling is active (repeats &gt; 1).\n\n\nmean_X\nndarray or None\nAggregated unique design points (if noise=True).\n\n\nmean_y\nndarray or None\nMean y values per design point (if noise=True).\n\n\nvar_y\nndarray or None\nVariance of y values per design point (if noise=True).\n\n\nmin_mean_X\nndarray or None\nX value of best mean y (if noise=True).\n\n\nmin_mean_y\nfloat or None\nBest mean y value (if noise=True).\n\n\nmin_var_y\nfloat or None\nVariance of best mean y (if noise=True).\n\n\nde_x0_prob\nfloat\nProbability of using the best point as starting point for differential evolution.\n\n\ntricands_fringe\nbool\nWhether to use the fringe of the design space for the initial design.\n\n\nprob_de_tricands\nfloat\nProbability of using differential evolution as an optimizer on the surrogate model.",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotOptim"
    ]
  },
  {
    "objectID": "docs/reference/SpotOptim.SpotOptim.html#examples",
    "href": "docs/reference/SpotOptim.SpotOptim.html#examples",
    "title": "SpotOptim.SpotOptim",
    "section": "Examples",
    "text": "Examples\n\nimport numpy as np\nfrom spotoptim import SpotOptim\n\ndef objective(X):\n    return np.sum(X**2, axis=1)\n\n# Example 1: Basic usage (deterministic function)\nbounds = [(-5, 5), (-5, 5)]\noptimizer = SpotOptim(fun=objective, bounds=bounds, max_iter=10, n_initial=5, verbose=True)\nresult = optimizer.optimize()\nprint(\"Best x:\", result.x)\nprint(\"Best f(x):\", result.fun)\n\nTensorBoard logging disabled\nInitial best: f(x) = 10.182354\nIter 1 | Best: 10.182354 | Curr: 15.327035 | Rate: 0.00 | Evals: 60.0%\nIter 2 | Best: 10.182354 | Curr: 10.194670 | Rate: 0.00 | Evals: 70.0%\nIter 3 | Best: 10.182354 | Curr: 10.188564 | Rate: 0.00 | Evals: 80.0%\nIter 4 | Best: 7.722703 | Rate: 0.25 | Evals: 90.0%\nIter 5 | Best: 3.348711 | Rate: 0.40 | Evals: 100.0%\nBest x: [1.67323285 0.74094706]\nBest f(x): 3.3487107206705296\n\n\n\nimport numpy as np\nfrom spotoptim import SpotOptim\n\ndef objective(X):\n    return np.sum(X**2, axis=1)\n\n# Example 2: With custom variable names\noptimizer = SpotOptim(\n    fun=objective,\n    bounds=[(-5, 5), (-5, 5)],\n    var_name=[\"param1\", \"param2\"],\n    max_iter=10,\n    n_initial=5\n)\nresult = optimizer.optimize()\n# Ensure we can use custom names in plots\noptimizer.plot_surrogate(show=False)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom spotoptim import SpotOptim\n\n# Example 3: Noisy function with repeated evaluations\ndef noisy_objective(X):\n    base = np.sum(X**2, axis=1)\n    noise = np.random.normal(0, 0.1, size=base.shape)\n    return base + noise\n\noptimizer = SpotOptim(\n    fun=noisy_objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=30,\n    n_initial=10,\n    repeats_initial=3,      # Evaluate each initial point 3 times\n    repeats_surrogate=2,    # Evaluate each new point 2 times\n    seed=42,                # For reproducibility\n    verbose=True\n)\nresult = optimizer.optimize()\n\n# Access noise statistics\nprint(\"Unique design points:\", optimizer.mean_X.shape[0])\nprint(\"Best mean value:\", optimizer.min_mean_y)\nprint(\"Variance at best point:\", optimizer.min_var_y)\n\nTensorBoard logging disabled\nInitial best: f(x) = 2.305707, mean best: f(x) = 2.367991\nUnique design points: 10\nBest mean value: 2.3679913784564808\nVariance at best point: 0.0026553713273186792\n\n\n\nimport numpy as np\nfrom spotoptim import SpotOptim\n\ndef noisy_objective(X):\n    base = np.sum(X**2, axis=1)\n    noise = np.random.normal(0, 0.1, size=base.shape)\n    return base + noise\n\n# Example 4: Noisy function with OCBA (Optimal Computing Budget Allocation)\noptimizer_ocba = SpotOptim(\n    fun=noisy_objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=50,\n    n_initial=10,\n    repeats_initial=2,      # Initial repeats\n    repeats_surrogate=1,    # Surrogate repeats\n    ocba_delta=3,           # Allocate 3 additional evaluations per iteration\n    seed=42,\n    verbose=True\n)\nresult = optimizer_ocba.optimize()\n\n# OCBA intelligently re-evaluates promising points to reduce uncertainty\nprint(\"Total evaluations:\", result.nfev)\nprint(\"Unique design points:\", optimizer_ocba.mean_X.shape[0])\nprint(\"Best mean value:\", optimizer_ocba.min_mean_y)\nprint(\"Variance at best point:\", optimizer_ocba.min_var_y)\n\nTensorBoard logging disabled\nInitial best: f(x) = 2.319523, mean best: f(x) = 2.385877\n\nIn _get_ocba():\nmeans: [25.8857338  11.32118531 10.14985518  2.38587731 20.7190555  21.36600973\n 16.36904227 14.1099758  18.36970272 20.14080533]\nvars: [6.73858271e-13 1.33640624e-08 1.00799409e-03 4.40284305e-03\n 2.56053422e-03 6.35744853e-04 1.16126758e-02 3.37927306e-03\n 1.64745915e-03 1.91555606e-03]\ndelta: 3\nn_designs: 10\nRatios: [7.29707371e-11 1.00099067e-05 1.00000000e+00 3.61962598e+00\n 4.55581054e-01 1.05534624e-01 3.55166445e+00 1.47019506e+00\n 3.85623766e-01 3.63385525e-01]\nBest: 3, Second best: 2\n  OCBA: Adding 3 re-evaluation(s)\nIter 1 | Best: 2.240418 | Rate: 0.25 | Evals: 48.0% | Mean Best: 2.240418\nIter 2 | Best: 1.594455 | Rate: 0.40 | Evals: 50.0% | Mean Best: 1.594455\nIter 3 | Best: 0.500204 | Rate: 0.50 | Evals: 52.0% | Mean Best: 0.500204\nIter 4 | Best: -0.050742 | Rate: 0.57 | Evals: 54.0% | Mean Best: -0.050742\nIter 5 | Best: -0.050742 | Curr: 0.109333 | Rate: 0.50 | Evals: 56.0% | Mean Curr: 0.109333\nIter 6 | Best: -0.050742 | Curr: -0.016351 | Rate: 0.44 | Evals: 58.0% | Mean Curr: -0.016351\nIter 7 | Best: -0.050742 | Curr: 0.029190 | Rate: 0.40 | Evals: 60.0% | Mean Curr: 0.029190\nIter 8 | Best: -0.050742 | Curr: 0.720357 | Rate: 0.36 | Evals: 62.0% | Mean Curr: 0.720357\nIter 9 | Best: -0.050742 | Curr: 0.440561 | Rate: 0.33 | Evals: 64.0% | Mean Curr: 0.440561\nIter 10 | Best: -0.050742 | Curr: 0.134039 | Rate: 0.31 | Evals: 66.0% | Mean Curr: 0.134039\nIter 11 | Best: -0.104359 | Rate: 0.36 | Evals: 68.0% | Mean Best: -0.104359\nIter 12 | Best: -0.104359 | Curr: 0.093697 | Rate: 0.33 | Evals: 70.0% | Mean Curr: 0.093697\nIter 13 | Best: -0.104359 | Curr: -0.011963 | Rate: 0.31 | Evals: 72.0% | Mean Curr: -0.011963\nIter 14 | Best: -0.104359 | Curr: 0.058689 | Rate: 0.29 | Evals: 74.0% | Mean Curr: 0.058689\nIter 15 | Best: -0.104359 | Curr: -0.016010 | Rate: 0.28 | Evals: 76.0% | Mean Curr: -0.016010\nIter 16 | Best: -0.104359 | Curr: 0.098153 | Rate: 0.26 | Evals: 78.0% | Mean Curr: 0.098153\nIter 17 | Best: -0.104359 | Curr: 0.038064 | Rate: 0.25 | Evals: 80.0% | Mean Curr: 0.038064\nIter 18 | Best: -0.104359 | Curr: 0.742283 | Rate: 0.24 | Evals: 82.0% | Mean Curr: 0.742283\nIter 19 | Best: -0.104359 | Curr: 0.262925 | Rate: 0.23 | Evals: 84.0% | Mean Curr: 0.262925\nIter 20 | Best: -0.104359 | Curr: -0.006529 | Rate: 0.22 | Evals: 86.0% | Mean Curr: -0.006529\nIter 21 | Best: -0.104359 | Curr: 0.077486 | Rate: 0.21 | Evals: 88.0% | Mean Curr: 0.077486\nIter 22 | Best: -0.146086 | Rate: 0.24 | Evals: 90.0% | Mean Best: -0.146086\nIter 23 | Best: -0.146086 | Curr: -0.069235 | Rate: 0.23 | Evals: 92.0% | Mean Curr: -0.069235\nIter 24 | Best: -0.146086 | Curr: 0.137256 | Rate: 0.22 | Evals: 94.0% | Mean Curr: 0.137256\nIter 25 | Best: -0.146086 | Curr: 0.108752 | Rate: 0.21 | Evals: 96.0% | Mean Curr: 0.108752\nIter 26 | Best: -0.146086 | Curr: 0.099482 | Rate: 0.21 | Evals: 98.0% | Mean Curr: 0.099482\nIter 27 | Best: -0.175205 | Rate: 0.23 | Evals: 100.0% | Mean Best: -0.175205\nTotal evaluations: 50\nUnique design points: 37\nBest mean value: -0.1752048981960286\nVariance at best point: 0.0\n\n\n\nimport numpy as np\nimport shutil\nimport os\nfrom spotoptim import SpotOptim\n\ndef objective(X):\n    return np.sum(X**2, axis=1)\n\n# Example 5: With TensorBoard logging\ntb_dir = \"runs/my_optimization\"\noptimizer_tb = SpotOptim(\n    fun=objective,\n    bounds=[(-5, 5), (-5, 5)],\n    max_iter=15,\n    n_initial=5,\n    tensorboard_log=True,   # Enable TensorBoard\n    tensorboard_path=tb_dir,  # Optional custom path\n    verbose=True\n)\nresult = optimizer_tb.optimize()\n\n# View logs in browser: tensorboard --logdir=runs/my_optimization\nprint(\"Logs saved to:\", optimizer_tb.tensorboard_path)\n\n# Cleanup log dir\nif os.path.exists(tb_dir):\n    shutil.rmtree(tb_dir)\n\nTensorBoard logging enabled: runs/my_optimization\nInitial best: f(x) = 3.173328\nIter 1 | Best: 1.413576 | Rate: 1.00 | Evals: 40.0%\nIter 2 | Best: 0.243346 | Rate: 1.00 | Evals: 46.7%\nIter 3 | Best: 0.107059 | Rate: 1.00 | Evals: 53.3%\nIter 4 | Best: 0.028948 | Rate: 1.00 | Evals: 60.0%\nIter 5 | Best: 0.000136 | Rate: 1.00 | Evals: 66.7%\nIter 6 | Best: 0.000078 | Rate: 1.00 | Evals: 73.3%\nIter 7 | Best: 0.000021 | Rate: 1.00 | Evals: 80.0%\nIter 8 | Best: 0.000007 | Rate: 1.00 | Evals: 86.7%\nIter 9 | Best: 0.000004 | Rate: 1.00 | Evals: 93.3%\nIter 10 | Best: 0.000004 | Rate: 1.00 | Evals: 100.0%\nTensorBoard writer closed. View logs with: tensorboard --logdir=runs/my_optimization\nLogs saved to: runs/my_optimization\n\n\n\nimport numpy as np\nfrom spotoptim import SpotOptim\nfrom spotoptim.surrogate import Kriging\n\ndef objective(X):\n    return np.sum(X**2, axis=1)\n\n# Example 6: Using SpotOptim's Kriging surrogate\nkriging_model = Kriging(\n    noise=1e-10,           # Regularization parameter\n    kernel='gauss',         # Gaussian/RBF kernel\n    min_theta=-3.0,         # Min log10(theta) bound\n    max_theta=2.0,          # Max log10(theta) bound\n    seed=42\n)\noptimizer_kriging = SpotOptim(\n    fun=objective,\n    bounds=[(-5, 5), (-5, 5)],\n    surrogate=kriging_model,\n    max_iter=15,\n    n_initial=5,\n    seed=42,\n    verbose=True\n)\nresult = optimizer_kriging.optimize()\nprint(\"Best solution found:\", result.x)\nprint(\"Best value:\", result.fun)\n\nTensorBoard logging disabled\nInitial best: f(x) = 2.510989\nIter 1 | Best: 2.510989 | Curr: 3.613946 | Rate: 0.00 | Evals: 40.0%\nIter 2 | Best: 0.757941 | Rate: 0.50 | Evals: 46.7%\nIter 3 | Best: 0.462652 | Rate: 0.67 | Evals: 53.3%\nIter 4 | Best: 0.048538 | Rate: 0.75 | Evals: 60.0%\nIter 5 | Best: 0.000429 | Rate: 0.80 | Evals: 66.7%\nIter 6 | Best: 0.000429 | Curr: 0.000477 | Rate: 0.67 | Evals: 73.3%\nIter 7 | Best: 0.000322 | Rate: 0.71 | Evals: 80.0%\nIter 8 | Best: 0.000118 | Rate: 0.75 | Evals: 86.7%\nIter 9 | Best: 0.000032 | Rate: 0.78 | Evals: 93.3%\nIter 10 | Best: 0.000009 | Rate: 0.80 | Evals: 100.0%\nBest solution found: [-0.00214389 -0.00207764]\nBest value: 8.912866053316175e-06\n\n\n\nimport numpy as np\nfrom spotoptim import SpotOptim\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\n\ndef objective(X):\n    return np.sum(X**2, axis=1)\n\n# Example 7: Using sklearn Gaussian Process with custom kernel\n# Custom kernel: constant * RBF + white noise\ncustom_kernel = ConstantKernel(1.0, (1e-2, 1e2)) * RBF(\n    length_scale=1.0, length_scale_bounds=(1e-1, 10.0)\n) + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-10, 1e-1))\n\ngp_custom = GaussianProcessRegressor(\n    kernel=custom_kernel,\n    n_restarts_optimizer=15,\n    normalize_y=True,\n    random_state=42\n)\n\noptimizer_custom_gp = SpotOptim(\n    fun=objective,\n    bounds=[(-5, 5), (-5, 5)],\n    surrogate=gp_custom,\n    max_iter=15,\n    n_initial=5,\n    seed=42\n)\nresult = optimizer_custom_gp.optimize()\n\n\nimport numpy as np\nfrom spotoptim import SpotOptim\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef objective(X):\n    return np.sum(X**2, axis=1)\n\n# Example 8: Using Random Forest as surrogate\nrf_model = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=10,\n    random_state=42\n)\n\noptimizer_rf = SpotOptim(\n    fun=objective,\n    bounds=[(-5, 5), (-5, 5)],\n    surrogate=rf_model,\n    max_iter=15,\n    n_initial=5,\n    seed=42\n)\nresult = optimizer_rf.optimize()\n\n# Note: Random Forests don't provide uncertainty estimates,\n# so Expected Improvement (EI) may be less effective.\n# Consider using acquisition='y' for pure exploitation.\n\n\nimport numpy as np\nfrom spotoptim import SpotOptim\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, RationalQuadratic, ConstantKernel, RBF\n\ndef objective(X):\n    return np.sum(X**2, axis=1)\n\n# Example 9: Comparing different kernels for Gaussian Process\n# Matern kernel with nu=1.5 (once differentiable)\nkernel_matern15 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=1.5)\ngp_matern15 = GaussianProcessRegressor(kernel=kernel_matern15, normalize_y=True)\n\n# Matern kernel with nu=2.5 (twice differentiable, DEFAULT)\nkernel_matern25 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)\ngp_matern25 = GaussianProcessRegressor(kernel=kernel_matern25, normalize_y=True)\n\n# RBF kernel (infinitely differentiable, smooth)\nkernel_rbf = ConstantKernel(1.0) * RBF(length_scale=1.0)\ngp_rbf = GaussianProcessRegressor(kernel=kernel_rbf, normalize_y=True)\n\n# Rational Quadratic kernel (mixture of RBF kernels)\nkernel_rq = ConstantKernel(1.0) * RationalQuadratic(length_scale=1.0, alpha=1.0)\ngp_rq = GaussianProcessRegressor(kernel=kernel_rq, normalize_y=True)\n\n# Use any of these as surrogate\noptimizer_rbf = SpotOptim(fun=objective, bounds=[(-5, 5), (-5, 5)],\n                          surrogate=gp_rbf, max_iter=15, n_initial=5)\nresult = optimizer_rbf.optimize()",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotOptim"
    ]
  },
  {
    "objectID": "docs/reference/SpotOptim.SpotOptim.html#methods",
    "href": "docs/reference/SpotOptim.SpotOptim.html#methods",
    "title": "SpotOptim.SpotOptim",
    "section": "Methods",
    "text": "Methods\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndetect_var_type\nAuto-detect variable types based on factor mappings.\n\n\ngen_design_table\nGenerate a table of the design or results.\n\n\nget_best_hyperparameters\nGet the best hyperparameter configuration found during optimization.\n\n\nget_design_table\nGet a table string showing the search space design before optimization.\n\n\nget_importance\nCalculate variable importance scores.\n\n\nget_initial_design\nGenerate or process initial design points.\n\n\nget_results_table\nGet a comprehensive table string of optimization results.\n\n\nget_stars\nConverts a list of values to a list of stars.\n\n\nhandle_default_var_trans\nHandle default variable transformations.\n\n\ninverse_transform_value\nApply inverse transformation to a single float value.\n\n\nload_experiment\nLoad an experiment configuration from a pickle file.\n\n\nload_result\nLoad complete optimization results from a pickle file.\n\n\nmodify_bounds_based_on_var_type\nModify bounds based on variable types.\n\n\noptimize\nRun the optimization process.\n\n\noptimize_acquisition_func\nOptimize the acquisition function to find the next point to evaluate.\n\n\nplot_importance\nPlot variable importance.\n\n\nplot_important_hyperparameter_contour\nPlot surrogate contours using spotoptim.plot.visualization.plot_important_hyperparameter_contour.\n\n\nplot_parameter_scatter\nPlot parameter distributions showing relationship between each parameter and objective.\n\n\nplot_progress\nPlot optimization progress using spotoptim.plot.visualization.plot_progress.\n\n\nplot_surrogate\nPlot the surrogate model for two dimensions.\n\n\nprint_best\nPrint the best solution found during optimization.\n\n\nprint_design_table\nPrint (and return) a table showing the search space design before optimization.\n\n\nprint_results\nAlias for print_results_table for compatibility.\n\n\nprint_results_table\nPrint (and return) a comprehensive table of optimization results.\n\n\nprocess_factor_bounds\nProcess bounds to handle factor variables.\n\n\nsave_experiment\nSave the experiment configuration to a pickle file.\n\n\nsave_result\nSave the complete optimization results to a pickle file.\n\n\nselect_new\nSelect rows from A that are not in X.\n\n\nsensitivity_spearman\nCompute and print Spearman correlation between parameters and objective values.\n\n\nsuggest_next_infill_point\nSuggest next point to evaluate (dispatcher).\n\n\nto_all_dim\nExpand reduced-dimensional points to full-dimensional representation.\n\n\nto_red_dim\nReduce full-dimensional points to optimization space.\n\n\ntransform_bounds\nTransform bounds from original to internal scale.\n\n\ntransform_value\nApply transformation to a single float value.\n\n\nupdate_stats\nUpdate optimization statistics.",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotOptim"
    ]
  },
  {
    "objectID": "docs/reference/SpotOptim.SpotOptimState.html",
    "href": "docs/reference/SpotOptim.SpotOptimState.html",
    "title": "SpotOptim.SpotOptimState",
    "section": "",
    "text": "SpotOptim.SpotOptimState\nSpotOptim.SpotOptimState(\n    X_=None,\n    y_=None,\n    y_mo=None,\n    best_x_=None,\n    best_y_=None,\n    n_iter_=0,\n    counter=0,\n    success_rate=0.0,\n    success_counter=0,\n    _success_history=list(),\n    _zero_success_count=0,\n    mean_X=None,\n    mean_y=None,\n    var_y=None,\n    min_mean_X=None,\n    min_mean_y=None,\n    min_var_y=None,\n    min_X=None,\n    min_y=None,\n    restarts_results_=list(),\n)\nMutable state of the optimization process.",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotOptimState"
    ]
  },
  {
    "objectID": "docs/reference/core.data.SpotDataFromArray.html",
    "href": "docs/reference/core.data.SpotDataFromArray.html",
    "title": "core.data.SpotDataFromArray",
    "section": "",
    "text": "core.data.SpotDataFromArray\ncore.data.SpotDataFromArray(\n    x_train,\n    y_train,\n    x_val=None,\n    y_val=None,\n    x_test=None,\n    y_test=None,\n    target_column=None,\n)\nData handler for numpy arrays or torch tensors.",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotDataFromArray"
    ]
  },
  {
    "objectID": "docs/reference/core.data.SpotDataSet.html",
    "href": "docs/reference/core.data.SpotDataSet.html",
    "title": "core.data.SpotDataSet",
    "section": "",
    "text": "core.data.SpotDataSet(input_dim, output_dim, target_column=None)\nAbstract base class for data handling in SpotOptim.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninput_dim\nint\nNumber of input features.\n\n\noutput_dim\nint\nNumber of output features.\n\n\ntarget_column\nstr\nName of the target column if applicable.\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_test_data\nReturns the test data.\n\n\nget_train_data\nReturns the training data.\n\n\nget_validation_data\nReturns the validation data.",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotDataSet"
    ]
  },
  {
    "objectID": "docs/reference/core.data.SpotDataSet.html#attributes",
    "href": "docs/reference/core.data.SpotDataSet.html#attributes",
    "title": "core.data.SpotDataSet",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ninput_dim\nint\nNumber of input features.\n\n\noutput_dim\nint\nNumber of output features.\n\n\ntarget_column\nstr\nName of the target column if applicable.",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotDataSet"
    ]
  },
  {
    "objectID": "docs/reference/core.data.SpotDataSet.html#methods",
    "href": "docs/reference/core.data.SpotDataSet.html#methods",
    "title": "core.data.SpotDataSet",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_test_data\nReturns the test data.\n\n\nget_train_data\nReturns the training data.\n\n\nget_validation_data\nReturns the validation data.",
    "crumbs": [
      "API Reference",
      "Core",
      "SpotDataSet"
    ]
  },
  {
    "objectID": "docs/reference/core.experiment.ExperimentControl.html",
    "href": "docs/reference/core.experiment.ExperimentControl.html",
    "title": "core.experiment.ExperimentControl",
    "section": "",
    "text": "core.experiment.ExperimentControl(\n    dataset,\n    model_class,\n    hyperparameters,\n    seed=123,\n    device='cpu',\n    num_workers=0,\n    epochs=None,\n    batch_size=32,\n    optimizer_class=None,\n    loss_function=None,\n    metrics=list(),\n    n_initial=10,\n    max_evals=50,\n    experiment_name='default_experiment',\n    verbosity=0,\n)\nControls the experiment configuration, replacing the legacy fun_control dictionary.\nThis class serves as the central configuration object for optimization experiments, holding the dataset, model configuration, hyperparameters, and other settings.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndataset\nSpotDataSet\nThe dataset object containing the data.\n\n\nmodel_class\nAny\nThe class of the model to be instantiated.\n\n\nhyperparameters\nAny\nThe hyperparameters for the model.\n\n\nseed\nint\nThe random seed for reproducibility.\n\n\ndevice\nstr\nThe device to run the model on (e.g. “cpu” or “cuda”).\n\n\nnum_workers\nint\nThe number of workers to use for data loading.\n\n\nepochs\nOptional[int]\nThe number of epochs to train the model.\n\n\nbatch_size\nint\nThe batch size for training.\n\n\noptimizer_class\nOptional[Any]\nThe optimizer class to use for training.\n\n\nloss_function\nOptional[Any]\nThe loss function to use for training.\n\n\nmetrics\nList[str]\nThe metrics to track during training.\n\n\nn_initial\nint\nThe number of initial design points.\n\n\nmax_evals\nint\nThe maximum number of evaluations.\n\n\nexperiment_name\nstr\nThe name of the experiment.\n\n\nverbosity\nint\nThe verbosity level.\n\n\n\n\n\n\nto_dict(): Convert the object to a dictionary. torch_device(): Return the torch device object.\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n&gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n&gt;&gt;&gt; from spotoptim.nn.mlp import MLP\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 1. Prepare Data\n&gt;&gt;&gt; X = np.array([[0.1, 0.2], [0.3, 0.4]])\n&gt;&gt;&gt; y = np.array([[1.0], [2.0]])\n&gt;&gt;&gt; dataset = SpotDataFromArray(x_train=X, y_train=y)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 2. Define Hyperparameters\n&gt;&gt;&gt; params = {\"l1\": 16, \"num_hidden_layers\": 1, \"lr\": 1e-3}\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 3. Initialize Control with Real Model\n&gt;&gt;&gt; exp = ExperimentControl(\n...     dataset=dataset,\n...     model_class=MLP,\n...     hyperparameters=params,\n...     experiment_name=\"real_model_run\",\n...     seed=42\n... )\n&gt;&gt;&gt; print(exp.experiment_name)\nreal_model_run\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nto_dict\nConvert to dictionary for partial backward compatibility or logging.",
    "crumbs": [
      "API Reference",
      "Core",
      "ExperimentControl"
    ]
  },
  {
    "objectID": "docs/reference/core.experiment.ExperimentControl.html#attributes",
    "href": "docs/reference/core.experiment.ExperimentControl.html#attributes",
    "title": "core.experiment.ExperimentControl",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ndataset\nSpotDataSet\nThe dataset object containing the data.\n\n\nmodel_class\nAny\nThe class of the model to be instantiated.\n\n\nhyperparameters\nAny\nThe hyperparameters for the model.\n\n\nseed\nint\nThe random seed for reproducibility.\n\n\ndevice\nstr\nThe device to run the model on (e.g. “cpu” or “cuda”).\n\n\nnum_workers\nint\nThe number of workers to use for data loading.\n\n\nepochs\nOptional[int]\nThe number of epochs to train the model.\n\n\nbatch_size\nint\nThe batch size for training.\n\n\noptimizer_class\nOptional[Any]\nThe optimizer class to use for training.\n\n\nloss_function\nOptional[Any]\nThe loss function to use for training.\n\n\nmetrics\nList[str]\nThe metrics to track during training.\n\n\nn_initial\nint\nThe number of initial design points.\n\n\nmax_evals\nint\nThe maximum number of evaluations.\n\n\nexperiment_name\nstr\nThe name of the experiment.\n\n\nverbosity\nint\nThe verbosity level.",
    "crumbs": [
      "API Reference",
      "Core",
      "ExperimentControl"
    ]
  },
  {
    "objectID": "docs/reference/core.experiment.ExperimentControl.html#public-methods",
    "href": "docs/reference/core.experiment.ExperimentControl.html#public-methods",
    "title": "core.experiment.ExperimentControl",
    "section": "",
    "text": "to_dict(): Convert the object to a dictionary. torch_device(): Return the torch device object.",
    "crumbs": [
      "API Reference",
      "Core",
      "ExperimentControl"
    ]
  },
  {
    "objectID": "docs/reference/core.experiment.ExperimentControl.html#examples",
    "href": "docs/reference/core.experiment.ExperimentControl.html#examples",
    "title": "core.experiment.ExperimentControl",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n&gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n&gt;&gt;&gt; from spotoptim.nn.mlp import MLP\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 1. Prepare Data\n&gt;&gt;&gt; X = np.array([[0.1, 0.2], [0.3, 0.4]])\n&gt;&gt;&gt; y = np.array([[1.0], [2.0]])\n&gt;&gt;&gt; dataset = SpotDataFromArray(x_train=X, y_train=y)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 2. Define Hyperparameters\n&gt;&gt;&gt; params = {\"l1\": 16, \"num_hidden_layers\": 1, \"lr\": 1e-3}\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 3. Initialize Control with Real Model\n&gt;&gt;&gt; exp = ExperimentControl(\n...     dataset=dataset,\n...     model_class=MLP,\n...     hyperparameters=params,\n...     experiment_name=\"real_model_run\",\n...     seed=42\n... )\n&gt;&gt;&gt; print(exp.experiment_name)\nreal_model_run",
    "crumbs": [
      "API Reference",
      "Core",
      "ExperimentControl"
    ]
  },
  {
    "objectID": "docs/reference/core.experiment.ExperimentControl.html#methods",
    "href": "docs/reference/core.experiment.ExperimentControl.html#methods",
    "title": "core.experiment.ExperimentControl",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nto_dict\nConvert to dictionary for partial backward compatibility or logging.",
    "crumbs": [
      "API Reference",
      "Core",
      "ExperimentControl"
    ]
  },
  {
    "objectID": "docs/reference/core.html",
    "href": "docs/reference/core.html",
    "title": "core",
    "section": "",
    "text": "core\ncore",
    "crumbs": [
      "API Reference",
      "Core",
      "core"
    ]
  },
  {
    "objectID": "docs/reference/data.base.FileConfig.html",
    "href": "docs/reference/data.base.FileConfig.html",
    "title": "data.base.FileConfig",
    "section": "",
    "text": "data.base.FileConfig(filename, directory=None, **desc)\nBase class for configurations that are stored in a local file.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nThe file’s name.\nrequired\n\n\ndirectory\nOptional[str]\nThe directory where the file is contained. Defaults to the location of the datasets module.\nNone\n\n\ndesc\ndict\nExtra config parameters to pass as keyword arguments.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\npath\nThe path to the configuration file.",
    "crumbs": [
      "API Reference",
      "Data",
      "FileConfig"
    ]
  },
  {
    "objectID": "docs/reference/data.base.FileConfig.html#parameters",
    "href": "docs/reference/data.base.FileConfig.html#parameters",
    "title": "data.base.FileConfig",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nThe file’s name.\nrequired\n\n\ndirectory\nOptional[str]\nThe directory where the file is contained. Defaults to the location of the datasets module.\nNone\n\n\ndesc\ndict\nExtra config parameters to pass as keyword arguments.\n{}",
    "crumbs": [
      "API Reference",
      "Data",
      "FileConfig"
    ]
  },
  {
    "objectID": "docs/reference/data.base.FileConfig.html#attributes",
    "href": "docs/reference/data.base.FileConfig.html#attributes",
    "title": "data.base.FileConfig",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npath\nThe path to the configuration file.",
    "crumbs": [
      "API Reference",
      "Data",
      "FileConfig"
    ]
  },
  {
    "objectID": "docs/reference/data.diabetes.DiabetesDataset.html",
    "href": "docs/reference/data.diabetes.DiabetesDataset.html",
    "title": "data.diabetes.DiabetesDataset",
    "section": "",
    "text": "data.diabetes.DiabetesDataset\ndata.diabetes.DiabetesDataset(\n    X=None,\n    y=None,\n    transform=None,\n    target_transform=None,\n)\nDiabetes dataset wrapping sklearn’s diabetes dataset or custom data.",
    "crumbs": [
      "API Reference",
      "Data",
      "DiabetesDataset"
    ]
  },
  {
    "objectID": "docs/reference/data.diabetes.html",
    "href": "docs/reference/data.diabetes.html",
    "title": "data.diabetes",
    "section": "",
    "text": "data.diabetes\n\n\n\n\n\nName\nDescription\n\n\n\n\nDiabetesDataset\nDiabetes dataset wrapping sklearn’s diabetes dataset or custom data.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_diabetes_dataloaders\nReturns train and test dataloaders for the Diabetes dataset.",
    "crumbs": [
      "API Reference",
      "Data",
      "data.diabetes"
    ]
  },
  {
    "objectID": "docs/reference/data.diabetes.html#classes",
    "href": "docs/reference/data.diabetes.html#classes",
    "title": "data.diabetes",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nDiabetesDataset\nDiabetes dataset wrapping sklearn’s diabetes dataset or custom data.",
    "crumbs": [
      "API Reference",
      "Data",
      "data.diabetes"
    ]
  },
  {
    "objectID": "docs/reference/data.diabetes.html#functions",
    "href": "docs/reference/data.diabetes.html#functions",
    "title": "data.diabetes",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_diabetes_dataloaders\nReturns train and test dataloaders for the Diabetes dataset.",
    "crumbs": [
      "API Reference",
      "Data",
      "data.diabetes"
    ]
  },
  {
    "objectID": "docs/reference/eda.plots.plot_ip_boxplots.html",
    "href": "docs/reference/eda.plots.plot_ip_boxplots.html",
    "title": "eda.plots.plot_ip_boxplots",
    "section": "",
    "text": "eda.plots.plot_ip_boxplots(\n    df,\n    category_column_name=None,\n    num_cols=2,\n    figwidth=10,\n    box_width=0.2,\n    both_names=True,\n    height_per_subplot=2.0,\n    add_points=None,\n    add_points_col=['red'],\n)\nGenerate infill-point boxplots (ip-boxplots). A separate ip-boxplot is generated for each numerical column in a DataFrame, arranged in a grid. Each subplot has its own scale, similar to how histograms are shown in plot_histograms(). Additional points can be added and highlighted in red.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nDataFrame containing the data to plot.\nrequired\n\n\ncategory_column_name\nstr\nColumn name for categorical grouping. Defaults to None.\nNone\n\n\nnum_cols\nint\nNumber of columns in the subplot grid. Defaults to 2.\n2\n\n\nfigwidth\nint\nWidth of the entire figure. Defaults to 10.\n10\n\n\nbox_width\nfloat\nWidth of the boxplots. Defaults to 0.2.\n0.2\n\n\nboth_names\nbool\nWhether to show both variable names and categories in titles. Defaults to True.\nTrue\n\n\nheight_per_subplot\nfloat\nHeight per subplot row. Defaults to 2.0.\n2.0\n\n\nadd_points\npd.DataFrame\nDataFrame containing additional points to highlight. Defaults to None.\nNone\n\n\nadd_points_col\nlist\nList of colors for the additional points. Defaults to [“red”].\n['red']\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotoptim.eda.plots import plot_ip_boxplots\n&gt;&gt;&gt; data = {'A': [1, 2, 2, 3, 4, 5, 100], 'B': [10, 10, 10, 10, 10, 10, 10]}\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; plot_ip_boxplots(df, num_cols=1)\n&gt;&gt;&gt; # Example with multiple added points and colors\n&gt;&gt;&gt; add_points = pd.DataFrame({'A': [1.5, 3.5], 'B': [10, 10]})\n&gt;&gt;&gt; plot_ip_boxplots(df, add_points=add_points, add_points_col=[\"red\", \"blue\"])",
    "crumbs": [
      "API Reference",
      "Exploratory Data Analysis",
      "plot_ip_boxplots"
    ]
  },
  {
    "objectID": "docs/reference/eda.plots.plot_ip_boxplots.html#parameters",
    "href": "docs/reference/eda.plots.plot_ip_boxplots.html#parameters",
    "title": "eda.plots.plot_ip_boxplots",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nDataFrame containing the data to plot.\nrequired\n\n\ncategory_column_name\nstr\nColumn name for categorical grouping. Defaults to None.\nNone\n\n\nnum_cols\nint\nNumber of columns in the subplot grid. Defaults to 2.\n2\n\n\nfigwidth\nint\nWidth of the entire figure. Defaults to 10.\n10\n\n\nbox_width\nfloat\nWidth of the boxplots. Defaults to 0.2.\n0.2\n\n\nboth_names\nbool\nWhether to show both variable names and categories in titles. Defaults to True.\nTrue\n\n\nheight_per_subplot\nfloat\nHeight per subplot row. Defaults to 2.0.\n2.0\n\n\nadd_points\npd.DataFrame\nDataFrame containing additional points to highlight. Defaults to None.\nNone\n\n\nadd_points_col\nlist\nList of colors for the additional points. Defaults to [“red”].\n['red']",
    "crumbs": [
      "API Reference",
      "Exploratory Data Analysis",
      "plot_ip_boxplots"
    ]
  },
  {
    "objectID": "docs/reference/eda.plots.plot_ip_boxplots.html#returns",
    "href": "docs/reference/eda.plots.plot_ip_boxplots.html#returns",
    "title": "eda.plots.plot_ip_boxplots",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "API Reference",
      "Exploratory Data Analysis",
      "plot_ip_boxplots"
    ]
  },
  {
    "objectID": "docs/reference/eda.plots.plot_ip_boxplots.html#examples",
    "href": "docs/reference/eda.plots.plot_ip_boxplots.html#examples",
    "title": "eda.plots.plot_ip_boxplots",
    "section": "",
    "text": "&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotoptim.eda.plots import plot_ip_boxplots\n&gt;&gt;&gt; data = {'A': [1, 2, 2, 3, 4, 5, 100], 'B': [10, 10, 10, 10, 10, 10, 10]}\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; plot_ip_boxplots(df, num_cols=1)\n&gt;&gt;&gt; # Example with multiple added points and colors\n&gt;&gt;&gt; add_points = pd.DataFrame({'A': [1.5, 3.5], 'B': [10, 10]})\n&gt;&gt;&gt; plot_ip_boxplots(df, add_points=add_points, add_points_col=[\"red\", \"blue\"])",
    "crumbs": [
      "API Reference",
      "Exploratory Data Analysis",
      "plot_ip_boxplots"
    ]
  },
  {
    "objectID": "docs/reference/eda.plots.html",
    "href": "docs/reference/eda.plots.html",
    "title": "eda.plots",
    "section": "",
    "text": "eda.plots\n\n\n\n\n\nName\nDescription\n\n\n\n\nplot_ip_boxplots\nGenerate infill-point boxplots (ip-boxplots). A separate ip-boxplot is generated for each numerical column in a DataFrame, arranged in a grid.\n\n\nplot_ip_histograms\nGenerate infill-point histograms (ip-histograms) for each numerical column in the DataFrame within a single figure.",
    "crumbs": [
      "API Reference",
      "Exploratory Data Analysis",
      "eda.plots"
    ]
  },
  {
    "objectID": "docs/reference/eda.plots.html#functions",
    "href": "docs/reference/eda.plots.html#functions",
    "title": "eda.plots",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nplot_ip_boxplots\nGenerate infill-point boxplots (ip-boxplots). A separate ip-boxplot is generated for each numerical column in a DataFrame, arranged in a grid.\n\n\nplot_ip_histograms\nGenerate infill-point histograms (ip-histograms) for each numerical column in the DataFrame within a single figure.",
    "crumbs": [
      "API Reference",
      "Exploratory Data Analysis",
      "eda.plots"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.html",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.html",
    "title": "factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer",
    "section": "",
    "text": "factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer(\n    specification=None,\n    n_obs=None,\n    is_cov_matrix=False,\n    bounds=None,\n    max_iter=200,\n    tol=None,\n    impute='median',\n    disp=True,\n)\nFit a confirmatory factor analysis model using maximum likelihood.\n\n\nspecification : :class:ModelSpecification or None, optional A model specification. This must be a :class:ModelSpecification object or None. If None, a :class:ModelSpecification object will be generated assuming that n_factors == n_variables, and that all variables load on all factors. Note that this could mean the factor model is not identified, and the optimization could fail. Defaults to None. n_obs : int or None, optional The number of observations in the original data set. If this is not passed and is_cov_matrix is True, then an error will be raised. Defaults to None. is_cov_matrix : bool, optional Whether the input X is a covariance matrix. If False, assume it is the full data set. Defaults to False. bounds : list of tuples or None, optional A list of minimum and maximum boundaries for each element of the input array. This must equal x0, which is the input array from your parsed and combined model specification.\nThe length is:\n((n_factors * n_variables) + n_variables + n_factors +\n(((n_factors * n_factors) - n_factors) // 2)\n\nIf `None`, nothing will be bounded.\nDefaults to ``None``.\nmax_iter : int, optional The maximum number of iterations for the optimization routine. Defaults to 200. tol : float or None, optional The tolerance for convergence. Defaults to None. disp : bool, optional Whether to print the scipy optimization fmin message to standard output. Defaults to True.\n\n\n\nValueError If is_cov_matrix is True, and n_obs is not provided.\n\n\n\nmodel : ModelSpecification The model specification object. loadings_ : :obj:numpy.ndarray The factor loadings matrix. None, if `fit()``` has not been called. error_vars_ : :obj:numpy.ndarrayThe error variance matrix factor_varcovs_ : :obj:numpy.ndarray` The factor covariance matrix. log_likelihood_ : float The log likelihood from the optimization routine. aic_ : float The Akaike information criterion. bic_ : float The Bayesian information criterion.\n\n\n\n\n\n\nimport pandas as pd from factor_analyzer import (ConfirmatoryFactorAnalyzer, … ModelSpecificationParser) X = pd.read_csv(‘tests/data/test11.csv’) model_dict = {“F1”: [“V1”, “V2”, “V3”, “V4”], … “F2”: [“V5”, “V6”, “V7”, “V8”]} model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict) cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False) cfa.fit(X.values) cfa.loadings_ array([[0.99131285, 0. ], [0.46074919, 0. ], [0.3502267 , 0. ], [0.58331488, 0. ], [0. , 0.98621042], [0. , 0.73389239], [0. , 0.37602988], [0. , 0.50049507]]) cfa.factor_varcovs_ array([[1. , 0.17385704], [0.17385704, 1. ]]) cfa.get_standard_errors() (array([[0.06779949, 0. ], [0.04369956, 0. ], [0.04153113, 0. ], [0.04766645, 0. ], [0. , 0.06025341], [0. , 0.04913149], [0. , 0.0406604 ], [0. , 0.04351208]]), array([0.11929873, 0.05043616, 0.04645803, 0.05803088, 0.10176889, 0.06607524, 0.04742321, 0.05373646])) cfa.transform(X.values) array([[-0.46852166, -1.08708035], [ 2.59025301, 1.20227783], [-0.47215977, 2.65697245], …, [-1.5930886 , -0.91804114], [ 0.19430887, 0.88174818], [-0.27863554, -0.7695101 ]])\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfit\nPerform confirmatory factor analysis.\n\n\nget_model_implied_cov\nGet the model-implied covariance matrix (sigma) for an estimated model.\n\n\nget_standard_errors\nGet standard errors from the implied covariance matrix and implied means.\n\n\ntransform\nGet the factor scores for a new data set.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "ConfirmatoryFactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.html#parameters",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.html#parameters",
    "title": "factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer",
    "section": "",
    "text": "specification : :class:ModelSpecification or None, optional A model specification. This must be a :class:ModelSpecification object or None. If None, a :class:ModelSpecification object will be generated assuming that n_factors == n_variables, and that all variables load on all factors. Note that this could mean the factor model is not identified, and the optimization could fail. Defaults to None. n_obs : int or None, optional The number of observations in the original data set. If this is not passed and is_cov_matrix is True, then an error will be raised. Defaults to None. is_cov_matrix : bool, optional Whether the input X is a covariance matrix. If False, assume it is the full data set. Defaults to False. bounds : list of tuples or None, optional A list of minimum and maximum boundaries for each element of the input array. This must equal x0, which is the input array from your parsed and combined model specification.\nThe length is:\n((n_factors * n_variables) + n_variables + n_factors +\n(((n_factors * n_factors) - n_factors) // 2)\n\nIf `None`, nothing will be bounded.\nDefaults to ``None``.\nmax_iter : int, optional The maximum number of iterations for the optimization routine. Defaults to 200. tol : float or None, optional The tolerance for convergence. Defaults to None. disp : bool, optional Whether to print the scipy optimization fmin message to standard output. Defaults to True.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "ConfirmatoryFactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.html#raises",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.html#raises",
    "title": "factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer",
    "section": "",
    "text": "ValueError If is_cov_matrix is True, and n_obs is not provided.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "ConfirmatoryFactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.html#attributes",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.html#attributes",
    "title": "factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer",
    "section": "",
    "text": "model : ModelSpecification The model specification object. loadings_ : :obj:numpy.ndarray The factor loadings matrix. None, if `fit()``` has not been called. error_vars_ : :obj:numpy.ndarrayThe error variance matrix factor_varcovs_ : :obj:numpy.ndarray` The factor covariance matrix. log_likelihood_ : float The log likelihood from the optimization routine. aic_ : float The Akaike information criterion. bic_ : float The Bayesian information criterion.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "ConfirmatoryFactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.html#examples",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.html#examples",
    "title": "factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer",
    "section": "",
    "text": "import pandas as pd from factor_analyzer import (ConfirmatoryFactorAnalyzer, … ModelSpecificationParser) X = pd.read_csv(‘tests/data/test11.csv’) model_dict = {“F1”: [“V1”, “V2”, “V3”, “V4”], … “F2”: [“V5”, “V6”, “V7”, “V8”]} model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict) cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False) cfa.fit(X.values) cfa.loadings_ array([[0.99131285, 0. ], [0.46074919, 0. ], [0.3502267 , 0. ], [0.58331488, 0. ], [0. , 0.98621042], [0. , 0.73389239], [0. , 0.37602988], [0. , 0.50049507]]) cfa.factor_varcovs_ array([[1. , 0.17385704], [0.17385704, 1. ]]) cfa.get_standard_errors() (array([[0.06779949, 0. ], [0.04369956, 0. ], [0.04153113, 0. ], [0.04766645, 0. ], [0. , 0.06025341], [0. , 0.04913149], [0. , 0.0406604 ], [0. , 0.04351208]]), array([0.11929873, 0.05043616, 0.04645803, 0.05803088, 0.10176889, 0.06607524, 0.04742321, 0.05373646])) cfa.transform(X.values) array([[-0.46852166, -1.08708035], [ 2.59025301, 1.20227783], [-0.47215977, 2.65697245], …, [-1.5930886 , -0.91804114], [ 0.19430887, 0.88174818], [-0.27863554, -0.7695101 ]])",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "ConfirmatoryFactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.html#methods",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.html#methods",
    "title": "factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfit\nPerform confirmatory factor analysis.\n\n\nget_model_implied_cov\nGet the model-implied covariance matrix (sigma) for an estimated model.\n\n\nget_standard_errors\nGet standard errors from the implied covariance matrix and implied means.\n\n\ntransform\nGet the factor scores for a new data set.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "ConfirmatoryFactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ModelSpecificationParser.html",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ModelSpecificationParser.html",
    "title": "factor_analyzer.confirmatory_factor_analyzer.ModelSpecificationParser",
    "section": "",
    "text": "factor_analyzer.confirmatory_factor_analyzer.ModelSpecificationParser()\nGenerate the model specification for CFA.\nThis class includes two static methods to generate a :class:ModelSpecification object from either a dictionary or a numpy array.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nparse_model_specification_from_array\nGenerate the model specification from a numpy array.\n\n\nparse_model_specification_from_dict\nGenerate the model specification from a dictionary.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "ModelSpecificationParser"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ModelSpecificationParser.html#methods",
    "href": "docs/reference/factor_analyzer.confirmatory_factor_analyzer.ModelSpecificationParser.html#methods",
    "title": "factor_analyzer.confirmatory_factor_analyzer.ModelSpecificationParser",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nparse_model_specification_from_array\nGenerate the model specification from a numpy array.\n\n\nparse_model_specification_from_dict\nGenerate the model specification from a dictionary.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "ModelSpecificationParser"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html",
    "title": "factor_analyzer.factor_analyzer.FactorAnalyzer",
    "section": "",
    "text": "factor_analyzer.factor_analyzer.FactorAnalyzer(\n    n_factors=3,\n    rotation='promax',\n    method='minres',\n    use_smc=True,\n    is_corr_matrix=False,\n    bounds=(0.005, 1),\n    impute='median',\n    svd_method='randomized',\n    rotation_kwargs=None,\n)\nThe main exploratory factor analysis class.\n\n\n\nFits a factor analysis model using minres, maximum likelihood, or principal factor extraction and returns the loading matrix\nOptionally performs a rotation, with method including:\n\nvarimax (orthogonal rotation)\npromax (oblique rotation)\noblimin (oblique rotation)\noblimax (orthogonal rotation)\nquartimin (oblique rotation)\nquartimax (orthogonal rotation)\nequamax (orthogonal rotation)\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn_factors\nint\nThe number of factors to select. Defaults to 3.\n3\n\n\nrotation\nstr\nThe type of rotation to perform after fitting the factor analysis model. If set to None, no rotation will be performed, nor will any associated Kaiser normalization. Possible values include: (a) varimax (orthogonal rotation) (b) promax (oblique rotation) (c) oblimin (oblique rotation) (d) oblimax (orthogonal rotation) (e) quartimin (oblique rotation) (f) quartimax (orthogonal rotation) (g) equamax (orthogonal rotation) Defaults to ‘promax’.\n'promax'\n\n\nmethod\nstr\nThe fitting method to use, either ‘minres’, ‘ml’, or ‘principal’. Defaults to ‘minres’.\n'minres'\n\n\nuse_smc\nbool\nWhether to use squared multiple correlation as starting guesses for factor analysis. Defaults to True.\nTrue\n\n\nbounds\ntuple\nThe lower and upper bounds on the variables for “L-BFGS-B” optimization. Defaults to (0.005, 1).\n(0.005, 1)\n\n\nimpute\nstr\nHow to handle missing values, if any, in the data: (a) use list-wise deletion (‘drop’), or (b) impute the column median (‘median’), or impute the column mean (‘mean’). Defaults to ‘median’.\n'median'\n\n\nis_corr_matrix\nbool\nSet to True if the data is the correlation matrix. Defaults to False.\nFalse\n\n\nsvd_method\nstr\nThe SVD method to use when method is ‘principal’. If ‘lapack’, use standard SVD from scipy.linalg. If ‘randomized’, use faster randomized_svd function from scikit-learn. Defaults to ‘randomized’.\n'randomized'\n\n\nrotation_kwargs\ndict\nDictionary containing keyword arguments for the rotation method. Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nloadings_\nnumpy.ndarray\nThe factor loadings matrix. None, if fit() has not been called.\n\n\ncorr_\nnumpy.ndarray\nThe original correlation matrix. None, if fit() has not been called.\n\n\nrotation_matrix_\nnumpy.ndarray\nThe rotation matrix, if a rotation has been performed. None otherwise.\n\n\nstructure_\nnumpy.ndarray or None\nThe structure loading matrix. This only exists if rotation is ‘promax’.\n\n\nphi_\nnumpy.ndarray or None\nThe factor correlations matrix. This only exists if rotation is ‘oblique’.\n\n\n\n\n\n\nThis code was partly derived from the excellent R package psych.\n\n\n\n[1] https://github.com/cran/psych/blob/master/R/fa.R\n\n\n\n\n\n\nimport pandas as pd from factor_analyzer import FactorAnalyzer df_features = pd.read_csv(‘tests/data/test02.csv’) fa = FactorAnalyzer(rotation=None) fa.fit(df_features) FactorAnalyzer(bounds=(0.005, 1), impute=‘median’, is_corr_matrix=False, method=‘minres’, n_factors=3, rotation=None, rotation_kwargs={}, use_smc=True) fa.loadings_ array([[-0.12991218, 0.16398154, 0.73823498], [ 0.03899558, 0.04658425, 0.01150343], [ 0.34874135, 0.61452341, -0.07255667], [ 0.45318006, 0.71926681, -0.07546472], [ 0.36688794, 0.44377343, -0.01737067], [ 0.74141382, -0.15008235, 0.29977512], [ 0.741675 , -0.16123009, -0.20744495], [ 0.82910167, -0.20519428, 0.04930817], [ 0.76041819, -0.23768727, -0.1206858 ], [ 0.81533404, -0.12494695, 0.17639683]]) fa.get_communalities() array([0.588758 , 0.00382308, 0.50452402, 0.72841183, 0.33184336, 0.66208428, 0.61911036, 0.73194557, 0.64929612, 0.71149718])\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfit\nFit factor analysis model using either MINRES, ML, or principal factor analysis.\n\n\nget_communalities\nCalculate the communalities, given the factor loading matrix.\n\n\nget_eigenvalues\nCalculate the eigenvalues, given the factor correlation matrix.\n\n\nget_factor_variance\nCalculate factor variance information.\n\n\nget_uniquenesses\nCalculate the uniquenesses, given the factor loading matrix.\n\n\nsufficiency\nPerform the sufficiency test.\n\n\ntransform\nGet factor scores for a new data set.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "FactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#this-class",
    "href": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#this-class",
    "title": "factor_analyzer.factor_analyzer.FactorAnalyzer",
    "section": "",
    "text": "Fits a factor analysis model using minres, maximum likelihood, or principal factor extraction and returns the loading matrix\nOptionally performs a rotation, with method including:\n\nvarimax (orthogonal rotation)\npromax (oblique rotation)\noblimin (oblique rotation)\noblimax (orthogonal rotation)\nquartimin (oblique rotation)\nquartimax (orthogonal rotation)\nequamax (orthogonal rotation)",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "FactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#parameters",
    "title": "factor_analyzer.factor_analyzer.FactorAnalyzer",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nn_factors\nint\nThe number of factors to select. Defaults to 3.\n3\n\n\nrotation\nstr\nThe type of rotation to perform after fitting the factor analysis model. If set to None, no rotation will be performed, nor will any associated Kaiser normalization. Possible values include: (a) varimax (orthogonal rotation) (b) promax (oblique rotation) (c) oblimin (oblique rotation) (d) oblimax (orthogonal rotation) (e) quartimin (oblique rotation) (f) quartimax (orthogonal rotation) (g) equamax (orthogonal rotation) Defaults to ‘promax’.\n'promax'\n\n\nmethod\nstr\nThe fitting method to use, either ‘minres’, ‘ml’, or ‘principal’. Defaults to ‘minres’.\n'minres'\n\n\nuse_smc\nbool\nWhether to use squared multiple correlation as starting guesses for factor analysis. Defaults to True.\nTrue\n\n\nbounds\ntuple\nThe lower and upper bounds on the variables for “L-BFGS-B” optimization. Defaults to (0.005, 1).\n(0.005, 1)\n\n\nimpute\nstr\nHow to handle missing values, if any, in the data: (a) use list-wise deletion (‘drop’), or (b) impute the column median (‘median’), or impute the column mean (‘mean’). Defaults to ‘median’.\n'median'\n\n\nis_corr_matrix\nbool\nSet to True if the data is the correlation matrix. Defaults to False.\nFalse\n\n\nsvd_method\nstr\nThe SVD method to use when method is ‘principal’. If ‘lapack’, use standard SVD from scipy.linalg. If ‘randomized’, use faster randomized_svd function from scikit-learn. Defaults to ‘randomized’.\n'randomized'\n\n\nrotation_kwargs\ndict\nDictionary containing keyword arguments for the rotation method. Defaults to None.\nNone",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "FactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#attributes",
    "href": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#attributes",
    "title": "factor_analyzer.factor_analyzer.FactorAnalyzer",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nloadings_\nnumpy.ndarray\nThe factor loadings matrix. None, if fit() has not been called.\n\n\ncorr_\nnumpy.ndarray\nThe original correlation matrix. None, if fit() has not been called.\n\n\nrotation_matrix_\nnumpy.ndarray\nThe rotation matrix, if a rotation has been performed. None otherwise.\n\n\nstructure_\nnumpy.ndarray or None\nThe structure loading matrix. This only exists if rotation is ‘promax’.\n\n\nphi_\nnumpy.ndarray or None\nThe factor correlations matrix. This only exists if rotation is ‘oblique’.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "FactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#notes",
    "href": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#notes",
    "title": "factor_analyzer.factor_analyzer.FactorAnalyzer",
    "section": "",
    "text": "This code was partly derived from the excellent R package psych.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "FactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#references",
    "href": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#references",
    "title": "factor_analyzer.factor_analyzer.FactorAnalyzer",
    "section": "",
    "text": "[1] https://github.com/cran/psych/blob/master/R/fa.R",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "FactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#examples",
    "href": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#examples",
    "title": "factor_analyzer.factor_analyzer.FactorAnalyzer",
    "section": "",
    "text": "import pandas as pd from factor_analyzer import FactorAnalyzer df_features = pd.read_csv(‘tests/data/test02.csv’) fa = FactorAnalyzer(rotation=None) fa.fit(df_features) FactorAnalyzer(bounds=(0.005, 1), impute=‘median’, is_corr_matrix=False, method=‘minres’, n_factors=3, rotation=None, rotation_kwargs={}, use_smc=True) fa.loadings_ array([[-0.12991218, 0.16398154, 0.73823498], [ 0.03899558, 0.04658425, 0.01150343], [ 0.34874135, 0.61452341, -0.07255667], [ 0.45318006, 0.71926681, -0.07546472], [ 0.36688794, 0.44377343, -0.01737067], [ 0.74141382, -0.15008235, 0.29977512], [ 0.741675 , -0.16123009, -0.20744495], [ 0.82910167, -0.20519428, 0.04930817], [ 0.76041819, -0.23768727, -0.1206858 ], [ 0.81533404, -0.12494695, 0.17639683]]) fa.get_communalities() array([0.588758 , 0.00382308, 0.50452402, 0.72841183, 0.33184336, 0.66208428, 0.61911036, 0.73194557, 0.64929612, 0.71149718])",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "FactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#methods",
    "href": "docs/reference/factor_analyzer.factor_analyzer.FactorAnalyzer.html#methods",
    "title": "factor_analyzer.factor_analyzer.FactorAnalyzer",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfit\nFit factor analysis model using either MINRES, ML, or principal factor analysis.\n\n\nget_communalities\nCalculate the communalities, given the factor loading matrix.\n\n\nget_eigenvalues\nCalculate the eigenvalues, given the factor correlation matrix.\n\n\nget_factor_variance\nCalculate factor variance information.\n\n\nget_uniquenesses\nCalculate the uniquenesses, given the factor loading matrix.\n\n\nsufficiency\nPerform the sufficiency test.\n\n\ntransform\nGet factor scores for a new data set.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "FactorAnalyzer"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.calculate_kmo.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer.calculate_kmo.html",
    "title": "factor_analyzer.factor_analyzer.calculate_kmo",
    "section": "",
    "text": "factor_analyzer.factor_analyzer.calculate_kmo(x)\nCalculate the Kaiser-Meyer-Olkin criterion for items and overall.\nThis statistic represents the degree to which each observed variable is predicted, without error, by the other variables in the dataset. In general, a KMO &lt; 0.6 is considered inadequate.\n\n\nx : array-like The array from which to calculate KMOs.\n\n\n\nkmo_per_variable : :obj:numpy.ndarray The KMO score per item. kmo_total : float The overall KMO score.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "calculate_kmo"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.calculate_kmo.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer.calculate_kmo.html#parameters",
    "title": "factor_analyzer.factor_analyzer.calculate_kmo",
    "section": "",
    "text": "x : array-like The array from which to calculate KMOs.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "calculate_kmo"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer.calculate_kmo.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer.calculate_kmo.html#returns",
    "title": "factor_analyzer.factor_analyzer.calculate_kmo",
    "section": "",
    "text": "kmo_per_variable : :obj:numpy.ndarray The KMO score per item. kmo_total : float The overall KMO score.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "calculate_kmo"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html",
    "title": "factor_analyzer.factor_analyzer_rotator.Rotator",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_rotator.Rotator(\n    method='varimax',\n    normalize=True,\n    power=4,\n    kappa=0,\n    gamma=0,\n    delta=0.01,\n    max_iter=500,\n    tol=1e-05,\n)\nPerform rotations on an unrotated factor loading matrix.\nThe Rotator class takes an (unrotated) factor loading matrix and performs one of several rotations.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmethod\nstr\nThe factor rotation method. Options include: (a) varimax (orthogonal rotation) (b) promax (oblique rotation) (c) oblimin (oblique rotation) (d) oblimax (orthogonal rotation) (e) quartimin (oblique rotation) (f) quartimax (orthogonal rotation) (g) equamax (orthogonal rotation) (h) geomin_obl (oblique rotation) (i) geomin_ort (orthogonal rotation) Defaults to ‘varimax’.\n'varimax'\n\n\nnormalize\nbool\nWhether to perform Kaiser normalization and de-normalization prior to and following rotation. Used for ‘varimax’ and ‘promax’ rotations. If None, default for ‘promax’ is False, and default for ‘varimax’ is True. Defaults to None.\nTrue\n\n\npower\nint\nThe exponent to which to raise the promax loadings (minus 1). Numbers should generally range from 2 to 4. Defaults to 4.\n4\n\n\nkappa\nfloat\nThe kappa value for the ‘equamax’ objective. Ignored if the method is not ‘equamax’. Defaults to 0.\n0\n\n\ngamma\nint\nThe gamma level for the ‘oblimin’ objective. Ignored if the method is not ‘oblimin’. Defaults to 0.\n0\n\n\ndelta\nfloat\nThe delta level for ‘geomin’ objectives. Ignored if the method is not ’geomin_*’. Defaults to 0.01.\n0.01\n\n\nmax_iter\nint\nThe maximum number of iterations. Used for ‘varimax’ and ‘oblique’ rotations. Defaults to 1000.\n500\n\n\ntol\nfloat\nThe convergence threshold. Used for ‘varimax’ and ‘oblique’ rotations. Defaults to 1e-5.\n1e-05\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nloadings_\nnumpy.ndarray\nThe loadings matrix. Shape (n_features, n_factors).\n\n\nrotation_\nnumpy.ndarray\nThe rotation matrix. Shape (n_factors, n_factors).\n\n\nphi_\nnumpy.ndarray or None\nThe factor correlations matrix. This only exists if method is ‘oblique’.\n\n\n\n\n\n\nMost of the rotations in this class are ported from R’s GPARotation package.\n\n\n\n[1] https://cran.r-project.org/web/packages/GPArotation/index.html\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from factor_analyzer import FactorAnalyzer, Rotator\n&gt;&gt;&gt; df_features = pd.read_csv('test02.csv')\n&gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n&gt;&gt;&gt; fa.fit(df_features)\n&gt;&gt;&gt; rotator = Rotator()\n&gt;&gt;&gt; rotator.fit_transform(fa.loadings_)\narray([[-0.07693215,  0.04499572,  0.76211208],\n       [ 0.01842035,  0.05757874,  0.01297908],\n       [ 0.06067925,  0.70692662, -0.03311798],\n       [ 0.11314343,  0.84525117, -0.03407129],\n       [ 0.15307233,  0.5553474 , -0.00121802],\n       [ 0.77450832,  0.1474666 ,  0.20118338],\n       [ 0.7063001 ,  0.17229555, -0.30093981],\n       [ 0.83990851,  0.15058874, -0.06182469],\n       [ 0.76620579,  0.1045194 , -0.22649615],\n       [ 0.81372945,  0.20915845,  0.07479506]])\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfit\nCompute the factor rotation.\n\n\nfit_transform\nCompute the factor rotation, and return the new loading matrix.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "Rotator"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html#parameters",
    "title": "factor_analyzer.factor_analyzer_rotator.Rotator",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmethod\nstr\nThe factor rotation method. Options include: (a) varimax (orthogonal rotation) (b) promax (oblique rotation) (c) oblimin (oblique rotation) (d) oblimax (orthogonal rotation) (e) quartimin (oblique rotation) (f) quartimax (orthogonal rotation) (g) equamax (orthogonal rotation) (h) geomin_obl (oblique rotation) (i) geomin_ort (orthogonal rotation) Defaults to ‘varimax’.\n'varimax'\n\n\nnormalize\nbool\nWhether to perform Kaiser normalization and de-normalization prior to and following rotation. Used for ‘varimax’ and ‘promax’ rotations. If None, default for ‘promax’ is False, and default for ‘varimax’ is True. Defaults to None.\nTrue\n\n\npower\nint\nThe exponent to which to raise the promax loadings (minus 1). Numbers should generally range from 2 to 4. Defaults to 4.\n4\n\n\nkappa\nfloat\nThe kappa value for the ‘equamax’ objective. Ignored if the method is not ‘equamax’. Defaults to 0.\n0\n\n\ngamma\nint\nThe gamma level for the ‘oblimin’ objective. Ignored if the method is not ‘oblimin’. Defaults to 0.\n0\n\n\ndelta\nfloat\nThe delta level for ‘geomin’ objectives. Ignored if the method is not ’geomin_*’. Defaults to 0.01.\n0.01\n\n\nmax_iter\nint\nThe maximum number of iterations. Used for ‘varimax’ and ‘oblique’ rotations. Defaults to 1000.\n500\n\n\ntol\nfloat\nThe convergence threshold. Used for ‘varimax’ and ‘oblique’ rotations. Defaults to 1e-5.\n1e-05",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "Rotator"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html#attributes",
    "href": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html#attributes",
    "title": "factor_analyzer.factor_analyzer_rotator.Rotator",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nloadings_\nnumpy.ndarray\nThe loadings matrix. Shape (n_features, n_factors).\n\n\nrotation_\nnumpy.ndarray\nThe rotation matrix. Shape (n_factors, n_factors).\n\n\nphi_\nnumpy.ndarray or None\nThe factor correlations matrix. This only exists if method is ‘oblique’.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "Rotator"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html#notes",
    "href": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html#notes",
    "title": "factor_analyzer.factor_analyzer_rotator.Rotator",
    "section": "",
    "text": "Most of the rotations in this class are ported from R’s GPARotation package.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "Rotator"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html#references",
    "href": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html#references",
    "title": "factor_analyzer.factor_analyzer_rotator.Rotator",
    "section": "",
    "text": "[1] https://cran.r-project.org/web/packages/GPArotation/index.html",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "Rotator"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html#examples",
    "href": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html#examples",
    "title": "factor_analyzer.factor_analyzer_rotator.Rotator",
    "section": "",
    "text": "&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from factor_analyzer import FactorAnalyzer, Rotator\n&gt;&gt;&gt; df_features = pd.read_csv('test02.csv')\n&gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n&gt;&gt;&gt; fa.fit(df_features)\n&gt;&gt;&gt; rotator = Rotator()\n&gt;&gt;&gt; rotator.fit_transform(fa.loadings_)\narray([[-0.07693215,  0.04499572,  0.76211208],\n       [ 0.01842035,  0.05757874,  0.01297908],\n       [ 0.06067925,  0.70692662, -0.03311798],\n       [ 0.11314343,  0.84525117, -0.03407129],\n       [ 0.15307233,  0.5553474 , -0.00121802],\n       [ 0.77450832,  0.1474666 ,  0.20118338],\n       [ 0.7063001 ,  0.17229555, -0.30093981],\n       [ 0.83990851,  0.15058874, -0.06182469],\n       [ 0.76620579,  0.1045194 , -0.22649615],\n       [ 0.81372945,  0.20915845,  0.07479506]])",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "Rotator"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html#methods",
    "href": "docs/reference/factor_analyzer.factor_analyzer_rotator.Rotator.html#methods",
    "title": "factor_analyzer.factor_analyzer_rotator.Rotator",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfit\nCompute the factor rotation.\n\n\nfit_transform\nCompute the factor rotation, and return the new loading matrix.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "Rotator"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.apply_impute_nan.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.apply_impute_nan.html",
    "title": "factor_analyzer.factor_analyzer_utils.apply_impute_nan",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.apply_impute_nan(x, how='mean')\nApply a function to impute np.nan values with the mean or the median.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe 1-D array to impute.\nrequired\n\n\nhow\nstr\nWhether to impute the ‘mean’ or ‘median’. Defaults to ‘mean’.\n'mean'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nx\nnumpy.ndarray\nThe array, with the missing values imputed.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "apply_impute_nan"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.apply_impute_nan.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.apply_impute_nan.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.apply_impute_nan",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe 1-D array to impute.\nrequired\n\n\nhow\nstr\nWhether to impute the ‘mean’ or ‘median’. Defaults to ‘mean’.\n'mean'",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "apply_impute_nan"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.apply_impute_nan.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.apply_impute_nan.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.apply_impute_nan",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nx\nnumpy.ndarray\nThe array, with the missing values imputed.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "apply_impute_nan"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.corr.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.corr.html",
    "title": "factor_analyzer.factor_analyzer_utils.corr",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.corr(x)\nCalculate the correlation matrix.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nA 1-D or 2-D array containing multiple variables and observations. Each column of x represents a variable, and each row a single observation of all those variables.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nr\nnumpy.ndarray\nThe correlation matrix of the variables.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "corr"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.corr.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.corr.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.corr",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nA 1-D or 2-D array containing multiple variables and observations. Each column of x represents a variable, and each row a single observation of all those variables.\nrequired",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "corr"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.corr.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.corr.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.corr",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nr\nnumpy.ndarray\nThe correlation matrix of the variables.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "corr"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.covariance_to_correlation.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.covariance_to_correlation.html",
    "title": "factor_analyzer.factor_analyzer_utils.covariance_to_correlation",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.covariance_to_correlation(m)\nCompute cross-correlations from the given covariance matrix.\nThis is a port of R cov2cor() function.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nm\narray - like\nThe covariance matrix.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nretval\nnumpy.ndarray\nThe cross-correlation matrix.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the input matrix is not square.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "covariance_to_correlation"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.covariance_to_correlation.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.covariance_to_correlation.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.covariance_to_correlation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nm\narray - like\nThe covariance matrix.\nrequired",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "covariance_to_correlation"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.covariance_to_correlation.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.covariance_to_correlation.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.covariance_to_correlation",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nretval\nnumpy.ndarray\nThe cross-correlation matrix.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "covariance_to_correlation"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.covariance_to_correlation.html#raises",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.covariance_to_correlation.html#raises",
    "title": "factor_analyzer.factor_analyzer_utils.covariance_to_correlation",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf the input matrix is not square.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "covariance_to_correlation"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post.html",
    "title": "factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post(x)\nTransform given input symmetric matrix using pre-post duplication.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe input matrix.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nout\n:obj:numpy.ndarray\nThe transformed matrix.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nAssertionError\nIf x is not symmetric.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "duplication_matrix_pre_post"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe input matrix.\nrequired",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "duplication_matrix_pre_post"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nout\n:obj:numpy.ndarray\nThe transformed matrix.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "duplication_matrix_pre_post"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post.html#raises",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post.html#raises",
    "title": "factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nAssertionError\nIf x is not symmetric.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "duplication_matrix_pre_post"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.get_first_idxs_from_values.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.get_first_idxs_from_values.html",
    "title": "factor_analyzer.factor_analyzer_utils.get_first_idxs_from_values",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.get_first_idxs_from_values(\n    x,\n    eq=1,\n    use_columns=True,\n)\nGet the indexes for a given value.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe input matrix.\nrequired\n\n\neq\nstr or int\nThe given value to find. Defaults to 1.\n1\n\n\nuse_columns\nbool\nWhether to get the first indexes using the columns. If False, then use the rows instead. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\nTuple[List[int], List[int]]\n- row_idx (list): A list of row indexes. - col_idx (list): A list of column indexes.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "get_first_idxs_from_values"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.get_first_idxs_from_values.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.get_first_idxs_from_values.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.get_first_idxs_from_values",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nThe input matrix.\nrequired\n\n\neq\nstr or int\nThe given value to find. Defaults to 1.\n1\n\n\nuse_columns\nbool\nWhether to get the first indexes using the columns. If False, then use the rows instead. Defaults to True.\nTrue",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "get_first_idxs_from_values"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.get_first_idxs_from_values.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.get_first_idxs_from_values.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.get_first_idxs_from_values",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ntuple\nTuple[List[int], List[int]]\n- row_idx (list): A list of row indexes. - col_idx (list): A list of column indexes.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "get_first_idxs_from_values"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.get_symmetric_lower_idxs.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.get_symmetric_lower_idxs.html",
    "title": "factor_analyzer.factor_analyzer_utils.get_symmetric_lower_idxs",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.get_symmetric_lower_idxs(n=1, diag=True)\nGet the indices for the lower triangle of a symmetric matrix.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nThe dimension of the n x n symmetric matrix. Defaults to 1.\n1\n\n\ndiag\nbool\nWhether to include the diagonal.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nindices\n:obj:numpy.ndarray\nThe indices for the lower triangle.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "get_symmetric_lower_idxs"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.get_symmetric_lower_idxs.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.get_symmetric_lower_idxs.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.get_symmetric_lower_idxs",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nn\nint\nThe dimension of the n x n symmetric matrix. Defaults to 1.\n1\n\n\ndiag\nbool\nWhether to include the diagonal.\nTrue",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "get_symmetric_lower_idxs"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.get_symmetric_lower_idxs.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.get_symmetric_lower_idxs.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.get_symmetric_lower_idxs",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nindices\n:obj:numpy.ndarray\nThe indices for the lower triangle.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "get_symmetric_lower_idxs"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.impute_values.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.impute_values.html",
    "title": "factor_analyzer.factor_analyzer_utils.impute_values",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.impute_values(x, how='mean')\nImpute np.nan values with the mean or median, or drop the containing rows.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nAn array to impute.\nrequired\n\n\nhow\nstr\nWhether to impute the ‘mean’ or ‘median’. Defaults to ‘mean’.\n'mean'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nx\nnumpy.ndarray\nThe array, with the missing values imputed or with rows dropped.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "impute_values"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.impute_values.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.impute_values.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.impute_values",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\narray - like\nAn array to impute.\nrequired\n\n\nhow\nstr\nWhether to impute the ‘mean’ or ‘median’. Defaults to ‘mean’.\n'mean'",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "impute_values"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.impute_values.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.impute_values.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.impute_values",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nx\nnumpy.ndarray\nThe array, with the missing values imputed or with rows dropped.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "impute_values"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.merge_variance_covariance.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.merge_variance_covariance.html",
    "title": "factor_analyzer.factor_analyzer_utils.merge_variance_covariance",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.merge_variance_covariance(\n    variances,\n    covariances=None,\n)\nMerge variances and covariances into a single variance-covariance matrix.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariances\narray - like\nThe variances that will be used to fill the diagonal of the square matrix.\nrequired\n\n\ncovariances\narray - like or None\nThe flattened input matrix that will be used to fill the lower and upper diagonal of the square matrix. If None, then only the variances will be used. Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nvariance_covariance\n:obj:numpy.ndarray\nThe variance-covariance matrix.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "merge_variance_covariance"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.merge_variance_covariance.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.merge_variance_covariance.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.merge_variance_covariance",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nvariances\narray - like\nThe variances that will be used to fill the diagonal of the square matrix.\nrequired\n\n\ncovariances\narray - like or None\nThe flattened input matrix that will be used to fill the lower and upper diagonal of the square matrix. If None, then only the variances will be used. Defaults to None.\nNone",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "merge_variance_covariance"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.merge_variance_covariance.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.merge_variance_covariance.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.merge_variance_covariance",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nvariance_covariance\n:obj:numpy.ndarray\nThe variance-covariance matrix.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "merge_variance_covariance"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.html",
    "title": "factor_analyzer.factor_analyzer_utils",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils\nUtility functions, used primarily by the confirmatory factor analysis module.\nConfirmatory factor analysis using machine learning methods. Re-implementation of the factor-analyzer package.\nSee https://factor-analyzer.readthedocs.io/en/latest/introduction.html for more details.\nAuthors of the original implementation: * Jeremy Biggs (jeremy.m.biggs@gmail.com) * Nitin Madnani (nmadnani@ets.org) Organization: Educational Testing Service Date: 2022-09-05\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\n\n\n\n\nName\nDescription\n\n\n\n\napply_impute_nan\nApply a function to impute np.nan values with the mean or the median.\n\n\ncommutation_matrix\nCalculate the commutation matrix.\n\n\ncorr\nCalculate the correlation matrix.\n\n\ncov\nCalculate the covariance matrix.\n\n\ncovariance_to_correlation\nCompute cross-correlations from the given covariance matrix.\n\n\nduplication_matrix\nCalculate the duplication matrix.\n\n\nduplication_matrix_pre_post\nTransform given input symmetric matrix using pre-post duplication.\n\n\nfill_lower_diag\nFill the lower diagonal of a square matrix, given a 1-D input array.\n\n\nget_first_idxs_from_values\nGet the indexes for a given value.\n\n\nget_free_parameter_idxs\nGet the free parameter indices from the flattened matrix.\n\n\nget_symmetric_lower_idxs\nGet the indices for the lower triangle of a symmetric matrix.\n\n\nget_symmetric_upper_idxs\nGet the indices for the upper triangle of a symmetric matrix.\n\n\nimpute_values\nImpute np.nan values with the mean or median, or drop the containing rows.\n\n\ninv_chol\nCalculate matrix inverse using Cholesky decomposition.\n\n\nmerge_variance_covariance\nMerge variances and covariances into a single variance-covariance matrix.\n\n\npartial_correlations\nCompute partial correlations between variable pairs.\n\n\nsmc\nCalculate the squared multiple correlations.\n\n\nunique_elements\nGet first unique instance of every list element, while maintaining order.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "factor_analyzer.factor_analyzer_utils"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.html#functions",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.html#functions",
    "title": "factor_analyzer.factor_analyzer_utils",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\napply_impute_nan\nApply a function to impute np.nan values with the mean or the median.\n\n\ncommutation_matrix\nCalculate the commutation matrix.\n\n\ncorr\nCalculate the correlation matrix.\n\n\ncov\nCalculate the covariance matrix.\n\n\ncovariance_to_correlation\nCompute cross-correlations from the given covariance matrix.\n\n\nduplication_matrix\nCalculate the duplication matrix.\n\n\nduplication_matrix_pre_post\nTransform given input symmetric matrix using pre-post duplication.\n\n\nfill_lower_diag\nFill the lower diagonal of a square matrix, given a 1-D input array.\n\n\nget_first_idxs_from_values\nGet the indexes for a given value.\n\n\nget_free_parameter_idxs\nGet the free parameter indices from the flattened matrix.\n\n\nget_symmetric_lower_idxs\nGet the indices for the lower triangle of a symmetric matrix.\n\n\nget_symmetric_upper_idxs\nGet the indices for the upper triangle of a symmetric matrix.\n\n\nimpute_values\nImpute np.nan values with the mean or median, or drop the containing rows.\n\n\ninv_chol\nCalculate matrix inverse using Cholesky decomposition.\n\n\nmerge_variance_covariance\nMerge variances and covariances into a single variance-covariance matrix.\n\n\npartial_correlations\nCompute partial correlations between variable pairs.\n\n\nsmc\nCalculate the squared multiple correlations.\n\n\nunique_elements\nGet first unique instance of every list element, while maintaining order.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "factor_analyzer.factor_analyzer_utils"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.unique_elements.html",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.unique_elements.html",
    "title": "factor_analyzer.factor_analyzer_utils.unique_elements",
    "section": "",
    "text": "factor_analyzer.factor_analyzer_utils.unique_elements(seq)\nGet first unique instance of every list element, while maintaining order.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nseq\nlist - like\nThe list of elements.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nseq\nlist\nThe updated list of elements.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "unique_elements"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.unique_elements.html#parameters",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.unique_elements.html#parameters",
    "title": "factor_analyzer.factor_analyzer_utils.unique_elements",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nseq\nlist - like\nThe list of elements.\nrequired",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "unique_elements"
    ]
  },
  {
    "objectID": "docs/reference/factor_analyzer.factor_analyzer_utils.unique_elements.html#returns",
    "href": "docs/reference/factor_analyzer.factor_analyzer_utils.unique_elements.html#returns",
    "title": "factor_analyzer.factor_analyzer_utils.unique_elements",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nseq\nlist\nThe updated list of elements.",
    "crumbs": [
      "API Reference",
      "Factor Analyzer",
      "unique_elements"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.aerofoilcd.html",
    "href": "docs/reference/function.forr08a.aerofoilcd.html",
    "title": "function.forr08a.aerofoilcd",
    "section": "",
    "text": "function.forr08a.aerofoilcd(X)\nComputes the drag coefficient (cd) of an aerofoil based on the shape parameter X.\nThis function reads the drag coefficient data from the “cd_data.csv” file and uses the input X (rounded to the nearest 0.01) to return the corresponding drag coefficients.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nA 1D NumPy array of values in the range [0, 1] representing the shape parameters.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 1D NumPy array of drag coefficients (cd) corresponding to the input X.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf any value in X is outside the range [0, 1].\n\n\n\n\n\n\n&gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import aerofoilcd\n&gt;&gt;&gt; X = np.array([0.5, 0.75])\n&gt;&gt;&gt; aerofoilcd(X)\narray([0.029975, 0.033375])",
    "crumbs": [
      "API Reference",
      "Functions",
      "aerofoilcd"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.aerofoilcd.html#parameters",
    "href": "docs/reference/function.forr08a.aerofoilcd.html#parameters",
    "title": "function.forr08a.aerofoilcd",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nA 1D NumPy array of values in the range [0, 1] representing the shape parameters.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "aerofoilcd"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.aerofoilcd.html#returns",
    "href": "docs/reference/function.forr08a.aerofoilcd.html#returns",
    "title": "function.forr08a.aerofoilcd",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 1D NumPy array of drag coefficients (cd) corresponding to the input X.",
    "crumbs": [
      "API Reference",
      "Functions",
      "aerofoilcd"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.aerofoilcd.html#raises",
    "href": "docs/reference/function.forr08a.aerofoilcd.html#raises",
    "title": "function.forr08a.aerofoilcd",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf any value in X is outside the range [0, 1].",
    "crumbs": [
      "API Reference",
      "Functions",
      "aerofoilcd"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.aerofoilcd.html#examples",
    "href": "docs/reference/function.forr08a.aerofoilcd.html#examples",
    "title": "function.forr08a.aerofoilcd",
    "section": "",
    "text": "&gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import aerofoilcd\n&gt;&gt;&gt; X = np.array([0.5, 0.75])\n&gt;&gt;&gt; aerofoilcd(X)\narray([0.029975, 0.033375])",
    "crumbs": [
      "API Reference",
      "Functions",
      "aerofoilcd"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.onevar.html",
    "href": "docs/reference/function.forr08a.onevar.html",
    "title": "function.forr08a.onevar",
    "section": "",
    "text": "function.forr08a.onevar(x)\nOne-variable test function that takes a scalar or 1D array input x in the range [0, 1] and returns the corresponding function values. The function is vectorized to handle multiple inputs.\n\n\nf(x) = ((6x - 2)^2) * np.sin((6x - 2) * 2)\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nnp.ndarray\nA scalar or 1D NumPy array of values in the range [0, 1].\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: The calculated function values for the input x.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf any value in x is outside the range [0, 1].\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import onevar\n&gt;&gt;&gt; # Single input\n&gt;&gt;&gt; print(onevar(np.array([0.5])))\n[0.9093]\n&gt;&gt;&gt; # Multiple inputs\n&gt;&gt;&gt; x = np.array([0.0, 0.25, 0.5, 0.75, 1.0])\n&gt;&gt;&gt; print(onevar(x))\n[3.0272, -0.2104, 0.9093,  -5.9933, 15.8297]",
    "crumbs": [
      "API Reference",
      "Functions",
      "onevar"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.onevar.html#the-function-is-defined-as",
    "href": "docs/reference/function.forr08a.onevar.html#the-function-is-defined-as",
    "title": "function.forr08a.onevar",
    "section": "",
    "text": "f(x) = ((6x - 2)^2) * np.sin((6x - 2) * 2)",
    "crumbs": [
      "API Reference",
      "Functions",
      "onevar"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.onevar.html#parameters",
    "href": "docs/reference/function.forr08a.onevar.html#parameters",
    "title": "function.forr08a.onevar",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\nnp.ndarray\nA scalar or 1D NumPy array of values in the range [0, 1].\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "onevar"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.onevar.html#returns",
    "href": "docs/reference/function.forr08a.onevar.html#returns",
    "title": "function.forr08a.onevar",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: The calculated function values for the input x.",
    "crumbs": [
      "API Reference",
      "Functions",
      "onevar"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.onevar.html#raises",
    "href": "docs/reference/function.forr08a.onevar.html#raises",
    "title": "function.forr08a.onevar",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf any value in x is outside the range [0, 1].",
    "crumbs": [
      "API Reference",
      "Functions",
      "onevar"
    ]
  },
  {
    "objectID": "docs/reference/function.forr08a.onevar.html#examples",
    "href": "docs/reference/function.forr08a.onevar.html#examples",
    "title": "function.forr08a.onevar",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import onevar\n&gt;&gt;&gt; # Single input\n&gt;&gt;&gt; print(onevar(np.array([0.5])))\n[0.9093]\n&gt;&gt;&gt; # Multiple inputs\n&gt;&gt;&gt; x = np.array([0.0, 0.25, 0.5, 0.75, 1.0])\n&gt;&gt;&gt; print(onevar(x))\n[3.0272, -0.2104, 0.9093,  -5.9933, 15.8297]",
    "crumbs": [
      "API Reference",
      "Functions",
      "onevar"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.activity_pred.html",
    "href": "docs/reference/function.mo.activity_pred.html",
    "title": "function.mo.activity_pred",
    "section": "",
    "text": "function.mo.activity_pred(X)\nCompute activity predictions for each row in the input array.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\n2D array where each row is a configuration.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: 1D array of activity predictions.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import activity_pred\n&gt;&gt;&gt; # Example input data\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; activity_pred(X)\narray([  1.5,  10.5])",
    "crumbs": [
      "API Reference",
      "Functions",
      "activity_pred"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.activity_pred.html#parameters",
    "href": "docs/reference/function.mo.activity_pred.html#parameters",
    "title": "function.mo.activity_pred",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\n2D array where each row is a configuration.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "activity_pred"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.activity_pred.html#returns",
    "href": "docs/reference/function.mo.activity_pred.html#returns",
    "title": "function.mo.activity_pred",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: 1D array of activity predictions.",
    "crumbs": [
      "API Reference",
      "Functions",
      "activity_pred"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.activity_pred.html#examples",
    "href": "docs/reference/function.mo.activity_pred.html#examples",
    "title": "function.mo.activity_pred",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import activity_pred\n&gt;&gt;&gt; # Example input data\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; activity_pred(X)\narray([  1.5,  10.5])",
    "crumbs": [
      "API Reference",
      "Functions",
      "activity_pred"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz1.html",
    "href": "docs/reference/function.mo.dtlz1.html",
    "title": "function.mo.dtlz1",
    "section": "",
    "text": "function.mo.dtlz1(X, n_obj=3)\nDTLZ1 multi-objective test function (scalable objectives).\nDTLZ1 is a scalable test problem with a linear Pareto front. It has (11^k - 1) local Pareto fronts where k = n - n_obj + 1.\n\n\nf_i(X) = 0.5 * x1 * … * x_{M-i} * [1 + g(X)] for i = 1, …, M-1 f_M(X) = 0.5 * [1 - x_{M-1}] * [1 + g(X)] g(X) = 100 * [k + sum((x_i - 0.5)^2 - cos(20π(x_i - 0.5)) for i in X_M)]\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.\nrequired\n\n\nn_obj\nint\nNumber of objectives. Defaults to 3. Must be at least 2 and at most n_features.\n3\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, n_obj).\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf n_obj is invalid or X has insufficient dimensions.\n\n\n\n\n\n\n\nNumber of objectives: Scalable (typically 3)\nTypical number of variables: n_obj + k - 1 (often k = 5, so n = 7 for 3 objectives)\nSearch domain: [0, 1]^n\nPareto front: Linear hyperplane\nCharacteristics: Multimodal, many local fronts\n\n\n\n\n&gt;&gt;&gt; from spotoptim.function.mo import dtlz1\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n&gt;&gt;&gt; result = dtlz1(X, n_obj=3)\n&gt;&gt;&gt; result.shape\n(1, 3)\n\n\n\nDeb, K., Thiele, L., Laumanns, M., & Zitzler, E. (2005). “Scalable test problems for evolutionary multiobjective optimization.” In Evolutionary multiobjective optimization (pp. 105-145). Springer.",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz1.html#mathematical-formulation",
    "href": "docs/reference/function.mo.dtlz1.html#mathematical-formulation",
    "title": "function.mo.dtlz1",
    "section": "",
    "text": "f_i(X) = 0.5 * x1 * … * x_{M-i} * [1 + g(X)] for i = 1, …, M-1 f_M(X) = 0.5 * [1 - x_{M-1}] * [1 + g(X)] g(X) = 100 * [k + sum((x_i - 0.5)^2 - cos(20π(x_i - 0.5)) for i in X_M)]",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz1.html#parameters",
    "href": "docs/reference/function.mo.dtlz1.html#parameters",
    "title": "function.mo.dtlz1",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.\nrequired\n\n\nn_obj\nint\nNumber of objectives. Defaults to 3. Must be at least 2 and at most n_features.\n3",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz1.html#returns",
    "href": "docs/reference/function.mo.dtlz1.html#returns",
    "title": "function.mo.dtlz1",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, n_obj).",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz1.html#raises",
    "href": "docs/reference/function.mo.dtlz1.html#raises",
    "title": "function.mo.dtlz1",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf n_obj is invalid or X has insufficient dimensions.",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz1.html#note",
    "href": "docs/reference/function.mo.dtlz1.html#note",
    "title": "function.mo.dtlz1",
    "section": "",
    "text": "Number of objectives: Scalable (typically 3)\nTypical number of variables: n_obj + k - 1 (often k = 5, so n = 7 for 3 objectives)\nSearch domain: [0, 1]^n\nPareto front: Linear hyperplane\nCharacteristics: Multimodal, many local fronts",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz1.html#examples",
    "href": "docs/reference/function.mo.dtlz1.html#examples",
    "title": "function.mo.dtlz1",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.function.mo import dtlz1\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n&gt;&gt;&gt; result = dtlz1(X, n_obj=3)\n&gt;&gt;&gt; result.shape\n(1, 3)",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.dtlz1.html#references",
    "href": "docs/reference/function.mo.dtlz1.html#references",
    "title": "function.mo.dtlz1",
    "section": "",
    "text": "Deb, K., Thiele, L., Laumanns, M., & Zitzler, E. (2005). “Scalable test problems for evolutionary multiobjective optimization.” In Evolutionary multiobjective optimization (pp. 105-145). Springer.",
    "crumbs": [
      "API Reference",
      "Functions",
      "dtlz1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.fonseca_fleming.html",
    "href": "docs/reference/function.mo.fonseca_fleming.html",
    "title": "function.mo.fonseca_fleming",
    "section": "",
    "text": "function.mo.fonseca_fleming(X)\nFonseca-Fleming multi-objective test function (2 objectives).\nThe Fonseca-Fleming function is a classical bi-objective problem with a concave Pareto front. The difficulty increases with the number of variables.\n\n\nf1(X) = 1 - exp(-sum((x_i - 1/sqrt(n))^2 for i=1 to n)) f2(X) = 1 - exp(-sum((x_i + 1/sqrt(n))^2 for i=1 to n))\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values\n\n\n\n\n\n\n\nNumber of objectives: 2\nTypical number of variables: 2-10\nSearch domain: [-4, 4]^n\nPareto front: Concave\nCharacteristics: Concave, symmetric\n\n\n\n\n&gt;&gt;&gt; from spotoptim.function.mo import fonseca_fleming\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0])\n&gt;&gt;&gt; result = fonseca_fleming(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [1.0, 1.0]])\n&gt;&gt;&gt; result = fonseca_fleming(X)\n&gt;&gt;&gt; result.shape\n(2, 2)\n\n\n\nFonseca, C. M., & Fleming, P. J. (1995). “An overview of evolutionary algorithms in multiobjective optimization.” Evolutionary computation, 3(1), 1-16.",
    "crumbs": [
      "API Reference",
      "Functions",
      "fonseca_fleming"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.fonseca_fleming.html#mathematical-formulation",
    "href": "docs/reference/function.mo.fonseca_fleming.html#mathematical-formulation",
    "title": "function.mo.fonseca_fleming",
    "section": "",
    "text": "f1(X) = 1 - exp(-sum((x_i - 1/sqrt(n))^2 for i=1 to n)) f2(X) = 1 - exp(-sum((x_i + 1/sqrt(n))^2 for i=1 to n))",
    "crumbs": [
      "API Reference",
      "Functions",
      "fonseca_fleming"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.fonseca_fleming.html#parameters",
    "href": "docs/reference/function.mo.fonseca_fleming.html#parameters",
    "title": "function.mo.fonseca_fleming",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "fonseca_fleming"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.fonseca_fleming.html#returns",
    "href": "docs/reference/function.mo.fonseca_fleming.html#returns",
    "title": "function.mo.fonseca_fleming",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values",
    "crumbs": [
      "API Reference",
      "Functions",
      "fonseca_fleming"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.fonseca_fleming.html#note",
    "href": "docs/reference/function.mo.fonseca_fleming.html#note",
    "title": "function.mo.fonseca_fleming",
    "section": "",
    "text": "Number of objectives: 2\nTypical number of variables: 2-10\nSearch domain: [-4, 4]^n\nPareto front: Concave\nCharacteristics: Concave, symmetric",
    "crumbs": [
      "API Reference",
      "Functions",
      "fonseca_fleming"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.fonseca_fleming.html#examples",
    "href": "docs/reference/function.mo.fonseca_fleming.html#examples",
    "title": "function.mo.fonseca_fleming",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.function.mo import fonseca_fleming\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0])\n&gt;&gt;&gt; result = fonseca_fleming(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [1.0, 1.0]])\n&gt;&gt;&gt; result = fonseca_fleming(X)\n&gt;&gt;&gt; result.shape\n(2, 2)",
    "crumbs": [
      "API Reference",
      "Functions",
      "fonseca_fleming"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.fonseca_fleming.html#references",
    "href": "docs/reference/function.mo.fonseca_fleming.html#references",
    "title": "function.mo.fonseca_fleming",
    "section": "",
    "text": "Fonseca, C. M., & Fleming, P. J. (1995). “An overview of evolutionary algorithms in multiobjective optimization.” Evolutionary computation, 3(1), 1-16.",
    "crumbs": [
      "API Reference",
      "Functions",
      "fonseca_fleming"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.kursawe.html",
    "href": "docs/reference/function.mo.kursawe.html",
    "title": "function.mo.kursawe",
    "section": "",
    "text": "function.mo.kursawe(X)\nKursawe multi-objective test function (2 objectives, minimization).\nThe Kursawe function is a classic bi-objective minimization benchmark with a non-convex, disconnected Pareto front, often used to test an optimizer’s ability to maintain diversity and avoid getting trapped in local fronts.\n\n\nf1(X) = sum(-10 * exp(-0.2 * sqrt(x_i^2 + x_{i+1}^2)) for i=1 to n-1) f2(X) = sum(|x_i|^0.8 + 5 * sin(x_i^3) for i=1 to n)\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.\n\n\n\n\n\n\n\nNumber of objectives: 2\nTypical number of variables: 3\nSearch domain: [-5, 5]^n\nPareto front: Disconnected\nCharacteristics: Non-convex, disconnected, multimodal\n\n\n\n\n&gt;&gt;&gt; from spotoptim.function.mo import kursawe\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; result = kursawe(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; X = np.array([[0.0, 0.0, 0.0], [1.0, 1.0, 1.0]])\n&gt;&gt;&gt; result = kursawe(X)\n&gt;&gt;&gt; result.shape\n(2, 2)\n\n\n\nKursawe, F. (1991). “A variant of evolution strategies for vector optimization.” In International Conference on Parallel Problem Solving from Nature (pp. 193-197). Springer.",
    "crumbs": [
      "API Reference",
      "Functions",
      "kursawe"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.kursawe.html#mathematical-formulation",
    "href": "docs/reference/function.mo.kursawe.html#mathematical-formulation",
    "title": "function.mo.kursawe",
    "section": "",
    "text": "f1(X) = sum(-10 * exp(-0.2 * sqrt(x_i^2 + x_{i+1}^2)) for i=1 to n-1) f2(X) = sum(|x_i|^0.8 + 5 * sin(x_i^3) for i=1 to n)",
    "crumbs": [
      "API Reference",
      "Functions",
      "kursawe"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.kursawe.html#parameters",
    "href": "docs/reference/function.mo.kursawe.html#parameters",
    "title": "function.mo.kursawe",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "kursawe"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.kursawe.html#returns",
    "href": "docs/reference/function.mo.kursawe.html#returns",
    "title": "function.mo.kursawe",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values",
    "crumbs": [
      "API Reference",
      "Functions",
      "kursawe"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.kursawe.html#raises",
    "href": "docs/reference/function.mo.kursawe.html#raises",
    "title": "function.mo.kursawe",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.",
    "crumbs": [
      "API Reference",
      "Functions",
      "kursawe"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.kursawe.html#note",
    "href": "docs/reference/function.mo.kursawe.html#note",
    "title": "function.mo.kursawe",
    "section": "",
    "text": "Number of objectives: 2\nTypical number of variables: 3\nSearch domain: [-5, 5]^n\nPareto front: Disconnected\nCharacteristics: Non-convex, disconnected, multimodal",
    "crumbs": [
      "API Reference",
      "Functions",
      "kursawe"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.kursawe.html#examples",
    "href": "docs/reference/function.mo.kursawe.html#examples",
    "title": "function.mo.kursawe",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.function.mo import kursawe\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; result = kursawe(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; X = np.array([[0.0, 0.0, 0.0], [1.0, 1.0, 1.0]])\n&gt;&gt;&gt; result = kursawe(X)\n&gt;&gt;&gt; result.shape\n(2, 2)",
    "crumbs": [
      "API Reference",
      "Functions",
      "kursawe"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.kursawe.html#references",
    "href": "docs/reference/function.mo.kursawe.html#references",
    "title": "function.mo.kursawe",
    "section": "",
    "text": "Kursawe, F. (1991). “A variant of evolution strategies for vector optimization.” In International Conference on Parallel Problem Solving from Nature (pp. 193-197). Springer.",
    "crumbs": [
      "API Reference",
      "Functions",
      "kursawe"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_min.html",
    "href": "docs/reference/function.mo.mo_conv2_min.html",
    "title": "function.mo.mo_conv2_min",
    "section": "",
    "text": "function.mo.mo_conv2_min(X)\nConvex bi-objective minimization test function (2 objectives).\nA smooth, convex two-objective problem on [0, 1]^2: f1(x, y) = x^2 + y^2 f2(x, y) = (x - 1)^2 + (y - 1)^2\n\n\n\nDomain: [0, 1]^2\nObjectives: minimize both f1 and f2\nIdeal points: (0, 0) for f1; (1, 1) for f2\nPareto set: line x = y in [0, 1]\nPareto front: convex quadratic trade-off f1 = 2t^2, f2 = 2(1 - t)^2, t ∈ [0, 1]\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have exactly 2 dimensions.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values (to be minimized) - Column 1: f2 values (to be minimized)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf X does not have exactly 2 dimensions.\n\n\n\n\n\n\n\nNumber of objectives: 2\nNumber of variables: 2\nSearch domain: [0, 1]^2\nIdeal points: (0, 0) for f1, (1, 1) for f2\nPareto front: Convex, quadratic\nProblem type: Minimization\nCharacteristics: Convex, smooth, bounded\n\n\n\n\nSingle point evaluation:\n&gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_min\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0])\n&gt;&gt;&gt; result = mo_conv2_min(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; result[0]  # f1 minimum\narray([0., 2.])\n&gt;&gt;&gt; X = np.array([1.0, 1.0])\n&gt;&gt;&gt; result = mo_conv2_min(X)\n&gt;&gt;&gt; result[0]  # f2 minimum\narray([2., 0.])\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; result = mo_conv2_min(X)\n&gt;&gt;&gt; result.shape\n(3, 2)\n&gt;&gt;&gt; result[1]  # Pareto front\narray([0.5, 0.5])",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_min"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_min.html#properties",
    "href": "docs/reference/function.mo.mo_conv2_min.html#properties",
    "title": "function.mo.mo_conv2_min",
    "section": "",
    "text": "Domain: [0, 1]^2\nObjectives: minimize both f1 and f2\nIdeal points: (0, 0) for f1; (1, 1) for f2\nPareto set: line x = y in [0, 1]\nPareto front: convex quadratic trade-off f1 = 2t^2, f2 = 2(1 - t)^2, t ∈ [0, 1]",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_min"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_min.html#parameters",
    "href": "docs/reference/function.mo.mo_conv2_min.html#parameters",
    "title": "function.mo.mo_conv2_min",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have exactly 2 dimensions.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_min"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_min.html#returns",
    "href": "docs/reference/function.mo.mo_conv2_min.html#returns",
    "title": "function.mo.mo_conv2_min",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values (to be minimized) - Column 1: f2 values (to be minimized)",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_min"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_min.html#raises",
    "href": "docs/reference/function.mo.mo_conv2_min.html#raises",
    "title": "function.mo.mo_conv2_min",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf X does not have exactly 2 dimensions.",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_min"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_min.html#note",
    "href": "docs/reference/function.mo.mo_conv2_min.html#note",
    "title": "function.mo.mo_conv2_min",
    "section": "",
    "text": "Number of objectives: 2\nNumber of variables: 2\nSearch domain: [0, 1]^2\nIdeal points: (0, 0) for f1, (1, 1) for f2\nPareto front: Convex, quadratic\nProblem type: Minimization\nCharacteristics: Convex, smooth, bounded",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_min"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.mo_conv2_min.html#examples",
    "href": "docs/reference/function.mo.mo_conv2_min.html#examples",
    "title": "function.mo.mo_conv2_min",
    "section": "",
    "text": "Single point evaluation:\n&gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_min\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0])\n&gt;&gt;&gt; result = mo_conv2_min(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; result[0]  # f1 minimum\narray([0., 2.])\n&gt;&gt;&gt; X = np.array([1.0, 1.0])\n&gt;&gt;&gt; result = mo_conv2_min(X)\n&gt;&gt;&gt; result[0]  # f2 minimum\narray([2., 0.])\nMultiple points evaluation:\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; result = mo_conv2_min(X)\n&gt;&gt;&gt; result.shape\n(3, 2)\n&gt;&gt;&gt; result[1]  # Pareto front\narray([0.5, 0.5])",
    "crumbs": [
      "API Reference",
      "Functions",
      "mo_conv2_min"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.schaffer_n1.html",
    "href": "docs/reference/function.mo.schaffer_n1.html",
    "title": "function.mo.schaffer_n1",
    "section": "",
    "text": "function.mo.schaffer_n1(X)\nSchaffer N1 multi-objective test function (2 objectives).\nSchaffer N1 is a simple bi-objective problem with a convex Pareto front. It is one of the earliest multi-objective test functions.\n\n\nf1(X) = x^2 f2(X) = (x - 2)^2\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. This function uses only the first variable.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values\n\n\n\n\n\n\n\nNumber of objectives: 2\nNumber of variables: 1 (only first variable is used)\nSearch domain: [-10, 10] or [-A, A]\nPareto front: x ∈ [0, 2]\nCharacteristics: Convex, simple, unimodal\n\n\n\n\n&gt;&gt;&gt; from spotoptim.function.mo import schaffer_n1\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0])\n&gt;&gt;&gt; result = schaffer_n1(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; result[0]\narray([0., 4.])\n&gt;&gt;&gt; X = np.array([[0.0], [1.0], [2.0]])\n&gt;&gt;&gt; result = schaffer_n1(X)\n&gt;&gt;&gt; result.shape\n(3, 2)\n\n\n\nSchaffer, J. D. (1985). “Multiple objective optimization with vector evaluated genetic algorithms.” In Proceedings of the 1st international Conference on Genetic Algorithms (pp. 93-100).",
    "crumbs": [
      "API Reference",
      "Functions",
      "schaffer_n1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.schaffer_n1.html#mathematical-formulation",
    "href": "docs/reference/function.mo.schaffer_n1.html#mathematical-formulation",
    "title": "function.mo.schaffer_n1",
    "section": "",
    "text": "f1(X) = x^2 f2(X) = (x - 2)^2",
    "crumbs": [
      "API Reference",
      "Functions",
      "schaffer_n1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.schaffer_n1.html#parameters",
    "href": "docs/reference/function.mo.schaffer_n1.html#parameters",
    "title": "function.mo.schaffer_n1",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. This function uses only the first variable.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "schaffer_n1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.schaffer_n1.html#returns",
    "href": "docs/reference/function.mo.schaffer_n1.html#returns",
    "title": "function.mo.schaffer_n1",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values",
    "crumbs": [
      "API Reference",
      "Functions",
      "schaffer_n1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.schaffer_n1.html#note",
    "href": "docs/reference/function.mo.schaffer_n1.html#note",
    "title": "function.mo.schaffer_n1",
    "section": "",
    "text": "Number of objectives: 2\nNumber of variables: 1 (only first variable is used)\nSearch domain: [-10, 10] or [-A, A]\nPareto front: x ∈ [0, 2]\nCharacteristics: Convex, simple, unimodal",
    "crumbs": [
      "API Reference",
      "Functions",
      "schaffer_n1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.schaffer_n1.html#examples",
    "href": "docs/reference/function.mo.schaffer_n1.html#examples",
    "title": "function.mo.schaffer_n1",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.function.mo import schaffer_n1\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0])\n&gt;&gt;&gt; result = schaffer_n1(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; result[0]\narray([0., 4.])\n&gt;&gt;&gt; X = np.array([[0.0], [1.0], [2.0]])\n&gt;&gt;&gt; result = schaffer_n1(X)\n&gt;&gt;&gt; result.shape\n(3, 2)",
    "crumbs": [
      "API Reference",
      "Functions",
      "schaffer_n1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.schaffer_n1.html#references",
    "href": "docs/reference/function.mo.schaffer_n1.html#references",
    "title": "function.mo.schaffer_n1",
    "section": "",
    "text": "Schaffer, J. D. (1985). “Multiple objective optimization with vector evaluated genetic algorithms.” In Proceedings of the 1st international Conference on Genetic Algorithms (pp. 93-100).",
    "crumbs": [
      "API Reference",
      "Functions",
      "schaffer_n1"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt2.html",
    "href": "docs/reference/function.mo.zdt2.html",
    "title": "function.mo.zdt2",
    "section": "",
    "text": "function.mo.zdt2(X)\nZDT2 multi-objective test function (2 objectives).\nZDT2 is similar to ZDT1 but has a non-convex Pareto front.\n\n\nf1(X) = x1 f2(X) = g(X) * [1 - (x1 / g(X))^2] g(X) = 1 + 9 * sum(x_i for i=2 to n) / (n - 1)\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.\n\n\n\n\n\n\n\nNumber of objectives: 2\nTypical number of variables: 30\nSearch domain: [0, 1]^n\nPareto front: Non-convex, f1 ∈ [0, 1], f2 = 1 - f1^2\nCharacteristics: Non-convex, unimodal\n\n\n\n\n&gt;&gt;&gt; from spotoptim.function.mo import zdt2\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; result = zdt2(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n\n\n\nZitzler, E., Deb, K., & Thiele, L. (2000). “Comparison of multiobjective evolutionary algorithms: Empirical results.” Evolutionary computation, 8(2), 173-195.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt2.html#mathematical-formulation",
    "href": "docs/reference/function.mo.zdt2.html#mathematical-formulation",
    "title": "function.mo.zdt2",
    "section": "",
    "text": "f1(X) = x1 f2(X) = g(X) * [1 - (x1 / g(X))^2] g(X) = 1 + 9 * sum(x_i for i=2 to n) / (n - 1)",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt2.html#parameters",
    "href": "docs/reference/function.mo.zdt2.html#parameters",
    "title": "function.mo.zdt2",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt2.html#returns",
    "href": "docs/reference/function.mo.zdt2.html#returns",
    "title": "function.mo.zdt2",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt2.html#raises",
    "href": "docs/reference/function.mo.zdt2.html#raises",
    "title": "function.mo.zdt2",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt2.html#note",
    "href": "docs/reference/function.mo.zdt2.html#note",
    "title": "function.mo.zdt2",
    "section": "",
    "text": "Number of objectives: 2\nTypical number of variables: 30\nSearch domain: [0, 1]^n\nPareto front: Non-convex, f1 ∈ [0, 1], f2 = 1 - f1^2\nCharacteristics: Non-convex, unimodal",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt2.html#examples",
    "href": "docs/reference/function.mo.zdt2.html#examples",
    "title": "function.mo.zdt2",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.function.mo import zdt2\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; result = zdt2(X)\n&gt;&gt;&gt; result.shape\n(1, 2)",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt2.html#references",
    "href": "docs/reference/function.mo.zdt2.html#references",
    "title": "function.mo.zdt2",
    "section": "",
    "text": "Zitzler, E., Deb, K., & Thiele, L. (2000). “Comparison of multiobjective evolutionary algorithms: Empirical results.” Evolutionary computation, 8(2), 173-195.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt2"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt4.html",
    "href": "docs/reference/function.mo.zdt4.html",
    "title": "function.mo.zdt4",
    "section": "",
    "text": "function.mo.zdt4(X)\nZDT4 multi-objective test function (2 objectives).\nZDT4 has 21^9 local Pareto fronts, testing the algorithm’s ability to deal with multimodality.\n\n\nf1(X) = x1 f2(X) = g(X) * [1 - sqrt(x1 / g(X))] g(X) = 1 + 10 * (n - 1) + sum(x_i^2 - 10 * cos(4 * π * x_i) for i=2 to n)\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.\n\n\n\n\n\n\n\nNumber of objectives: 2\nTypical number of variables: 10\nSearch domain: x1 ∈ [0, 1], x_i ∈ [-5, 5] for i = 2, …, n\nPareto front: Convex, same as ZDT1\nCharacteristics: Multimodal (many local fronts)\n\n\n\n\n&gt;&gt;&gt; from spotoptim.function.mo import zdt4\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.5, 0.0, 0.0])\n&gt;&gt;&gt; result = zdt4(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n\n\n\nZitzler, E., Deb, K., & Thiele, L. (2000). “Comparison of multiobjective evolutionary algorithms: Empirical results.” Evolutionary computation, 8(2), 173-195.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt4"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt4.html#mathematical-formulation",
    "href": "docs/reference/function.mo.zdt4.html#mathematical-formulation",
    "title": "function.mo.zdt4",
    "section": "",
    "text": "f1(X) = x1 f2(X) = g(X) * [1 - sqrt(x1 / g(X))] g(X) = 1 + 10 * (n - 1) + sum(x_i^2 - 10 * cos(4 * π * x_i) for i=2 to n)",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt4"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt4.html#parameters",
    "href": "docs/reference/function.mo.zdt4.html#parameters",
    "title": "function.mo.zdt4",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt4"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt4.html#returns",
    "href": "docs/reference/function.mo.zdt4.html#returns",
    "title": "function.mo.zdt4",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt4"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt4.html#raises",
    "href": "docs/reference/function.mo.zdt4.html#raises",
    "title": "function.mo.zdt4",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf X has fewer than 2 dimensions.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt4"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt4.html#note",
    "href": "docs/reference/function.mo.zdt4.html#note",
    "title": "function.mo.zdt4",
    "section": "",
    "text": "Number of objectives: 2\nTypical number of variables: 10\nSearch domain: x1 ∈ [0, 1], x_i ∈ [-5, 5] for i = 2, …, n\nPareto front: Convex, same as ZDT1\nCharacteristics: Multimodal (many local fronts)",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt4"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt4.html#examples",
    "href": "docs/reference/function.mo.zdt4.html#examples",
    "title": "function.mo.zdt4",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.function.mo import zdt4\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.5, 0.0, 0.0])\n&gt;&gt;&gt; result = zdt4(X)\n&gt;&gt;&gt; result.shape\n(1, 2)",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt4"
    ]
  },
  {
    "objectID": "docs/reference/function.mo.zdt4.html#references",
    "href": "docs/reference/function.mo.zdt4.html#references",
    "title": "function.mo.zdt4",
    "section": "",
    "text": "Zitzler, E., Deb, K., & Thiele, L. (2000). “Comparison of multiobjective evolutionary algorithms: Empirical results.” Evolutionary computation, 8(2), 173-195.",
    "crumbs": [
      "API Reference",
      "Functions",
      "zdt4"
    ]
  },
  {
    "objectID": "docs/reference/function.html",
    "href": "docs/reference/function.html",
    "title": "function",
    "section": "",
    "text": "function\nfunction\nAnalytical test functions for optimization.",
    "crumbs": [
      "API Reference",
      "Functions",
      "function"
    ]
  },
  {
    "objectID": "docs/reference/function.remote.html",
    "href": "docs/reference/function.remote.html",
    "title": "function.remote",
    "section": "",
    "text": "function.remote\n\n\n\n\n\nName\nDescription\n\n\n\n\nobjective_remote\nEvaluates an objective function remotely via an HTTP POST request.",
    "crumbs": [
      "API Reference",
      "Functions",
      "function.remote"
    ]
  },
  {
    "objectID": "docs/reference/function.remote.html#functions",
    "href": "docs/reference/function.remote.html#functions",
    "title": "function.remote",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nobjective_remote\nEvaluates an objective function remotely via an HTTP POST request.",
    "crumbs": [
      "API Reference",
      "Functions",
      "function.remote"
    ]
  },
  {
    "objectID": "docs/reference/function.so.lennard_jones.html",
    "href": "docs/reference/function.so.lennard_jones.html",
    "title": "function.so.lennard_jones",
    "section": "",
    "text": "function.so.lennard_jones(X, n_atoms=13)\nLennard-Jones Atomic Cluster Potential Energy.\nCalculates the potential energy of a cluster of N atoms interacting via the Lennard-Jones potential. The optimization problem involves finding the atomic coordinates that minimize the total potential energy. This is a classic benchmark problem known for its high difficulty due to the exponential growth of local minima with N.\n\n\nThe function accepts inputs in the range [0, 1] and internally maps them to the search domain [-2, 2] for each coordinate.\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput points. - Shape (n_samples, 3 * n_atoms) for batch evaluation. - Shape (3 * n_atoms,) for single evaluation. The input represents the flattened [x1, y1, z1, x2, y2, z2, …] coordinates.\nrequired\n\n\nn_atoms\nint\nNumber of atoms in the cluster. Defaults to 13.\n13\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Potential energy values. Shape (n_samples,).\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf input dimensions do not match 3 * n_atoms.\n\n\n\n\n\n\n\nGlobal minimum for N=13: E ≈ -44.3268\nSearch domain: [-2, 2]^(3N) (mapped from [0, 1])\nCharacteristics: Extremely rugged landscape, non-convex, many local minima.\n\n\n\n\nSingle point evaluation (random configuration):\n&gt;&gt;&gt; from spotoptim.function import lennard_jones\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; rng = np.random.default_rng(42)\n&gt;&gt;&gt; X = rng.random(39)  # 13 atoms * 3 coords, in [0, 1]\n&gt;&gt;&gt; lennard_jones(X)\narray([9.5...e+...])\nBatch evaluation:\n&gt;&gt;&gt; X = rng.random((5, 39))\n&gt;&gt;&gt; lennard_jones(X).shape\n(5,)\n\n\n\nWales, D. J., & Doye, J. P. (1997). Global optimization by basin-hopping and the lowest energy structures of Lennard-Jones clusters containing up to 110 atoms. The Journal of Physical Chemistry A, 101(28), 5111-5116.",
    "crumbs": [
      "API Reference",
      "Functions",
      "lennard_jones"
    ]
  },
  {
    "objectID": "docs/reference/function.so.lennard_jones.html#input-domain-handling",
    "href": "docs/reference/function.so.lennard_jones.html#input-domain-handling",
    "title": "function.so.lennard_jones",
    "section": "",
    "text": "The function accepts inputs in the range [0, 1] and internally maps them to the search domain [-2, 2] for each coordinate.",
    "crumbs": [
      "API Reference",
      "Functions",
      "lennard_jones"
    ]
  },
  {
    "objectID": "docs/reference/function.so.lennard_jones.html#parameters",
    "href": "docs/reference/function.so.lennard_jones.html#parameters",
    "title": "function.so.lennard_jones",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput points. - Shape (n_samples, 3 * n_atoms) for batch evaluation. - Shape (3 * n_atoms,) for single evaluation. The input represents the flattened [x1, y1, z1, x2, y2, z2, …] coordinates.\nrequired\n\n\nn_atoms\nint\nNumber of atoms in the cluster. Defaults to 13.\n13",
    "crumbs": [
      "API Reference",
      "Functions",
      "lennard_jones"
    ]
  },
  {
    "objectID": "docs/reference/function.so.lennard_jones.html#returns",
    "href": "docs/reference/function.so.lennard_jones.html#returns",
    "title": "function.so.lennard_jones",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Potential energy values. Shape (n_samples,).",
    "crumbs": [
      "API Reference",
      "Functions",
      "lennard_jones"
    ]
  },
  {
    "objectID": "docs/reference/function.so.lennard_jones.html#raises",
    "href": "docs/reference/function.so.lennard_jones.html#raises",
    "title": "function.so.lennard_jones",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf input dimensions do not match 3 * n_atoms.",
    "crumbs": [
      "API Reference",
      "Functions",
      "lennard_jones"
    ]
  },
  {
    "objectID": "docs/reference/function.so.lennard_jones.html#note",
    "href": "docs/reference/function.so.lennard_jones.html#note",
    "title": "function.so.lennard_jones",
    "section": "",
    "text": "Global minimum for N=13: E ≈ -44.3268\nSearch domain: [-2, 2]^(3N) (mapped from [0, 1])\nCharacteristics: Extremely rugged landscape, non-convex, many local minima.",
    "crumbs": [
      "API Reference",
      "Functions",
      "lennard_jones"
    ]
  },
  {
    "objectID": "docs/reference/function.so.lennard_jones.html#examples",
    "href": "docs/reference/function.so.lennard_jones.html#examples",
    "title": "function.so.lennard_jones",
    "section": "",
    "text": "Single point evaluation (random configuration):\n&gt;&gt;&gt; from spotoptim.function import lennard_jones\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; rng = np.random.default_rng(42)\n&gt;&gt;&gt; X = rng.random(39)  # 13 atoms * 3 coords, in [0, 1]\n&gt;&gt;&gt; lennard_jones(X)\narray([9.5...e+...])\nBatch evaluation:\n&gt;&gt;&gt; X = rng.random((5, 39))\n&gt;&gt;&gt; lennard_jones(X).shape\n(5,)",
    "crumbs": [
      "API Reference",
      "Functions",
      "lennard_jones"
    ]
  },
  {
    "objectID": "docs/reference/function.so.lennard_jones.html#references",
    "href": "docs/reference/function.so.lennard_jones.html#references",
    "title": "function.so.lennard_jones",
    "section": "",
    "text": "Wales, D. J., & Doye, J. P. (1997). Global optimization by basin-hopping and the lowest energy structures of Lennard-Jones clusters containing up to 110 atoms. The Journal of Physical Chemistry A, 101(28), 5111-5116.",
    "crumbs": [
      "API Reference",
      "Functions",
      "lennard_jones"
    ]
  },
  {
    "objectID": "docs/reference/function.so.html",
    "href": "docs/reference/function.so.html",
    "title": "function.so",
    "section": "",
    "text": "function.so\nAnalytical single-objective test functions for optimization benchmarking.\nThis module provides well-known analytical test functions commonly used for evaluating and benchmarking optimization algorithms.\n\n\n\n\n\nName\nDescription\n\n\n\n\nackley\nN-dimensional Ackley function.\n\n\nlennard_jones\nLennard-Jones Atomic Cluster Potential Energy.\n\n\nmichalewicz\nN-dimensional Michalewicz function.\n\n\nrobot_arm_hard\n10-Link Robot Arm with Maze-Like Hard Constraints.\n\n\nrobot_arm_obstacle\n10-Link Planar Robot Arm Inverse Kinematics with Obstacle Avoidance.\n\n\nrosenbrock\nN-dimensional Rosenbrock function.\n\n\nwingwt\nAircraft Wing Weight function.",
    "crumbs": [
      "API Reference",
      "Functions",
      "function.so"
    ]
  },
  {
    "objectID": "docs/reference/function.so.html#functions",
    "href": "docs/reference/function.so.html#functions",
    "title": "function.so",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nackley\nN-dimensional Ackley function.\n\n\nlennard_jones\nLennard-Jones Atomic Cluster Potential Energy.\n\n\nmichalewicz\nN-dimensional Michalewicz function.\n\n\nrobot_arm_hard\n10-Link Robot Arm with Maze-Like Hard Constraints.\n\n\nrobot_arm_obstacle\n10-Link Planar Robot Arm Inverse Kinematics with Obstacle Avoidance.\n\n\nrosenbrock\nN-dimensional Rosenbrock function.\n\n\nwingwt\nAircraft Wing Weight function.",
    "crumbs": [
      "API Reference",
      "Functions",
      "function.so"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_obstacle.html",
    "href": "docs/reference/function.so.robot_arm_obstacle.html",
    "title": "function.so.robot_arm_obstacle",
    "section": "",
    "text": "function.so.robot_arm_obstacle(X)\n10-Link Planar Robot Arm Inverse Kinematics with Obstacle Avoidance.\nThe goal is to minimize the distance of the end-effector to a target point while avoiding collision with a set of circular obstacles. This problem mimics a real-world inverse kinematics solver for a redundant manipulator.\n\n\nThe function accepts inputs in the range [0, 1] and internally maps them to the search domain [-pi, pi] for each joint angle (radians).\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput angles (normalized). - Shape (n_samples, 10). The input contains the normalized relative angles for the 10 links.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Cost values (Weighted sum of Distance + Penalty). Shape (n_samples,).\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf input dimensions do not match 10.\n\n\n\n\n\n\n\nTarget: (5.0, 5.0)\nObstacles:\n\nOrder 1: (2,2), r=1\nOrder 2: (4,3), r=1.5\nOrder 3: (3,6), r=1\n\nDimensions: 10 (fixed)\nSearch domain: [-pi, pi]^10 (mapped from [0, 1])\nCharacteristics: Multimodal, disjoint feasible regions, constrained.\n\n\n\n\nSingle point evaluation:\n&gt;&gt;&gt; from spotoptim.function.so import robot_arm_obstacle\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; rng = np.random.default_rng(42)\n&gt;&gt;&gt; X = rng.random(10)  # Random angles in [0, 1]\n&gt;&gt;&gt; robot_arm_obstacle(X)\narray([2547...])\nBatch evaluation:\n&gt;&gt;&gt; X = rng.random((5, 10))\n&gt;&gt;&gt; robot_arm_obstacle(X).shape\n(5,)",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_obstacle"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_obstacle.html#input-domain-handling",
    "href": "docs/reference/function.so.robot_arm_obstacle.html#input-domain-handling",
    "title": "function.so.robot_arm_obstacle",
    "section": "",
    "text": "The function accepts inputs in the range [0, 1] and internally maps them to the search domain [-pi, pi] for each joint angle (radians).",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_obstacle"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_obstacle.html#parameters",
    "href": "docs/reference/function.so.robot_arm_obstacle.html#parameters",
    "title": "function.so.robot_arm_obstacle",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput angles (normalized). - Shape (n_samples, 10). The input contains the normalized relative angles for the 10 links.\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_obstacle"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_obstacle.html#returns",
    "href": "docs/reference/function.so.robot_arm_obstacle.html#returns",
    "title": "function.so.robot_arm_obstacle",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Cost values (Weighted sum of Distance + Penalty). Shape (n_samples,).",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_obstacle"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_obstacle.html#raises",
    "href": "docs/reference/function.so.robot_arm_obstacle.html#raises",
    "title": "function.so.robot_arm_obstacle",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf input dimensions do not match 10.",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_obstacle"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_obstacle.html#note",
    "href": "docs/reference/function.so.robot_arm_obstacle.html#note",
    "title": "function.so.robot_arm_obstacle",
    "section": "",
    "text": "Target: (5.0, 5.0)\nObstacles:\n\nOrder 1: (2,2), r=1\nOrder 2: (4,3), r=1.5\nOrder 3: (3,6), r=1\n\nDimensions: 10 (fixed)\nSearch domain: [-pi, pi]^10 (mapped from [0, 1])\nCharacteristics: Multimodal, disjoint feasible regions, constrained.",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_obstacle"
    ]
  },
  {
    "objectID": "docs/reference/function.so.robot_arm_obstacle.html#examples",
    "href": "docs/reference/function.so.robot_arm_obstacle.html#examples",
    "title": "function.so.robot_arm_obstacle",
    "section": "",
    "text": "Single point evaluation:\n&gt;&gt;&gt; from spotoptim.function.so import robot_arm_obstacle\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; rng = np.random.default_rng(42)\n&gt;&gt;&gt; X = rng.random(10)  # Random angles in [0, 1]\n&gt;&gt;&gt; robot_arm_obstacle(X)\narray([2547...])\nBatch evaluation:\n&gt;&gt;&gt; X = rng.random((5, 10))\n&gt;&gt;&gt; robot_arm_obstacle(X).shape\n(5,)",
    "crumbs": [
      "API Reference",
      "Functions",
      "robot_arm_obstacle"
    ]
  },
  {
    "objectID": "docs/reference/function.so.wingwt.html",
    "href": "docs/reference/function.so.wingwt.html",
    "title": "function.so.wingwt",
    "section": "",
    "text": "function.so.wingwt(X)\nAircraft Wing Weight function.\nThe example models the weight of an unpainted light aircraft wing. The function accepts inputs in the unit cube [0,1]^9 and returns the wing weight.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, 9) or (9,) or (10,) or (n_samples, 10). Input variables order: [Sw, Wfw, A, L, q, l, Rtc, Nz, Wdg, Wp(optional)]\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Wing weight values at the input points with shape (n_samples,).\n\n\n\n\n\n\nSingle point evaluation (Baseline Cessna C172 - Unpainted):\n&gt;&gt;&gt; from spotoptim.function.so import wingwt\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Baseline configuration in unit cube\n&gt;&gt;&gt; x_base = np.array([0.48, 0.4, 0.38, 0.5, 0.62, 0.344, 0.4, 0.37, 0.38])\n&gt;&gt;&gt; wingwt(x_base)\narray([233.90...])\nBatch evaluation:\n&gt;&gt;&gt; X = np.vstack([x_base, x_base])\n&gt;&gt;&gt; wingwt(X)\narray([233.90..., 233.90...])\n\n\n\nForrester, A., Sobester, A., & Keane, A. (2008). Engineering design via surrogate modelling: a practical guide. John Wiley & Sons.",
    "crumbs": [
      "API Reference",
      "Functions",
      "wingwt"
    ]
  },
  {
    "objectID": "docs/reference/function.so.wingwt.html#parameters",
    "href": "docs/reference/function.so.wingwt.html#parameters",
    "title": "function.so.wingwt",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\narray - like\nInput points with shape (n_samples, 9) or (9,) or (10,) or (n_samples, 10). Input variables order: [Sw, Wfw, A, L, q, l, Rtc, Nz, Wdg, Wp(optional)]\nrequired",
    "crumbs": [
      "API Reference",
      "Functions",
      "wingwt"
    ]
  },
  {
    "objectID": "docs/reference/function.so.wingwt.html#returns",
    "href": "docs/reference/function.so.wingwt.html#returns",
    "title": "function.so.wingwt",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Wing weight values at the input points with shape (n_samples,).",
    "crumbs": [
      "API Reference",
      "Functions",
      "wingwt"
    ]
  },
  {
    "objectID": "docs/reference/function.so.wingwt.html#examples",
    "href": "docs/reference/function.so.wingwt.html#examples",
    "title": "function.so.wingwt",
    "section": "",
    "text": "Single point evaluation (Baseline Cessna C172 - Unpainted):\n&gt;&gt;&gt; from spotoptim.function.so import wingwt\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Baseline configuration in unit cube\n&gt;&gt;&gt; x_base = np.array([0.48, 0.4, 0.38, 0.5, 0.62, 0.344, 0.4, 0.37, 0.38])\n&gt;&gt;&gt; wingwt(x_base)\narray([233.90...])\nBatch evaluation:\n&gt;&gt;&gt; X = np.vstack([x_base, x_base])\n&gt;&gt;&gt; wingwt(X)\narray([233.90..., 233.90...])",
    "crumbs": [
      "API Reference",
      "Functions",
      "wingwt"
    ]
  },
  {
    "objectID": "docs/reference/function.so.wingwt.html#references",
    "href": "docs/reference/function.so.wingwt.html#references",
    "title": "function.so.wingwt",
    "section": "",
    "text": "Forrester, A., Sobester, A., & Keane, A. (2008). Engineering design via surrogate modelling: a practical guide. John Wiley & Sons.",
    "crumbs": [
      "API Reference",
      "Functions",
      "wingwt"
    ]
  },
  {
    "objectID": "docs/reference/function.torch_objective.html",
    "href": "docs/reference/function.torch_objective.html",
    "title": "function.torch_objective",
    "section": "",
    "text": "function.torch_objective\n\n\n\n\n\nName\nDescription\n\n\n\n\nTorchObjective\nA callable objective function for SpotOptim that trains and evaluates a PyTorch model.",
    "crumbs": [
      "API Reference",
      "Functions",
      "function.torch_objective"
    ]
  },
  {
    "objectID": "docs/reference/function.torch_objective.html#classes",
    "href": "docs/reference/function.torch_objective.html#classes",
    "title": "function.torch_objective",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nTorchObjective\nA callable objective function for SpotOptim that trains and evaluates a PyTorch model.",
    "crumbs": [
      "API Reference",
      "Functions",
      "function.torch_objective"
    ]
  },
  {
    "objectID": "docs/reference/hyperparameters.parameters.html",
    "href": "docs/reference/hyperparameters.parameters.html",
    "title": "hyperparameters.parameters",
    "section": "",
    "text": "hyperparameters.parameters\n\n\n\n\n\nName\nDescription\n\n\n\n\nParameterSet\nUser-friendly interface for defining hyperparameters.",
    "crumbs": [
      "API Reference",
      "Hyperparameters",
      "hyperparameters.parameters"
    ]
  },
  {
    "objectID": "docs/reference/hyperparameters.parameters.html#classes",
    "href": "docs/reference/hyperparameters.parameters.html#classes",
    "title": "hyperparameters.parameters",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nParameterSet\nUser-friendly interface for defining hyperparameters.",
    "crumbs": [
      "API Reference",
      "Hyperparameters",
      "hyperparameters.parameters"
    ]
  },
  {
    "objectID": "docs/reference/hyperparameters.repr_helpers.Bounds.html",
    "href": "docs/reference/hyperparameters.repr_helpers.Bounds.html",
    "title": "hyperparameters.repr_helpers.Bounds",
    "section": "",
    "text": "hyperparameters.repr_helpers.Bounds\nhyperparameters.repr_helpers.Bounds(low, high)",
    "crumbs": [
      "API Reference",
      "Hyperparameters",
      "Bounds"
    ]
  },
  {
    "objectID": "docs/reference/hyperparameters.repr_helpers.html",
    "href": "docs/reference/hyperparameters.repr_helpers.html",
    "title": "hyperparameters.repr_helpers",
    "section": "",
    "text": "hyperparameters.repr_helpers\nhyperparameters.repr_helpers",
    "crumbs": [
      "API Reference",
      "Hyperparameters",
      "hyperparameters.repr_helpers"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.generate_imp.html",
    "href": "docs/reference/inspection.importance.generate_imp.html",
    "title": "inspection.importance.generate_imp",
    "section": "",
    "text": "inspection.importance.generate_imp(\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n    random_state=42,\n    n_repeats=10,\n    use_test=True,\n)\nGenerates permutation importances from a RandomForestRegressor.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX_train\npd.DataFrame or np.ndarray\nThe training feature set.\nrequired\n\n\nX_test\npd.DataFrame or np.ndarray\nThe test feature set.\nrequired\n\n\ny_train\npd.Series or np.ndarray\nThe training target variable.\nrequired\n\n\ny_test\npd.Series or np.ndarray\nThe test target variable.\nrequired\n\n\nrandom_state\nint\nRandom state for the RandomForestRegressor. Defaults to 42.\n42\n\n\nn_repeats\nint\nNumber of repeats for permutation importance. Defaults to 10.\n10\n\n\nuse_test\nbool\nIf True, computes permutation importance on the test set. If False, uses the training set. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npermutation_importance\npermutation_importance\nPermutation importances object.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.sensitivity.importance import generate_imp\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; X_train, X_test = X[:80], X[80:]\n&gt;&gt;&gt; y_train, y_test = y[:80], y[80:]\n&gt;&gt;&gt; X_train_df = pd.DataFrame(X_train)\n&gt;&gt;&gt; X_test_df = pd.DataFrame(X_test)\n&gt;&gt;&gt; y_train_series = pd.Series(y_train)\n&gt;&gt;&gt; y_test_series = pd.Series(y_test)\n&gt;&gt;&gt; perm_imp = generate_imp(X_train_df, X_test_df, y_train_series, y_test_series)\n&gt;&gt;&gt; print(perm_imp)",
    "crumbs": [
      "API Reference",
      "Inspection",
      "generate_imp"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.generate_imp.html#parameters",
    "href": "docs/reference/inspection.importance.generate_imp.html#parameters",
    "title": "inspection.importance.generate_imp",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX_train\npd.DataFrame or np.ndarray\nThe training feature set.\nrequired\n\n\nX_test\npd.DataFrame or np.ndarray\nThe test feature set.\nrequired\n\n\ny_train\npd.Series or np.ndarray\nThe training target variable.\nrequired\n\n\ny_test\npd.Series or np.ndarray\nThe test target variable.\nrequired\n\n\nrandom_state\nint\nRandom state for the RandomForestRegressor. Defaults to 42.\n42\n\n\nn_repeats\nint\nNumber of repeats for permutation importance. Defaults to 10.\n10\n\n\nuse_test\nbool\nIf True, computes permutation importance on the test set. If False, uses the training set. Defaults to True.\nTrue",
    "crumbs": [
      "API Reference",
      "Inspection",
      "generate_imp"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.generate_imp.html#returns",
    "href": "docs/reference/inspection.importance.generate_imp.html#returns",
    "title": "inspection.importance.generate_imp",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\npermutation_importance\npermutation_importance\nPermutation importances object.",
    "crumbs": [
      "API Reference",
      "Inspection",
      "generate_imp"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.generate_imp.html#examples",
    "href": "docs/reference/inspection.importance.generate_imp.html#examples",
    "title": "inspection.importance.generate_imp",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.sensitivity.importance import generate_imp\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; X_train, X_test = X[:80], X[80:]\n&gt;&gt;&gt; y_train, y_test = y[:80], y[80:]\n&gt;&gt;&gt; X_train_df = pd.DataFrame(X_train)\n&gt;&gt;&gt; X_test_df = pd.DataFrame(X_test)\n&gt;&gt;&gt; y_train_series = pd.Series(y_train)\n&gt;&gt;&gt; y_test_series = pd.Series(y_test)\n&gt;&gt;&gt; perm_imp = generate_imp(X_train_df, X_test_df, y_train_series, y_test_series)\n&gt;&gt;&gt; print(perm_imp)",
    "crumbs": [
      "API Reference",
      "Inspection",
      "generate_imp"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.plot_feature_importances.html",
    "href": "docs/reference/inspection.importance.plot_feature_importances.html",
    "title": "inspection.importance.plot_feature_importances",
    "section": "",
    "text": "inspection.importance.plot_feature_importances(\n    X,\n    y,\n    feature_names,\n    target_names,\n    target_index,\n    n_top_features=10,\n    figsize=(6, 6),\n)\nGenerate and plot feature importances using MDI and permutation importance.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput features array\nrequired\n\n\ny\nnp.ndarray\nTarget array\nrequired\n\n\nfeature_names\nlist\nList of feature names\nrequired\n\n\ntarget_names\nlist\nList of target names\nrequired\n\n\ntarget_index\nint\nIndex of target variable to analyze\nrequired\n\n\nn_top_features\nint\nNumber of top features to show\n10\n\n\nfigsize\ntuple\nSize of the figure\n(6, 6)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\ntuple\n(top_features, importance_df)\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.sensitivity import plot_feature_importances\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n&gt;&gt;&gt; target_names = [\"target\"]\n&gt;&gt;&gt; top_features, imp_df = plot_feature_importances(X, y, feature_names, target_names, target_index=0)\n&gt;&gt;&gt; print(\"Top features:\", top_features)",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_feature_importances"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.plot_feature_importances.html#parameters",
    "href": "docs/reference/inspection.importance.plot_feature_importances.html#parameters",
    "title": "inspection.importance.plot_feature_importances",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput features array\nrequired\n\n\ny\nnp.ndarray\nTarget array\nrequired\n\n\nfeature_names\nlist\nList of feature names\nrequired\n\n\ntarget_names\nlist\nList of target names\nrequired\n\n\ntarget_index\nint\nIndex of target variable to analyze\nrequired\n\n\nn_top_features\nint\nNumber of top features to show\n10\n\n\nfigsize\ntuple\nSize of the figure\n(6, 6)",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_feature_importances"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.plot_feature_importances.html#returns",
    "href": "docs/reference/inspection.importance.plot_feature_importances.html#returns",
    "title": "inspection.importance.plot_feature_importances",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ntuple\ntuple\n(top_features, importance_df)",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_feature_importances"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.plot_feature_importances.html#examples",
    "href": "docs/reference/inspection.importance.plot_feature_importances.html#examples",
    "title": "inspection.importance.plot_feature_importances",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.sensitivity import plot_feature_importances\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n&gt;&gt;&gt; target_names = [\"target\"]\n&gt;&gt;&gt; top_features, imp_df = plot_feature_importances(X, y, feature_names, target_names, target_index=0)\n&gt;&gt;&gt; print(\"Top features:\", top_features)",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_feature_importances"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.plot_importances.html",
    "href": "docs/reference/inspection.importance.plot_importances.html",
    "title": "inspection.importance.plot_importances",
    "section": "",
    "text": "inspection.importance.plot_importances(\n    df_mdi,\n    perm_imp,\n    X_test,\n    target_name=None,\n    feature_names=None,\n    k=10,\n    figsize=(12, 8),\n    show=True,\n)\nPlots the impurity-based and permutation-based feature importances for a given classifier.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf_mdi\npd.DataFrame\nDataFrame with Gini importances.\nrequired\n\n\nperm_imp\nobject\nPermutation importances object.\nrequired\n\n\nX_test\npd.DataFrame\nThe test feature set for permutation importance.\nrequired\n\n\ntarget_name\nstr\nName of the target variable for labeling. Defaults to None.\nNone\n\n\nfeature_names\nlist\nList of feature names for labeling. Defaults to None.\nNone\n\n\nk\nint\nNumber of top features to display based on importance. Default is 10.\n10\n\n\nfigsize\ntuple\nSize of the figure (width, height) in inches. Default is (12, 8).\n(12, 8)\n\n\nshow\nbool\nIf True, displays the plot immediately. Default is True.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.sensitivity.importance import generate_mdi, generate_imp, plot_importances\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; X_train, X_test = X[:80], X[80:]\n&gt;&gt;&gt; y_train, y_test = y[:80], y[80:]\n&gt;&gt;&gt; X_train_df = pd.DataFrame(X_train)\n&gt;&gt;&gt; X_test_df = pd.DataFrame(X_test)\n&gt;&gt;&gt; y_train_series = pd.Series(y_train)\n&gt;&gt;&gt; y_test_series = pd.Series(y_test)\n&gt;&gt;&gt; df_mdi = generate_mdi(X_train_df, y_train_series)\n&gt;&gt;&gt; perm_imp = generate_imp(X_train_df, X_test_df, y_train_series, y_test_series)\n&gt;&gt;&gt; plot_importances(df_mdi, perm_imp, X_test_df, figsize=(15, 10))",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_importances"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.plot_importances.html#parameters",
    "href": "docs/reference/inspection.importance.plot_importances.html#parameters",
    "title": "inspection.importance.plot_importances",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndf_mdi\npd.DataFrame\nDataFrame with Gini importances.\nrequired\n\n\nperm_imp\nobject\nPermutation importances object.\nrequired\n\n\nX_test\npd.DataFrame\nThe test feature set for permutation importance.\nrequired\n\n\ntarget_name\nstr\nName of the target variable for labeling. Defaults to None.\nNone\n\n\nfeature_names\nlist\nList of feature names for labeling. Defaults to None.\nNone\n\n\nk\nint\nNumber of top features to display based on importance. Default is 10.\n10\n\n\nfigsize\ntuple\nSize of the figure (width, height) in inches. Default is (12, 8).\n(12, 8)\n\n\nshow\nbool\nIf True, displays the plot immediately. Default is True.\nTrue",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_importances"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.plot_importances.html#returns",
    "href": "docs/reference/inspection.importance.plot_importances.html#returns",
    "title": "inspection.importance.plot_importances",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_importances"
    ]
  },
  {
    "objectID": "docs/reference/inspection.importance.plot_importances.html#examples",
    "href": "docs/reference/inspection.importance.plot_importances.html#examples",
    "title": "inspection.importance.plot_importances",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.sensitivity.importance import generate_mdi, generate_imp, plot_importances\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; X_train, X_test = X[:80], X[80:]\n&gt;&gt;&gt; y_train, y_test = y[:80], y[80:]\n&gt;&gt;&gt; X_train_df = pd.DataFrame(X_train)\n&gt;&gt;&gt; X_test_df = pd.DataFrame(X_test)\n&gt;&gt;&gt; y_train_series = pd.Series(y_train)\n&gt;&gt;&gt; y_test_series = pd.Series(y_test)\n&gt;&gt;&gt; df_mdi = generate_mdi(X_train_df, y_train_series)\n&gt;&gt;&gt; perm_imp = generate_imp(X_train_df, X_test_df, y_train_series, y_test_series)\n&gt;&gt;&gt; plot_importances(df_mdi, perm_imp, X_test_df, figsize=(15, 10))",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_importances"
    ]
  },
  {
    "objectID": "docs/reference/inspection.predictions.plot_actual_vs_predicted.html",
    "href": "docs/reference/inspection.predictions.plot_actual_vs_predicted.html",
    "title": "inspection.predictions.plot_actual_vs_predicted",
    "section": "",
    "text": "inspection.predictions.plot_actual_vs_predicted(\n    y_test,\n    y_pred,\n    title=None,\n    show=True,\n    filename=None,\n)\nPlot actual vs. predicted values.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ny_test\nnp.ndarray\nTrue values.\nrequired\n\n\ny_pred\nnp.ndarray\nPredicted values.\nrequired\n\n\ntitle\nstr\nTitle of the plot. Defaults to None.\nNone\n\n\nshow\nbool\nIf True, the plot is shown. Defaults to True.\nTrue\n\n\nfilename\nstr\nName of the file to save the plot. Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNoneType\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n    from sklearn.linear_model import LinearRegression\n    from spotoptim.inspection import plot_actual_vs_predicted\n    X, y = load_diabetes(return_X_y=True)\n    lr = LinearRegression()\n    lr.fit(X, y)\n    y_pred = lr.predict(X)\n    plot_actual_vs_predicted(y, y_pred)",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_actual_vs_predicted"
    ]
  },
  {
    "objectID": "docs/reference/inspection.predictions.plot_actual_vs_predicted.html#parameters",
    "href": "docs/reference/inspection.predictions.plot_actual_vs_predicted.html#parameters",
    "title": "inspection.predictions.plot_actual_vs_predicted",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ny_test\nnp.ndarray\nTrue values.\nrequired\n\n\ny_pred\nnp.ndarray\nPredicted values.\nrequired\n\n\ntitle\nstr\nTitle of the plot. Defaults to None.\nNone\n\n\nshow\nbool\nIf True, the plot is shown. Defaults to True.\nTrue\n\n\nfilename\nstr\nName of the file to save the plot. Defaults to None.\nNone",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_actual_vs_predicted"
    ]
  },
  {
    "objectID": "docs/reference/inspection.predictions.plot_actual_vs_predicted.html#returns",
    "href": "docs/reference/inspection.predictions.plot_actual_vs_predicted.html#returns",
    "title": "inspection.predictions.plot_actual_vs_predicted",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNoneType\nNone",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_actual_vs_predicted"
    ]
  },
  {
    "objectID": "docs/reference/inspection.predictions.plot_actual_vs_predicted.html#examples",
    "href": "docs/reference/inspection.predictions.plot_actual_vs_predicted.html#examples",
    "title": "inspection.predictions.plot_actual_vs_predicted",
    "section": "",
    "text": "&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n    from sklearn.linear_model import LinearRegression\n    from spotoptim.inspection import plot_actual_vs_predicted\n    X, y = load_diabetes(return_X_y=True)\n    lr = LinearRegression()\n    lr.fit(X, y)\n    y_pred = lr.predict(X)\n    plot_actual_vs_predicted(y, y_pred)",
    "crumbs": [
      "API Reference",
      "Inspection",
      "plot_actual_vs_predicted"
    ]
  },
  {
    "objectID": "docs/reference/inspection.html",
    "href": "docs/reference/inspection.html",
    "title": "inspection",
    "section": "",
    "text": "inspection\ninspection\nInspection (sensitivity analysis) module for spotoptim.\nProvides feature importance utilities based on impurity (MDI) and permutation importance, plus plotting helpers.",
    "crumbs": [
      "API Reference",
      "Inspection",
      "inspection"
    ]
  },
  {
    "objectID": "docs/reference/mo.mo_mm.mo_mm_desirability_optimizer.html",
    "href": "docs/reference/mo.mo_mm.mo_mm_desirability_optimizer.html",
    "title": "mo.mo_mm.mo_mm_desirability_optimizer",
    "section": "",
    "text": "mo.mo_mm.mo_mm_desirability_optimizer(\n    X_base,\n    models,\n    bounds,\n    obj_func,\n    **kwargs,\n)\nOptimizes the multi-objective function to find the next best point. Returns the best point, its desirability, and the history of objective values.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX_base\nnp.ndarray\nExisting design points.\nrequired\n\n\nmodels\nlist\nList of trained surrogate models for each objective.\nrequired\n\n\nbounds\nlist\nBounds for each dimension.\nrequired\n\n\nobj_func\ncallable\nObjective function to compute desirability and objectives.\nrequired\n\n\n**kwargs\nAny\nAdditional arguments for the objective function.\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[np.ndarray, float, np.ndarray]\nTuple[np.ndarray, float, np.ndarray]: A tuple containing: - Best point (np.ndarray) - Best desirability (float) - History of objective values (np.ndarray)",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_mm_desirability_optimizer"
    ]
  },
  {
    "objectID": "docs/reference/mo.mo_mm.mo_mm_desirability_optimizer.html#parameters",
    "href": "docs/reference/mo.mo_mm.mo_mm_desirability_optimizer.html#parameters",
    "title": "mo.mo_mm.mo_mm_desirability_optimizer",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX_base\nnp.ndarray\nExisting design points.\nrequired\n\n\nmodels\nlist\nList of trained surrogate models for each objective.\nrequired\n\n\nbounds\nlist\nBounds for each dimension.\nrequired\n\n\nobj_func\ncallable\nObjective function to compute desirability and objectives.\nrequired\n\n\n**kwargs\nAny\nAdditional arguments for the objective function.\n{}",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_mm_desirability_optimizer"
    ]
  },
  {
    "objectID": "docs/reference/mo.mo_mm.mo_mm_desirability_optimizer.html#returns",
    "href": "docs/reference/mo.mo_mm.mo_mm_desirability_optimizer.html#returns",
    "title": "mo.mo_mm.mo_mm_desirability_optimizer",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nTuple[np.ndarray, float, np.ndarray]\nTuple[np.ndarray, float, np.ndarray]: A tuple containing: - Best point (np.ndarray) - Best desirability (float) - History of objective values (np.ndarray)",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_mm_desirability_optimizer"
    ]
  },
  {
    "objectID": "docs/reference/mo.mo_mm.html",
    "href": "docs/reference/mo.mo_mm.html",
    "title": "mo.mo_mm",
    "section": "",
    "text": "mo.mo_mm\n\n\n\n\n\nName\nDescription\n\n\n\n\nmo_mm_desirability_function\nCalculates the negative combined desirability for a candidate point x. Can be used by the mo_mm_desirability_optimizer.\n\n\nmo_mm_desirability_optimizer\nOptimizes the multi-objective function to find the next best point.\n\n\nmo_xy_desirability_plot\nGenerates a plot of the desirability landscape.",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo.mo_mm"
    ]
  },
  {
    "objectID": "docs/reference/mo.mo_mm.html#functions",
    "href": "docs/reference/mo.mo_mm.html#functions",
    "title": "mo.mo_mm",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nmo_mm_desirability_function\nCalculates the negative combined desirability for a candidate point x. Can be used by the mo_mm_desirability_optimizer.\n\n\nmo_mm_desirability_optimizer\nOptimizes the multi-objective function to find the next best point.\n\n\nmo_xy_desirability_plot\nGenerates a plot of the desirability landscape.",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo.mo_mm"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.mo_pareto_optx_plot.html",
    "href": "docs/reference/mo.pareto.mo_pareto_optx_plot.html",
    "title": "mo.pareto.mo_pareto_optx_plot",
    "section": "",
    "text": "mo.pareto.mo_pareto_optx_plot(\n    X,\n    Y,\n    minimize=True,\n    feature_names=None,\n    target_names=None,\n    **kwargs,\n)\nVisualizes the Pareto-optimal points in the input space for each pair of inputs x_i and x_j (with i &lt; j) and each objective f_k.\nPlots are placed on a grid where rows correspond to input pairs and columns correspond to objectives.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nAn (N,D) array of input points, where N is the number of points and D is the number of variables (dimensions).\nrequired\n\n\nY\nnp.ndarray\nAn (N,M) array of objective values, where N is the number of points and M is the number of objectives.\nrequired\n\n\nminimize\nbool\nIf True, assumes minimization of objectives. Defaults to True.\nTrue\n\n\nfeature_names\nlist\nList of names for the input variables. Defaults to None.\nNone\n\n\ntarget_names\nlist\nList of names for the objectives. Defaults to None.\nNone\n\n\n**kwargs\nAny\nAdditional arguments passed to plt.subplots (e.g., figsize).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.mo.pareto import mo_pareto_optx_plot\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; Y = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; mo_pareto_optx_plot(X, Y)",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_pareto_optx_plot"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.mo_pareto_optx_plot.html#parameters",
    "href": "docs/reference/mo.pareto.mo_pareto_optx_plot.html#parameters",
    "title": "mo.pareto.mo_pareto_optx_plot",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nAn (N,D) array of input points, where N is the number of points and D is the number of variables (dimensions).\nrequired\n\n\nY\nnp.ndarray\nAn (N,M) array of objective values, where N is the number of points and M is the number of objectives.\nrequired\n\n\nminimize\nbool\nIf True, assumes minimization of objectives. Defaults to True.\nTrue\n\n\nfeature_names\nlist\nList of names for the input variables. Defaults to None.\nNone\n\n\ntarget_names\nlist\nList of names for the objectives. Defaults to None.\nNone\n\n\n**kwargs\nAny\nAdditional arguments passed to plt.subplots (e.g., figsize).\n{}",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_pareto_optx_plot"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.mo_pareto_optx_plot.html#returns",
    "href": "docs/reference/mo.pareto.mo_pareto_optx_plot.html#returns",
    "title": "mo.pareto.mo_pareto_optx_plot",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_pareto_optx_plot"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.mo_pareto_optx_plot.html#examples",
    "href": "docs/reference/mo.pareto.mo_pareto_optx_plot.html#examples",
    "title": "mo.pareto.mo_pareto_optx_plot",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.mo.pareto import mo_pareto_optx_plot\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; Y = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; mo_pareto_optx_plot(X, Y)",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_pareto_optx_plot"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.mo_xy_surface.html",
    "href": "docs/reference/mo.pareto.mo_xy_surface.html",
    "title": "mo.pareto.mo_xy_surface",
    "section": "",
    "text": "mo.pareto.mo_xy_surface(\n    models,\n    bounds,\n    target_names=None,\n    feature_names=None,\n    resolution=50,\n    feature_pairs=None,\n    **kwargs,\n)\nGenerates surface plots of every combination of two input variables x_i and x_j (where i &lt; j) and for each of the multiple objectives f_k.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodels\nlist\nList of trained models (one per objective).\nrequired\n\n\nbounds\nlist\nList of tuples (min, max) for each input variable.\nrequired\n\n\ntarget_names\nlist\nList of names for the objectives. Defaults to None.\nNone\n\n\nfeature_names\nlist\nList of names for the input variables. Defaults to None.\nNone\n\n\nresolution\nint\nGrid resolution for the surface plot. Defaults to 50.\n50\n\n\nfeature_pairs\nlist\nList of tuples (i, j) specifying which feature pairs to plot. If None, all combinations are plotted. Defaults to None.\nNone\n\n\n**kwargs\nAny\nAdditional keyword arguments passed to plt.subplots (e.g., figsize).\n{}\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotoptim.mo.pareto import mo_xy_surface\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Train dummy models\n&gt;&gt;&gt; X = np.random.rand(10, 2)\n&gt;&gt;&gt; y1 = X[:, 0] + X[:, 1]\n&gt;&gt;&gt; y2 = X[:, 0] * X[:, 1]\n&gt;&gt;&gt; m1 = RandomForestRegressor().fit(X, y1)\n&gt;&gt;&gt; m2 = RandomForestRegressor().fit(X, y2)\n&gt;&gt;&gt; # Plot\n&gt;&gt;&gt; mo_xy_surface([m1, m2], bounds=[(0, 1), (0, 1)], target_names=[\"Sum\", \"Prod\"])",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_xy_surface"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.mo_xy_surface.html#parameters",
    "href": "docs/reference/mo.pareto.mo_xy_surface.html#parameters",
    "title": "mo.pareto.mo_xy_surface",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmodels\nlist\nList of trained models (one per objective).\nrequired\n\n\nbounds\nlist\nList of tuples (min, max) for each input variable.\nrequired\n\n\ntarget_names\nlist\nList of names for the objectives. Defaults to None.\nNone\n\n\nfeature_names\nlist\nList of names for the input variables. Defaults to None.\nNone\n\n\nresolution\nint\nGrid resolution for the surface plot. Defaults to 50.\n50\n\n\nfeature_pairs\nlist\nList of tuples (i, j) specifying which feature pairs to plot. If None, all combinations are plotted. Defaults to None.\nNone\n\n\n**kwargs\nAny\nAdditional keyword arguments passed to plt.subplots (e.g., figsize).\n{}",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_xy_surface"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.mo_xy_surface.html#returns",
    "href": "docs/reference/mo.pareto.mo_xy_surface.html#returns",
    "title": "mo.pareto.mo_xy_surface",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_xy_surface"
    ]
  },
  {
    "objectID": "docs/reference/mo.pareto.mo_xy_surface.html#examples",
    "href": "docs/reference/mo.pareto.mo_xy_surface.html#examples",
    "title": "mo.pareto.mo_xy_surface",
    "section": "",
    "text": "&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotoptim.mo.pareto import mo_xy_surface\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Train dummy models\n&gt;&gt;&gt; X = np.random.rand(10, 2)\n&gt;&gt;&gt; y1 = X[:, 0] + X[:, 1]\n&gt;&gt;&gt; y2 = X[:, 0] * X[:, 1]\n&gt;&gt;&gt; m1 = RandomForestRegressor().fit(X, y1)\n&gt;&gt;&gt; m2 = RandomForestRegressor().fit(X, y2)\n&gt;&gt;&gt; # Plot\n&gt;&gt;&gt; mo_xy_surface([m1, m2], bounds=[(0, 1), (0, 1)], target_names=[\"Sum\", \"Prod\"])",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo_xy_surface"
    ]
  },
  {
    "objectID": "docs/reference/mo.html",
    "href": "docs/reference/mo.html",
    "title": "mo",
    "section": "",
    "text": "mo\nmo",
    "crumbs": [
      "API Reference",
      "Multi-Objective",
      "mo"
    ]
  },
  {
    "objectID": "docs/reference/nn.linear_regressor.html",
    "href": "docs/reference/nn.linear_regressor.html",
    "title": "nn.linear_regressor",
    "section": "",
    "text": "nn.linear_regressor\n\n\n\n\n\nName\nDescription\n\n\n\n\nLinearRegressor\nPyTorch neural network for regression with configurable architecture.",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "nn.linear_regressor"
    ]
  },
  {
    "objectID": "docs/reference/nn.linear_regressor.html#classes",
    "href": "docs/reference/nn.linear_regressor.html#classes",
    "title": "nn.linear_regressor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nLinearRegressor\nPyTorch neural network for regression with configurable architecture.",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "nn.linear_regressor"
    ]
  },
  {
    "objectID": "docs/reference/nn.mlp.html",
    "href": "docs/reference/nn.mlp.html",
    "title": "nn.mlp",
    "section": "",
    "text": "nn.mlp\n\n\n\n\n\nName\nDescription\n\n\n\n\nMLP\nThis block implements the multi-layer perceptron (MLP) module.",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "nn.mlp"
    ]
  },
  {
    "objectID": "docs/reference/nn.mlp.html#classes",
    "href": "docs/reference/nn.mlp.html#classes",
    "title": "nn.mlp",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nMLP\nThis block implements the multi-layer perceptron (MLP) module.",
    "crumbs": [
      "API Reference",
      "Neural Networks",
      "nn.mlp"
    ]
  },
  {
    "objectID": "docs/reference/optimizer.html",
    "href": "docs/reference/optimizer.html",
    "title": "optimizer",
    "section": "",
    "text": "optimizer\noptimizer",
    "crumbs": [
      "API Reference",
      "Optimization",
      "optimizer"
    ]
  },
  {
    "objectID": "docs/reference/optimizer.schedule_free.html",
    "href": "docs/reference/optimizer.schedule_free.html",
    "title": "optimizer.schedule_free",
    "section": "",
    "text": "optimizer.schedule_free\n\n\n\n\n\nName\nDescription\n\n\n\n\nAdamWScheduleFree\nSchedule-Free AdamW in PyTorch.",
    "crumbs": [
      "API Reference",
      "Optimization",
      "optimizer.schedule_free"
    ]
  },
  {
    "objectID": "docs/reference/optimizer.schedule_free.html#classes",
    "href": "docs/reference/optimizer.schedule_free.html#classes",
    "title": "optimizer.schedule_free",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nAdamWScheduleFree\nSchedule-Free AdamW in PyTorch.",
    "crumbs": [
      "API Reference",
      "Optimization",
      "optimizer.schedule_free"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.mo_generate_plot_grid.html",
    "href": "docs/reference/plot.contour.mo_generate_plot_grid.html",
    "title": "plot.contour.mo_generate_plot_grid",
    "section": "",
    "text": "plot.contour.mo_generate_plot_grid(variables, resolutions, functions)\nGenerate a grid of input variables and apply objective functions.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvariables\ndict\nA dictionary where keys are variable names (e.g., “time”, “temperature”) and values are tuples of (min_value, max_value).\nrequired\n\n\nresolutions\ndict\nA dictionary where keys are variable names and values are the number of points.\nrequired\n\n\nfunctions\ndict\nA dictionary where keys are function names and values are callable functions.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: A DataFrame containing the grid and the results of the objective functions.",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "mo_generate_plot_grid"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.mo_generate_plot_grid.html#parameters",
    "href": "docs/reference/plot.contour.mo_generate_plot_grid.html#parameters",
    "title": "plot.contour.mo_generate_plot_grid",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nvariables\ndict\nA dictionary where keys are variable names (e.g., “time”, “temperature”) and values are tuples of (min_value, max_value).\nrequired\n\n\nresolutions\ndict\nA dictionary where keys are variable names and values are the number of points.\nrequired\n\n\nfunctions\ndict\nA dictionary where keys are function names and values are callable functions.\nrequired",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "mo_generate_plot_grid"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.mo_generate_plot_grid.html#returns",
    "href": "docs/reference/plot.contour.mo_generate_plot_grid.html#returns",
    "title": "plot.contour.mo_generate_plot_grid",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: A DataFrame containing the grid and the results of the objective functions.",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "mo_generate_plot_grid"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.html",
    "href": "docs/reference/plot.contour.html",
    "title": "plot.contour",
    "section": "",
    "text": "plot.contour\n\n\n\n\n\nName\nDescription\n\n\n\n\ncontourf_plot\nCreates contour plots (single or faceted) using matplotlib.\n\n\nmo_generate_plot_grid\nGenerate a grid of input variables and apply objective functions.\n\n\nplotModel\nGenerate 2D contour and optionally 3D surface plots for a model’s predictions.\n\n\nsimple_contour\nSimple contour plot",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot.contour"
    ]
  },
  {
    "objectID": "docs/reference/plot.contour.html#functions",
    "href": "docs/reference/plot.contour.html#functions",
    "title": "plot.contour",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncontourf_plot\nCreates contour plots (single or faceted) using matplotlib.\n\n\nmo_generate_plot_grid\nGenerate a grid of input variables and apply objective functions.\n\n\nplotModel\nGenerate 2D contour and optionally 3D surface plots for a model’s predictions.\n\n\nsimple_contour\nSimple contour plot",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot.contour"
    ]
  },
  {
    "objectID": "docs/reference/plot.mo.plot_mo.html",
    "href": "docs/reference/plot.mo.plot_mo.html",
    "title": "plot.mo.plot_mo",
    "section": "",
    "text": "plot.mo.plot_mo(\n    target_names,\n    combinations,\n    pareto,\n    y_rf=None,\n    pareto_front=False,\n    y_best=None,\n    y_add=None,\n    y_add2=None,\n    y_add_color='blue',\n    y_add2_color='green',\n    title='',\n    y_orig=None,\n    pareto_front_orig=False,\n    pareto_label=False,\n    y_rf_color='blue',\n    y_best_color='red',\n    x_axis_transformation='id',\n    y_axis_transformation='id',\n    y_best_label='Best',\n    y_add_label='Add',\n    y_add2_label='Add2',\n    filename=None,\n    figsize=(9, 6),\n)\nGenerates scatter plots for each combination of two targets from a multi-output prediction while highlighting Pareto optimal points.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ny_rf\nnp.ndarray\nThe predicted target values with shape (n_samples, n_targets).\nNone\n\n\ntarget_names\nlist\nA list of target names corresponding to the columns of y_rf.\nrequired\n\n\ncombinations\nlist\nA list of tuples, where each tuple contains the indices of the target combinations to plot.\nrequired\n\n\npareto\nstr\nSpecifies whether to compute Pareto front based on ‘min’ or ‘max’ criterion.\nrequired\n\n\npareto_front\nbool\nIf True, connect Pareto optimal points with a red line for y_rf.\nFalse\n\n\ny_best\nnp.ndarray\nA NumPy array representing the best point to highlight in red. Defaults to None.\nNone\n\n\ny_add\nnp.ndarray\nA NumPy array representing the additional points to highlight in blue. Defaults to None.\nNone\n\n\ny_add2\nnp.ndarray\nA NumPy array representing the additional points to highlight in green. Defaults to None.\nNone\n\n\ny_add_color\nstr\nThe color of the additional points. Defaults to “blue”.\n'blue'\n\n\ny_add2_color\nstr\nThe color of the additional points. Defaults to “green”.\n'green'\n\n\ny_best_label\nstr\nThe label for the best point. Defaults to “Best”.\n'Best'\n\n\ny_add_label\nstr\nThe label for the additional points. Defaults to “Add”.\n'Add'\n\n\ny_add2_label\nstr\nThe label for the additional points. Defaults to “Add2”.\n'Add2'\n\n\ntitle\nstr\nThe title of the plot. Defaults to “” (empty string).\n''\n\n\ny_orig\nnp.ndarray\nThe original target values with shape (n_samples, n_targets). Defaults to None.\nNone\n\n\npareto_front_orig\nbool\nIf True, connect Pareto optimal points with a light blue line for y_orig. Defaults to False.\nFalse\n\n\npareto_label\nbool\nIf True, label Pareto points with their index. Defaults to False.\nFalse\n\n\ny_rf_color\nstr\nThe color of the predicted points. Defaults to “blue”.\n'blue'\n\n\ny_best_color\nstr\nThe color of the best point. Defaults to “red”.\n'red'\n\n\nx_axis_transformation\nstr\nTransformation for the x-axis. Options are “id” (linear), “log” (logarithmic), and “loglog” (log-log). Defaults to “id”.\n'id'\n\n\ny_axis_transformation\nstr\nTransformation for the y-axis. Options are “id” (linear), “log” (logarithmic), and “loglog” (log-log). Defaults to “id”.\n'id'\n\n\nfilename\nstr\nIf provided, saves the plot to the specified file. Supports “pdf” and “png” formats. Defaults to None.\nNone\n\n\nfigsize\ntuple\nFigure size (width, height) in inches. Default is (9, 6).\n(9, 6)\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nNone\nNone\nDisplays or saves the plot.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.plot.mo import plot_mo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; target_names = [\"Target 1\", \"Target 2\"]\n&gt;&gt;&gt; combinations = [(0, 1)]\n&gt;&gt;&gt; pareto = \"min\"\n&gt;&gt;&gt; y_rf = np.random.rand(100, 2)\n&gt;&gt;&gt; y_orig = np.random.rand(100, 2)\n&gt;&gt;&gt; plot_mo(target_names, combinations, pareto, y_rf=y_rf, y_orig=y_orig, filename=\"plot.png\")",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_mo"
    ]
  },
  {
    "objectID": "docs/reference/plot.mo.plot_mo.html#parameters",
    "href": "docs/reference/plot.mo.plot_mo.html#parameters",
    "title": "plot.mo.plot_mo",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ny_rf\nnp.ndarray\nThe predicted target values with shape (n_samples, n_targets).\nNone\n\n\ntarget_names\nlist\nA list of target names corresponding to the columns of y_rf.\nrequired\n\n\ncombinations\nlist\nA list of tuples, where each tuple contains the indices of the target combinations to plot.\nrequired\n\n\npareto\nstr\nSpecifies whether to compute Pareto front based on ‘min’ or ‘max’ criterion.\nrequired\n\n\npareto_front\nbool\nIf True, connect Pareto optimal points with a red line for y_rf.\nFalse\n\n\ny_best\nnp.ndarray\nA NumPy array representing the best point to highlight in red. Defaults to None.\nNone\n\n\ny_add\nnp.ndarray\nA NumPy array representing the additional points to highlight in blue. Defaults to None.\nNone\n\n\ny_add2\nnp.ndarray\nA NumPy array representing the additional points to highlight in green. Defaults to None.\nNone\n\n\ny_add_color\nstr\nThe color of the additional points. Defaults to “blue”.\n'blue'\n\n\ny_add2_color\nstr\nThe color of the additional points. Defaults to “green”.\n'green'\n\n\ny_best_label\nstr\nThe label for the best point. Defaults to “Best”.\n'Best'\n\n\ny_add_label\nstr\nThe label for the additional points. Defaults to “Add”.\n'Add'\n\n\ny_add2_label\nstr\nThe label for the additional points. Defaults to “Add2”.\n'Add2'\n\n\ntitle\nstr\nThe title of the plot. Defaults to “” (empty string).\n''\n\n\ny_orig\nnp.ndarray\nThe original target values with shape (n_samples, n_targets). Defaults to None.\nNone\n\n\npareto_front_orig\nbool\nIf True, connect Pareto optimal points with a light blue line for y_orig. Defaults to False.\nFalse\n\n\npareto_label\nbool\nIf True, label Pareto points with their index. Defaults to False.\nFalse\n\n\ny_rf_color\nstr\nThe color of the predicted points. Defaults to “blue”.\n'blue'\n\n\ny_best_color\nstr\nThe color of the best point. Defaults to “red”.\n'red'\n\n\nx_axis_transformation\nstr\nTransformation for the x-axis. Options are “id” (linear), “log” (logarithmic), and “loglog” (log-log). Defaults to “id”.\n'id'\n\n\ny_axis_transformation\nstr\nTransformation for the y-axis. Options are “id” (linear), “log” (logarithmic), and “loglog” (log-log). Defaults to “id”.\n'id'\n\n\nfilename\nstr\nIf provided, saves the plot to the specified file. Supports “pdf” and “png” formats. Defaults to None.\nNone\n\n\nfigsize\ntuple\nFigure size (width, height) in inches. Default is (9, 6).\n(9, 6)",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_mo"
    ]
  },
  {
    "objectID": "docs/reference/plot.mo.plot_mo.html#returns",
    "href": "docs/reference/plot.mo.plot_mo.html#returns",
    "title": "plot.mo.plot_mo",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nNone\nNone\nDisplays or saves the plot.",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_mo"
    ]
  },
  {
    "objectID": "docs/reference/plot.mo.plot_mo.html#examples",
    "href": "docs/reference/plot.mo.plot_mo.html#examples",
    "title": "plot.mo.plot_mo",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.plot.mo import plot_mo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; target_names = [\"Target 1\", \"Target 2\"]\n&gt;&gt;&gt; combinations = [(0, 1)]\n&gt;&gt;&gt; pareto = \"min\"\n&gt;&gt;&gt; y_rf = np.random.rand(100, 2)\n&gt;&gt;&gt; y_orig = np.random.rand(100, 2)\n&gt;&gt;&gt; plot_mo(target_names, combinations, pareto, y_rf=y_rf, y_orig=y_orig, filename=\"plot.png\")",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_mo"
    ]
  },
  {
    "objectID": "docs/reference/plot.html",
    "href": "docs/reference/plot.html",
    "title": "plot",
    "section": "",
    "text": "plot\nplot",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.plot_progress.html",
    "href": "docs/reference/plot.visualization.plot_progress.html",
    "title": "plot.visualization.plot_progress",
    "section": "",
    "text": "plot.visualization.plot_progress(\n    optimizer,\n    show=True,\n    log_y=False,\n    figsize=(10, 6),\n    ylabel='Objective Value',\n    mo=False,\n)\nPlot optimization progress showing all evaluations and best-so-far curve.\nThis method visualizes the optimization history, displaying both individual function evaluations and the cumulative best value found. Initial design points are shown as individual scatter points with a light grey background region, while sequential optimization iterations are connected with lines.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noptimizer\nobject\nSpotOptim instance containing optimization data.\nrequired\n\n\nshow\nbool\nWhether to display the plot. Defaults to True.\nTrue\n\n\nlog_y\nbool\nWhether to use log scale for y-axis. Defaults to False.\nFalse\n\n\nfigsize\ntuple\nFigure size as (width, height). Defaults to (10, 6).\n(10, 6)\n\n\nylabel\nstr\nLabel for y-axis. Defaults to “Objective Value”.\n'Objective Value'\n\n\nmo\nbool\nWhether to plot individual objectives if available. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf optimization hasn’t been run yet.\n\n\n\nImportError\nIf matplotlib is not installed.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; from spotoptim.plot.visualization import plot_progress\n&gt;&gt;&gt;\n&gt;&gt;&gt; def sphere(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize and run optimizer\n&gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5)]*2,\n...                 max_iter=20, n_initial=10, seed=42)\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Plot optimization progress (linear scale)\n&gt;&gt;&gt; plot_progress(opt, log_y=False, show=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Plot with log scale\n&gt;&gt;&gt; plot_progress(opt, log_y=True, show=False)",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_progress"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.plot_progress.html#parameters",
    "href": "docs/reference/plot.visualization.plot_progress.html#parameters",
    "title": "plot.visualization.plot_progress",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\noptimizer\nobject\nSpotOptim instance containing optimization data.\nrequired\n\n\nshow\nbool\nWhether to display the plot. Defaults to True.\nTrue\n\n\nlog_y\nbool\nWhether to use log scale for y-axis. Defaults to False.\nFalse\n\n\nfigsize\ntuple\nFigure size as (width, height). Defaults to (10, 6).\n(10, 6)\n\n\nylabel\nstr\nLabel for y-axis. Defaults to “Objective Value”.\n'Objective Value'\n\n\nmo\nbool\nWhether to plot individual objectives if available. Defaults to False.\nFalse",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_progress"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.plot_progress.html#raises",
    "href": "docs/reference/plot.visualization.plot_progress.html#raises",
    "title": "plot.visualization.plot_progress",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf optimization hasn’t been run yet.\n\n\n\nImportError\nIf matplotlib is not installed.",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_progress"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.plot_progress.html#examples",
    "href": "docs/reference/plot.visualization.plot_progress.html#examples",
    "title": "plot.visualization.plot_progress",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; from spotoptim.plot.visualization import plot_progress\n&gt;&gt;&gt;\n&gt;&gt;&gt; def sphere(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize and run optimizer\n&gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5)]*2,\n...                 max_iter=20, n_initial=10, seed=42)\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Plot optimization progress (linear scale)\n&gt;&gt;&gt; plot_progress(opt, log_y=False, show=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Plot with log scale\n&gt;&gt;&gt; plot_progress(opt, log_y=True, show=False)",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot_progress"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.html",
    "href": "docs/reference/plot.visualization.html",
    "title": "plot.visualization",
    "section": "",
    "text": "plot.visualization\n\n\n\n\n\nName\nDescription\n\n\n\n\nplot_important_hyperparameter_contour\nPlot surrogate contours for all combinations of the top max_imp important parameters.\n\n\nplot_progress\nPlot optimization progress showing all evaluations and best-so-far curve.\n\n\nplot_surrogate\nPlot the surrogate model for two dimensions.",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot.visualization"
    ]
  },
  {
    "objectID": "docs/reference/plot.visualization.html#functions",
    "href": "docs/reference/plot.visualization.html#functions",
    "title": "plot.visualization",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nplot_important_hyperparameter_contour\nPlot surrogate contours for all combinations of the top max_imp important parameters.\n\n\nplot_progress\nPlot optimization progress showing all evaluations and best-so-far curve.\n\n\nplot_surrogate\nPlot the surrogate model for two dimensions.",
    "crumbs": [
      "API Reference",
      "Plotting & Visualization",
      "plot.visualization"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_clustered_design.html",
    "href": "docs/reference/sampling.design.generate_clustered_design.html",
    "title": "sampling.design.generate_clustered_design",
    "section": "",
    "text": "sampling.design.generate_clustered_design(\n    bounds,\n    n_design,\n    n_clusters,\n    seed=None,\n)\nGenerates a clustered design.\nGenerates clusters of points using sklearn.datasets.make_blobs. Points are scaled to the provided bounds.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbounds\nUnion[List[Tuple[float, float]], np.ndarray]\nDesign space bounds.\nrequired\n\n\nn_design\nint\nThe number of points to generate.\nrequired\n\n\nn_clusters\nint\nThe number of clusters.\nrequired\n\n\nseed\nOptional[Union[int, Generator]]\nRandom seed or generator.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 2D array of shape (n_design, n_dim) with clustered points.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_clustered_design\n&gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n&gt;&gt;&gt; X = generate_clustered_design(bounds, n_design=5, n_clusters=2, seed=42)\n&gt;&gt;&gt; X.shape\n(5, 2)\n&gt;&gt;&gt; np.all((X &gt;= [-5, 0]) & (X &lt;= [5, 10]))\nTrue",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_clustered_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_clustered_design.html#parameters",
    "href": "docs/reference/sampling.design.generate_clustered_design.html#parameters",
    "title": "sampling.design.generate_clustered_design",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbounds\nUnion[List[Tuple[float, float]], np.ndarray]\nDesign space bounds.\nrequired\n\n\nn_design\nint\nThe number of points to generate.\nrequired\n\n\nn_clusters\nint\nThe number of clusters.\nrequired\n\n\nseed\nOptional[Union[int, Generator]]\nRandom seed or generator.\nNone",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_clustered_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_clustered_design.html#returns",
    "href": "docs/reference/sampling.design.generate_clustered_design.html#returns",
    "title": "sampling.design.generate_clustered_design",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 2D array of shape (n_design, n_dim) with clustered points.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_clustered_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_clustered_design.html#examples",
    "href": "docs/reference/sampling.design.generate_clustered_design.html#examples",
    "title": "sampling.design.generate_clustered_design",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_clustered_design\n&gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n&gt;&gt;&gt; X = generate_clustered_design(bounds, n_design=5, n_clusters=2, seed=42)\n&gt;&gt;&gt; X.shape\n(5, 2)\n&gt;&gt;&gt; np.all((X &gt;= [-5, 0]) & (X &lt;= [5, 10]))\nTrue",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_clustered_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_grid_design.html",
    "href": "docs/reference/sampling.design.generate_grid_design.html",
    "title": "sampling.design.generate_grid_design",
    "section": "",
    "text": "sampling.design.generate_grid_design(bounds, n_design, seed=None)\nGenerates a regular grid design.\nPoints are generated by creating a regular grid where the number of points per dimension is derived from n_design (floor(n_design^(1/n_dim))).\nNote: The actual number of points returned might be less than n_design if n_design is not a perfect power of n_dim.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbounds\nUnion[List[Tuple[float, float]], np.ndarray]\nDesign space bounds.\nrequired\n\n\nn_design\nint\nThe target number of points. Used to determine points per dimension.\nrequired\n\n\nseed\nOptional[Union[int, Generator]]\nUnused, kept for API consistency.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 2D array of shape (points_per_dim^n_dim, n_dim) with grid points.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_grid_design\n&gt;&gt;&gt; bounds = [(0, 1), (0, 1)]\n&gt;&gt;&gt; X = generate_grid_design(bounds, n_design=25) # 5^2 = 25\n&gt;&gt;&gt; X.shape\n(25, 2)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_grid_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_grid_design.html#parameters",
    "href": "docs/reference/sampling.design.generate_grid_design.html#parameters",
    "title": "sampling.design.generate_grid_design",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbounds\nUnion[List[Tuple[float, float]], np.ndarray]\nDesign space bounds.\nrequired\n\n\nn_design\nint\nThe target number of points. Used to determine points per dimension.\nrequired\n\n\nseed\nOptional[Union[int, Generator]]\nUnused, kept for API consistency.\nNone",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_grid_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_grid_design.html#returns",
    "href": "docs/reference/sampling.design.generate_grid_design.html#returns",
    "title": "sampling.design.generate_grid_design",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 2D array of shape (points_per_dim^n_dim, n_dim) with grid points.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_grid_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_grid_design.html#examples",
    "href": "docs/reference/sampling.design.generate_grid_design.html#examples",
    "title": "sampling.design.generate_grid_design",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_grid_design\n&gt;&gt;&gt; bounds = [(0, 1), (0, 1)]\n&gt;&gt;&gt; X = generate_grid_design(bounds, n_design=25) # 5^2 = 25\n&gt;&gt;&gt; X.shape\n(25, 2)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_grid_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_sobol_design.html",
    "href": "docs/reference/sampling.design.generate_sobol_design.html",
    "title": "sampling.design.generate_sobol_design",
    "section": "",
    "text": "sampling.design.generate_sobol_design(bounds, n_design, seed=None)\nGenerates a Sobol sequence design.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbounds\nUnion[List[Tuple[float, float]], np.ndarray]\nDesign space bounds.\nrequired\n\n\nn_design\nint\nThe number of points to generate.\nrequired\n\n\nseed\nOptional[Union[int, Generator]]\nRandom seed or generator.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: An array of shape (n_design, n_dim) containing the generated Sobol sequence points.\n\n\n\n\n\n\n\nThe Sobol sequence is generated with a length that is a power of 2.\nScrambling is enabled for improved uniformity.\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_sobol_design\n&gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n&gt;&gt;&gt; X = generate_sobol_design(bounds, n_design=5, seed=42)\n&gt;&gt;&gt; X.shape\n(5, 2)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_sobol_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_sobol_design.html#parameters",
    "href": "docs/reference/sampling.design.generate_sobol_design.html#parameters",
    "title": "sampling.design.generate_sobol_design",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbounds\nUnion[List[Tuple[float, float]], np.ndarray]\nDesign space bounds.\nrequired\n\n\nn_design\nint\nThe number of points to generate.\nrequired\n\n\nseed\nOptional[Union[int, Generator]]\nRandom seed or generator.\nNone",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_sobol_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_sobol_design.html#returns",
    "href": "docs/reference/sampling.design.generate_sobol_design.html#returns",
    "title": "sampling.design.generate_sobol_design",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: An array of shape (n_design, n_dim) containing the generated Sobol sequence points.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_sobol_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_sobol_design.html#notes",
    "href": "docs/reference/sampling.design.generate_sobol_design.html#notes",
    "title": "sampling.design.generate_sobol_design",
    "section": "",
    "text": "The Sobol sequence is generated with a length that is a power of 2.\nScrambling is enabled for improved uniformity.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_sobol_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.generate_sobol_design.html#examples",
    "href": "docs/reference/sampling.design.generate_sobol_design.html#examples",
    "title": "sampling.design.generate_sobol_design",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_sobol_design\n&gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n&gt;&gt;&gt; X = generate_sobol_design(bounds, n_design=5, seed=42)\n&gt;&gt;&gt; X.shape\n(5, 2)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "generate_sobol_design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.html",
    "href": "docs/reference/sampling.design.html",
    "title": "sampling.design",
    "section": "",
    "text": "sampling.design\n\n\n\n\n\nName\nDescription\n\n\n\n\nfullfactorial\nGenerates a full factorial sampling plan in the unit cube.\n\n\ngenerate_clustered_design\nGenerates a clustered design.\n\n\ngenerate_collinear_design\nGenerates a collinear design (poorly projected).\n\n\ngenerate_grid_design\nGenerates a regular grid design.\n\n\ngenerate_qmc_lhs_design\nGenerates a Latin Hypercube Sampling design using QMC.\n\n\ngenerate_sobol_design\nGenerates a Sobol sequence design.\n\n\ngenerate_uniform_design\nGenerate a uniform random experimental design.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "sampling.design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.design.html#functions",
    "href": "docs/reference/sampling.design.html#functions",
    "title": "sampling.design",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfullfactorial\nGenerates a full factorial sampling plan in the unit cube.\n\n\ngenerate_clustered_design\nGenerates a clustered design.\n\n\ngenerate_collinear_design\nGenerates a collinear design (poorly projected).\n\n\ngenerate_grid_design\nGenerates a regular grid design.\n\n\ngenerate_qmc_lhs_design\nGenerates a Latin Hypercube Sampling design using QMC.\n\n\ngenerate_sobol_design\nGenerates a Sobol sequence design.\n\n\ngenerate_uniform_design\nGenerate a uniform random experimental design.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "sampling.design"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.html",
    "href": "docs/reference/sampling.effects.html",
    "title": "sampling.effects",
    "section": "",
    "text": "sampling.effects\n\n\n\n\n\nName\nDescription\n\n\n\n\nplot_all_partial_dependence\nGenerates Partial Dependence Plots (PDPs) for every feature in a DataFrame against a target variable,\n\n\nrandorient\nGenerates a random orientation of a sampling matrix.\n\n\nscreening_plot\nGenerates a plot with elementary effect screening metrics.\n\n\nscreening_print\nGenerates a DataFrame with elementary effect screening metrics.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "sampling.effects"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.html#functions",
    "href": "docs/reference/sampling.effects.html#functions",
    "title": "sampling.effects",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nplot_all_partial_dependence\nGenerates Partial Dependence Plots (PDPs) for every feature in a DataFrame against a target variable,\n\n\nrandorient\nGenerates a random orientation of a sampling matrix.\n\n\nscreening_plot\nGenerates a plot with elementary effect screening metrics.\n\n\nscreening_print\nGenerates a DataFrame with elementary effect screening metrics.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "sampling.effects"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.screening_plot.html",
    "href": "docs/reference/sampling.effects.screening_plot.html",
    "title": "sampling.effects.screening_plot",
    "section": "",
    "text": "sampling.effects.screening_plot(X, fun, xi, p, labels, bounds=None, show=True)\nGenerates a plot with elementary effect screening metrics.\nThis function calculates the mean and standard deviation of the elementary effects for a given set of design variables and plots the results.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nThe screening plan matrix, typically structured within a [0,1]^k box.\nrequired\n\n\nfun\nobject\nThe objective function to evaluate at each design point in the screening plan.\nrequired\n\n\nxi\nfloat\nThe elementary effect step length factor.\nrequired\n\n\np\nint\nNumber of discrete levels along each dimension.\nrequired\n\n\nlabels\nlist of str\nA list of variable names corresponding to the design variables.\nrequired\n\n\nbounds\nnp.ndarray\nA 2xk matrix where the first row contains lower bounds and the second row contains upper bounds for each variable.\nNone\n\n\nshow\nbool\nIf True, the plot is displayed. Defaults to True.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nNone\nNone\nThe function generates a plot of the results.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.effects import screening, screeningplan\n    from spotpython.fun.objectivefunctions import Analytical\n    fun = Analytical()\n    k = 10\n    p = 10\n    xi = 1\n    r = 25\n    X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n    # Provide real-world bounds from the wing weight docs (2 x 10).\n    value_range = np.array([\n        [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n        [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n    ])\n    labels = [\n        \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n        \"q\",   \"lambda\", \"tc\", \"N_z\",\n        \"W_dg\", \"W_p\"\n    ]\n    screening(\n        X=X,\n        fun=fun.fun_wingwt,\n        bounds=value_range,\n        xi=xi,\n        p=p,\n        labels=labels,\n        print=False,\n    )",
    "crumbs": [
      "API Reference",
      "Sampling",
      "screening_plot"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.screening_plot.html#parameters",
    "href": "docs/reference/sampling.effects.screening_plot.html#parameters",
    "title": "sampling.effects.screening_plot",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nThe screening plan matrix, typically structured within a [0,1]^k box.\nrequired\n\n\nfun\nobject\nThe objective function to evaluate at each design point in the screening plan.\nrequired\n\n\nxi\nfloat\nThe elementary effect step length factor.\nrequired\n\n\np\nint\nNumber of discrete levels along each dimension.\nrequired\n\n\nlabels\nlist of str\nA list of variable names corresponding to the design variables.\nrequired\n\n\nbounds\nnp.ndarray\nA 2xk matrix where the first row contains lower bounds and the second row contains upper bounds for each variable.\nNone\n\n\nshow\nbool\nIf True, the plot is displayed. Defaults to True.\nTrue",
    "crumbs": [
      "API Reference",
      "Sampling",
      "screening_plot"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.screening_plot.html#returns",
    "href": "docs/reference/sampling.effects.screening_plot.html#returns",
    "title": "sampling.effects.screening_plot",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nNone\nNone\nThe function generates a plot of the results.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "screening_plot"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.screening_plot.html#examples",
    "href": "docs/reference/sampling.effects.screening_plot.html#examples",
    "title": "sampling.effects.screening_plot",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.effects import screening, screeningplan\n    from spotpython.fun.objectivefunctions import Analytical\n    fun = Analytical()\n    k = 10\n    p = 10\n    xi = 1\n    r = 25\n    X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n    # Provide real-world bounds from the wing weight docs (2 x 10).\n    value_range = np.array([\n        [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n        [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n    ])\n    labels = [\n        \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n        \"q\",   \"lambda\", \"tc\", \"N_z\",\n        \"W_dg\", \"W_p\"\n    ]\n    screening(\n        X=X,\n        fun=fun.fun_wingwt,\n        bounds=value_range,\n        xi=xi,\n        p=p,\n        labels=labels,\n        print=False,\n    )",
    "crumbs": [
      "API Reference",
      "Sampling",
      "screening_plot"
    ]
  },
  {
    "objectID": "docs/reference/sampling.effects.screeningplan.html",
    "href": "docs/reference/sampling.effects.screeningplan.html",
    "title": "sampling.effects.screeningplan",
    "section": "",
    "text": "sampling.effects.screeningplan\nsampling.effects.screeningplan(k, p, xi, r)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "screeningplan"
    ]
  },
  {
    "objectID": "docs/reference/sampling.lhs.rlh.html",
    "href": "docs/reference/sampling.lhs.rlh.html",
    "title": "sampling.lhs.rlh",
    "section": "",
    "text": "sampling.lhs.rlh(n, k, edges=0, seed=None)\nGenerates a random Latin hypercube within the [0,1]^k hypercube.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nDesired number of points.\nrequired\n\n\nk\nint\nNumber of design variables (dimensions).\nrequired\n\n\nedges\nint\nIf 1, places centers of the extreme bins at the domain edges ([0,1]). Otherwise, bins are fully contained within the domain, i.e. midpoints. Defaults to 0.\n0\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A Latin hypercube sampling plan of n points in k dimensions, with each coordinate in the range [0,1].\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.sampling.lhs import rlh\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Generate a 2D Latin hypercube with 5 points and edges=0\n&gt;&gt;&gt; X = rlh(n=5, k=2, edges=0)\n&gt;&gt;&gt; print(X)\n# Example output (values vary due to randomness):\n# [[0.1  0.5 ]\n#  [0.7  0.1 ]\n#  [0.9  0.7 ]\n#  [0.3  0.9 ]\n#  [0.5  0.3 ]]",
    "crumbs": [
      "API Reference",
      "Sampling",
      "rlh"
    ]
  },
  {
    "objectID": "docs/reference/sampling.lhs.rlh.html#parameters",
    "href": "docs/reference/sampling.lhs.rlh.html#parameters",
    "title": "sampling.lhs.rlh",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nn\nint\nDesired number of points.\nrequired\n\n\nk\nint\nNumber of design variables (dimensions).\nrequired\n\n\nedges\nint\nIf 1, places centers of the extreme bins at the domain edges ([0,1]). Otherwise, bins are fully contained within the domain, i.e. midpoints. Defaults to 0.\n0",
    "crumbs": [
      "API Reference",
      "Sampling",
      "rlh"
    ]
  },
  {
    "objectID": "docs/reference/sampling.lhs.rlh.html#returns",
    "href": "docs/reference/sampling.lhs.rlh.html#returns",
    "title": "sampling.lhs.rlh",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A Latin hypercube sampling plan of n points in k dimensions, with each coordinate in the range [0,1].",
    "crumbs": [
      "API Reference",
      "Sampling",
      "rlh"
    ]
  },
  {
    "objectID": "docs/reference/sampling.lhs.rlh.html#examples",
    "href": "docs/reference/sampling.lhs.rlh.html#examples",
    "title": "sampling.lhs.rlh",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.sampling.lhs import rlh\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Generate a 2D Latin hypercube with 5 points and edges=0\n&gt;&gt;&gt; X = rlh(n=5, k=2, edges=0)\n&gt;&gt;&gt; print(X)\n# Example output (values vary due to randomness):\n# [[0.1  0.5 ]\n#  [0.7  0.1 ]\n#  [0.9  0.7 ]\n#  [0.3  0.9 ]\n#  [0.5  0.3 ]]",
    "crumbs": [
      "API Reference",
      "Sampling",
      "rlh"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.jd.html",
    "href": "docs/reference/sampling.mm.jd.html",
    "title": "sampling.mm.jd",
    "section": "",
    "text": "sampling.mm.jd(X, p=1.0)\nComputes and counts the distinct p-norm distances between all pairs of points in X. It returns: 1) A list of distinct distances (sorted), and 2) A corresponding multiplicity array that indicates how often each distance occurs.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nA 2D array of shape (n, d) representing n points in d-dimensional space.\nrequired\n\n\np\nfloat\nThe distance norm to use. p=1 uses the Manhattan (L1) norm, while p=2 uses the Euclidean (L2) norm. Defaults to 1.0 (Manhattan norm).\n1.0\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n(np.ndarray, np.ndarray)\nA tuple (J, distinct_d), where: - distinct_d is a 1D float array of unique, sorted distances between points. - J is a 1D integer array that provides the multiplicity (occurrence count) of each distance in distinct_d.\n\n\n\n\n\n\nMany thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import jd\n&gt;&gt;&gt; # A small 3-point set in 2D\n&gt;&gt;&gt; X = np.array([[0.0, 0.0],\n...               [1.0, 1.0],\n...               [2.0, 2.0]])\n&gt;&gt;&gt; J, distinct_d = jd(X, p=2.0)\n&gt;&gt;&gt; print(\"Distinct distances:\", distinct_d)\n&gt;&gt;&gt; print(\"Occurrences:\", J)\n# Possible output (using Euclidean norm):\n# Distinct distances: [1.41421356 2.82842712]\n# Occurrences: [1 1]\n# Explanation: Distances are sqrt(2) between consecutive points and 2*sqrt(2) for the farthest pair.\n    Distinct distances: [1.41421356 2.82842712]\n    Occurrences: [2 1]",
    "crumbs": [
      "API Reference",
      "Sampling",
      "jd"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.jd.html#parameters",
    "href": "docs/reference/sampling.mm.jd.html#parameters",
    "title": "sampling.mm.jd",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nA 2D array of shape (n, d) representing n points in d-dimensional space.\nrequired\n\n\np\nfloat\nThe distance norm to use. p=1 uses the Manhattan (L1) norm, while p=2 uses the Euclidean (L2) norm. Defaults to 1.0 (Manhattan norm).\n1.0",
    "crumbs": [
      "API Reference",
      "Sampling",
      "jd"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.jd.html#returns",
    "href": "docs/reference/sampling.mm.jd.html#returns",
    "title": "sampling.mm.jd",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n(np.ndarray, np.ndarray)\nA tuple (J, distinct_d), where: - distinct_d is a 1D float array of unique, sorted distances between points. - J is a 1D integer array that provides the multiplicity (occurrence count) of each distance in distinct_d.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "jd"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.jd.html#notes",
    "href": "docs/reference/sampling.mm.jd.html#notes",
    "title": "sampling.mm.jd",
    "section": "",
    "text": "Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”",
    "crumbs": [
      "API Reference",
      "Sampling",
      "jd"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.jd.html#examples",
    "href": "docs/reference/sampling.mm.jd.html#examples",
    "title": "sampling.mm.jd",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import jd\n&gt;&gt;&gt; # A small 3-point set in 2D\n&gt;&gt;&gt; X = np.array([[0.0, 0.0],\n...               [1.0, 1.0],\n...               [2.0, 2.0]])\n&gt;&gt;&gt; J, distinct_d = jd(X, p=2.0)\n&gt;&gt;&gt; print(\"Distinct distances:\", distinct_d)\n&gt;&gt;&gt; print(\"Occurrences:\", J)\n# Possible output (using Euclidean norm):\n# Distinct distances: [1.41421356 2.82842712]\n# Occurrences: [1 1]\n# Explanation: Distances are sqrt(2) between consecutive points and 2*sqrt(2) for the farthest pair.\n    Distinct distances: [1.41421356 2.82842712]\n    Occurrences: [2 1]",
    "crumbs": [
      "API Reference",
      "Sampling",
      "jd"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mm_improvement.html",
    "href": "docs/reference/sampling.mm.mm_improvement.html",
    "title": "sampling.mm.mm_improvement",
    "section": "",
    "text": "sampling.mm.mm_improvement(\n    x,\n    X_base,\n    phi_base=None,\n    J_base=None,\n    d_base=None,\n    q=2,\n    p=2,\n    normalize_flag=False,\n    verbose=False,\n    exponential=True,\n)\nCalculates the Morris-Mitchell improvement for a candidate point x.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\nnp.ndarray\nCandidate point (1D array).\nrequired\n\n\nX_base\nnp.ndarray\nExisting design points.\nrequired\n\n\nJ_base\nnp.ndarray\nMultiplicities of distances for X_base.\nNone\n\n\nd_base\nnp.ndarray\nUnique distances for X_base.\nNone\n\n\nq\nint\nNumber of nearest neighbors for MM metric.\n2\n\n\np\nint\nPower for MM metric.\n2\n\n\nnormalize_flag\nbool\nIf True, normalizes the X array and candidate point before computing distances. Defaults to False.\nFalse\n\n\nexponential\nbool\nIf True, the exponential is applied.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nfloat\nfloat\nMorris-Mitchell improvement.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mm_improvement\n&gt;&gt;&gt; X_base = np.array([[0.1, 0.2], [0.4, 0.5], [0.7, 0.8]])\n&gt;&gt;&gt; x = np.array([0.5, 0.5])\n&gt;&gt;&gt; improvement = mm_improvement(x, X_base, q=2, p=2)\n&gt;&gt;&gt; print(improvement)\n0.123456789",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mm_improvement"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mm_improvement.html#parameters",
    "href": "docs/reference/sampling.mm.mm_improvement.html#parameters",
    "title": "sampling.mm.mm_improvement",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\nnp.ndarray\nCandidate point (1D array).\nrequired\n\n\nX_base\nnp.ndarray\nExisting design points.\nrequired\n\n\nJ_base\nnp.ndarray\nMultiplicities of distances for X_base.\nNone\n\n\nd_base\nnp.ndarray\nUnique distances for X_base.\nNone\n\n\nq\nint\nNumber of nearest neighbors for MM metric.\n2\n\n\np\nint\nPower for MM metric.\n2\n\n\nnormalize_flag\nbool\nIf True, normalizes the X array and candidate point before computing distances. Defaults to False.\nFalse\n\n\nexponential\nbool\nIf True, the exponential is applied.\nTrue",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mm_improvement"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mm_improvement.html#returns",
    "href": "docs/reference/sampling.mm.mm_improvement.html#returns",
    "title": "sampling.mm.mm_improvement",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nfloat\nfloat\nMorris-Mitchell improvement.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mm_improvement"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mm_improvement.html#examples",
    "href": "docs/reference/sampling.mm.mm_improvement.html#examples",
    "title": "sampling.mm.mm_improvement",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mm_improvement\n&gt;&gt;&gt; X_base = np.array([[0.1, 0.2], [0.4, 0.5], [0.7, 0.8]])\n&gt;&gt;&gt; x = np.array([0.5, 0.5])\n&gt;&gt;&gt; improvement = mm_improvement(x, X_base, q=2, p=2)\n&gt;&gt;&gt; print(improvement)\n0.123456789",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mm_improvement"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmlhs.html",
    "href": "docs/reference/sampling.mm.mmlhs.html",
    "title": "sampling.mm.mmlhs",
    "section": "",
    "text": "sampling.mm.mmlhs(X_start, population, iterations, q=2.0, plot=False)\nPerforms an evolutionary search (using perturbations) to find a Morris-Mitchell optimal Latin hypercube, starting from an initial plan X_start.\n\n\n\nInitializes a “best” Latin hypercube (X_best) from the provided X_start.\nIteratively perturbs X_best to create offspring.\nEvaluates the space-fillingness of each offspring via the Morris-Mitchell metric (using mmphi).\nUpdates the best plan whenever a better offspring is found.\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX_start\nnp.ndarray\nA 2D array of shape (n, k) providing the initial Latin hypercube (n points in k dimensions).\nrequired\n\n\npopulation\nint\nNumber of offspring to create in each generation.\nrequired\n\n\niterations\nint\nTotal number of generations to run the evolutionary search.\nrequired\n\n\nq\nfloat\nThe exponent used by the Morris-Mitchell space-filling criterion. Defaults to 2.0.\n2.0\n\n\nplot\nbool\nIf True, a simple scatter plot of the first two dimensions will be displayed at each iteration. Only if k &gt;= 2. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 2D array representing the most space-filling Latin hypercube found after all iterations, of the same shape as X_start.\n\n\n\n\n\n\nMany thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmlhs\n&gt;&gt;&gt; # Suppose we have an initial 4x2 plan\n&gt;&gt;&gt; X_start = np.array([\n...     [0, 0],\n...     [1, 3],\n...     [2, 1],\n...     [3, 2]\n... ])\n&gt;&gt;&gt; # Search for a more space-filling plan\n&gt;&gt;&gt; X_opt = mmlhs(X_start, population=5, iterations=10, q=2)\n&gt;&gt;&gt; print(\"Optimized plan:\")\n&gt;&gt;&gt; print(X_opt)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmlhs"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmlhs.html#this-function-does-the-following",
    "href": "docs/reference/sampling.mm.mmlhs.html#this-function-does-the-following",
    "title": "sampling.mm.mmlhs",
    "section": "",
    "text": "Initializes a “best” Latin hypercube (X_best) from the provided X_start.\nIteratively perturbs X_best to create offspring.\nEvaluates the space-fillingness of each offspring via the Morris-Mitchell metric (using mmphi).\nUpdates the best plan whenever a better offspring is found.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmlhs"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmlhs.html#parameters",
    "href": "docs/reference/sampling.mm.mmlhs.html#parameters",
    "title": "sampling.mm.mmlhs",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX_start\nnp.ndarray\nA 2D array of shape (n, k) providing the initial Latin hypercube (n points in k dimensions).\nrequired\n\n\npopulation\nint\nNumber of offspring to create in each generation.\nrequired\n\n\niterations\nint\nTotal number of generations to run the evolutionary search.\nrequired\n\n\nq\nfloat\nThe exponent used by the Morris-Mitchell space-filling criterion. Defaults to 2.0.\n2.0\n\n\nplot\nbool\nIf True, a simple scatter plot of the first two dimensions will be displayed at each iteration. Only if k &gt;= 2. Defaults to False.\nFalse",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmlhs"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmlhs.html#returns",
    "href": "docs/reference/sampling.mm.mmlhs.html#returns",
    "title": "sampling.mm.mmlhs",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 2D array representing the most space-filling Latin hypercube found after all iterations, of the same shape as X_start.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmlhs"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmlhs.html#notes",
    "href": "docs/reference/sampling.mm.mmlhs.html#notes",
    "title": "sampling.mm.mmlhs",
    "section": "",
    "text": "Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmlhs"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmlhs.html#examples",
    "href": "docs/reference/sampling.mm.mmlhs.html#examples",
    "title": "sampling.mm.mmlhs",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmlhs\n&gt;&gt;&gt; # Suppose we have an initial 4x2 plan\n&gt;&gt;&gt; X_start = np.array([\n...     [0, 0],\n...     [1, 3],\n...     [2, 1],\n...     [3, 2]\n... ])\n&gt;&gt;&gt; # Search for a more space-filling plan\n&gt;&gt;&gt; X_opt = mmlhs(X_start, population=5, iterations=10, q=2)\n&gt;&gt;&gt; print(\"Optimized plan:\")\n&gt;&gt;&gt; print(X_opt)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmlhs"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmphi_intensive.html",
    "href": "docs/reference/sampling.mm.mmphi_intensive.html",
    "title": "sampling.mm.mmphi_intensive",
    "section": "",
    "text": "sampling.mm.mmphi_intensive(X, q=2.0, p=2.0, normalize_flag=False)\nCalculates a size-invariant Morris-Mitchell criterion.\nThis “intensive” version of the criterion allows for the comparison of sampling plans with different sample sizes by normalizing for the number of point pairs. A smaller value indicates a better (more space-filling) design.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nA 2D array representing the sampling plan (shape: (n, d)).\nrequired\n\n\nq\nfloat\nThe exponent used in the computation of the metric. Defaults to 2.0.\n2.0\n\n\np\nfloat\nThe distance norm to use (e.g., p=1 for Manhattan, p=2 for Euclidean). Defaults to 2.0.\n2.0\n\n\nnormalize_flag\nbool\nIf True, normalizes the X array before computing distances. Defaults to False.\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[float, np.ndarray, np.ndarray]\ntuple[float, np.ndarray, np.ndarray]: A tuple containing: - intensive_phiq: The intensive space-fillingness metric. - J: Multiplicities of distances. - d: Unique distances.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive\n&gt;&gt;&gt; # Create a simple 3-point sampling plan in 2D\n&gt;&gt;&gt; X = np.array([\n...     [0.0, 0.0],\n...     [0.5, 0.5],\n...     [1.0, 1.0]\n... ])\n&gt;&gt;&gt; # Calculate the intensive space-fillingness metric with q=2, using Euclidean distances (p=2)\n&gt;&gt;&gt; quality, J, d = mmphi_intensive(X, q=2, p=2)\n&gt;&gt;&gt; print(quality)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmphi_intensive"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmphi_intensive.html#parameters",
    "href": "docs/reference/sampling.mm.mmphi_intensive.html#parameters",
    "title": "sampling.mm.mmphi_intensive",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nA 2D array representing the sampling plan (shape: (n, d)).\nrequired\n\n\nq\nfloat\nThe exponent used in the computation of the metric. Defaults to 2.0.\n2.0\n\n\np\nfloat\nThe distance norm to use (e.g., p=1 for Manhattan, p=2 for Euclidean). Defaults to 2.0.\n2.0\n\n\nnormalize_flag\nbool\nIf True, normalizes the X array before computing distances. Defaults to False.\nFalse",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmphi_intensive"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmphi_intensive.html#returns",
    "href": "docs/reference/sampling.mm.mmphi_intensive.html#returns",
    "title": "sampling.mm.mmphi_intensive",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\ntuple[float, np.ndarray, np.ndarray]\ntuple[float, np.ndarray, np.ndarray]: A tuple containing: - intensive_phiq: The intensive space-fillingness metric. - J: Multiplicities of distances. - d: Unique distances.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmphi_intensive"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmphi_intensive.html#examples",
    "href": "docs/reference/sampling.mm.mmphi_intensive.html#examples",
    "title": "sampling.mm.mmphi_intensive",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive\n&gt;&gt;&gt; # Create a simple 3-point sampling plan in 2D\n&gt;&gt;&gt; X = np.array([\n...     [0.0, 0.0],\n...     [0.5, 0.5],\n...     [1.0, 1.0]\n... ])\n&gt;&gt;&gt; # Calculate the intensive space-fillingness metric with q=2, using Euclidean distances (p=2)\n&gt;&gt;&gt; quality, J, d = mmphi_intensive(X, q=2, p=2)\n&gt;&gt;&gt; print(quality)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmphi_intensive"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmsort.html",
    "href": "docs/reference/sampling.mm.mmsort.html",
    "title": "sampling.mm.mmsort",
    "section": "",
    "text": "sampling.mm.mmsort(X3D, p=1.0)\nRanks multiple sampling plans stored in a 3D array according to the Morris-Mitchell criterion, using a simple bubble sort.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX3D\nnp.ndarray\nA 3D NumPy array of shape (n, d, m), where m is the number of sampling plans, and each plan is an (n, d) matrix of points.\nrequired\n\n\np\nfloat\nThe distance metric to use. p=1 for Manhattan (L1), p=2 for Euclidean (L2). Defaults to 1.0.\n1.0\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 1D integer array of length m that holds the plan indices in ascending order of space-filling quality. The first index in the returned array corresponds to the most space-filling plan.\n\n\n\n\n\n\nMany thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmsort\n&gt;&gt;&gt; # Suppose we have two 3-point sampling plans in 2D, stored in X3D:\n&gt;&gt;&gt; X1 = np.array([[0.0, 0.0],\n...                [0.5, 0.5],\n...                [1.0, 1.0]])\n&gt;&gt;&gt; X2 = np.array([[0.2, 0.2],\n...                [0.6, 0.4],\n...                [0.9, 0.9]])\n&gt;&gt;&gt; # Stack them along the third dimension: shape will be (3, 2, 2)\n&gt;&gt;&gt; X3D = np.stack([X1, X2], axis=2)\n&gt;&gt;&gt; # Sort them using the Morris-Mitchell criterion with p=2\n&gt;&gt;&gt; ranking = mmsort(X3D, p=2.0)\n&gt;&gt;&gt; print(ranking)\n# It might print [2 1] or [1 2], depending on which plan is more space-filling.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmsort"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmsort.html#parameters",
    "href": "docs/reference/sampling.mm.mmsort.html#parameters",
    "title": "sampling.mm.mmsort",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX3D\nnp.ndarray\nA 3D NumPy array of shape (n, d, m), where m is the number of sampling plans, and each plan is an (n, d) matrix of points.\nrequired\n\n\np\nfloat\nThe distance metric to use. p=1 for Manhattan (L1), p=2 for Euclidean (L2). Defaults to 1.0.\n1.0",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmsort"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmsort.html#returns",
    "href": "docs/reference/sampling.mm.mmsort.html#returns",
    "title": "sampling.mm.mmsort",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 1D integer array of length m that holds the plan indices in ascending order of space-filling quality. The first index in the returned array corresponds to the most space-filling plan.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmsort"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmsort.html#notes",
    "href": "docs/reference/sampling.mm.mmsort.html#notes",
    "title": "sampling.mm.mmsort",
    "section": "",
    "text": "Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmsort"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.mmsort.html#examples",
    "href": "docs/reference/sampling.mm.mmsort.html#examples",
    "title": "sampling.mm.mmsort",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmsort\n&gt;&gt;&gt; # Suppose we have two 3-point sampling plans in 2D, stored in X3D:\n&gt;&gt;&gt; X1 = np.array([[0.0, 0.0],\n...                [0.5, 0.5],\n...                [1.0, 1.0]])\n&gt;&gt;&gt; X2 = np.array([[0.2, 0.2],\n...                [0.6, 0.4],\n...                [0.9, 0.9]])\n&gt;&gt;&gt; # Stack them along the third dimension: shape will be (3, 2, 2)\n&gt;&gt;&gt; X3D = np.stack([X1, X2], axis=2)\n&gt;&gt;&gt; # Sort them using the Morris-Mitchell criterion with p=2\n&gt;&gt;&gt; ranking = mmsort(X3D, p=2.0)\n&gt;&gt;&gt; print(ranking)\n# It might print [2 1] or [1 2], depending on which plan is more space-filling.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "mmsort"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.phisort.html",
    "href": "docs/reference/sampling.mm.phisort.html",
    "title": "sampling.mm.phisort",
    "section": "",
    "text": "sampling.mm.phisort(X3D, q=2.0, p=1.0)\nRanks multiple sampling plans stored in a 3D array by the Morris-Mitchell numerical quality metric (mmphi). Uses a simple bubble-sort: sampling plans with smaller mmphi values are placed first in the index array.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX3D\nnp.ndarray\nA 3D array of shape (n, d, m), where m is the number of sampling plans.\nrequired\n\n\nq\nfloat\nExponent for the mmphi metric. Defaults to 2.0.\n2.0\n\n\np\nfloat\nDistance norm for mmphi. p=1 is Manhattan; p=2 is Euclidean. Defaults to 1.0.\n1.0\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 1D integer array of length m, giving the plan indices in ascending order of mmphi. The first index in the returned array corresponds to the numerically lowest mmphi value.\n\n\n\n\n\n\nMany thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”\n\n\n\n&gt;&gt;&gt; import numpy as np\n    from spotoptim.sampling.mm import phisort\n    from spotoptim.sampling.mm import bestlh\n    X1 = bestlh(n=5, k=2, population=5, iterations=10)\n    X2 = bestlh(n=5, k=2, population=15, iterations=20)\n    X3 = bestlh(n=5, k=2, population=25, iterations=30)\n    # Map X1 and X2 so that X3D has the two sampling plans in X3D[:, :, 0] and X3D[:, :, 1]\n    X3D = np.array([X1, X2])\n    print(phisort(X3D))\n    X3D = np.array([X3, X2])\n    print(phisort(X3D))\n        [2 1]\n        [1 2]",
    "crumbs": [
      "API Reference",
      "Sampling",
      "phisort"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.phisort.html#parameters",
    "href": "docs/reference/sampling.mm.phisort.html#parameters",
    "title": "sampling.mm.phisort",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX3D\nnp.ndarray\nA 3D array of shape (n, d, m), where m is the number of sampling plans.\nrequired\n\n\nq\nfloat\nExponent for the mmphi metric. Defaults to 2.0.\n2.0\n\n\np\nfloat\nDistance norm for mmphi. p=1 is Manhattan; p=2 is Euclidean. Defaults to 1.0.\n1.0",
    "crumbs": [
      "API Reference",
      "Sampling",
      "phisort"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.phisort.html#returns",
    "href": "docs/reference/sampling.mm.phisort.html#returns",
    "title": "sampling.mm.phisort",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: A 1D integer array of length m, giving the plan indices in ascending order of mmphi. The first index in the returned array corresponds to the numerically lowest mmphi value.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "phisort"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.phisort.html#notes",
    "href": "docs/reference/sampling.mm.phisort.html#notes",
    "title": "sampling.mm.phisort",
    "section": "",
    "text": "Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: “This program is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.”",
    "crumbs": [
      "API Reference",
      "Sampling",
      "phisort"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.phisort.html#examples",
    "href": "docs/reference/sampling.mm.phisort.html#examples",
    "title": "sampling.mm.phisort",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n    from spotoptim.sampling.mm import phisort\n    from spotoptim.sampling.mm import bestlh\n    X1 = bestlh(n=5, k=2, population=5, iterations=10)\n    X2 = bestlh(n=5, k=2, population=15, iterations=20)\n    X3 = bestlh(n=5, k=2, population=25, iterations=30)\n    # Map X1 and X2 so that X3D has the two sampling plans in X3D[:, :, 0] and X3D[:, :, 1]\n    X3D = np.array([X1, X2])\n    print(phisort(X3D))\n    X3D = np.array([X3, X2])\n    print(phisort(X3D))\n        [2 1]\n        [1 2]",
    "crumbs": [
      "API Reference",
      "Sampling",
      "phisort"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.plot_mmphi_vs_points.html",
    "href": "docs/reference/sampling.mm.plot_mmphi_vs_points.html",
    "title": "sampling.mm.plot_mmphi_vs_points",
    "section": "",
    "text": "sampling.mm.plot_mmphi_vs_points(\n    X_base,\n    x_min,\n    x_max,\n    p_min=10,\n    p_max=100,\n    p_step=10,\n    n_repeats=5,\n)\nPlot the Morris-Mitchell criterion versus the number of added points.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX_base\nnp.ndarray\nBase design matrix\nrequired\n\n\nx_min\nnp.ndarray\nLower bounds for variables\nrequired\n\n\nx_max\nnp.ndarray\nUpper bounds for variables\nrequired\n\n\np_min\nint\nMinimum number of points to add\n10\n\n\np_max\nint\nMaximum number of points to add\n100\n\n\np_step\nint\nStep size for number of points\n10\n\n\nn_repeats\nint\nNumber of repetitions for each point count\n5\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Summary DataFrame with mean and std of mmphi for each number of added points.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import plot_mmphi_vs_points\n&gt;&gt;&gt; # Define base design\n&gt;&gt;&gt; X_base = np.array([[0.1, 0.2], [0.4, 0.5], [0.7, 0.8]])\n&gt;&gt;&gt; # Define variable bounds\n&gt;&gt;&gt; x_min = np.array([0.0, 0.0])\n&gt;&gt;&gt; x_max = np.array([1.0, 1.0])\n&gt;&gt;&gt; # Plot mmphi vs number of added points\n&gt;&gt;&gt; df_summary = plot_mmphi_vs_points(X_base, x_min, x_max, p_min=10, p_max=50, p_step=10, n_repeats=3)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "plot_mmphi_vs_points"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.plot_mmphi_vs_points.html#parameters",
    "href": "docs/reference/sampling.mm.plot_mmphi_vs_points.html#parameters",
    "title": "sampling.mm.plot_mmphi_vs_points",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX_base\nnp.ndarray\nBase design matrix\nrequired\n\n\nx_min\nnp.ndarray\nLower bounds for variables\nrequired\n\n\nx_max\nnp.ndarray\nUpper bounds for variables\nrequired\n\n\np_min\nint\nMinimum number of points to add\n10\n\n\np_max\nint\nMaximum number of points to add\n100\n\n\np_step\nint\nStep size for number of points\n10\n\n\nn_repeats\nint\nNumber of repetitions for each point count\n5",
    "crumbs": [
      "API Reference",
      "Sampling",
      "plot_mmphi_vs_points"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.plot_mmphi_vs_points.html#returns",
    "href": "docs/reference/sampling.mm.plot_mmphi_vs_points.html#returns",
    "title": "sampling.mm.plot_mmphi_vs_points",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Summary DataFrame with mean and std of mmphi for each number of added points.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "plot_mmphi_vs_points"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.plot_mmphi_vs_points.html#examples",
    "href": "docs/reference/sampling.mm.plot_mmphi_vs_points.html#examples",
    "title": "sampling.mm.plot_mmphi_vs_points",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import plot_mmphi_vs_points\n&gt;&gt;&gt; # Define base design\n&gt;&gt;&gt; X_base = np.array([[0.1, 0.2], [0.4, 0.5], [0.7, 0.8]])\n&gt;&gt;&gt; # Define variable bounds\n&gt;&gt;&gt; x_min = np.array([0.0, 0.0])\n&gt;&gt;&gt; x_max = np.array([1.0, 1.0])\n&gt;&gt;&gt; # Plot mmphi vs number of added points\n&gt;&gt;&gt; df_summary = plot_mmphi_vs_points(X_base, x_min, x_max, p_min=10, p_max=50, p_step=10, n_repeats=3)",
    "crumbs": [
      "API Reference",
      "Sampling",
      "plot_mmphi_vs_points"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.html",
    "href": "docs/reference/sampling.mm.html",
    "title": "sampling.mm",
    "section": "",
    "text": "sampling.mm\n\n\n\n\n\nName\nDescription\n\n\n\n\nbestlh\nGenerates an optimized Latin hypercube by evolving the Morris-Mitchell\n\n\njd\nComputes and counts the distinct p-norm distances between all pairs of points in X.\n\n\nmm\nDetermines which of two sampling plans has better space-filling properties\n\n\nmm_improvement\nCalculates the Morris-Mitchell improvement for a candidate point x.\n\n\nmm_improvement_contour\nGenerates a contour plot of the Morris-Mitchell improvement over a grid defined by x1 and x2.\n\n\nmmlhs\nPerforms an evolutionary search (using perturbations) to find a Morris-Mitchell\n\n\nmmphi\nCalculates the Morris-Mitchell sampling plan quality criterion.\n\n\nmmphi_intensive\nCalculates a size-invariant Morris-Mitchell criterion.\n\n\nmmphi_intensive_update\nUpdates the Morris-Mitchell intensive criterion for n+1 points by adding a new point to the design.\n\n\nmmsort\nRanks multiple sampling plans stored in a 3D array according to the\n\n\nperturb\nPerforms a specified number of random element swaps on a sampling plan.\n\n\nphisort\nRanks multiple sampling plans stored in a 3D array by the Morris-Mitchell\n\n\nplot_mmphi_vs_n_lhs\nGenerates LHS designs for varying n, calculates mmphi and mmphi_intensive,\n\n\nplot_mmphi_vs_points\nPlot the Morris-Mitchell criterion versus the number of added points.\n\n\npropose_mmphi_intensive_minimizing_point\nPropose a new point that, when added to X, minimizes the intensive Morris-Mitchell (mmphi_intensive) criterion.\n\n\nsubset\nReturns a space-filling subset of a given size from a sampling plan, along with",
    "crumbs": [
      "API Reference",
      "Sampling",
      "sampling.mm"
    ]
  },
  {
    "objectID": "docs/reference/sampling.mm.html#functions",
    "href": "docs/reference/sampling.mm.html#functions",
    "title": "sampling.mm",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nbestlh\nGenerates an optimized Latin hypercube by evolving the Morris-Mitchell\n\n\njd\nComputes and counts the distinct p-norm distances between all pairs of points in X.\n\n\nmm\nDetermines which of two sampling plans has better space-filling properties\n\n\nmm_improvement\nCalculates the Morris-Mitchell improvement for a candidate point x.\n\n\nmm_improvement_contour\nGenerates a contour plot of the Morris-Mitchell improvement over a grid defined by x1 and x2.\n\n\nmmlhs\nPerforms an evolutionary search (using perturbations) to find a Morris-Mitchell\n\n\nmmphi\nCalculates the Morris-Mitchell sampling plan quality criterion.\n\n\nmmphi_intensive\nCalculates a size-invariant Morris-Mitchell criterion.\n\n\nmmphi_intensive_update\nUpdates the Morris-Mitchell intensive criterion for n+1 points by adding a new point to the design.\n\n\nmmsort\nRanks multiple sampling plans stored in a 3D array according to the\n\n\nperturb\nPerforms a specified number of random element swaps on a sampling plan.\n\n\nphisort\nRanks multiple sampling plans stored in a 3D array by the Morris-Mitchell\n\n\nplot_mmphi_vs_n_lhs\nGenerates LHS designs for varying n, calculates mmphi and mmphi_intensive,\n\n\nplot_mmphi_vs_points\nPlot the Morris-Mitchell criterion versus the number of added points.\n\n\npropose_mmphi_intensive_minimizing_point\nPropose a new point that, when added to X, minimizes the intensive Morris-Mitchell (mmphi_intensive) criterion.\n\n\nsubset\nReturns a space-filling subset of a given size from a sampling plan, along with",
    "crumbs": [
      "API Reference",
      "Sampling",
      "sampling.mm"
    ]
  },
  {
    "objectID": "docs/reference/sampling.html",
    "href": "docs/reference/sampling.html",
    "title": "sampling",
    "section": "",
    "text": "sampling\nsampling\nSampling methods for design of experiments.",
    "crumbs": [
      "API Reference",
      "Sampling",
      "sampling"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.Kernel.html",
    "href": "docs/reference/surrogate.kernels.Kernel.html",
    "title": "surrogate.kernels.Kernel",
    "section": "",
    "text": "surrogate.kernels.Kernel()\nBase class for Kernels.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndiag\nReturns the diagonal of the kernel k(X, X).",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Kernel"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.Kernel.html#methods",
    "href": "docs/reference/surrogate.kernels.Kernel.html#methods",
    "title": "surrogate.kernels.Kernel",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndiag\nReturns the diagonal of the kernel k(X, X).",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Kernel"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.RBF.html",
    "href": "docs/reference/surrogate.kernels.RBF.html",
    "title": "surrogate.kernels.RBF",
    "section": "",
    "text": "surrogate.kernels.RBF(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0))\nRadial Basis Function (RBF) kernel.\nAlso known as the “squared exponential” kernel. It is given by: k(x_i, x_j) = exp(-0.5 * d(x_i, x_j)^2 / length_scale^2)\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlength_scale\nfloat or np.ndarray\nThe length scale of the kernel. If a float, using isotropic distances. If an array, using anisotropic distances. Defaults to 1.0.\n1.0\n\n\nlength_scale_bounds\ntuple\nThe lower and upper bound on length_scale. Defaults to (1e-5, 1e5).\n(1e-05, 100000.0)",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "RBF"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.RBF.html#parameters",
    "href": "docs/reference/surrogate.kernels.RBF.html#parameters",
    "title": "surrogate.kernels.RBF",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nlength_scale\nfloat or np.ndarray\nThe length scale of the kernel. If a float, using isotropic distances. If an array, using anisotropic distances. Defaults to 1.0.\n1.0\n\n\nlength_scale_bounds\ntuple\nThe lower and upper bound on length_scale. Defaults to (1e-5, 1e5).\n(1e-05, 100000.0)",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "RBF"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.Sum.html",
    "href": "docs/reference/surrogate.kernels.Sum.html",
    "title": "surrogate.kernels.Sum",
    "section": "",
    "text": "surrogate.kernels.Sum(k1, k2)\nThe Sum kernel k1 + k2.\nThe kernel value is k1(X, Y) + k2(X, Y).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nk1\nKernel\nFirst kernel.\nrequired\n\n\nk2\nKernel\nSecond kernel.\nrequired",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Sum"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.Sum.html#parameters",
    "href": "docs/reference/surrogate.kernels.Sum.html#parameters",
    "title": "surrogate.kernels.Sum",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nk1\nKernel\nFirst kernel.\nrequired\n\n\nk2\nKernel\nSecond kernel.\nrequired",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "Sum"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.html",
    "href": "docs/reference/surrogate.kernels.html",
    "title": "surrogate.kernels",
    "section": "",
    "text": "surrogate.kernels\n\n\n\n\n\nName\nDescription\n\n\n\n\nConstantKernel\nConstant kernel.\n\n\nKernel\nBase class for Kernels.\n\n\nProduct\nThe Product kernel k1 * k2.\n\n\nRBF\nRadial Basis Function (RBF) kernel.\n\n\nSpotOptimKernel\nKernel designed for SpotOptim’s Kriging with mixed variable support.\n\n\nSum\nThe Sum kernel k1 + k2.\n\n\nWhiteKernel\nWhite kernel.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate.kernels"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kernels.html#classes",
    "href": "docs/reference/surrogate.kernels.html#classes",
    "title": "surrogate.kernels",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nConstantKernel\nConstant kernel.\n\n\nKernel\nBase class for Kernels.\n\n\nProduct\nThe Product kernel k1 * k2.\n\n\nRBF\nRadial Basis Function (RBF) kernel.\n\n\nSpotOptimKernel\nKernel designed for SpotOptim’s Kriging with mixed variable support.\n\n\nSum\nThe Sum kernel k1 + k2.\n\n\nWhiteKernel\nWhite kernel.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate.kernels"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kriging.html",
    "href": "docs/reference/surrogate.kriging.html",
    "title": "surrogate.kriging",
    "section": "",
    "text": "surrogate.kriging\nKriging (Gaussian Process) surrogate model for SpotOptim.\nAdapted from spotpython.surrogate.kriging_basic for compatibility with SpotOptim. This implementation follows Forrester et al. (2008) “Engineering Design via Surrogate Modelling”.\nSpecific references: - Section 2.4 “Kriging”: Core implementation of the Kriging predictor and likelihood. - Section 6 “Surrogate Modeling of Noisy Data”: Implementation of “regression” and “reinterpolation” methods. - Validated against the book’s Matlab implementation: - likelihood.m: Concentrated log-likelihood calculation. - pred.m: Prediction and error estimation.\n\n\n\n\n\nName\nDescription\n\n\n\n\nKriging\nA scikit-learn compatible Kriging model class for regression tasks.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate.kriging"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.kriging.html#classes",
    "href": "docs/reference/surrogate.kriging.html#classes",
    "title": "surrogate.kriging",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nKriging\nA scikit-learn compatible Kriging model class for regression tasks.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate.kriging"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.mlp_surrogate.html",
    "href": "docs/reference/surrogate.mlp_surrogate.html",
    "title": "surrogate.mlp_surrogate",
    "section": "",
    "text": "surrogate.mlp_surrogate\nMLP Surrogate model for SpotOptim.\nThis module implements a standard Multi-Layer Perceptron (MLP) surrogate that enables uncertainty estimation via Monte Carlo Dropout (MC Dropout). It is designed to be a drop-in alternative to the Kriging surrogate within the SpotOptim framework.\n\n\n\n\n\nName\nDescription\n\n\n\n\nMLPSurrogate\nA scikit-learn compatible MLP surrogate model with uncertainty estimation.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate.mlp_surrogate"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.mlp_surrogate.html#classes",
    "href": "docs/reference/surrogate.mlp_surrogate.html#classes",
    "title": "surrogate.mlp_surrogate",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nMLPSurrogate\nA scikit-learn compatible MLP surrogate model with uncertainty estimation.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate.mlp_surrogate"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.nystroem.html",
    "href": "docs/reference/surrogate.nystroem.html",
    "title": "surrogate.nystroem",
    "section": "",
    "text": "surrogate.nystroem\n\n\n\n\n\nName\nDescription\n\n\n\n\nNystroem\nApproximate a feature map of a kernel using a subset of data.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate.nystroem"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.nystroem.html#classes",
    "href": "docs/reference/surrogate.nystroem.html#classes",
    "title": "surrogate.nystroem",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nNystroem\nApproximate a feature map of a kernel using a subset of data.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate.nystroem"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.pipeline.html",
    "href": "docs/reference/surrogate.pipeline.html",
    "title": "surrogate.pipeline",
    "section": "",
    "text": "surrogate.pipeline\n\n\n\n\n\nName\nDescription\n\n\n\n\nPipeline\nPipeline of transforms with a final estimator.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate.pipeline"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.pipeline.html#classes",
    "href": "docs/reference/surrogate.pipeline.html#classes",
    "title": "surrogate.pipeline",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nPipeline\nPipeline of transforms with a final estimator.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "surrogate.pipeline"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.simple_kriging.SimpleKriging.html",
    "href": "docs/reference/surrogate.simple_kriging.SimpleKriging.html",
    "title": "surrogate.simple_kriging.SimpleKriging",
    "section": "",
    "text": "surrogate.simple_kriging.SimpleKriging(\n    noise=None,\n    kernel='gauss',\n    n_theta=None,\n    min_theta=-3.0,\n    max_theta=2.0,\n    seed=None,\n)\nA simplified Kriging (Gaussian Process) surrogate model for SpotOptim.\nThis class provides a scikit-learn compatible interface with fit() and predict() methods, making it suitable for use as a surrogate in SpotOptim.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnoise\nfloat\nRegularization parameter (nugget effect). If None, uses sqrt(eps). Defaults to None.\nNone\n\n\nkernel\nstr\nKernel type. Currently only ‘gauss’ (Gaussian/RBF) is supported. Defaults to ‘gauss’.\n'gauss'\n\n\nn_theta\nint\nNumber of theta parameters. If None, uses k (number of dimensions). Defaults to None.\nNone\n\n\nmin_theta\nfloat\nMinimum log10(theta) bound for optimization. Defaults to -3.0.\n-3.0\n\n\nmax_theta\nfloat\nMaximum log10(theta) bound for optimization. Defaults to 2.0.\n2.0\n\n\nseed\nint\nRandom seed for reproducibility. Defaults to None.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nX_\nndarray\nTraining data, shape (n_samples, n_features).\n\n\ny_\nndarray\nTraining targets, shape (n_samples,).\n\n\ntheta_\nndarray\nOptimized theta parameters (log10 scale).\n\n\nmu_\nfloat\nMean of the SimpleKriging predictor.\n\n\nsigma2_\nfloat\nVariance of the SimpleKriging predictor.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.surrogate import SimpleKriging\n&gt;&gt;&gt; X = np.array([[0.0], [0.5], [1.0]])\n&gt;&gt;&gt; y = np.array([0.0, 0.25, 1.0])\n&gt;&gt;&gt; model = SimpleKriging()\n&gt;&gt;&gt; model.fit(X, y)\n&gt;&gt;&gt; predictions = model.predict(np.array([[0.25], [0.75]]))\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nfit\nFit the SimpleKriging model to training data.\n\n\npredict\nPredict using the SimpleKriging model.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "SimpleKriging"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.simple_kriging.SimpleKriging.html#parameters",
    "href": "docs/reference/surrogate.simple_kriging.SimpleKriging.html#parameters",
    "title": "surrogate.simple_kriging.SimpleKriging",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nnoise\nfloat\nRegularization parameter (nugget effect). If None, uses sqrt(eps). Defaults to None.\nNone\n\n\nkernel\nstr\nKernel type. Currently only ‘gauss’ (Gaussian/RBF) is supported. Defaults to ‘gauss’.\n'gauss'\n\n\nn_theta\nint\nNumber of theta parameters. If None, uses k (number of dimensions). Defaults to None.\nNone\n\n\nmin_theta\nfloat\nMinimum log10(theta) bound for optimization. Defaults to -3.0.\n-3.0\n\n\nmax_theta\nfloat\nMaximum log10(theta) bound for optimization. Defaults to 2.0.\n2.0\n\n\nseed\nint\nRandom seed for reproducibility. Defaults to None.\nNone",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "SimpleKriging"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.simple_kriging.SimpleKriging.html#attributes",
    "href": "docs/reference/surrogate.simple_kriging.SimpleKriging.html#attributes",
    "title": "surrogate.simple_kriging.SimpleKriging",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nX_\nndarray\nTraining data, shape (n_samples, n_features).\n\n\ny_\nndarray\nTraining targets, shape (n_samples,).\n\n\ntheta_\nndarray\nOptimized theta parameters (log10 scale).\n\n\nmu_\nfloat\nMean of the SimpleKriging predictor.\n\n\nsigma2_\nfloat\nVariance of the SimpleKriging predictor.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "SimpleKriging"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.simple_kriging.SimpleKriging.html#examples",
    "href": "docs/reference/surrogate.simple_kriging.SimpleKriging.html#examples",
    "title": "surrogate.simple_kriging.SimpleKriging",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.surrogate import SimpleKriging\n&gt;&gt;&gt; X = np.array([[0.0], [0.5], [1.0]])\n&gt;&gt;&gt; y = np.array([0.0, 0.25, 1.0])\n&gt;&gt;&gt; model = SimpleKriging()\n&gt;&gt;&gt; model.fit(X, y)\n&gt;&gt;&gt; predictions = model.predict(np.array([[0.25], [0.75]]))",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "SimpleKriging"
    ]
  },
  {
    "objectID": "docs/reference/surrogate.simple_kriging.SimpleKriging.html#methods",
    "href": "docs/reference/surrogate.simple_kriging.SimpleKriging.html#methods",
    "title": "surrogate.simple_kriging.SimpleKriging",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfit\nFit the SimpleKriging model to training data.\n\n\npredict\nPredict using the SimpleKriging model.",
    "crumbs": [
      "API Reference",
      "Surrogate Models",
      "SimpleKriging"
    ]
  },
  {
    "objectID": "docs/reference/tricands.html",
    "href": "docs/reference/tricands.html",
    "title": "tricands",
    "section": "",
    "text": "tricands\ntricands",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.tricands.html",
    "href": "docs/reference/tricands.tricands.tricands.html",
    "title": "tricands.tricands.tricands",
    "section": "",
    "text": "tricands.tricands.tricands(\n    X,\n    p=0.5,\n    fringe=True,\n    nmax=None,\n    best=None,\n    ordering=None,\n    vis=False,\n    imgname='tricands.pdf',\n    lower=0,\n    upper=1,\n)\nGenerate Triangulation Candidates for Bayesian Optimization. Assumes a bounding box of [lower, upper]^m.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nDesign matrix of shape (n_samples, n_features). Each row gives a design point and each column a feature.\nrequired\n\n\np\nfloat\nDistance to the boundary for fringe candidates (0 = on hull, 1 = on boundary). Defaults to 0.5.\n0.5\n\n\nfringe\nbool\nWhether to include fringe points to allow exploration outside the convex hull. Defaults to True.\nTrue\n\n\nnmax\nint\nMaximum size of candidate set. If output exceeds this, strategic subsetting is employed. Defaults to 100 * n_features.\nNone\n\n\nbest\nint\nIndex of the best (lowest) currently observed point. Used for strategic subsetting in Bayesian optimization. Defaults to None.\nNone\n\n\nordering\nnp.ndarray\nOrder of closeness of rows of X to a contour level. Used for contour location subsetting. Defaults to None.\nNone\n\n\nvis\nbool\nWhether to visualize the triangulation. Only applicable to 2D designs. Defaults to False.\nFalse\n\n\nimgname\nstr\nFile name for saved plot if vis=True. Defaults to ‘tricands.pdf’.\n'tricands.pdf'\n\n\nlower\nfloat\nLower bound of bounding box for all dimensions. Defaults to 0.\n0\n\n\nupper\nfloat\nUpper bound of bounding box for all dimensions. Defaults to 1.\n1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Array of candidate points, shape (n_candidates, n_features).\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nException\nIf visualization is requested for non-2D data.\n\n\n\nException\nIf number of points is less than n_features + 1.\n\n\n\nException\nIf both ‘best’ and ‘ordering’ are provided.\n\n\n\nException\nIf X contains values outside [lower, upper].\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.tricands import tricands\n&gt;&gt;&gt; X = np.array([[0.1, 0.1], [0.9, 0.1], [0.5, 0.9], [0.2, 0.5]])\n&gt;&gt;&gt; candidates = tricands(X, fringe=True, p=0.5)\n&gt;&gt;&gt; print(candidates.shape)\n(7, 2)",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.tricands.html#parameters",
    "href": "docs/reference/tricands.tricands.tricands.html#parameters",
    "title": "tricands.tricands.tricands",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nDesign matrix of shape (n_samples, n_features). Each row gives a design point and each column a feature.\nrequired\n\n\np\nfloat\nDistance to the boundary for fringe candidates (0 = on hull, 1 = on boundary). Defaults to 0.5.\n0.5\n\n\nfringe\nbool\nWhether to include fringe points to allow exploration outside the convex hull. Defaults to True.\nTrue\n\n\nnmax\nint\nMaximum size of candidate set. If output exceeds this, strategic subsetting is employed. Defaults to 100 * n_features.\nNone\n\n\nbest\nint\nIndex of the best (lowest) currently observed point. Used for strategic subsetting in Bayesian optimization. Defaults to None.\nNone\n\n\nordering\nnp.ndarray\nOrder of closeness of rows of X to a contour level. Used for contour location subsetting. Defaults to None.\nNone\n\n\nvis\nbool\nWhether to visualize the triangulation. Only applicable to 2D designs. Defaults to False.\nFalse\n\n\nimgname\nstr\nFile name for saved plot if vis=True. Defaults to ‘tricands.pdf’.\n'tricands.pdf'\n\n\nlower\nfloat\nLower bound of bounding box for all dimensions. Defaults to 0.\n0\n\n\nupper\nfloat\nUpper bound of bounding box for all dimensions. Defaults to 1.\n1",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.tricands.html#returns",
    "href": "docs/reference/tricands.tricands.tricands.html#returns",
    "title": "tricands.tricands.tricands",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Array of candidate points, shape (n_candidates, n_features).",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.tricands.html#raises",
    "href": "docs/reference/tricands.tricands.tricands.html#raises",
    "title": "tricands.tricands.tricands",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nException\nIf visualization is requested for non-2D data.\n\n\n\nException\nIf number of points is less than n_features + 1.\n\n\n\nException\nIf both ‘best’ and ‘ordering’ are provided.\n\n\n\nException\nIf X contains values outside [lower, upper].",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.tricands.html#examples",
    "href": "docs/reference/tricands.tricands.tricands.html#examples",
    "title": "tricands.tricands.tricands",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.tricands import tricands\n&gt;&gt;&gt; X = np.array([[0.1, 0.1], [0.9, 0.1], [0.5, 0.9], [0.2, 0.5]])\n&gt;&gt;&gt; candidates = tricands(X, fringe=True, p=0.5)\n&gt;&gt;&gt; print(candidates.shape)\n(7, 2)",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.tricands_interior.html",
    "href": "docs/reference/tricands.tricands.tricands_interior.html",
    "title": "tricands.tricands.tricands_interior",
    "section": "",
    "text": "tricands.tricands.tricands_interior(X)\nGenerate interior candidates using Delaunay triangulation.\nSubroutine used by tricands wrapper.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput design matrix of shape (n_samples, n_features).\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndict\ndict\nA dictionary containing: - ‘cand’ (np.ndarray): Candidate points (midpoints of triangles). - ‘tri’ (np.ndarray): Simplicies of the Delaunay triangulation.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nException\nIf the number of points is less than n_features + 1.",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands_interior"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.tricands_interior.html#parameters",
    "href": "docs/reference/tricands.tricands.tricands_interior.html#parameters",
    "title": "tricands.tricands.tricands_interior",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput design matrix of shape (n_samples, n_features).\nrequired",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands_interior"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.tricands_interior.html#returns",
    "href": "docs/reference/tricands.tricands.tricands_interior.html#returns",
    "title": "tricands.tricands.tricands_interior",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ndict\ndict\nA dictionary containing: - ‘cand’ (np.ndarray): Candidate points (midpoints of triangles). - ‘tri’ (np.ndarray): Simplicies of the Delaunay triangulation.",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands_interior"
    ]
  },
  {
    "objectID": "docs/reference/tricands.tricands.tricands_interior.html#raises",
    "href": "docs/reference/tricands.tricands.tricands_interior.html#raises",
    "title": "tricands.tricands.tricands_interior",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nException\nIf the number of points is less than n_features + 1.",
    "crumbs": [
      "API Reference",
      "Tricands",
      "tricands_interior"
    ]
  },
  {
    "objectID": "docs/reference/utils.boundaries.map_to_original_scale.html",
    "href": "docs/reference/utils.boundaries.map_to_original_scale.html",
    "title": "utils.boundaries.map_to_original_scale",
    "section": "",
    "text": "utils.boundaries.map_to_original_scale(X_search, x_min, x_max)\nMaps the values in X_search from the range [0, 1] to the original scale defined by x_min and x_max.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX_search\nUnion[pd.DataFrame, np.ndarray]\nA Pandas DataFrame or NumPy array containing the search points in the range [0, 1].\nrequired\n\n\nx_min\nnp.ndarray\nA NumPy array containing the minimum values for each feature in the original scale.\nrequired\n\n\nx_max\nnp.ndarray\nA NumPy array containing the maximum values for each feature in the original scale.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[pd.DataFrame, np.ndarray]\nUnion[pd.DataFrame, np.ndarray]: A Pandas DataFrame or NumPy array with the values mapped to the original scale.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.utils.boundaries import map_to_original_scale\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; X_search = pd.DataFrame([[0.5, 0.5], [0.25, 0.75]], columns=['x', 'y'])\n&gt;&gt;&gt; x_min = np.array([0, 0])\n&gt;&gt;&gt; x_max = np.array([10, 20])\n&gt;&gt;&gt; X_search_scaled = map_to_original_scale(X_search, x_min, x_max)\n&gt;&gt;&gt; print(X_search_scaled)\n      x     y\n0   5.0  10.0\n1   2.5  15.0",
    "crumbs": [
      "API Reference",
      "Utilities",
      "map_to_original_scale"
    ]
  },
  {
    "objectID": "docs/reference/utils.boundaries.map_to_original_scale.html#parameters",
    "href": "docs/reference/utils.boundaries.map_to_original_scale.html#parameters",
    "title": "utils.boundaries.map_to_original_scale",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX_search\nUnion[pd.DataFrame, np.ndarray]\nA Pandas DataFrame or NumPy array containing the search points in the range [0, 1].\nrequired\n\n\nx_min\nnp.ndarray\nA NumPy array containing the minimum values for each feature in the original scale.\nrequired\n\n\nx_max\nnp.ndarray\nA NumPy array containing the maximum values for each feature in the original scale.\nrequired",
    "crumbs": [
      "API Reference",
      "Utilities",
      "map_to_original_scale"
    ]
  },
  {
    "objectID": "docs/reference/utils.boundaries.map_to_original_scale.html#returns",
    "href": "docs/reference/utils.boundaries.map_to_original_scale.html#returns",
    "title": "utils.boundaries.map_to_original_scale",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nUnion[pd.DataFrame, np.ndarray]\nUnion[pd.DataFrame, np.ndarray]: A Pandas DataFrame or NumPy array with the values mapped to the original scale.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "map_to_original_scale"
    ]
  },
  {
    "objectID": "docs/reference/utils.boundaries.map_to_original_scale.html#examples",
    "href": "docs/reference/utils.boundaries.map_to_original_scale.html#examples",
    "title": "utils.boundaries.map_to_original_scale",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.utils.boundaries import map_to_original_scale\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; X_search = pd.DataFrame([[0.5, 0.5], [0.25, 0.75]], columns=['x', 'y'])\n&gt;&gt;&gt; x_min = np.array([0, 0])\n&gt;&gt;&gt; x_max = np.array([10, 20])\n&gt;&gt;&gt; X_search_scaled = map_to_original_scale(X_search, x_min, x_max)\n&gt;&gt;&gt; print(X_search_scaled)\n      x     y\n0   5.0  10.0\n1   2.5  15.0",
    "crumbs": [
      "API Reference",
      "Utilities",
      "map_to_original_scale"
    ]
  },
  {
    "objectID": "docs/reference/utils.eval.mo_cv_models.html",
    "href": "docs/reference/utils.eval.mo_cv_models.html",
    "title": "utils.eval.mo_cv_models",
    "section": "",
    "text": "utils.eval.mo_cv_models(X, y, model_define_func, cv=5, scores=None)\nPerforms cross-validation for separate models for each target in a multi-output problem.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame or np.ndarray\nFeature matrix.\nrequired\n\n\ny\npd.DataFrame or np.ndarray\nTarget matrix with multiple columns.\nrequired\n\n\nmodel_define_func\nCallable\nFunction that returns a fresh model or pipeline instance.\nrequired\n\n\ncv\nint or cross-validation generator, default=5\nNumber of folds or CV object.\n5\n\n\nscores\nUnion[Dict[str, str / Callable], str, Callable, None]\nScoring metric(s). - If None (default): Uses default scorer of the estimator. Returns List[np.ndarray]. - If str/Callable: Single scorer. Returns List[np.ndarray]. - If Dict: Dictionary of {name: scorer}. Returns Dict[str, List[np.ndarray]]. Note: Unlike mo_eval_models which takes raw functions, cross_val_score expects strings (e.g. ‘neg_mean_squared_error’) or make_scorer callables, or raw callables that fit the sklearn signature. To maintain similarity with mo_eval_models, we rely on cross_val_score’s flexibility.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nscores\nUnion[List[np.ndarray], Dict[str, List[np.ndarray]]]\n- List of arrays (one array of CV scores per target) if scores is None or single. - Dictionary of {name: List[np.ndarray]} if scores is a dictionary.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.utils.eval import mo_cv_models\n&gt;&gt;&gt; # Generate dummy data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; X = pd.DataFrame(np.random.rand(100, 5), columns=[f'x{i}' for i in range(5)])\n&gt;&gt;&gt; y = pd.DataFrame(np.random.rand(100, 3), columns=[f'y{i}' for i in range(3)])\n&gt;&gt;&gt; # Example 1: Default behavior (Default scorer, e.g. R2)\n&gt;&gt;&gt; def make_model():\n...     from sklearn.linear_model import Ridge\n...     return Ridge()\n&gt;&gt;&gt; cv_scores = mo_cv_models(X, y, make_model, cv=3)\nCross-validating target 1/3...\nCross-validating target 2/3...\nCross-validating target 3/3...\nCV Scores Mean: ['-0.14', '-0.07', '-0.20']\n&gt;&gt;&gt; # Example 2: Custom single score (NMSE - Negative Mean Squared Error)\n&gt;&gt;&gt; # Note: sklearn scoring strings are preferred for cross_val_score\n&gt;&gt;&gt; nmse_scores = mo_cv_models(X, y, make_model, cv=3, scores='neg_mean_squared_error')\nCross-validating target 1/3...\nCross-validating target 2/3...\nCross-validating target 3/3...\nCV Scores Mean: ['-0.09', '-0.08', '-0.10']\n&gt;&gt;&gt; # Example 3: Multiple custom scores\n&gt;&gt;&gt; my_scores = {'R2': 'r2', 'NMSE': 'neg_mean_squared_error'}\n&gt;&gt;&gt; all_cv_scores = mo_cv_models(X, y, make_model, cv=3, scores=my_scores)\nCross-validating target 1/3...\nCross-validating target 2/3...\nCross-validating target 3/3...\nCV Scores Mean:\n  R2: ['-0.14', '-0.07', '-0.20']\n  NMSE: ['-0.09', '-0.08', '-0.10']",
    "crumbs": [
      "API Reference",
      "Utilities",
      "mo_cv_models"
    ]
  },
  {
    "objectID": "docs/reference/utils.eval.mo_cv_models.html#parameters",
    "href": "docs/reference/utils.eval.mo_cv_models.html#parameters",
    "title": "utils.eval.mo_cv_models",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\npd.DataFrame or np.ndarray\nFeature matrix.\nrequired\n\n\ny\npd.DataFrame or np.ndarray\nTarget matrix with multiple columns.\nrequired\n\n\nmodel_define_func\nCallable\nFunction that returns a fresh model or pipeline instance.\nrequired\n\n\ncv\nint or cross-validation generator, default=5\nNumber of folds or CV object.\n5\n\n\nscores\nUnion[Dict[str, str / Callable], str, Callable, None]\nScoring metric(s). - If None (default): Uses default scorer of the estimator. Returns List[np.ndarray]. - If str/Callable: Single scorer. Returns List[np.ndarray]. - If Dict: Dictionary of {name: scorer}. Returns Dict[str, List[np.ndarray]]. Note: Unlike mo_eval_models which takes raw functions, cross_val_score expects strings (e.g. ‘neg_mean_squared_error’) or make_scorer callables, or raw callables that fit the sklearn signature. To maintain similarity with mo_eval_models, we rely on cross_val_score’s flexibility.\nNone",
    "crumbs": [
      "API Reference",
      "Utilities",
      "mo_cv_models"
    ]
  },
  {
    "objectID": "docs/reference/utils.eval.mo_cv_models.html#returns",
    "href": "docs/reference/utils.eval.mo_cv_models.html#returns",
    "title": "utils.eval.mo_cv_models",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nscores\nUnion[List[np.ndarray], Dict[str, List[np.ndarray]]]\n- List of arrays (one array of CV scores per target) if scores is None or single. - Dictionary of {name: List[np.ndarray]} if scores is a dictionary.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "mo_cv_models"
    ]
  },
  {
    "objectID": "docs/reference/utils.eval.mo_cv_models.html#examples",
    "href": "docs/reference/utils.eval.mo_cv_models.html#examples",
    "title": "utils.eval.mo_cv_models",
    "section": "",
    "text": "&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.utils.eval import mo_cv_models\n&gt;&gt;&gt; # Generate dummy data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; X = pd.DataFrame(np.random.rand(100, 5), columns=[f'x{i}' for i in range(5)])\n&gt;&gt;&gt; y = pd.DataFrame(np.random.rand(100, 3), columns=[f'y{i}' for i in range(3)])\n&gt;&gt;&gt; # Example 1: Default behavior (Default scorer, e.g. R2)\n&gt;&gt;&gt; def make_model():\n...     from sklearn.linear_model import Ridge\n...     return Ridge()\n&gt;&gt;&gt; cv_scores = mo_cv_models(X, y, make_model, cv=3)\nCross-validating target 1/3...\nCross-validating target 2/3...\nCross-validating target 3/3...\nCV Scores Mean: ['-0.14', '-0.07', '-0.20']\n&gt;&gt;&gt; # Example 2: Custom single score (NMSE - Negative Mean Squared Error)\n&gt;&gt;&gt; # Note: sklearn scoring strings are preferred for cross_val_score\n&gt;&gt;&gt; nmse_scores = mo_cv_models(X, y, make_model, cv=3, scores='neg_mean_squared_error')\nCross-validating target 1/3...\nCross-validating target 2/3...\nCross-validating target 3/3...\nCV Scores Mean: ['-0.09', '-0.08', '-0.10']\n&gt;&gt;&gt; # Example 3: Multiple custom scores\n&gt;&gt;&gt; my_scores = {'R2': 'r2', 'NMSE': 'neg_mean_squared_error'}\n&gt;&gt;&gt; all_cv_scores = mo_cv_models(X, y, make_model, cv=3, scores=my_scores)\nCross-validating target 1/3...\nCross-validating target 2/3...\nCross-validating target 3/3...\nCV Scores Mean:\n  R2: ['-0.14', '-0.07', '-0.20']\n  NMSE: ['-0.09', '-0.08', '-0.10']",
    "crumbs": [
      "API Reference",
      "Utilities",
      "mo_cv_models"
    ]
  },
  {
    "objectID": "docs/reference/utils.eval.html",
    "href": "docs/reference/utils.eval.html",
    "title": "utils.eval",
    "section": "",
    "text": "utils.eval\n\n\n\n\n\nName\nDescription\n\n\n\n\nmo_cv_models\nPerforms cross-validation for separate models for each target in a multi-output problem.\n\n\nmo_eval_models\nTrains and evaluates separate models for each target in a multi-output regression problem.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.eval"
    ]
  },
  {
    "objectID": "docs/reference/utils.eval.html#functions",
    "href": "docs/reference/utils.eval.html#functions",
    "title": "utils.eval",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nmo_cv_models\nPerforms cross-validation for separate models for each target in a multi-output problem.\n\n\nmo_eval_models\nTrains and evaluates separate models for each target in a multi-output regression problem.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.eval"
    ]
  },
  {
    "objectID": "docs/reference/utils.file.html",
    "href": "docs/reference/utils.file.html",
    "title": "utils.file",
    "section": "",
    "text": "utils.file\n\n\n\n\n\nName\nDescription\n\n\n\n\nget_experiment_filename\nGenerates a standardized filename for experiments.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.file"
    ]
  },
  {
    "objectID": "docs/reference/utils.file.html#functions",
    "href": "docs/reference/utils.file.html#functions",
    "title": "utils.file",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nget_experiment_filename\nGenerates a standardized filename for experiments.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.file"
    ]
  },
  {
    "objectID": "docs/reference/utils.mapping.html",
    "href": "docs/reference/utils.mapping.html",
    "title": "utils.mapping",
    "section": "",
    "text": "utils.mapping\nLearning rate mapping functions for unified optimizer interface.\nThis module provides utilities to map a unified learning rate scale to optimizer-specific learning rates, accounting for the different default and typical ranges used by different PyTorch optimizers.\n\n\n\n\n\nName\nDescription\n\n\n\n\nmap_lr\nMap a unified learning rate to an optimizer-specific learning rate.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.mapping"
    ]
  },
  {
    "objectID": "docs/reference/utils.mapping.html#functions",
    "href": "docs/reference/utils.mapping.html#functions",
    "title": "utils.mapping",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nmap_lr\nMap a unified learning rate to an optimizer-specific learning rate.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.mapping"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.get_pca.html",
    "href": "docs/reference/utils.pca.get_pca.html",
    "title": "utils.pca.get_pca",
    "section": "",
    "text": "utils.pca.get_pca(df, n_components=3)\nScale the numeric data and perform PCA.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nInput DataFrame.\nrequired\n\n\nn_components\nint\nNumber of principal components to compute. Defaults to 3.\n3\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntuple\ntuple\n- pca (PCA): Fitted PCA object. - scaled_data (np.ndarray): Scaled numeric data. - feature_names (pd.Index): Names of the numeric features. - sample_names (pd.Index): Index of the samples. - pca_data (np.ndarray): PCA-transformed data.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotpython.utils.pca import get_pca\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"A\": [1, 2, 3],\n...     \"B\": [4, 5, 6],\n...     \"C\": [\"x\", \"y\", \"z\"]  # Non-numeric column will be ignored\n... })\n&gt;&gt;&gt; pca, scaled_data, feature_names, sample_names, pca_data = get_pca(df)\n&gt;&gt;&gt; print(feature_names)\nIndex(['A', 'B'], dtype='object')\n&gt;&gt;&gt; print(pca_data.shape)\n(3, 2)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_pca"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.get_pca.html#parameters",
    "href": "docs/reference/utils.pca.get_pca.html#parameters",
    "title": "utils.pca.get_pca",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nInput DataFrame.\nrequired\n\n\nn_components\nint\nNumber of principal components to compute. Defaults to 3.\n3",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_pca"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.get_pca.html#returns",
    "href": "docs/reference/utils.pca.get_pca.html#returns",
    "title": "utils.pca.get_pca",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ntuple\ntuple\n- pca (PCA): Fitted PCA object. - scaled_data (np.ndarray): Scaled numeric data. - feature_names (pd.Index): Names of the numeric features. - sample_names (pd.Index): Index of the samples. - pca_data (np.ndarray): PCA-transformed data.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_pca"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.get_pca.html#examples",
    "href": "docs/reference/utils.pca.get_pca.html#examples",
    "title": "utils.pca.get_pca",
    "section": "",
    "text": "&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotpython.utils.pca import get_pca\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"A\": [1, 2, 3],\n...     \"B\": [4, 5, 6],\n...     \"C\": [\"x\", \"y\", \"z\"]  # Non-numeric column will be ignored\n... })\n&gt;&gt;&gt; pca, scaled_data, feature_names, sample_names, pca_data = get_pca(df)\n&gt;&gt;&gt; print(feature_names)\nIndex(['A', 'B'], dtype='object')\n&gt;&gt;&gt; print(pca_data.shape)\n(3, 2)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_pca"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.plot_loading_scores.html",
    "href": "docs/reference/utils.pca.plot_loading_scores.html",
    "title": "utils.pca.plot_loading_scores",
    "section": "",
    "text": "utils.pca.plot_loading_scores(loading_scores, figsize=(12, 8))\nCreates a heatmap visualization of PCA loading scores.\nGenerates a heatmap showing the relationship between original features and principal components, with color intensity indicating the strength and direction of the relationship.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nloading_scores\npd.DataFrame\nDataFrame containing the loading scores matrix with features as rows and principal components as columns.\nrequired\n\n\nfigsize\ntuple\nSize of the figure as (width, height). Defaults to (12, 8).\n(12, 8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nNone\nNone\nThe function creates and displays a matplotlib plot.\n\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA from sklearn.datasets import load_iris from spotpython.utils.pca import print_loading_scores, plot_loading_scores\n\niris = load_iris() X = iris.data feature_names = iris.feature_names\n\npca = PCA() pca.fit(X) scores_df = print_loading_scores(pca, feature_names)\n\nplot_loading_scores(scores_df, figsize=(10, 6))",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_loading_scores"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.plot_loading_scores.html#parameters",
    "href": "docs/reference/utils.pca.plot_loading_scores.html#parameters",
    "title": "utils.pca.plot_loading_scores",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nloading_scores\npd.DataFrame\nDataFrame containing the loading scores matrix with features as rows and principal components as columns.\nrequired\n\n\nfigsize\ntuple\nSize of the figure as (width, height). Defaults to (12, 8).\n(12, 8)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_loading_scores"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.plot_loading_scores.html#returns",
    "href": "docs/reference/utils.pca.plot_loading_scores.html#returns",
    "title": "utils.pca.plot_loading_scores",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nNone\nNone\nThe function creates and displays a matplotlib plot.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_loading_scores"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.plot_loading_scores.html#example",
    "href": "docs/reference/utils.pca.plot_loading_scores.html#example",
    "title": "utils.pca.plot_loading_scores",
    "section": "",
    "text": "from sklearn.decomposition import PCA from sklearn.datasets import load_iris from spotpython.utils.pca import print_loading_scores, plot_loading_scores\n\niris = load_iris() X = iris.data feature_names = iris.feature_names\n\npca = PCA() pca.fit(X) scores_df = print_loading_scores(pca, feature_names)\n\nplot_loading_scores(scores_df, figsize=(10, 6))",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_loading_scores"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.plot_pca_scree.html",
    "href": "docs/reference/utils.pca.plot_pca_scree.html",
    "title": "utils.pca.plot_pca_scree",
    "section": "",
    "text": "utils.pca.plot_pca_scree(pca, df_name='', max_scree=None, figsize=(12, 6))\nPlot the scree plot for Principal Component Analysis (PCA).\nA scree plot shows the percentage of variance explained by each principal component in descending order. It helps in determining the optimal number of components to retain.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npca\nsklearn.decomposition.PCA\nFitted PCA object containing the explained variance ratios.\nrequired\n\n\ndf_name\nstr\nName of the dataset to be displayed in the plot title. Defaults to empty string.\n''\n\n\nmax_scree\nint\nMaximum number of principal components to plot. If None, all components are plotted. Defaults to None.\nNone\n\n\nfigsize\ntuple\nSize of the figure as (width, height). Defaults to (12, 6).\n(12, 6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nNone\nNone\nThe function creates and displays a matplotlib plot.\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.decomposition import PCA\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from spotpython.utils.pca import plot_pca_scree\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load iris dataset\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X = iris.data\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit PCA\n&gt;&gt;&gt; pca = PCA()\n&gt;&gt;&gt; pca.fit(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create scree plot\n&gt;&gt;&gt; plot_pca_scree(pca,\n...                df_name=\"Iris Dataset\",\n...                max_scree=4,\n...                figsize=(10, 5))",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_pca_scree"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.plot_pca_scree.html#parameters",
    "href": "docs/reference/utils.pca.plot_pca_scree.html#parameters",
    "title": "utils.pca.plot_pca_scree",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npca\nsklearn.decomposition.PCA\nFitted PCA object containing the explained variance ratios.\nrequired\n\n\ndf_name\nstr\nName of the dataset to be displayed in the plot title. Defaults to empty string.\n''\n\n\nmax_scree\nint\nMaximum number of principal components to plot. If None, all components are plotted. Defaults to None.\nNone\n\n\nfigsize\ntuple\nSize of the figure as (width, height). Defaults to (12, 6).\n(12, 6)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_pca_scree"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.plot_pca_scree.html#returns",
    "href": "docs/reference/utils.pca.plot_pca_scree.html#returns",
    "title": "utils.pca.plot_pca_scree",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nNone\nNone\nThe function creates and displays a matplotlib plot.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_pca_scree"
    ]
  },
  {
    "objectID": "docs/reference/utils.pca.plot_pca_scree.html#examples",
    "href": "docs/reference/utils.pca.plot_pca_scree.html#examples",
    "title": "utils.pca.plot_pca_scree",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.decomposition import PCA\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from spotpython.utils.pca import plot_pca_scree\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load iris dataset\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X = iris.data\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit PCA\n&gt;&gt;&gt; pca = PCA()\n&gt;&gt;&gt; pca.fit(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create scree plot\n&gt;&gt;&gt; plot_pca_scree(pca,\n...                df_name=\"Iris Dataset\",\n...                max_scree=4,\n...                figsize=(10, 5))",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_pca_scree"
    ]
  },
  {
    "objectID": "docs/reference/utils.html",
    "href": "docs/reference/utils.html",
    "title": "utils",
    "section": "",
    "text": "utils\nutils\nUtility functions for spotoptim.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils"
    ]
  },
  {
    "objectID": "docs/reference/utils.scaler.html",
    "href": "docs/reference/utils.scaler.html",
    "title": "utils.scaler",
    "section": "",
    "text": "utils.scaler\n\n\n\n\n\nName\nDescription\n\n\n\n\nTorchStandardScaler\nA class for scaling data using standardization with torch tensors.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.scaler"
    ]
  },
  {
    "objectID": "docs/reference/utils.scaler.html#classes",
    "href": "docs/reference/utils.scaler.html#classes",
    "title": "utils.scaler",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nTorchStandardScaler\nA class for scaling data using standardization with torch tensors.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "utils.scaler"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.compute_coefficients_table.html",
    "href": "docs/reference/utils.stats.compute_coefficients_table.html",
    "title": "utils.stats.compute_coefficients_table",
    "section": "",
    "text": "utils.stats.compute_coefficients_table(model, X_encoded, y, vif_table=None)\n\n\n\nVariable name\nZero-order correlation\nPartial correlation\nSemipartial (part) correlation\nTolerance (1 / VIF)\nVIF\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodel\nstatsmodels.regression.linear_model.RegressionResultsWrapper\nA fitted OLS model from statsmodels.\nrequired\n\n\nX_encoded\npd.DataFrame\nThe DataFrame used to fit the model, including ‘const’.\nrequired\n\n\ny\npd.Series\nDependent variable used in fitting the model.\nrequired\n\n\nvif_table\npd.DataFrame\nA DataFrame with columns [“feature”, “VIF”] for each column in X_encoded (typ. from statsmodels.stats.outliers_influence.variance_inflation_factor). Default is None.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame with columns: - “Variable” - “Zero-Order r” - “Partial r” - “Semipartial r” - “Tolerance” - “VIF”\n\n\n\n\n\n\n&gt;&gt;&gt; from spotpython.utils.stats import compute_coefficients_table\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import statsmodels.api as sm\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; X = sm.add_constant(data)\n&gt;&gt;&gt; model = sm.OLS(y, X).fit()\n&gt;&gt;&gt; vif_table = pd.DataFrame({\n...     'feature': ['x1', 'x2', 'x3'],\n...     'VIF': [1, 2, 3]\n... })\n&gt;&gt;&gt; compute_coefficients_table(model, data, y, vif_table)\n   Variable  Zero-Order r  Partial r  Semipartial r  Tolerance  VIF\n0       x1           0.0        0.0            0.0        1.0  1.0\n1       x2           0.0        0.0            0.0        0.5  2.0\n2       x3           0.0        0.0            0.0        0.333333  3.0",
    "crumbs": [
      "API Reference",
      "Utilities",
      "compute_coefficients_table"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.compute_coefficients_table.html#compute-a-coefficients-table-containing",
    "href": "docs/reference/utils.stats.compute_coefficients_table.html#compute-a-coefficients-table-containing",
    "title": "utils.stats.compute_coefficients_table",
    "section": "",
    "text": "Variable name\nZero-order correlation\nPartial correlation\nSemipartial (part) correlation\nTolerance (1 / VIF)\nVIF",
    "crumbs": [
      "API Reference",
      "Utilities",
      "compute_coefficients_table"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.compute_coefficients_table.html#parameters",
    "href": "docs/reference/utils.stats.compute_coefficients_table.html#parameters",
    "title": "utils.stats.compute_coefficients_table",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmodel\nstatsmodels.regression.linear_model.RegressionResultsWrapper\nA fitted OLS model from statsmodels.\nrequired\n\n\nX_encoded\npd.DataFrame\nThe DataFrame used to fit the model, including ‘const’.\nrequired\n\n\ny\npd.Series\nDependent variable used in fitting the model.\nrequired\n\n\nvif_table\npd.DataFrame\nA DataFrame with columns [“feature”, “VIF”] for each column in X_encoded (typ. from statsmodels.stats.outliers_influence.variance_inflation_factor). Default is None.\nNone",
    "crumbs": [
      "API Reference",
      "Utilities",
      "compute_coefficients_table"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.compute_coefficients_table.html#returns",
    "href": "docs/reference/utils.stats.compute_coefficients_table.html#returns",
    "title": "utils.stats.compute_coefficients_table",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame with columns: - “Variable” - “Zero-Order r” - “Partial r” - “Semipartial r” - “Tolerance” - “VIF”",
    "crumbs": [
      "API Reference",
      "Utilities",
      "compute_coefficients_table"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.compute_coefficients_table.html#examples",
    "href": "docs/reference/utils.stats.compute_coefficients_table.html#examples",
    "title": "utils.stats.compute_coefficients_table",
    "section": "",
    "text": "&gt;&gt;&gt; from spotpython.utils.stats import compute_coefficients_table\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import statsmodels.api as sm\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; X = sm.add_constant(data)\n&gt;&gt;&gt; model = sm.OLS(y, X).fit()\n&gt;&gt;&gt; vif_table = pd.DataFrame({\n...     'feature': ['x1', 'x2', 'x3'],\n...     'VIF': [1, 2, 3]\n... })\n&gt;&gt;&gt; compute_coefficients_table(model, data, y, vif_table)\n   Variable  Zero-Order r  Partial r  Semipartial r  Tolerance  VIF\n0       x1           0.0        0.0            0.0        1.0  1.0\n1       x2           0.0        0.0            0.0        0.5  2.0\n2       x3           0.0        0.0            0.0        0.333333  3.0",
    "crumbs": [
      "API Reference",
      "Utilities",
      "compute_coefficients_table"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.condition_index.html",
    "href": "docs/reference/utils.stats.condition_index.html",
    "title": "utils.stats.condition_index",
    "section": "",
    "text": "utils.stats.condition_index(df)\nCalculates the Condition Index for a DataFrame to assess multicollinearity.\nThe Condition Index is computed based on the eigenvalues of the covariance matrix of the standardized data. High condition indices suggest potential multicollinearity issues.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npandas.DataFrame\nA DataFrame containing the independent variables.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npandas.DataFrame: A DataFrame with the following columns: - ‘Index’: The index of the eigenvalue. - ‘Eigenvalue’: The eigenvalue of the covariance matrix. - ‘Condition Index’: The Condition Index for the eigenvalue.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotpython.utils.stats import condition_index\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; condition_index(data)\n   Index  Eigenvalue  Condition Index\n0      0    1.140000         1.000000\n1      1    0.000000              inf\n2      2    0.002857        20.000000",
    "crumbs": [
      "API Reference",
      "Utilities",
      "condition_index"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.condition_index.html#parameters",
    "href": "docs/reference/utils.stats.condition_index.html#parameters",
    "title": "utils.stats.condition_index",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndf\npandas.DataFrame\nA DataFrame containing the independent variables.\nrequired",
    "crumbs": [
      "API Reference",
      "Utilities",
      "condition_index"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.condition_index.html#returns",
    "href": "docs/reference/utils.stats.condition_index.html#returns",
    "title": "utils.stats.condition_index",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\npd.DataFrame\npandas.DataFrame: A DataFrame with the following columns: - ‘Index’: The index of the eigenvalue. - ‘Eigenvalue’: The eigenvalue of the covariance matrix. - ‘Condition Index’: The Condition Index for the eigenvalue.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "condition_index"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.condition_index.html#examples",
    "href": "docs/reference/utils.stats.condition_index.html#examples",
    "title": "utils.stats.condition_index",
    "section": "",
    "text": "&gt;&gt;&gt; from spotpython.utils.stats import condition_index\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; condition_index(data)\n   Index  Eigenvalue  Condition Index\n0      0    1.140000         1.000000\n1      1    0.000000              inf\n2      2    0.002857        20.000000",
    "crumbs": [
      "API Reference",
      "Utilities",
      "condition_index"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.fit_all_lm.html",
    "href": "docs/reference/utils.stats.fit_all_lm.html",
    "title": "utils.stats.fit_all_lm",
    "section": "",
    "text": "utils.stats.fit_all_lm(basic, xlist, data, remove_na=True)\nFit a linear regression model for all possible combinations of independent variables.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbasic\nstr\nThe basic model formula.\nrequired\n\n\nxlist\nlist\nA list of independent variables.\nrequired\n\n\ndata\npandas.DataFrame\nThe data frame containing the variables.\nrequired\n\n\nremove_na\nbool\nWhether to remove missing values from the data frame.\nTrue\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndict\ndict\nA dictionary containing the estimated coefficients, confidence intervals, p-values, AIC values, sample size, and the basic model formula.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotpython.utils.stats import fit_all_lm\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'y': [1, 2, 3],\n&gt;&gt;&gt;     'x1': [4, 5, 6],\n&gt;&gt;&gt;     'x2': [7, 8, 9]\n&gt;&gt;&gt; })\n&gt;&gt;&gt; fit_all_lm(\"y ~ x1\", [\"x2\"], data)\n{'estimate':   variables  estimate  conf_low  conf_high    p         aic  n\n0    basic  1.000000  1.000000   1.000000  0.0  0.000000  3\n1       x2  1.000000  1.000000   1.000000  0.0  0.000000  3}",
    "crumbs": [
      "API Reference",
      "Utilities",
      "fit_all_lm"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.fit_all_lm.html#parameters",
    "href": "docs/reference/utils.stats.fit_all_lm.html#parameters",
    "title": "utils.stats.fit_all_lm",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbasic\nstr\nThe basic model formula.\nrequired\n\n\nxlist\nlist\nA list of independent variables.\nrequired\n\n\ndata\npandas.DataFrame\nThe data frame containing the variables.\nrequired\n\n\nremove_na\nbool\nWhether to remove missing values from the data frame.\nTrue",
    "crumbs": [
      "API Reference",
      "Utilities",
      "fit_all_lm"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.fit_all_lm.html#returns",
    "href": "docs/reference/utils.stats.fit_all_lm.html#returns",
    "title": "utils.stats.fit_all_lm",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ndict\ndict\nA dictionary containing the estimated coefficients, confidence intervals, p-values, AIC values, sample size, and the basic model formula.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "fit_all_lm"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.fit_all_lm.html#examples",
    "href": "docs/reference/utils.stats.fit_all_lm.html#examples",
    "title": "utils.stats.fit_all_lm",
    "section": "",
    "text": "&gt;&gt;&gt; from spotpython.utils.stats import fit_all_lm\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'y': [1, 2, 3],\n&gt;&gt;&gt;     'x1': [4, 5, 6],\n&gt;&gt;&gt;     'x2': [7, 8, 9]\n&gt;&gt;&gt; })\n&gt;&gt;&gt; fit_all_lm(\"y ~ x1\", [\"x2\"], data)\n{'estimate':   variables  estimate  conf_low  conf_high    p         aic  n\n0    basic  1.000000  1.000000   1.000000  0.0  0.000000  3\n1       x2  1.000000  1.000000   1.000000  0.0  0.000000  3}",
    "crumbs": [
      "API Reference",
      "Utilities",
      "fit_all_lm"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.get_combinations.html",
    "href": "docs/reference/utils.stats.get_combinations.html",
    "title": "utils.stats.get_combinations",
    "section": "",
    "text": "utils.stats.get_combinations(ind_list, type='indices')\nGenerates all possible combinations of two values from a list of values. Order is not important.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nind_list\nlist\nA list of target indices.\nrequired\n\n\ntype\nstr\nThe type of output, either ‘values’ or ‘indices’. Default is ‘indices’.\n'indices'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nlist\nlist\nA list of tuples, where each tuple contains a combination of two values. The order of the values within a tuple is not important, and each combination appears only once.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.utils import get_combinations\n&gt;&gt;&gt; ind_list = [0, 10, 20, 30]\n&gt;&gt;&gt; combinations = get_combinations(ind_list)\n&gt;&gt;&gt; combinations = get_combinations(ind_list, type='indices')\n    [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n&gt;&gt;&gt; print(combinations, type='values')\n    [(0, 10), (0, 20), (0, 30), (1, 20), (1, 30), (2, 30)]",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_combinations"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.get_combinations.html#parameters",
    "href": "docs/reference/utils.stats.get_combinations.html#parameters",
    "title": "utils.stats.get_combinations",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nind_list\nlist\nA list of target indices.\nrequired\n\n\ntype\nstr\nThe type of output, either ‘values’ or ‘indices’. Default is ‘indices’.\n'indices'",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_combinations"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.get_combinations.html#returns",
    "href": "docs/reference/utils.stats.get_combinations.html#returns",
    "title": "utils.stats.get_combinations",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nlist\nlist\nA list of tuples, where each tuple contains a combination of two values. The order of the values within a tuple is not important, and each combination appears only once.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_combinations"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.get_combinations.html#examples",
    "href": "docs/reference/utils.stats.get_combinations.html#examples",
    "title": "utils.stats.get_combinations",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.utils import get_combinations\n&gt;&gt;&gt; ind_list = [0, 10, 20, 30]\n&gt;&gt;&gt; combinations = get_combinations(ind_list)\n&gt;&gt;&gt; combinations = get_combinations(ind_list, type='indices')\n    [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n&gt;&gt;&gt; print(combinations, type='values')\n    [(0, 10), (0, 20), (0, 30), (1, 20), (1, 30), (2, 30)]",
    "crumbs": [
      "API Reference",
      "Utilities",
      "get_combinations"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.normalize_X.html",
    "href": "docs/reference/utils.stats.normalize_X.html",
    "title": "utils.stats.normalize_X",
    "section": "",
    "text": "utils.stats.normalize_X(X, eps=1e-12)\nNormalize array X to [0, 1] in each dimension.\nFor dimensions where all values are identical (X_max == X_min), the normalized value is set to 0.5 to avoid division by zero.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput array of shape (n, d) to normalize.\nrequired\n\n\neps\nfloat\nSmall value to avoid division by zero when range is very small. Defaults to 1e-12.\n1e-12\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Normalized array with values in [0, 1] for each dimension. For constant dimensions, values are set to 0.5.\n\n\n\n\n\n\n&gt;&gt;&gt; from spotoptim.utils.stats import normalize_X\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; normalize_X(X)\narray([[0. , 0. ],\n       [0.5, 0.5],\n       [1. , 1. ]])\n&gt;&gt;&gt; # Constant dimension example\n&gt;&gt;&gt; X_const = np.array([[1, 5], [1, 5], [1, 5]])\n&gt;&gt;&gt; normalize_X(X_const)\narray([[0.5, 0.5],\n       [0.5, 0.5],\n       [0.5, 0.5]])",
    "crumbs": [
      "API Reference",
      "Utilities",
      "normalize_X"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.normalize_X.html#parameters",
    "href": "docs/reference/utils.stats.normalize_X.html#parameters",
    "title": "utils.stats.normalize_X",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nX\nnp.ndarray\nInput array of shape (n, d) to normalize.\nrequired\n\n\neps\nfloat\nSmall value to avoid division by zero when range is very small. Defaults to 1e-12.\n1e-12",
    "crumbs": [
      "API Reference",
      "Utilities",
      "normalize_X"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.normalize_X.html#returns",
    "href": "docs/reference/utils.stats.normalize_X.html#returns",
    "title": "utils.stats.normalize_X",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Normalized array with values in [0, 1] for each dimension. For constant dimensions, values are set to 0.5.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "normalize_X"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.normalize_X.html#examples",
    "href": "docs/reference/utils.stats.normalize_X.html#examples",
    "title": "utils.stats.normalize_X",
    "section": "",
    "text": "&gt;&gt;&gt; from spotoptim.utils.stats import normalize_X\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; normalize_X(X)\narray([[0. , 0. ],\n       [0.5, 0.5],\n       [1. , 1. ]])\n&gt;&gt;&gt; # Constant dimension example\n&gt;&gt;&gt; X_const = np.array([[1, 5], [1, 5], [1, 5]])\n&gt;&gt;&gt; normalize_X(X_const)\narray([[0.5, 0.5],\n       [0.5, 0.5],\n       [0.5, 0.5]])",
    "crumbs": [
      "API Reference",
      "Utilities",
      "normalize_X"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.partial_correlation.html",
    "href": "docs/reference/utils.stats.partial_correlation.html",
    "title": "utils.stats.partial_correlation",
    "section": "",
    "text": "utils.stats.partial_correlation(x, method='pearson')\nCalculate the partial correlation matrix for a given data set.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nx\npandas.DataFrame or numpy.ndarray\nThe data matrix with variables as columns.\nrequired\n\n\nmethod\nstr\nCorrelation method, one of ‘pearson’, ‘kendall’, or ‘spearman’.\n'pearson'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndict\ndict\nA dictionary containing the partial correlation estimates, p-values, statistics, sample size (n), number of given parameters (gp), and method used.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf input is not a matrix-like structure or not numeric.\n\n\n\n\n\n\n\nKim, S. ppcor: An R package for a fast calculation to semi-partial correlation coefficients. Commun Stat Appl Methods 22, 6 (Nov 2015), 665–674.\n\n\n\n\n&gt;&gt;&gt; from spotpython.utils.stats import partial_correlation\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'A': [1, 2, 3],\n&gt;&gt;&gt;     'B': [4, 5, 6],\n&gt;&gt;&gt;     'C': [7, 8, 9]\n&gt;&gt;&gt; })\n&gt;&gt;&gt; partial_correlation(data, method='pearson')\n{'estimate': array([[ 1. , -1. ,  1. ],\n                    [-1. ,  1. , -1. ],\n                    [ 1. , -1. ,  1. ]]),\n'p_value': array([[0. , 0. , 0. ],\n                  [0. , 0. , 0. ],\n                  [0. , 0. , 0. ]]), ...\n}",
    "crumbs": [
      "API Reference",
      "Utilities",
      "partial_correlation"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.partial_correlation.html#parameters",
    "href": "docs/reference/utils.stats.partial_correlation.html#parameters",
    "title": "utils.stats.partial_correlation",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nx\npandas.DataFrame or numpy.ndarray\nThe data matrix with variables as columns.\nrequired\n\n\nmethod\nstr\nCorrelation method, one of ‘pearson’, ‘kendall’, or ‘spearman’.\n'pearson'",
    "crumbs": [
      "API Reference",
      "Utilities",
      "partial_correlation"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.partial_correlation.html#returns",
    "href": "docs/reference/utils.stats.partial_correlation.html#returns",
    "title": "utils.stats.partial_correlation",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ndict\ndict\nA dictionary containing the partial correlation estimates, p-values, statistics, sample size (n), number of given parameters (gp), and method used.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "partial_correlation"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.partial_correlation.html#raises",
    "href": "docs/reference/utils.stats.partial_correlation.html#raises",
    "title": "utils.stats.partial_correlation",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf input is not a matrix-like structure or not numeric.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "partial_correlation"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.partial_correlation.html#references",
    "href": "docs/reference/utils.stats.partial_correlation.html#references",
    "title": "utils.stats.partial_correlation",
    "section": "",
    "text": "Kim, S. ppcor: An R package for a fast calculation to semi-partial correlation coefficients. Commun Stat Appl Methods 22, 6 (Nov 2015), 665–674.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "partial_correlation"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.partial_correlation.html#examples",
    "href": "docs/reference/utils.stats.partial_correlation.html#examples",
    "title": "utils.stats.partial_correlation",
    "section": "",
    "text": "&gt;&gt;&gt; from spotpython.utils.stats import partial_correlation\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'A': [1, 2, 3],\n&gt;&gt;&gt;     'B': [4, 5, 6],\n&gt;&gt;&gt;     'C': [7, 8, 9]\n&gt;&gt;&gt; })\n&gt;&gt;&gt; partial_correlation(data, method='pearson')\n{'estimate': array([[ 1. , -1. ,  1. ],\n                    [-1. ,  1. , -1. ],\n                    [ 1. , -1. ,  1. ]]),\n'p_value': array([[0. , 0. , 0. ],\n                  [0. , 0. , 0. ],\n                  [0. , 0. , 0. ]]), ...\n}",
    "crumbs": [
      "API Reference",
      "Utilities",
      "partial_correlation"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.plot_coeff_vs_pvals.html",
    "href": "docs/reference/utils.stats.plot_coeff_vs_pvals.html",
    "title": "utils.stats.plot_coeff_vs_pvals",
    "section": "",
    "text": "utils.stats.plot_coeff_vs_pvals(\n    data,\n    xlabels=None,\n    xlim=(0, 1),\n    xlab='p-value',\n    ylim=None,\n    ylab=None,\n    xscale_log=True,\n    yscale_log=False,\n    title=None,\n    show=True,\n    y_scaler=1.1,\n)\nPlot the coefficient estimates from fit_all_lm against the corresponding p-values.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\ndict\nA dictionary containing the estimated coefficients, p-values, and other information. Generated by the fit_all_lm function.\nrequired\n\n\nxlabels\nlist\nA list of x-axis labels.\nNone\n\n\nxlim\ntuple\nA tuple of the x-axis limits.\n(0, 1)\n\n\nxlab\nstr\nThe x-axis label.\n'p-value'\n\n\nylim\ntuple\nA tuple of the y-axis limits.\nNone\n\n\nylab\nstr\nThe y-axis label.\nNone\n\n\nxscale_log\nbool\nWhether to use a log scale on the x-axis.\nTrue\n\n\nyscale_log\nbool\nWhether to use a log scale on the y-axis.\nFalse\n\n\ntitle\nstr\nThe plot title.\nNone\n\n\nshow\nbool\nWhether to display the plot.\nTrue\n\n\ny_scaler\nfloat\nA scaling factor for the y-axis limits. Default is 1.1, i.e., 10% more than the maximum value.\n1.1\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\nBased on the R package ‘allestimates’ by Zhiqiang Wang, see https://cran.r-project.org/package=allestimates\n\n\n\n\nWang, Z. (2007). Two Postestimation Commands for Assessing Confounding Effects in Epidemiological Studies. The Stata Journal, 7(2), 183-196. https://doi.org/10.1177/1536867X0700700203\n\n\n\n&gt;&gt;&gt; from spotpython.utils.stats import plot_coeff_vs_pvals, fit_all_lm\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'y': [1, 2, 3],\n&gt;&gt;&gt;     'x1': [4, 5, 6],\n&gt;&gt;&gt;     'x2': [7, 8, 9]\n&gt;&gt;&gt; })\n&gt;&gt;&gt; estimates = fit_all_lm(\"y ~ x1\", [\"x2\"], data)\n&gt;&gt;&gt; plot_coeff_vs_pvals(estimates)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_coeff_vs_pvals"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.plot_coeff_vs_pvals.html#parameters",
    "href": "docs/reference/utils.stats.plot_coeff_vs_pvals.html#parameters",
    "title": "utils.stats.plot_coeff_vs_pvals",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndata\ndict\nA dictionary containing the estimated coefficients, p-values, and other information. Generated by the fit_all_lm function.\nrequired\n\n\nxlabels\nlist\nA list of x-axis labels.\nNone\n\n\nxlim\ntuple\nA tuple of the x-axis limits.\n(0, 1)\n\n\nxlab\nstr\nThe x-axis label.\n'p-value'\n\n\nylim\ntuple\nA tuple of the y-axis limits.\nNone\n\n\nylab\nstr\nThe y-axis label.\nNone\n\n\nxscale_log\nbool\nWhether to use a log scale on the x-axis.\nTrue\n\n\nyscale_log\nbool\nWhether to use a log scale on the y-axis.\nFalse\n\n\ntitle\nstr\nThe plot title.\nNone\n\n\nshow\nbool\nWhether to display the plot.\nTrue\n\n\ny_scaler\nfloat\nA scaling factor for the y-axis limits. Default is 1.1, i.e., 10% more than the maximum value.\n1.1",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_coeff_vs_pvals"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.plot_coeff_vs_pvals.html#returns",
    "href": "docs/reference/utils.stats.plot_coeff_vs_pvals.html#returns",
    "title": "utils.stats.plot_coeff_vs_pvals",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nNone",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_coeff_vs_pvals"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.plot_coeff_vs_pvals.html#notes",
    "href": "docs/reference/utils.stats.plot_coeff_vs_pvals.html#notes",
    "title": "utils.stats.plot_coeff_vs_pvals",
    "section": "",
    "text": "Based on the R package ‘allestimates’ by Zhiqiang Wang, see https://cran.r-project.org/package=allestimates",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_coeff_vs_pvals"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.plot_coeff_vs_pvals.html#references",
    "href": "docs/reference/utils.stats.plot_coeff_vs_pvals.html#references",
    "title": "utils.stats.plot_coeff_vs_pvals",
    "section": "",
    "text": "Wang, Z. (2007). Two Postestimation Commands for Assessing Confounding Effects in Epidemiological Studies. The Stata Journal, 7(2), 183-196. https://doi.org/10.1177/1536867X0700700203",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_coeff_vs_pvals"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.plot_coeff_vs_pvals.html#examples",
    "href": "docs/reference/utils.stats.plot_coeff_vs_pvals.html#examples",
    "title": "utils.stats.plot_coeff_vs_pvals",
    "section": "",
    "text": "&gt;&gt;&gt; from spotpython.utils.stats import plot_coeff_vs_pvals, fit_all_lm\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'y': [1, 2, 3],\n&gt;&gt;&gt;     'x1': [4, 5, 6],\n&gt;&gt;&gt;     'x2': [7, 8, 9]\n&gt;&gt;&gt; })\n&gt;&gt;&gt; estimates = fit_all_lm(\"y ~ x1\", [\"x2\"], data)\n&gt;&gt;&gt; plot_coeff_vs_pvals(estimates)",
    "crumbs": [
      "API Reference",
      "Utilities",
      "plot_coeff_vs_pvals"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.preprocess_df_for_ols.html",
    "href": "docs/reference/utils.stats.preprocess_df_for_ols.html",
    "title": "utils.stats.preprocess_df_for_ols",
    "section": "",
    "text": "utils.stats.preprocess_df_for_ols(df, independent_var_columns, target_col)\nPreprocesses a df for fiitting an OLS regression model using the specified target column and predictors.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nInput DataFrame containing the data.\nrequired\n\n\nindependent_var_columns\nlist of str\nList of names for predictor columns.\nrequired\n\n\ntarget_col\nstr\nName of the target/dependent variable column.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nX_encoded\npd.DataFrame\nEncoded predictors with a constant term.\n\n\ny\npd.Series\nTarget variable.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "preprocess_df_for_ols"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.preprocess_df_for_ols.html#parameters",
    "href": "docs/reference/utils.stats.preprocess_df_for_ols.html#parameters",
    "title": "utils.stats.preprocess_df_for_ols",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nInput DataFrame containing the data.\nrequired\n\n\nindependent_var_columns\nlist of str\nList of names for predictor columns.\nrequired\n\n\ntarget_col\nstr\nName of the target/dependent variable column.\nrequired",
    "crumbs": [
      "API Reference",
      "Utilities",
      "preprocess_df_for_ols"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.preprocess_df_for_ols.html#returns",
    "href": "docs/reference/utils.stats.preprocess_df_for_ols.html#returns",
    "title": "utils.stats.preprocess_df_for_ols",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nX_encoded\npd.DataFrame\nEncoded predictors with a constant term.\n\n\ny\npd.Series\nTarget variable.",
    "crumbs": [
      "API Reference",
      "Utilities",
      "preprocess_df_for_ols"
    ]
  },
  {
    "objectID": "docs/reference/utils.stats.semi_partial_correlation.html",
    "href": "docs/reference/utils.stats.semi_partial_correlation.html",
    "title": "utils.stats.semi_partial_correlation",
    "section": "",
    "text": "utils.stats.semi_partial_correlation\nutils.stats.semi_partial_correlation(x, method='pearson')",
    "crumbs": [
      "API Reference",
      "Utilities",
      "semi_partial_correlation"
    ]
  },
  {
    "objectID": "docs/sequential-parameter-optimization-cookbook.html",
    "href": "docs/sequential-parameter-optimization-cookbook.html",
    "title": "Sequential Parameter Optimization Cookbook",
    "section": "",
    "text": "Sequential Parameter Optimization Cookbook\nThe following is a cookbook of optimization and hyperparameter tuning recipes. It is not meant to be exhaustive, but instead act as a place to capture a number of the common patterns used in hyperparameter tuning.\nSequential Parameter Optimization Cookbook",
    "crumbs": [
      "Getting Started"
    ]
  }
]