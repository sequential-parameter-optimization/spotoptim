{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6b5800",
   "metadata": {},
   "source": [
    "# SpotOptim Demonstrations\n",
    "\n",
    "This notebook demonstrates the usage of the SpotOptim optimizer for various optimization problems.\n",
    "\n",
    "## Example 1: 2-Dimensional Rosenbrock Function\n",
    "\n",
    "This example shows basic optimization on the classic 2D Rosenbrock function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotoptim import SpotOptim\n",
    "\n",
    "# Define Rosenbrock function\n",
    "def rosenbrock(X):\n",
    "    \"\"\"Rosenbrock function for optimization.\"\"\"\n",
    "    X = np.atleast_2d(X)\n",
    "    x = X[:, 0]\n",
    "    y = X[:, 1]\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "# Set up bounds for 2D problem\n",
    "bounds = [(-2, 2), (-2, 2)]\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = SpotOptim(\n",
    "    fun=rosenbrock,\n",
    "    bounds=bounds,\n",
    "    max_iter=50,\n",
    "    n_initial=5,\n",
    "    acquisition='ei',\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "result = optimizer.optimize()\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Optimization Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best point found: {result.x}\")\n",
    "print(f\"Best function value: {result.fun:.6f}\")\n",
    "print(f\"Number of function evaluations: {result.nfev}\")\n",
    "print(f\"Number of iterations: {result.nit}\")\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Message: {result.message}\")\n",
    "print(\"\\nTrue optimum: [1, 1] with f(x) = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b656a7",
   "metadata": {},
   "source": [
    "## Example 2: 6-Dimensional Rosenbrock Function\n",
    "\n",
    "This example demonstrates optimization of the 6-dimensional Rosenbrock function with a budget of 100 total function evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotoptim import SpotOptim\n",
    "\n",
    "# Define 6-dimensional Rosenbrock function\n",
    "def rosenbrock_6d(X):\n",
    "    \"\"\"\n",
    "    6-dimensional Rosenbrock function for optimization.\n",
    "    \n",
    "    The Rosenbrock function is defined as:\n",
    "    f(x) = sum_{i=1}^{n-1} [100*(x_{i+1} - x_i^2)^2 + (1 - x_i)^2]\n",
    "    \n",
    "    Global minimum: f(1, 1, 1, 1, 1, 1) = 0\n",
    "    \"\"\"\n",
    "    X = np.atleast_2d(X)\n",
    "    n_samples, n_dim = X.shape\n",
    "    \n",
    "    result = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        x = X[i]\n",
    "        total = 0\n",
    "        for j in range(n_dim - 1):\n",
    "            total += 100 * (x[j+1] - x[j]**2)**2 + (1 - x[j])**2\n",
    "        result[i] = total\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Set up bounds for 6D problem\n",
    "# Typical search domain for Rosenbrock is [-5, 10] for each dimension\n",
    "bounds_6d = [(-2, 2)] * 6\n",
    "\n",
    "# Budget: 100 total function evaluations\n",
    "# Split into initial design and optimization iterations\n",
    "n_total = 100\n",
    "n_initial = 6  # Initial Latin Hypercube Design points\n",
    "max_iter = n_total - n_initial   # Optimization iterations: 6 + 94 = 100 total evaluations\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"6D Rosenbrock Function Optimization\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Problem dimension: 6\")\n",
    "print(f\"Search bounds: [-2, 2] for each dimension\")\n",
    "print(f\"Total budget: {n_initial + max_iter} function evaluations\")\n",
    "print(f\"  - Initial design: {n_initial} points\")\n",
    "print(f\"  - Optimization iterations: {max_iter}\")\n",
    "print(f\"Global optimum: x* = [1, 1, 1, 1, 1, 1], f(x*) = 0\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create optimizer with Expected Improvement acquisition\n",
    "optimizer_6d = SpotOptim(\n",
    "    fun=rosenbrock_6d,\n",
    "    bounds=bounds_6d,\n",
    "    max_iter=max_iter,\n",
    "    n_initial=n_initial,\n",
    "    acquisition='y',  # Expected Improvement\n",
    "    seed=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "print(\"\\nStarting optimization...\\n\")\n",
    "result_6d = optimizer_6d.optimize()\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Optimization Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best point found: {result_6d.x}\")\n",
    "print(f\"Best function value: {result_6d.fun:.6e}\")\n",
    "print(f\"Number of function evaluations: {result_6d.nfev}\")\n",
    "print(f\"Number of iterations: {result_6d.nit}\")\n",
    "print(f\"Success: {result_6d.success}\")\n",
    "\n",
    "# Calculate distance from true optimum\n",
    "true_optimum = np.ones(6)\n",
    "distance_to_optimum = np.linalg.norm(result_6d.x - true_optimum)\n",
    "print(f\"\\nDistance to true optimum [1,1,1,1,1,1]: {distance_to_optimum:.6f}\")\n",
    "\n",
    "# Show improvement over initial design\n",
    "initial_best = np.min(result_6d.y[:n_initial])\n",
    "final_best = result_6d.fun\n",
    "improvement = initial_best - final_best\n",
    "improvement_pct = (improvement / initial_best) * 100\n",
    "\n",
    "print(f\"\\nImprovement analysis:\")\n",
    "print(f\"  Best initial value: {initial_best:.6e}\")\n",
    "print(f\"  Final best value: {final_best:.6e}\")\n",
    "print(f\"  Absolute improvement: {improvement:.6e}\")\n",
    "print(f\"  Relative improvement: {improvement_pct:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e047e",
   "metadata": {},
   "source": [
    "## Example 3: Using Kriging Surrogate\n",
    "\n",
    "This example demonstrates how to use the Kriging surrogate model instead of the default Gaussian Process from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotoptim import SpotOptim, Kriging\n",
    "\n",
    "# Define 2D Sphere function\n",
    "def sphere_2d(X):\n",
    "    \"\"\"\n",
    "    2D Sphere function: f(x, y) = x^2 + y^2\n",
    "    Global minimum: f(0, 0) = 0\n",
    "    \"\"\"\n",
    "    X = np.atleast_2d(X)\n",
    "    return np.sum(X**2, axis=1)\n",
    "\n",
    "# Set up bounds\n",
    "bounds_sphere = [(-5, 5), (-5, 5)]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Optimization with Kriging Surrogate\")\n",
    "print(\"=\"*60)\n",
    "print(\"Function: 2D Sphere (f(x,y) = x² + y²)\")\n",
    "print(\"Search bounds: [-5, 5] for each dimension\")\n",
    "print(\"Global optimum: x* = [0, 0], f(x*) = 0\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create Kriging surrogate with custom parameters\n",
    "kriging_surrogate = Kriging(\n",
    "    noise=1e-6,          # Small regularization\n",
    "    min_theta=-3.0,      # Bounds for length scale optimization\n",
    "    max_theta=2.0,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create optimizer with Kriging surrogate\n",
    "optimizer_kriging = SpotOptim(\n",
    "    fun=sphere_2d,\n",
    "    bounds=bounds_sphere,\n",
    "    max_iter=20,\n",
    "    n_initial=10,\n",
    "    surrogate=kriging_surrogate,  # Use Kriging instead of default GP\n",
    "    acquisition='ei',\n",
    "    seed=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "print(\"\\nStarting optimization with Kriging surrogate...\\n\")\n",
    "result_kriging = optimizer_kriging.optimize()\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Results (Kriging Surrogate)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best point found: {result_kriging.x}\")\n",
    "print(f\"Best function value: {result_kriging.fun:.6e}\")\n",
    "print(f\"Distance to optimum [0, 0]: {np.linalg.norm(result_kriging.x):.6f}\")\n",
    "print(f\"Number of function evaluations: {result_kriging.nfev}\")\n",
    "print(f\"Number of iterations: {result_kriging.nit}\")\n",
    "\n",
    "# Compare with default GP surrogate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparison: Running with Default GP Surrogate\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "optimizer_gp = SpotOptim(\n",
    "    fun=sphere_2d,\n",
    "    bounds=bounds_sphere,\n",
    "    max_iter=20,\n",
    "    n_initial=10,\n",
    "    acquisition='ei',\n",
    "    seed=42,\n",
    "    verbose=False  # Quiet for comparison\n",
    ")\n",
    "\n",
    "result_gp = optimizer_gp.optimize()\n",
    "\n",
    "print(f\"GP Best value: {result_gp.fun:.6e}\")\n",
    "print(f\"GP Distance to optimum: {np.linalg.norm(result_gp.x):.6f}\")\n",
    "print(f\"\\nKriging Best value: {result_kriging.fun:.6e}\")\n",
    "print(f\"Kriging Distance to optimum: {np.linalg.norm(result_kriging.x):.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary\")\n",
    "print(\"=\"*60)\n",
    "print(\"Both Kriging and GP surrogates successfully found the optimum.\")\n",
    "print(\"Kriging offers a simplified, self-contained surrogate model\")\n",
    "print(\"that can be customized with different hyperparameters.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6adad",
   "metadata": {},
   "source": [
    "## Example 4: Visualizing the Surrogate Model\n",
    "\n",
    "This example demonstrates the `plot_surrogate()` method, which visualizes the fitted surrogate model and its uncertainty. We'll use a 2D sphere function to clearly show the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95010c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from spotoptim import SpotOptim\n",
    "\n",
    "# Define a 2D sphere function\n",
    "def sphere_2d(X):\n",
    "    \"\"\"Simple 2D sphere function: f(x) = x1^2 + x2^2\"\"\"\n",
    "    X = np.atleast_2d(X)\n",
    "    return np.sum(X**2, axis=1)\n",
    "\n",
    "# Set up the optimization problem\n",
    "bounds = [(-5, 5), (-5, 5)]\n",
    "\n",
    "# Create optimizer with limited iterations to see exploration\n",
    "optimizer = SpotOptim(\n",
    "    fun=sphere_2d,\n",
    "    bounds=bounds,\n",
    "    max_iter=15,\n",
    "    n_initial=8,\n",
    "    acquisition='ei',\n",
    "    seed=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "print(\"Running optimization...\")\n",
    "result = optimizer.optimize()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Optimization Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best point found: {result.x}\")\n",
    "print(f\"Best function value: {result.fun:.6f}\")\n",
    "print(f\"Number of function evaluations: {result.nfev}\")\n",
    "print(f\"True optimum: [0, 0] with f(x) = 0\")\n",
    "print(f\"Distance to optimum: {np.linalg.norm(result.x):.6f}\")\n",
    "\n",
    "# Visualize the surrogate model\n",
    "print(\"\\nGenerating surrogate model visualization...\")\n",
    "optimizer.plot_surrogate(\n",
    "    i=0, j=1,\n",
    "    var_name=['x1', 'x2'],\n",
    "    add_points=True,\n",
    "    cmap='viridis',\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12324e4",
   "metadata": {},
   "source": [
    "### Visualizing Higher-Dimensional Problems\n",
    "\n",
    "For problems with more than 2 dimensions, `plot_surrogate()` shows a 2D slice by fixing all other dimensions at their mean values. This example demonstrates visualization of dimensions 0 and 2 from a 4D problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotoptim import SpotOptim\n",
    "\n",
    "# Define a 4D sphere function\n",
    "def sphere_4d(X):\n",
    "    \"\"\"4D sphere function: f(x) = sum(xi^2)\"\"\"\n",
    "    X = np.atleast_2d(X)\n",
    "    return np.sum(X**2, axis=1)\n",
    "\n",
    "# Set up bounds for 4D problem\n",
    "bounds = [(-3, 3), (-3, 3), (-3, 3), (-3, 3)]\n",
    "\n",
    "# Create optimizer\n",
    "optimizer_4d = SpotOptim(\n",
    "    fun=sphere_4d,\n",
    "    bounds=bounds,\n",
    "    max_iter=20,\n",
    "    n_initial=10,\n",
    "    acquisition='ei',\n",
    "    seed=123,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "result_4d = optimizer_4d.optimize()\n",
    "\n",
    "print(\"4D Optimization Results:\")\n",
    "print(f\"Best point: {result_4d.x}\")\n",
    "print(f\"Best value: {result_4d.fun:.6f}\")\n",
    "print(f\"Distance to optimum: {np.linalg.norm(result_4d.x):.6f}\")\n",
    "\n",
    "# Visualize dimensions 0 and 2 (dimensions 1 and 3 fixed at mean)\n",
    "print(\"\\nVisualizing dimensions 0 and 2 (others fixed at mean values)...\")\n",
    "optimizer_4d.plot_surrogate(\n",
    "    i=0, j=2,\n",
    "    var_name=['x0', 'x1', 'x2', 'x3'],\n",
    "    add_points=True,\n",
    "    cmap='coolwarm',\n",
    "    contour_levels=25,\n",
    "    show=True\n",
    ")\n",
    "\n",
    "# You can also visualize other dimension pairs\n",
    "print(\"\\nVisualizing dimensions 1 and 3...\")\n",
    "optimizer_4d.plot_surrogate(\n",
    "    i=1, j=3,\n",
    "    var_name=['x0', 'x1', 'x2', 'x3'],\n",
    "    add_points=True,\n",
    "    cmap='plasma',\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f83255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotoptim import SpotOptim\n",
    "opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n",
    "                 bounds=[(-5, 5), (-5, 5)],\n",
    "                 n_initial=10)\n",
    "X0 = opt._generate_initial_design()\n",
    "X0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotoptim as so\n",
    "opt = so.SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n",
    "                   bounds=[(-5, 5), (-5, 5)],\n",
    "                   n_initial=10)\n",
    "X0 = opt._generate_initial_design()\n",
    "X0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17455336",
   "metadata": {},
   "source": [
    "## Success Rate Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotoptim import SpotOptim\n",
    "\n",
    "def rosenbrock(X):\n",
    "    \"\"\"Rosenbrock function - challenging optimization problem\"\"\"\n",
    "    x = X[:, 0]\n",
    "    y = X[:, 1]\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "# Run optimization with periodic success rate checks\n",
    "optimizer = SpotOptim(\n",
    "    fun=rosenbrock,\n",
    "    bounds=[(-2, 2), (-2, 2)],\n",
    "    max_iter=200,\n",
    "    n_initial=20,\n",
    "    tensorboard_log=True,\n",
    "    tensorboard_clean=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "result = optimizer.optimize()\n",
    "\n",
    "# Analyze final success rate\n",
    "print(f\"\\nOptimization Results:\")\n",
    "print(f\"Best value: {result.fun:.6f}\")\n",
    "print(f\"Total evaluations: {optimizer.counter}\")\n",
    "print(f\"Final success rate: {optimizer.success_rate:.2%}\")\n",
    "\n",
    "# Interpret the result\n",
    "if optimizer.success_rate > 0.5:\n",
    "    print(\"→ High success rate: Optimization is still making good progress\")\n",
    "elif optimizer.success_rate > 0.2:\n",
    "    print(\"→ Medium success rate: Approaching convergence\")\n",
    "else:\n",
    "    print(\"→ Low success rate: Optimization has likely converged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd369e5",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d22c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed old TensorBoard logs: runs/spotoptim_20251115_210430\n",
      "Cleaned 1 old TensorBoard log directory\n",
      "TensorBoard logging enabled: runs/spotoptim_20251115_232906\n",
      "Initial best: f(x) = 2719.988281\n",
      "Iteration 1: f(x) = 2949.463867\n",
      "Initial best: f(x) = 2719.988281\n",
      "Iteration 1: f(x) = 2949.463867\n",
      "Iteration 2: f(x) = 3095.499674\n",
      "Iteration 2: f(x) = 3095.499674\n",
      "Iteration 3: f(x) = 4099.496582\n",
      "Iteration 3: f(x) = 4099.496582\n",
      "Iteration 4: f(x) = 3722.035156\n",
      "Iteration 4: f(x) = 3722.035156\n",
      "Iteration 5: f(x) = 2890.413737\n",
      "Iteration 6: f(x) = 2806.675618\n",
      "Iteration 5: f(x) = 2890.413737\n",
      "Iteration 6: f(x) = 2806.675618\n",
      "Iteration 7: f(x) = 3018.945719\n",
      "Iteration 8: f(x) = 3067.457194\n",
      "Iteration 7: f(x) = 3018.945719\n",
      "Iteration 8: f(x) = 3067.457194\n",
      "Iteration 9: f(x) = 3697.912191\n",
      "Iteration 9: f(x) = 3697.912191\n",
      "Iteration 10: f(x) = 2997.858073\n",
      "Iteration 10: f(x) = 2997.858073\n",
      "Iteration 11: f(x) = 3101.482666\n",
      "Iteration 12: f(x) = 3218.051514\n",
      "Iteration 11: f(x) = 3101.482666\n",
      "Iteration 12: f(x) = 3218.051514\n",
      "Iteration 13: f(x) = 6343.232910\n",
      "Iteration 13: f(x) = 6343.232910\n",
      "Iteration 14: f(x) = 2721.832275\n",
      "Iteration 15: New best f(x) = 2713.404378\n",
      "Iteration 14: f(x) = 2721.832275\n",
      "Iteration 15: New best f(x) = 2713.404378\n",
      "Iteration 16: f(x) = 26631.564453\n",
      "Iteration 16: f(x) = 26631.564453\n",
      "Iteration 17: f(x) = 3740.738932\n",
      "Iteration 18: f(x) = 2873.406006\n",
      "Iteration 17: f(x) = 3740.738932\n",
      "Iteration 18: f(x) = 2873.406006\n",
      "Iteration 19: f(x) = 2783.449788\n",
      "Iteration 19: f(x) = 2783.449788\n",
      "Iteration 20: New best f(x) = 2659.834717\n",
      "Iteration 21: f(x) = 3671.409912\n",
      "Iteration 20: New best f(x) = 2659.834717\n",
      "Iteration 21: f(x) = 3671.409912\n",
      "Iteration 22: f(x) = 2798.252604\n",
      "Iteration 22: f(x) = 2798.252604\n",
      "Iteration 23: f(x) = 2947.285889\n",
      "Iteration 23: f(x) = 2947.285889\n",
      "Iteration 24: f(x) = 2998.539225\n",
      "Iteration 25: f(x) = 24589.879557\n",
      "Iteration 24: f(x) = 2998.539225\n",
      "Iteration 25: f(x) = 24589.879557\n",
      "Iteration 26: f(x) = 3936.726318\n",
      "Iteration 26: f(x) = 3936.726318\n",
      "Iteration 27: f(x) = 2892.751139\n",
      "Iteration 27: f(x) = 2892.751139\n",
      "Iteration 28: f(x) = 2848.314860\n",
      "Iteration 28: f(x) = 2848.314860\n",
      "Iteration 29: f(x) = 3313.484049\n",
      "Iteration 29: f(x) = 3313.484049\n",
      "Iteration 30: f(x) = 3028.491862\n",
      "Iteration 31: f(x) = 3080.501139\n",
      "Iteration 30: f(x) = 3028.491862\n",
      "Iteration 31: f(x) = 3080.501139\n",
      "Iteration 32: f(x) = 3497.927897\n",
      "Iteration 33: f(x) = 3168.031006\n",
      "Iteration 32: f(x) = 3497.927897\n",
      "Iteration 33: f(x) = 3168.031006\n",
      "Iteration 34: f(x) = 2934.157308\n",
      "Iteration 34: f(x) = 2934.157308\n",
      "Iteration 35: f(x) = 3467.458089\n",
      "Iteration 35: f(x) = 3467.458089\n",
      "Iteration 36: f(x) = 2708.566976\n",
      "Iteration 37: f(x) = 24514.358724\n",
      "Iteration 36: f(x) = 2708.566976\n",
      "Iteration 37: f(x) = 24514.358724\n",
      "Iteration 38: f(x) = 3578.715169\n",
      "Iteration 38: f(x) = 3578.715169\n",
      "Iteration 39: f(x) = 2860.947428\n",
      "Iteration 39: f(x) = 2860.947428\n",
      "Iteration 40: f(x) = 2989.457031\n",
      "Iteration 40: f(x) = 2989.457031\n",
      "Iteration 41: f(x) = 2706.930827\n",
      "Iteration 42: f(x) = 2881.858805\n",
      "Iteration 41: f(x) = 2706.930827\n",
      "Iteration 42: f(x) = 2881.858805\n",
      "Iteration 43: f(x) = 3577.378255\n",
      "Iteration 43: f(x) = 3577.378255\n",
      "Iteration 44: f(x) = 2853.484945\n",
      "Iteration 44: f(x) = 2853.484945\n",
      "Iteration 45: f(x) = 3034.840739\n",
      "Iteration 46: f(x) = 24509.926432\n",
      "Iteration 45: f(x) = 3034.840739\n",
      "Iteration 46: f(x) = 24509.926432\n",
      "Iteration 47: f(x) = 3499.078613\n",
      "Iteration 48: f(x) = 3766.370117\n",
      "Iteration 47: f(x) = 3499.078613\n",
      "Iteration 48: f(x) = 3766.370117\n",
      "Iteration 49: f(x) = 18767.164062\n",
      "Iteration 49: f(x) = 18767.164062\n",
      "Iteration 50: f(x) = 2866.222168\n",
      "Iteration 51: f(x) = 3000.566569\n",
      "Iteration 50: f(x) = 2866.222168\n",
      "Iteration 51: f(x) = 3000.566569\n",
      "Iteration 52: New best f(x) = 2590.657796\n",
      "Iteration 53: f(x) = 3100.223389\n",
      "Iteration 52: New best f(x) = 2590.657796\n",
      "Iteration 53: f(x) = 3100.223389\n",
      "Iteration 54: f(x) = 2868.948893\n",
      "Iteration 54: f(x) = 2868.948893\n",
      "Iteration 55: f(x) = 3423.088704\n",
      "Iteration 56: f(x) = 3019.941406\n",
      "Iteration 55: f(x) = 3423.088704\n",
      "Iteration 56: f(x) = 3019.941406\n",
      "Iteration 57: f(x) = 3617.893311\n",
      "Iteration 57: f(x) = 3617.893311\n",
      "Iteration 58: f(x) = 3349.153809\n",
      "Iteration 58: f(x) = 3349.153809\n",
      "Iteration 59: f(x) = 3413.045736\n",
      "Iteration 60: f(x) = 3048.443197\n",
      "Iteration 59: f(x) = 3413.045736\n",
      "Iteration 60: f(x) = 3048.443197\n",
      "Iteration 61: f(x) = 3118.843913\n",
      "Iteration 61: f(x) = 3118.843913\n",
      "Iteration 62: f(x) = 15856.414388\n",
      "Iteration 63: f(x) = 3130.823242\n",
      "Iteration 62: f(x) = 15856.414388\n",
      "Iteration 63: f(x) = 3130.823242\n",
      "Iteration 64: f(x) = 2760.358480\n",
      "Iteration 65: f(x) = 26609.430990\n",
      "Iteration 64: f(x) = 2760.358480\n",
      "Iteration 65: f(x) = 26609.430990\n",
      "Iteration 66: f(x) = 2702.781250\n",
      "Iteration 66: f(x) = 2702.781250\n",
      "Iteration 67: f(x) = 2754.653890\n",
      "Iteration 68: f(x) = 2941.286296\n",
      "Iteration 67: f(x) = 2754.653890\n",
      "Iteration 68: f(x) = 2941.286296\n",
      "Iteration 69: f(x) = 2842.819743\n",
      "Iteration 69: f(x) = 2842.819743\n",
      "Iteration 70: f(x) = 2603.391764\n",
      "Iteration 70: f(x) = 2603.391764\n",
      "Iteration 71: f(x) = 2969.400146\n",
      "Iteration 71: f(x) = 2969.400146\n",
      "Iteration 72: f(x) = 2914.824137\n",
      "Iteration 72: f(x) = 2914.824137\n",
      "Iteration 73: f(x) = 3485.624349\n",
      "Iteration 73: f(x) = 3485.624349\n",
      "Iteration 74: f(x) = 2782.708171\n",
      "Iteration 74: f(x) = 2782.708171\n",
      "Iteration 75: f(x) = 2866.280029\n",
      "Iteration 75: f(x) = 2866.280029\n",
      "Iteration 76: f(x) = 2904.023763\n",
      "Iteration 76: f(x) = 2904.023763\n",
      "Iteration 77: f(x) = 2790.123698\n",
      "Iteration 77: f(x) = 2790.123698\n",
      "Iteration 78: f(x) = 3133.336589\n",
      "Iteration 78: f(x) = 3133.336589\n",
      "Iteration 79: f(x) = 2898.611165\n",
      "Iteration 80: f(x) = 24515.193359\n",
      "Iteration 79: f(x) = 2898.611165\n",
      "Iteration 80: f(x) = 24515.193359\n",
      "Iteration 81: f(x) = 11291.094889\n",
      "Iteration 82: f(x) = 2703.095378\n",
      "Iteration 81: f(x) = 11291.094889\n",
      "Iteration 82: f(x) = 2703.095378\n",
      "Iteration 83: f(x) = 2803.103109\n",
      "Iteration 83: f(x) = 2803.103109\n",
      "Iteration 84: f(x) = 2952.149089\n",
      "Iteration 84: f(x) = 2952.149089\n",
      "Iteration 85: f(x) = 2609.337484\n",
      "Iteration 85: f(x) = 2609.337484\n",
      "Iteration 86: f(x) = 2868.970622\n",
      "Iteration 87: f(x) = 26562.556641\n",
      "Iteration 86: f(x) = 2868.970622\n",
      "Iteration 87: f(x) = 26562.556641\n",
      "Iteration 88: f(x) = 26404.014974\n",
      "Iteration 88: f(x) = 26404.014974\n",
      "Iteration 89: f(x) = 2775.851156\n",
      "Iteration 89: f(x) = 2775.851156\n",
      "Iteration 90: f(x) = 2919.562012\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir=runs/spotoptim_20251115_232906\n",
      "Best hyperparameters found:\n",
      "  Learning rate: 0.010000\n",
      "  Hidden neurons (l1): 18\n",
      "  Hidden layers: 3\n",
      "  Best MSE: 2590.6578\n",
      "Iteration 90: f(x) = 2919.562012\n",
      "TensorBoard writer closed. View logs with: tensorboard --logdir=runs/spotoptim_20251115_232906\n",
      "Best hyperparameters found:\n",
      "  Learning rate: 0.010000\n",
      "  Hidden neurons (l1): 18\n",
      "  Hidden layers: 3\n",
      "  Best MSE: 2590.6578\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from spotoptim import SpotOptim\n",
    "from spotoptim.data import get_diabetes_dataloaders\n",
    "from spotoptim.nn.linear_regressor import LinearRegressor\n",
    "\n",
    "def evaluate_model(X):\n",
    "    \"\"\"Objective function for SpotOptim.\n",
    "\n",
    "    Args:\n",
    "        X: Array of hyperparameters [lr, l1, num_hidden_layers]\n",
    "\n",
    "    Returns:\n",
    "        Array of validation losses\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for params in X:\n",
    "        lr, l1, num_hidden_layers = params\n",
    "        lr = 10 ** lr  # Log scale for learning rate\n",
    "        l1 = int(l1)\n",
    "        num_hidden_layers = int(num_hidden_layers)\n",
    "\n",
    "        # Load data\n",
    "        train_loader, test_loader, _ = get_diabetes_dataloaders(\n",
    "            test_size=0.2,\n",
    "            batch_size=32,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Create model\n",
    "        model = LinearRegressor(\n",
    "            input_dim=10,\n",
    "            output_dim=1,\n",
    "            l1=l1,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            activation=\"ReLU\"\n",
    "        )\n",
    "\n",
    "        # Train briefly\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = model.get_optimizer(\"Adam\", lr=lr)\n",
    "\n",
    "        num_epochs = 50\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        results.append(test_loss / len(test_loader))\n",
    "\n",
    "    return np.array(results)\n",
    "\n",
    "# Optimize hyperparameters\n",
    "optimizer = SpotOptim(\n",
    "    fun=evaluate_model,\n",
    "    bounds=[\n",
    "        (-4, -2),   # log10(lr): 0.0001 to 0.01\n",
    "        (16, 128),  # l1: number of neurons\n",
    "        (0, 4)      # num_hidden_layers\n",
    "    ],\n",
    "    var_type=[\"num\", \"int\", \"int\"],\n",
    "    max_iter=100,\n",
    "    n_initial=10,\n",
    "    seed=42,\n",
    "    verbose=True,\n",
    "    tensorboard_clean=True,\n",
    "    tensorboard_log=True\n",
    ")\n",
    "\n",
    "result = optimizer.optimize()\n",
    "print(f\"Best hyperparameters found:\")\n",
    "print(f\"  Learning rate: {10**result.x[0]:.6f}\")\n",
    "print(f\"  Hidden neurons (l1): {int(result.x[1])}\")\n",
    "print(f\"  Hidden layers: {int(result.x[2])}\")\n",
    "print(f\"  Best MSE: {result.fun:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391cd676",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.plot_surrogate(i=0, j=1, var_name=['log10(lr)', 'l1', 'num_hidden_layers'], add_points=True, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc6c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotoptim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
