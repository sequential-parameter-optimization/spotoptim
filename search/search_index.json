{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"spotoptim","text":""},{"location":"#surrogate-model-based-optimization","title":"Surrogate Model Based Optimization","text":"<ul> <li>Documentation for spotpython see Sequential Parameter Optimization Cookbook.</li> <li>News and updates related to SPOT see SPOTSeven</li> </ul>"},{"location":"about/","title":"Contact/Privacy Policy","text":""},{"location":"about/#address","title":"Address","text":"<p>Prof. Dr. Thomas Bartz-Beielstein TH K\u00f6ln Raum 1.519 Steinm\u00fcllerallee 6 51643 Gummersbach +49 (0)2261 8196 6391 thomas.bartz-beielstein [at] th-koeln.de www.spotseven.de</p>"},{"location":"about/#privacy-policy","title":"Privacy Policy","text":"<p>We are very delighted that you have shown interest in our enterprise. Data protection is of a particularly high priority for the management of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. The use of the Internet pages of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is possible without any indication of personal data; however, if a data subject wants to use special enterprise services via our website, processing of personal data could become necessary. If the processing of personal data is necessary and there is no statutory basis for such processing, we generally obtain consent from the data subject.</p> <p>The processing of personal data, such as the name, address, e-mail address, or telephone number of a data subject shall always be in line with the General Data Protection Regulation (GDPR), and in accordance with the country-specific data protection regulations applicable to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab. By means of this data protection declaration, our enterprise would like to inform the general public of the nature, scope, and purpose of the personal data we collect, use and process. Furthermore, data subjects are informed, by means of this data protection declaration, of the rights to which they are entitled.</p> <p>As the controller, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab has implemented numerous technical and organizational measures to ensure the most complete protection of personal data processed through this website. However, Internet-based data transmissions may in principle have security gaps, so absolute protection may not be guaranteed. For this reason, every data subject is free to transfer personal data to us via alternative means, e.g. by telephone.</p> <ol> <li>Definitions</li> </ol> <p>The data protection declaration of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab is based on the terms used by the European legislator for the adoption of the General Data Protection Regulation (GDPR). Our data protection declaration should be legible and understandable for the general public, as well as our customers and business partners. To ensure this, we would like to first explain the terminology used.</p> <p>In this data protection declaration, we use, inter alia, the following terms:</p> <p>a)    Personal data</p> <p>Personal data means any information relating to an identified or identifiable natural person (\u201cdata subject\u201d). An identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person.</p> <p>b) Data subject</p> <p>Data subject is any identified or identifiable natural person, whose personal data is processed by the controller responsible for the processing.</p> <p>c)    Processing</p> <p>Processing is any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction.</p> <p>d)    Restriction of processing</p> <p>Restriction of processing is the marking of stored personal data with the aim of limiting their processing in the future.</p> <p>e)    Profiling</p> <p>Profiling means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person\u2019s performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements.</p> <p>f)     Pseudonymisation</p> <p>Pseudonymisation is the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person.</p> <p>g)    Controller or controller responsible for the processing</p> <p>Controller or controller responsible for the processing is the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law.</p> <p>h)    Processor</p> <p>Processor is a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller.</p> <p>i)      Recipient</p> <p>Recipient is a natural or legal person, public authority, agency or another body, to which the personal data are disclosed, whether a third party or not. However, public authorities which may receive personal data in the framework of a particular inquiry in accordance with Union or Member State law shall not be regarded as recipients; the processing of those data by those public authorities shall be in compliance with the applicable data protection rules according to the purposes of the processing.</p> <p>j)      Third party</p> <p>Third party is a natural or legal person, public authority, agency or body other than the data subject, controller, processor and persons who, under the direct authority of the controller or processor, are authorised to process personal data.</p> <p>k)    Consent</p> <p>Consent of the data subject is any freely given, specific, informed and unambiguous indication of the data subject\u2019s wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her.</p> <ol> <li>Name and Address of the controller</li> </ol> <p>Controller for the purposes of the General Data Protection Regulation (GDPR), other data protection laws applicable in Member states of the European Union and other provisions related to data protection is:</p> <p>TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab</p> <p>Steinm\u00fcllerallee 1</p> <p>51643 Gummersbach</p> <p>Deutschland</p> <p>Phone: +49 2261 81966391</p> <p>Email: thomas.bartz-beielstein@th-koeln.de</p> <p>Website: www.spotseven.de</p> <ol> <li>Collection of general data and information</li> </ol> <p>The website of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab collects a series of general data and information when a data subject or automated system calls up the website. This general data and information are stored in the server log files. Collected may be (1) the browser types and versions used, (2) the operating system used by the accessing system, (3) the website from which an accessing system reaches our website (so-called referrers), (4) the sub-websites, (5) the date and time of access to the Internet site, (6) an Internet protocol address (IP address), (7) the Internet service provider of the accessing system, and (8) any other similar data and information that may be used in the event of attacks on our information technology systems.</p> <p>When using these general data and information, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab does not draw any conclusions about the data subject. Rather, this information is needed to (1) deliver the content of our website correctly, (2) optimize the content of our website as well as its advertisement, (3) ensure the long-term viability of our information technology systems and website technology, and (4) provide law enforcement authorities with the information necessary for criminal prosecution in case of a cyber-attack. Therefore, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab analyzes anonymously collected data and information statistically, with the aim of increasing the data protection and data security of our enterprise, and to ensure an optimal level of protection for the personal data we process. The anonymous data of the server log files are stored separately from all personal data provided by a data subject.</p> <ol> <li>Comments function in the blog on the website</li> </ol> <p>The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab offers users the possibility to leave individual comments on individual blog contributions on a blog, which is on the website of the controller. A blog is a web-based, publicly-accessible portal, through which one or more people called bloggers or web-bloggers may post articles or write down thoughts in so-called blogposts. Blogposts may usually be commented by third parties.</p> <p>If a data subject leaves a comment on the blog published on this website, the comments made by the data subject are also stored and published, as well as information on the date of the commentary and on the user\u2019s (pseudonym) chosen by the data subject. In addition, the IP address assigned by the Internet service provider (ISP) to the data subject is also logged. This storage of the IP address takes place for security reasons, and in case the data subject violates the rights of third parties, or posts illegal content through a given comment. The storage of these personal data is, therefore, in the own interest of the data controller, so that he can exculpate in the event of an infringement. This collected personal data will not be passed to third parties, unless such a transfer is required by law or serves the aim of the defense of the data controller.</p> <ol> <li>Routine erasure and blocking of personal data</li> </ol> <p>The data controller shall process and store the personal data of the data subject only for the period necessary to achieve the purpose of storage, or as far as this is granted by the European legislator or other legislators in laws or regulations to which the controller is subject to.</p> <p>If the storage purpose is not applicable, or if a storage period prescribed by the European legislator or another competent legislator expires, the personal data are routinely blocked or erased in accordance with legal requirements.</p> <ol> <li>Rights of the data subject</li> </ol> <p>a) Right of confirmation</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller the confirmation as to whether or not personal data concerning him or her are being processed. If a data subject wishes to avail himself of this right of confirmation, he or she may, at any time, contact our Data Protection Officer or another employee of the controller.</p> <p>b) Right of access</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller free information about his or her personal data stored at any time and a copy of this information. Furthermore, the European directives and regulations grant the data subject access to the following information:</p> <p>the purposes of the processing; the categories of personal data concerned; the recipients or categories of recipients to whom the personal data have been or will be disclosed, in particular recipients in third countries or international organisations; where possible, the envisaged period for which the personal data will be stored, or, if not possible, the criteria used to determine that period; the existence of the right to request from the controller rectification or erasure of personal data, or restriction of processing of personal data concerning the data subject, or to object to such processing; the existence of the right to lodge a complaint with a supervisory authority; where the personal data are not collected from the data subject, any available information as to their source; the existence of automated decision-making, including profiling, referred to in Article 22(1) and (4) of the GDPR and, at least in those cases, meaningful information about the logic involved, as well as the significance and envisaged consequences of such processing for the data subject. Furthermore, the data subject shall have a right to obtain information as to whether personal data are transferred to a third country or to an international organisation. Where this is the case, the data subject shall have the right to be informed of the appropriate safeguards relating to the transfer.</p> <p>If a data subject wishes to avail himself of this right of access, he or she may at any time contact our Data Protection Officer or another employee of the controller.</p> <p>c) Right to rectification</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller without undue delay the rectification of inaccurate personal data concerning him or her. Taking into account the purposes of the processing, the data subject shall have the right to have incomplete personal data completed, including by means of providing a supplementary statement.</p> <p>If a data subject wishes to exercise this right to rectification, he or she may, at any time, contact our Data Protection Officer or another employee of the controller.</p> <p>d) Right to erasure (Right to be forgotten)</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller the erasure of personal data concerning him or her without undue delay, and the controller shall have the obligation to erase personal data without undue delay where one of the following grounds applies, as long as the processing is not necessary:</p> <p>The personal data are no longer necessary in relation to the purposes for which they were collected or otherwise processed. The data subject withdraws consent to which the processing is based according to point (a) of Article 6(1) of the GDPR, or point (a) of Article 9(2) of the GDPR, and where there is no other legal ground for the processing. The data subject objects to the processing pursuant to Article 21(1) of the GDPR and there are no overriding legitimate grounds for the processing, or the data subject objects to the processing pursuant to Article 21(2) of the GDPR. The personal data have been unlawfully processed. The personal data must be erased for compliance with a legal obligation in Union or Member State law to which the controller is subject. The personal data have been collected in relation to the offer of information society services referred to in Article 8(1) of the GDPR. If one of the aforementioned reasons applies, and a data subject wishes to request the erasure of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee shall promptly ensure that the erasure request is complied with immediately.</p> <p>Where the controller has made personal data public and is obliged pursuant to Article 17(1) to erase the personal data, the controller, taking account of available technology and the cost of implementation, shall take reasonable steps, including technical measures, to inform other controllers processing the personal data that the data subject has requested erasure by such controllers of any links to, or copy or replication of, those personal data, as far as processing is not required. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the necessary measures in individual cases.</p> <p>e) Right of restriction of processing</p> <p>Each data subject shall have the right granted by the European legislator to obtain from the controller restriction of processing where one of the following applies:</p> <p>The accuracy of the personal data is contested by the data subject, for a period enabling the controller to verify the accuracy of the personal data. The processing is unlawful and the data subject opposes the erasure of the personal data and requests instead the restriction of their use instead. The controller no longer needs the personal data for the purposes of the processing, but they are required by the data subject for the establishment, exercise or defence of legal claims. The data subject has objected to processing pursuant to Article 21(1) of the GDPR pending the verification whether the legitimate grounds of the controller override those of the data subject. If one of the aforementioned conditions is met, and a data subject wishes to request the restriction of the processing of personal data stored by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab, he or she may at any time contact our Data Protection Officer or another employee of the controller. The Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee will arrange the restriction of the processing.</p> <p>f) Right to data portability</p> <p>Each data subject shall have the right granted by the European legislator, to receive the personal data concerning him or her, which was provided to a controller, in a structured, commonly used and machine-readable format. He or she shall have the right to transmit those data to another controller without hindrance from the controller to which the personal data have been provided, as long as the processing is based on consent pursuant to point (a) of Article 6(1) of the GDPR or point (a) of Article 9(2) of the GDPR, or on a contract pursuant to point (b) of Article 6(1) of the GDPR, and the processing is carried out by automated means, as long as the processing is not necessary for the performance of a task carried out in the public interest or in the exercise of official authority vested in the controller.</p> <p>Furthermore, in exercising his or her right to data portability pursuant to Article 20(1) of the GDPR, the data subject shall have the right to have personal data transmitted directly from one controller to another, where technically feasible and when doing so does not adversely affect the rights and freedoms of others.</p> <p>In order to assert the right to data portability, the data subject may at any time contact the Data Protection Officer designated by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee.</p> <p>g) Right to object</p> <p>Each data subject shall have the right granted by the European legislator to object, on grounds relating to his or her particular situation, at any time, to processing of personal data concerning him or her, which is based on point (e) or (f) of Article 6(1) of the GDPR. This also applies to profiling based on these provisions.</p> <p>The TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall no longer process the personal data in the event of the objection, unless we can demonstrate compelling legitimate grounds for the processing which override the interests, rights and freedoms of the data subject, or for the establishment, exercise or defence of legal claims.</p> <p>If the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab processes personal data for direct marketing purposes, the data subject shall have the right to object at any time to processing of personal data concerning him or her for such marketing. This applies to profiling to the extent that it is related to such direct marketing. If the data subject objects to the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab to the processing for direct marketing purposes, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab will no longer process the personal data for these purposes.</p> <p>In addition, the data subject has the right, on grounds relating to his or her particular situation, to object to processing of personal data concerning him or her by the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab for scientific or historical research purposes, or for statistical purposes pursuant to Article 89(1) of the GDPR, unless the processing is necessary for the performance of a task carried out for reasons of public interest.</p> <p>In order to exercise the right to object, the data subject may directly contact the Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee. In addition, the data subject is free in the context of the use of information society services, and notwithstanding Directive 2002/58/EC, to use his or her right to object by automated means using technical specifications.</p> <p>h) Automated individual decision-making, including profiling</p> <p>Each data subject shall have the right granted by the European legislator not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her, or similarly significantly affects him or her, as long as the decision (1) is not is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) is not authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, or (3) is not based on the data subject\u2019s explicit consent.</p> <p>If the decision (1) is necessary for entering into, or the performance of, a contract between the data subject and a data controller, or (2) it is based on the data subject\u2019s explicit consent, the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab shall implement suitable measures to safeguard the data subject\u2019s rights and freedoms and legitimate interests, at least the right to obtain human intervention on the part of the controller, to express his or her point of view and contest the decision.</p> <p>If the data subject wishes to exercise the rights concerning automated individual decision-making, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller.</p> <p>i) Right to withdraw data protection consent</p> <p>Each data subject shall have the right granted by the European legislator to withdraw his or her consent to processing of his or her personal data at any time.</p> <p>f the data subject wishes to exercise the right to withdraw the consent, he or she may at any time directly contact our Data Protection Officer of the TH K\u00f6ln, Fakult\u00e4t Informatik und Ingenieurwissenschaften, SPOTSeven Lab or another employee of the controller.</p> <ol> <li>Data protection provisions about the application and use of Facebook</li> </ol> <p>On this website, the controller has integrated components of the enterprise Facebook. Facebook is a social network.</p> <p>A social network is a place for social meetings on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Facebook allows social network users to include the creation of private profiles, upload photos, and network through friend requests.</p> <p>The operating company of Facebook is Facebook, Inc., 1 Hacker Way, Menlo Park, CA 94025, United States. If a person lives outside of the United States or Canada, the controller is the Facebook Ireland Ltd., 4 Grand Canal Square, Grand Canal Harbour, Dublin 2, Ireland.</p> <p>With each call-up to one of the individual pages of this Internet website, which is operated by the controller and into which a Facebook component (Facebook plug-ins) was integrated, the web browser on the information technology system of the data subject is automatically prompted to download display of the corresponding Facebook component from Facebook through the Facebook component. An overview of all the Facebook Plug-ins may be accessed under https://developers.facebook.com/docs/plugins/. During the course of this technical procedure, Facebook is made aware of what specific sub-site of our website was visited by the data subject.</p> <p>If the data subject is logged in at the same time on Facebook, Facebook detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-site of our Internet page was visited by the data subject. This information is collected through the Facebook component and associated with the respective Facebook account of the data subject. If the data subject clicks on one of the Facebook buttons integrated into our website, e.g. the \u201cLike\u201d button, or if the data subject submits a comment, then Facebook matches this information with the personal Facebook user account of the data subject and stores the personal data.</p> <p>Facebook always receives, through the Facebook component, information about a visit to our website by the data subject, whenever the data subject is logged in at the same time on Facebook during the time of the call-up to our website. This occurs regardless of whether the data subject clicks on the Facebook component or not. If such a transmission of information to Facebook is not desirable for the data subject, then he or she may prevent this by logging off from their Facebook account before a call-up to our website is made.</p> <p>The data protection guideline published by Facebook, which is available at https://facebook.com/about/privacy/, provides information about the collection, processing and use of personal data by Facebook. In addition, it is explained there what setting options Facebook offers to protect the privacy of the data subject. In addition, different configuration options are made available to allow the elimination of data transmission to Facebook, e.g. the Facebook blocker of the provider Webgraph, which may be obtained under http://webgraph.com/resources/facebookblocker/. These applications may be used by the data subject to eliminate a data transmission to Facebook.</p> <ol> <li>Data protection provisions about the application and use of Google+</li> </ol> <p>On this website, the controller has integrated the Google+ button as a component. Google+ is a so-called social network. A social network is a social meeting place on the Internet, an online community, which usually allows users to communicate with each other and interact in a virtual space. A social network may serve as a platform for the exchange of opinions and experiences, or enable the Internet community to provide personal or business-related information. Google+ allows users of the social network to include the creation of private profiles, upload photos and network through friend requests.</p> <p>The operating company of Google+ is Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this website, which is operated by the controller and on which a Google+ button has been integrated, the Internet browser on the information technology system of the data subject automatically downloads a display of the corresponding Google+ button of Google through the respective Google+ button component. During the course of this technical procedure, Google is made aware of what specific sub-page of our website was visited by the data subject. More detailed information about Google+ is available under https://developers.google.com/+/.</p> <p>If the data subject is logged in at the same time to Google+, Google recognizes with each call-up to our website by the data subject and for the entire duration of his or her stay on our Internet site, which specific sub-pages of our Internet page were visited by the data subject. This information is collected through the Google+ button and Google matches this with the respective Google+ account associated with the data subject.</p> <p>If the data subject clicks on the Google+ button integrated on our website and thus gives a Google+ 1 recommendation, then Google assigns this information to the personal Google+ user account of the data subject and stores the personal data. Google stores the Google+ 1 recommendation of the data subject, making it publicly available in accordance with the terms and conditions accepted by the data subject in this regard. Subsequently, a Google+ 1 recommendation given by the data subject on this website together with other personal data, such as the Google+ account name used by the data subject and the stored photo, is stored and processed on other Google services, such as search-engine results of the Google search engine, the Google account of the data subject or in other places, e.g. on Internet pages, or in relation to advertisements. Google is also able to link the visit to this website with other personal data stored on Google. Google further records this personal information with the purpose of improving or optimizing the various Google services.</p> <p>Through the Google+ button, Google receives information that the data subject visited our website, if the data subject at the time of the call-up to our website is logged in to Google+. This occurs regardless of whether the data subject clicks or doesn\u2019t click on the Google+ button.</p> <p>If the data subject does not wish to transmit personal data to Google, he or she may prevent such transmission by logging out of his Google+ account before calling up our website.</p> <p>Further information and the data protection provisions of Google may be retrieved under https://www.google.com/intl/en/policies/privacy/. More references from Google about the Google+ 1 button may be obtained under https://developers.google.com/+/web/buttons-policy.</p> <ol> <li>Data protection provisions about the application and use of Jetpack for WordPress</li> </ol> <p>On this website, the controller has integrated Jetpack. Jetpack is a WordPress plug-in, which provides additional features to the operator of a website based on WordPress. Jetpack allows the Internet site operator, inter alia, an overview of the visitors of the site. By displaying related posts and publications, or the ability to share content on the page, it is also possible to increase visitor numbers. In addition, security features are integrated into Jetpack, so a Jetpack-using site is better protected against brute-force attacks. Jetpack also optimizes and accelerates the loading of images on the website.</p> <p>The operating company of Jetpack Plug-Ins for WordPress is the Automattic Inc., 132 Hawthorne Street, San Francisco, CA 94107, UNITED STATES. The operating enterprise uses the tracking technology created by Quantcast Inc., 201 Third Street, San Francisco, CA 94103, UNITED STATES.</p> <p>Jetpack sets a cookie on the information technology system used by the data subject. The definition of cookies is explained above. With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Jetpack component was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to submit data through the Jetpack component for analysis purposes to Automattic. During the course of this technical procedure Automattic receives data that is used to create an overview of website visits. The data obtained in this way serves the analysis of the behaviour of the data subject, which has access to the Internet page of the controller and is analyzed with the aim to optimize the website. The data collected through the Jetpack component is not used to identify the data subject without a prior obtaining of a separate express consent of the data subject. The data comes also to the notice of Quantcast. Quantcast uses the data for the same purposes as Automattic.</p> <p>The data subject can, as stated above, prevent the setting of cookies through our website at any time by means of a corresponding adjustment of the web browser used and thus permanently deny the setting of cookies. Such an adjustment to the Internet browser used would also prevent Automattic/Quantcast from setting a cookie on the information technology system of the data subject. In addition, cookies already in use by Automattic/Quantcast may be deleted at any time via a web browser or other software programs.</p> <p>In addition, the data subject has the possibility of objecting to a collection of data relating to a use of this Internet site that are generated by the Jetpack cookie as well as the processing of these data by Automattic/Quantcast and the chance to preclude any such. For this purpose, the data subject must press the \u2018opt-out\u2019 button under the link https://www.quantcast.com/opt-out/ which sets an opt-out cookie. The opt-out cookie set with this purpose is placed on the information technology system used by the data subject. If the cookies are deleted on the system of the data subject, then the data subject must call up the link again and set a new opt-out cookie.</p> <p>With the setting of the opt-out cookie, however, the possibility exists that the websites of the controller are not fully usable anymore by the data subject.</p> <p>The applicable data protection provisions of Automattic may be accessed under https://automattic.com/privacy/. The applicable data protection provisions of Quantcast can be accessed under https://www.quantcast.com/privacy/.</p> <ol> <li>Data protection provisions about the application and use of LinkedIn</li> </ol> <p>The controller has integrated components of the LinkedIn Corporation on this website. LinkedIn is a web-based social network that enables users with existing business contacts to connect and to make new business contacts. Over 400 million registered people in more than 200 countries use LinkedIn. Thus, LinkedIn is currently the largest platform for business contacts and one of the most visited websites in the world.</p> <p>The operating company of LinkedIn is LinkedIn Corporation, 2029 Stierlin Court Mountain View, CA 94043, UNITED STATES. For privacy matters outside of the UNITED STATES LinkedIn Ireland, Privacy Policy Issues, Wilton Plaza, Wilton Place, Dublin 2, Ireland, is responsible.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a LinkedIn component (LinkedIn plug-in) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to the download of a display of the corresponding LinkedIn component of LinkedIn. Further information about the LinkedIn plug-in may be accessed under https://developer.linkedin.com/plugins. During the course of this technical procedure, LinkedIn gains knowledge of what specific sub-page of our website was visited by the data subject.</p> <p>If the data subject is logged in at the same time on LinkedIn, LinkedIn detects with every call-up to our website by the data subject\u2014and for the entire duration of their stay on our Internet site\u2014which specific sub-page of our Internet page was visited by the data subject. This information is collected through the LinkedIn component and associated with the respective LinkedIn account of the data subject. If the data subject clicks on one of the LinkedIn buttons integrated on our website, then LinkedIn assigns this information to the personal LinkedIn user account of the data subject and stores the personal data.</p> <p>LinkedIn receives information via the LinkedIn component that the data subject has visited our website, provided that the data subject is logged in at LinkedIn at the time of the call-up to our website. This occurs regardless of whether the person clicks on the LinkedIn button or not. If such a transmission of information to LinkedIn is not desirable for the data subject, then he or she may prevent this by logging off from their LinkedIn account before a call-up to our website is made.</p> <p>LinkedIn provides under https://www.linkedin.com/psettings/guest-controls the possibility to unsubscribe from e-mail messages, SMS messages and targeted ads, as well as the ability to manage ad settings. LinkedIn also uses affiliates such as Eire, Google Analytics, BlueKai, DoubleClick, Nielsen, Comscore, Eloqua, and Lotame. The setting of such cookies may be denied under https://www.linkedin.com/legal/cookie-policy. The applicable privacy policy for LinkedIn is available under https://www.linkedin.com/legal/privacy-policy. The LinkedIn Cookie Policy is available under https://www.linkedin.com/legal/cookie-policy.</p> <ol> <li>Data protection provisions about the application and use of Twitter</li> </ol> <p>On this website, the controller has integrated components of Twitter. Twitter is a multilingual, publicly-accessible microblogging service on which users may publish and spread so-called \u2018tweets,\u2019 e.g. short messages, which are limited to 140 characters. These short messages are available for everyone, including those who are not logged on to Twitter. The tweets are also displayed to so-called followers of the respective user. Followers are other Twitter users who follow a user\u2019s tweets. Furthermore, Twitter allows you to address a wide audience via hashtags, links or retweets.</p> <p>The operating company of Twitter is Twitter, Inc., 1355 Market Street, Suite 900, San Francisco, CA 94103, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a Twitter component (Twitter button) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding Twitter component of Twitter. Further information about the Twitter buttons is available under https://about.twitter.com/de/resources/buttons. During the course of this technical procedure, Twitter gains knowledge of what specific sub-page of our website was visited by the data subject. The purpose of the integration of the Twitter component is a retransmission of the contents of this website to allow our users to introduce this web page to the digital world and increase our visitor numbers.</p> <p>If the data subject is logged in at the same time on Twitter, Twitter detects with every call-up to our website by the data subject and for the entire duration of their stay on our Internet site which specific sub-page of our Internet page was visited by the data subject. This information is collected through the Twitter component and associated with the respective Twitter account of the data subject. If the data subject clicks on one of the Twitter buttons integrated on our website, then Twitter assigns this information to the personal Twitter user account of the data subject and stores the personal data.</p> <p>Twitter receives information via the Twitter component that the data subject has visited our website, provided that the data subject is logged in on Twitter at the time of the call-up to our website. This occurs regardless of whether the person clicks on the Twitter component or not. If such a transmission of information to Twitter is not desirable for the data subject, then he or she may prevent this by logging off from their Twitter account before a call-up to our website is made.</p> <p>The applicable data protection provisions of Twitter may be accessed under https://twitter.com/privacy?lang=en.</p> <ol> <li>Data protection provisions about the application and use of YouTube</li> </ol> <p>On this website, the controller has integrated components of YouTube. YouTube is an Internet video portal that enables video publishers to set video clips and other users free of charge, which also provides free viewing, review and commenting on them. YouTube allows you to publish all kinds of videos, so you can access both full movies and TV broadcasts, as well as music videos, trailers, and videos made by users via the Internet portal.</p> <p>The operating company of YouTube is YouTube, LLC, 901 Cherry Ave., San Bruno, CA 94066, UNITED STATES. The YouTube, LLC is a subsidiary of Google Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043-1351, UNITED STATES.</p> <p>With each call-up to one of the individual pages of this Internet site, which is operated by the controller and on which a YouTube component (YouTube video) was integrated, the Internet browser on the information technology system of the data subject is automatically prompted to download a display of the corresponding YouTube component. Further information about YouTube may be obtained under https://www.youtube.com/yt/about/en/. During the course of this technical procedure, YouTube and Google gain knowledge of what specific sub-page of our website was visited by the data subject.</p> <p>If the data subject is logged in on YouTube, YouTube recognizes with each call-up to a sub-page that contains a YouTube video, which specific sub-page of our Internet site was visited by the data subject. This information is collected by YouTube and Google and assigned to the respective YouTube account of the data subject.</p> <p>YouTube and Google will receive information through the YouTube component that the data subject has visited our website, if the data subject at the time of the call to our website is logged in on YouTube; this occurs regardless of whether the person clicks on a YouTube video or not. If such a transmission of this information to YouTube and Google is not desirable for the data subject, the delivery may be prevented if the data subject logs off from their own YouTube account before a call-up to our website is made.</p> <p>YouTube\u2019s data protection provisions, available at https://www.google.com/intl/en/policies/privacy/, provide information about the collection, processing and use of personal data by YouTube and Google.</p> <ol> <li>Legal basis for the processing</li> </ol> <p>Art. 6(1) lit. a GDPR serves as the legal basis for processing operations for which we obtain consent for a specific processing purpose. If the processing of personal data is necessary for the performance of a contract to which the data subject is party, as is the case, for example, when processing operations are necessary for the supply of goods or to provide any other service, the processing is based on Article 6(1) lit. b GDPR. The same applies to such processing operations which are necessary for carrying out pre-contractual measures, for example in the case of inquiries concerning our products or services. Is our company subject to a legal obligation by which processing of personal data is required, such as for the fulfillment of tax obligations, the processing is based on Art. 6(1) lit. c GDPR. In rare cases, the processing of personal data may be necessary to protect the vital interests of the data subject or of another natural person. This would be the case, for example, if a visitor were injured in our company and his name, age, health insurance data or other vital information would have to be passed on to a doctor, hospital or other third party. Then the processing would be based on Art. 6(1) lit. d GDPR. Finally, processing operations could be based on Article 6(1) lit. f GDPR. This legal basis is used for processing operations which are not covered by any of the abovementioned legal grounds, if processing is necessary for the purposes of the legitimate interests pursued by our company or by a third party, except where such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require protection of personal data. Such processing operations are particularly permissible because they have been specifically mentioned by the European legislator. He considered that a legitimate interest could be assumed if the data subject is a client of the controller (Recital 47 Sentence 2 GDPR).</p> <ol> <li>The legitimate interests pursued by the controller or by a third party</li> </ol> <p>Where the processing of personal data is based on Article 6(1) lit. f GDPR our legitimate interest is to carry out our business in favor of the well-being of all our employees and the shareholders.</p> <ol> <li>Period for which the personal data will be stored</li> </ol> <p>The criteria used to determine the period of storage of personal data is the respective statutory retention period. After expiration of that period, the corresponding data is routinely deleted, as long as it is no longer necessary for the fulfillment of the contract or the initiation of a contract.</p> <ol> <li>Provision of personal data as statutory or contractual requirement; Requirement necessary to enter into a contract; Obligation of the data subject to provide the personal data; possible consequences of failure to provide such data</li> </ol> <p>We clarify that the provision of personal data is partly required by law (e.g. tax regulations) or can also result from contractual provisions (e.g. information on the contractual partner). Sometimes it may be necessary to conclude a contract that the data subject provides us with personal data, which must subsequently be processed by us. The data subject is, for example, obliged to provide us with personal data when our company signs a contract with him or her. The non-provision of the personal data would have the consequence that the contract with the data subject could not be concluded. Before personal data is provided by the data subject, the data subject must contact our Data Protection Officer. Our Data Protection Officer clarifies to the data subject whether the provision of the personal data is required by law or contract or is necessary for the conclusion of the contract, whether there is an obligation to provide the personal data and the consequences of non-provision of the personal data.</p> <ol> <li>Existence of automated decision-making</li> </ol> <p>As a responsible company, we do not use automatic decision-making or profiling.</p> <p>This Privacy Policy has been generated by the Privacy Policy Generator of the External Data Protection Officers that was developed in cooperation with RC GmbH, which sells used notebooks and the Media Law Lawyers from WBS-LAW.</p>"},{"location":"download/","title":"Install spotpython","text":"<pre><code>pip install spotoptim\n</code></pre>"},{"location":"examples/","title":"SPOT Examples","text":""},{"location":"examples/#simple-spotpython-run","title":"Simple spotpython run","text":"<pre><code>import numpy as np\nfrom spotpython.spot import spot\nfrom spotpython.fun.objectivefunctions import analytical\nfrom spotpython.utils.init import fun_control_init, design_control_init, surrogate_control_init\n\nfun = analytical().fun_branin\nfun_control = fun_control_init(lower = np.array([-5, 0]),\n                               upper = np.array([10, 15]),\n                               fun_evals=20)\ndesign_control = design_control_init(init_size=10)\nsurrogate_control = surrogate_control_init(n_theta=2)\nS = spot.Spot(fun=fun, fun_control=fun_control, design_control=design_control)\nS.run()\n</code></pre> <pre><code>spotpython tuning: 3.146824136952164 [######----] 55.00% \nspotpython tuning: 3.146824136952164 [######----] 60.00% \nspotpython tuning: 3.146824136952164 [######----] 65.00% \nspotpython tuning: 3.146824136952164 [#######---] 70.00% \nspotpython tuning: 1.1487233101571483 [########--] 75.00% \nspotpython tuning: 1.0236891516766402 [########--] 80.00% \nspotpython tuning: 0.41994270072214057 [########--] 85.00% \nspotpython tuning: 0.40193544341108023 [#########-] 90.00% \nspotpython tuning: 0.3991519598268951 [##########] 95.00% \nspotpython tuning: 0.3991519598268951 [##########] 100.00% Done...\n</code></pre> <pre><code>S.print_results()\n</code></pre> <pre><code>min y: 0.3991519598268951\nx0: 3.1546575195040987\nx1: 2.285931113926263\n</code></pre> <pre><code>S.plot_progress(log_y=True)\n</code></pre> <pre><code>S.surrogate.plot()\n</code></pre>"},{"location":"examples/#further-examples","title":"Further Examples","text":"<p>Examples can be found in the Hyperparameter Tuning Cookbook, e.g., Documentation of the Sequential Parameter Optimization.</p>"},{"location":"sequential-parameter-optimization-cookbook/","title":"Sequential Parameter Optimization Cookbook","text":"<p>The following is a cookbook of optimization and hyperparameter tuning recipes. It is not meant to be exhaustive, but instead act as a place to capture a number of the common patterns used in hyperparameter tuning.</p> <p>Sequential Parameter Optimization Cookbook</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>spotoptim<ul> <li>SpotOptim</li> <li>core<ul> <li>data</li> <li>experiment</li> </ul> </li> <li>data<ul> <li>base</li> <li>diabetes</li> </ul> </li> <li>eda<ul> <li>plots</li> </ul> </li> <li>factor_analyzer<ul> <li>confirmatory_factor_analyzer</li> <li>factor_analyzer</li> <li>factor_analyzer_rotator</li> <li>factor_analyzer_utils</li> </ul> </li> <li>function<ul> <li>forr08a</li> <li>mo</li> <li>remote</li> <li>so</li> <li>torch_objective</li> </ul> </li> <li>hyperparameters<ul> <li>parameters</li> <li>repr_helpers</li> </ul> </li> <li>inspection<ul> <li>importance</li> <li>predictions</li> </ul> </li> <li>mo<ul> <li>mo_mm</li> <li>pareto</li> </ul> </li> <li>nn<ul> <li>linear_regressor</li> <li>mlp</li> </ul> </li> <li>optimizer<ul> <li>schedule_free</li> </ul> </li> <li>plot<ul> <li>contour</li> <li>mo</li> <li>visualization</li> </ul> </li> <li>sampling<ul> <li>design</li> <li>effects</li> <li>lhs</li> <li>mm</li> </ul> </li> <li>surrogate<ul> <li>kernels</li> <li>kriging</li> <li>mlp_surrogate</li> <li>nystroem</li> <li>pipeline</li> <li>simple_kriging</li> </ul> </li> <li>tricands<ul> <li>tricands</li> </ul> </li> <li>utils<ul> <li>boundaries</li> <li>eval</li> <li>file</li> <li>mapping</li> <li>pca</li> <li>scaler</li> <li>stats</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/spotoptim/SpotOptim/","title":"SpotOptim","text":""},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim","title":"<code>SpotOptim</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>SPOT optimizer compatible with scipy.optimize interface.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>callable</code> <p>Objective function to minimize. Should accept array of shape (n_samples, n_features).</p> required <code>bounds</code> <code>list of tuple</code> <p>Bounds for each dimension as [(low, high), \u2026].</p> <code>None</code> <code>max_iter</code> <code>int</code> <p>Maximum number of total function evaluations (including initial design). For example, max_iter=30 with n_initial=10 will perform 10 initial evaluations plus 20 sequential optimization iterations. Defaults to 20.</p> <code>20</code> <code>n_initial</code> <code>int</code> <p>Number of initial design points. Defaults to 10.</p> <code>10</code> <code>surrogate</code> <code>object</code> <p>Surrogate model with scikit-learn interface (fit/predict methods). If None, uses a Gaussian Process Regressor with Matern kernel. Default configuration::</p> <pre><code>from sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, ConstantKernel\n\nkernel = ConstantKernel(1.0, (1e-2, 1e12)) * Matern(length_scale=1.0, length_scale_bounds=(1e-4, 1e2), nu=2.5)\nsurrogate = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=100)\nsurrogate = GaussianProcessRegressor(\n    kernel=kernel,\n    n_restarts_optimizer=10,\n    normalize_y=True,\n    random_state=self.seed,\n)\n</code></pre> <p>Alternative surrogates can be provided, including SpotOptim\u2019s Kriging model, Random Forests, or any scikit-learn compatible regressor. See Examples section. Defaults to None (uses default Gaussian Process configuration).</p> <code>None</code> <code>acquisition</code> <code>str</code> <p>Acquisition function (\u2018ei\u2019, \u2018y\u2019, \u2018pi\u2019). Defaults to \u2018y\u2019.</p> <code>'y'</code> <code>var_type</code> <code>list of str</code> <p>Variable types for each dimension. Supported types: - \u2018float\u2019: Python floats, continuous optimization (no rounding) - \u2018int\u2019: Python int, float values will be rounded to integers - \u2018factor\u2019: Unordered categorical data, internally mapped to int values   (e.g., \u201cred\u201d-&gt;0, \u201cgreen\u201d-&gt;1, etc.) Defaults to None (which sets all dimensions to \u2018float\u2019).</p> <code>None</code> <code>var_name</code> <code>list of str</code> <p>Variable names for each dimension. If None, uses default names [\u2018x0\u2019, \u2018x1\u2019, \u2018x2\u2019, \u2026]. Defaults to None.</p> <code>None</code> <code>tolerance_x</code> <code>float</code> <p>Minimum distance between points. Defaults to np.sqrt(np.spacing(1))</p> <code>None</code> <code>var_trans</code> <code>list of str</code> <p>Variable transformations for each dimension. Supported: - \u2018log\u2019: Logarithmic transformation, e.g. \u201clog10\u201d for base-10 log - \u2018sqrt\u2019: Square root transformation, \u201csqrt\u201d for square root - None or \u2018id\u2019 or \u2018None\u2019: No transformation Defaults to None (no transformations).</p> <code>None</code> <code>max_time</code> <code>float</code> <p>Maximum runtime in minutes. If np.inf (default), no time limit. The optimization terminates when either max_iter evaluations are reached OR max_time minutes have elapsed, whichever comes first. Defaults to np.inf.</p> <code>inf</code> <code>repeats_initial</code> <code>int</code> <p>Number of times to evaluate each initial design point. Useful for noisy objective functions. If &gt; 1, noise handling is activated and statistics (mean, variance) are tracked. Defaults to 1.</p> <code>1</code> <code>repeats_surrogate</code> <code>int</code> <p>Number of times to evaluate each surrogate-suggested point. Useful for noisy objective functions. If &gt; 1, noise handling is activated and statistics (mean, variance) are tracked. Defaults to 1.</p> <code>1</code> <code>ocba_delta</code> <code>int</code> <p>Number of additional evaluations to allocate using Optimal Computing Budget Allocation (OCBA) when noise handling is active. OCBA determines which existing design points should be re-evaluated to best distinguish between alternatives. Only used when noise=True (repeats &gt; 1) and ocba_delta &gt; 0. Requires at least 3 design points with variance information. Defaults to 0 (no OCBA).</p> <code>0</code> <code>tensorboard_log</code> <code>bool</code> <p>Enable TensorBoard logging. If True, optimization metrics and hyperparameters are logged to TensorBoard. View logs by running: <code>tensorboard --logdir=&lt;tensorboard_path&gt;</code> in a separate terminal. Defaults to False.</p> <code>False</code> <code>tensorboard_path</code> <code>str</code> <p>Path for TensorBoard log files. If None and tensorboard_log is True, creates a default path: runs/spotoptim_YYYYMMDD_HHMMSS. Defaults to None.</p> <code>None</code> <code>tensorboard_clean</code> <code>bool</code> <p>If True, removes all old TensorBoard log directories from the \u2018runs\u2019 folder before starting optimization. Use with caution as this permanently deletes all subdirectories in \u2018runs\u2019. Defaults to False.</p> <code>False</code> <code>fun_mo2so</code> <code>callable</code> <p>Function to convert multi-objective values to single-objective. Takes an array of shape (n_samples, n_objectives) and returns array of shape (n_samples,). If None and objective function returns multi-objective values, uses first objective. Defaults to None.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print progress information. Defaults to False.</p> <code>False</code> <code>warnings_filter</code> <code>str</code> <p>Filter for warnings. One of \u201cerror\u201d, \u201cignore\u201d, \u201calways\u201d, \u201call\u201d, \u201cdefault\u201d, \u201cmodule\u201d, or \u201conce\u201d. Defaults to \u201cignore\u201d.</p> <code>'ignore'</code> <code>n_infill_points</code> <code>int</code> <p>Number of infill points to suggest at each iteration. Defaults to 1. If &gt; 1, multiple distinct points are proposed using the optimizer and fallback strategies.</p> <code>1</code> <code>max_surrogate_points</code> <code>int</code> <p>Maximum number of points to use for surrogate model fitting. If None, all points are used. If the number of evaluated points exceeds this limit, a subset is selected using the selection method. Defaults to None.</p> <code>None</code> <code>selection_method</code> <code>str</code> <p>Method for selecting points when max_surrogate_points is exceeded. Options: \u2018distant\u2019 (Select points that are distant from each other via K-means clustering) or \u2018best\u2019 (Select all points from the cluster with the best mean objective value). Defaults to \u2018distant\u2019.</p> <code>'distant'</code> <code>acquisition_failure_strategy</code> <code>str</code> <p>Strategy for handling acquisition function failures. Options: \u2018random\u2019 (space-filling design via Latin Hypercube Sampling) Defaults to \u2018random\u2019.</p> <code>'random'</code> <code>penalty</code> <code>bool</code> <p>Whether to use penalty for handling NaN/inf values in objective function evaluations. Defaults to False.</p> <code>False</code> <code>penalty_val</code> <code>float</code> <p>Penalty value to replace NaN/inf values in objective function evaluations. When the objective function returns NaN or inf, these values are replaced with penalty plus a small random noise (sampled from N(0, 0.1)) to avoid identical penalty values. This allows optimization to continue despite occasional function evaluation failures. Defaults to None.</p> <code>None</code> <code>acquisition_fun_return_size</code> <code>int</code> <p>Number of top candidates to return from acquisition function optimization. Defaults to 3.</p> <code>3</code> <code>acquisition_optimizer</code> <code>str or callable</code> <p>Optimizer to use for maximizing acquisition function. Can be \u201cdifferential_evolution\u201d (default) or any method name supported by scipy.optimize.minimize (e.g., \u201cNelder-Mead\u201d, \u201cL-BFGS-B\u201d). Can also be a callable with signature compatible with scipy.optimize.minimize (fun, x0, bounds, \u2026). A specific version is \u201cde_tricands\u201d, which combines DE with Tricands. It can be parameterized with \u201cprob_de_tricands\u201d (probability of using DE). Defaults to \u201cdifferential_evolution\u201d.</p> <code>'differential_evolution'</code> <code>acquisition_optimizer_kwargs</code> <code>dict</code> <p>Kwargs passed to the acquisition function optimizer and GPR surrogate optimizer. Defaults to {\u2018maxiter\u2019: 10000, \u2018gtol\u2019: 1e-9}.</p> <code>None</code> <code>restart_after_n</code> <code>int</code> <p>Number of consecutive iterations with zero success rate before triggering a restart. Defaults to 100.</p> <code>100</code> <code>restart_inject_best</code> <code>bool</code> <p>Whether to inject the best solution found so far as a starting point for the next restart. Defaults to True.</p> <code>True</code> <code>x0</code> <code>array - like</code> <p>Starting point for optimization, shape (n_features,). If provided, this point will be evaluated first and included in the initial design. The point should be within the bounds and will be validated before use. Defaults to None (no starting point, uses only LHS design).</p> <code>None</code> <code>de_x0_prob</code> <code>float</code> <p>Probability of using the best point as starting point for differential evolution. Defaults to 0.1.</p> <code>0.1</code> <code>tricands_fringe</code> <code>bool</code> <p>Whether to use the fringe of the design space for the initial design. Defaults to False.</p> <code>False</code> <code>prob_de_tricands</code> <code>float</code> <p>Probability of using differential evolution as an optimizer on the surrogate model. 1 - prob_de_tricands is the probability of using tricands. Defaults to 0.8.</p> <code>0.8</code> <code>window_size</code> <code>int</code> <p>Window size for success rate calculation.</p> <code>None</code> <code>min_tol_metric</code> <code>str</code> <p>Distance metric used when checking <code>tolerance_x</code> for duplicate detection. Default is \u201cchebyshev\u201d. Supports all metrics from scipy.spatial.distance.cdist, including: - \u201cchebyshev\u201d: L-infinity distance (hypercube). Default. Matches previous behavior. - \u201ceuclidean\u201d: L2 distance (hypersphere). - \u201cminkowski\u201d: Lp distance (default p=2). - \u201ccityblock\u201d: Manhattan/L1 distance. - \u201ccosine\u201d: Cosine distance. - \u201ccorrelation\u201d: Correlation distance. - \u201ccanberra\u201d, \u201cbraycurtis\u201d, \u201csqeuclidean\u201d, etc.</p> <code>'chebyshev'</code> <p>Attributes:</p> Name Type Description <code>X_</code> <code>ndarray</code> <p>All evaluated points, shape (n_samples, n_features).</p> <code>y_</code> <code>ndarray</code> <p>Function values at X_, shape (n_samples,). For multi-objective problems, these are the converted single-objective values.</p> <code>y_mo</code> <code>ndarray or None</code> <p>Multi-objective function values, shape (n_samples, n_objectives). None for single-objective problems.</p> <code>best_x_</code> <code>ndarray</code> <p>Best point found, shape (n_features,).</p> <code>best_y_</code> <code>float</code> <p>Best function value found.</p> <code>n_iter_</code> <code>int</code> <p>Number of iterations performed. This is not the same as counter. Provided for compatibility with scipy.optimize routines.</p> <code>counter</code> <code>int</code> <p>Total number of function evaluations.</p> <code>success_rate</code> <code>float</code> <p>Rolling success rate over the last window_size evaluations. A success is counted when a new evaluation improves upon the best value found so far.</p> <code>warnings_filter</code> <code>str</code> <p>Filter for warnings during optimization.</p> <code>max_surrogate_points</code> <code>int or None</code> <p>Maximum number of points for surrogate fitting.</p> <code>selection_method</code> <code>str</code> <p>Point selection method.</p> <code>acquisition_failure_strategy</code> <code>str</code> <p>Strategy for handling acquisition failures (\u2018random\u2019).</p> <code>noise</code> <code>bool</code> <p>True if noise handling is active (repeats &gt; 1).</p> <code>mean_X</code> <code>ndarray or None</code> <p>Aggregated unique design points (if noise=True).</p> <code>mean_y</code> <code>ndarray or None</code> <p>Mean y values per design point (if noise=True).</p> <code>var_y</code> <code>ndarray or None</code> <p>Variance of y values per design point (if noise=True).</p> <code>min_mean_X</code> <code>ndarray or None</code> <p>X value of best mean y (if noise=True).</p> <code>min_mean_y</code> <code>float or None</code> <p>Best mean y value (if noise=True).</p> <code>min_var_y</code> <code>float or None</code> <p>Variance of best mean y (if noise=True).</p> <code>de_x0_prob</code> <code>float</code> <p>Probability of using the best point as starting point for differential evolution.</p> <code>tricands_fringe</code> <code>bool</code> <p>Whether to use the fringe of the design space for the initial design.</p> <code>prob_de_tricands</code> <code>float</code> <p>Probability of using differential evolution as an optimizer on the surrogate model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; def objective(X):\n...     return np.sum(X**2, axis=1)\n...\n&gt;&gt;&gt; # Example 1: Basic usage (deterministic function)\n&gt;&gt;&gt; bounds = [(-5, 5), (-5, 5)]\n&gt;&gt;&gt; optimizer = SpotOptim(fun=objective, bounds=bounds, max_iter=10, n_initial=5, verbose=True)\n&gt;&gt;&gt; result = optimizer.optimize()\n&gt;&gt;&gt; print(\"Best x:\", result.x)\n&gt;&gt;&gt; print(\"Best f(x):\", result.fun)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 2: With custom variable names\n&gt;&gt;&gt; optimizer = SpotOptim(\n...     fun=objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     var_name=[\"param1\", \"param2\"],\n...     max_iter=10,\n...     n_initial=5\n... )\n&gt;&gt;&gt; result = optimizer.optimize()\n&gt;&gt;&gt; optimizer.plot_surrogate()  # Uses custom names in plot labels\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 3: Noisy function with repeated evaluations\n&gt;&gt;&gt; def noisy_objective(X):\n...     import numpy as np\n...     base = np.sum(X**2, axis=1)\n...     noise = np.random.normal(0, 0.1, size=base.shape)\n...     return base + noise\n...\n&gt;&gt;&gt; optimizer = SpotOptim(\n...     fun=noisy_objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     max_iter=30,\n...     n_initial=10,\n...     repeats_initial=3,      # Evaluate each initial point 3 times\n...     repeats_surrogate=2,    # Evaluate each new point 2 times\n...     seed=42,                # For reproducibility\n...     verbose=True\n... )\n&gt;&gt;&gt; result = optimizer.optimize()\n&gt;&gt;&gt; # Access noise statistics\n&gt;&gt;&gt; print(\"Unique design points:\", optimizer.mean_X.shape[0])\n&gt;&gt;&gt; print(\"Best mean value:\", optimizer.min_mean_y)\n&gt;&gt;&gt; print(\"Variance at best point:\", optimizer.min_var_y)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 4: Noisy function with OCBA (Optimal Computing Budget Allocation)\n&gt;&gt;&gt; optimizer_ocba = SpotOptim(\n...     fun=noisy_objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     max_iter=50,\n...     n_initial=10,\n...     repeats_initial=2,      # Initial repeats\n...     repeats_surrogate=1,    # Surrogate repeats\n...     ocba_delta=3,           # Allocate 3 additional evaluations per iteration\n...     seed=42,\n...     verbose=True\n... )\n&gt;&gt;&gt; result = optimizer_ocba.optimize()\n&gt;&gt;&gt; # OCBA intelligently re-evaluates promising points to reduce uncertainty\n&gt;&gt;&gt; print(\"Total evaluations:\", result.nfev)\n&gt;&gt;&gt; print(\"Unique design points:\", optimizer_ocba.mean_X.shape[0])\n&gt;&gt;&gt; print(\"Best mean value:\", optimizer.min_mean_y)\n&gt;&gt;&gt; print(\"Variance at best point:\", optimizer.min_var_y)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 5: With TensorBoard logging\n&gt;&gt;&gt; optimizer_tb = SpotOptim(\n...     fun=objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     max_iter=30,\n...     n_initial=10,\n...     tensorboard_log=True,   # Enable TensorBoard\n...     tensorboard_path=\"runs/my_optimization\",  # Optional custom path\n...     verbose=True\n... )\n&gt;&gt;&gt; result = optimizer_tb.optimize()\n&gt;&gt;&gt; # View logs in browser: tensorboard --logdir=runs/my_optimization\n&gt;&gt;&gt; print(\"Logs saved to:\", optimizer_tb.tensorboard_path)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 6: Using SpotOptim's Kriging surrogate\n&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt; kriging_model = Kriging(\n...     noise=1e-10,           # Regularization parameter\n...     kernel='gauss',         # Gaussian/RBF kernel\n...     min_theta=-3.0,         # Min log10(theta) bound\n...     max_theta=2.0,          # Max log10(theta) bound\n...     seed=42\n... )\n&gt;&gt;&gt; optimizer_kriging = SpotOptim(\n...     fun=objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     surrogate=kriging_model,\n...     max_iter=30,\n...     n_initial=10,\n...     seed=42,\n...     verbose=True\n... )\n&gt;&gt;&gt; result = optimizer_kriging.optimize()\n&gt;&gt;&gt; print(\"Best solution found:\", result.x)\n&gt;&gt;&gt; print(\"Best value:\", result.fun)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 7: Using sklearn Gaussian Process with custom kernel\n&gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor\n&gt;&gt;&gt; from sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\n&gt;&gt;&gt; # Custom kernel: constant * RBF + white noise\n&gt;&gt;&gt; custom_kernel = ConstantKernel(1.0, (1e-2, 1e2)) * RBF(\n...     length_scale=1.0, length_scale_bounds=(1e-1, 10.0)\n... ) + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-10, 1e-1))\n&gt;&gt;&gt; gp_custom = GaussianProcessRegressor(\n...     kernel=custom_kernel,\n...     n_restarts_optimizer=15,\n...     normalize_y=True,\n...     random_state=42\n... )\n&gt;&gt;&gt; optimizer_custom_gp = SpotOptim(\n...     fun=objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     surrogate=gp_custom,\n...     max_iter=30,\n...     n_initial=10,\n...     seed=42\n... )\n&gt;&gt;&gt; result = optimizer_custom_gp.optimize()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 8: Using Random Forest as surrogate\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; rf_model = RandomForestRegressor(\n...     n_estimators=100,\n...     max_depth=10,\n...     random_state=42\n... )\n&gt;&gt;&gt; optimizer_rf = SpotOptim(\n...     fun=objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     surrogate=rf_model,\n...     max_iter=30,\n...     n_initial=10,\n...     seed=42\n... )\n&gt;&gt;&gt; result = optimizer_rf.optimize()\n&gt;&gt;&gt; # Note: Random Forests don't provide uncertainty estimates,\n&gt;&gt;&gt; # so Expected Improvement (EI) may be less effective.\n&gt;&gt;&gt; # Consider using acquisition='y' for pure exploitation.\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 9: Comparing different kernels for Gaussian Process\n&gt;&gt;&gt; from sklearn.gaussian_process.kernels import Matern, RationalQuadratic\n&gt;&gt;&gt; # Matern kernel with nu=1.5 (once differentiable)\n&gt;&gt;&gt; kernel_matern15 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=1.5)\n&gt;&gt;&gt; gp_matern15 = GaussianProcessRegressor(kernel=kernel_matern15, normalize_y=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Matern kernel with nu=2.5 (twice differentiable, DEFAULT)\n&gt;&gt;&gt; kernel_matern25 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)\n&gt;&gt;&gt; gp_matern25 = GaussianProcessRegressor(kernel=kernel_matern25, normalize_y=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # RBF kernel (infinitely differentiable, smooth)\n&gt;&gt;&gt; kernel_rbf = ConstantKernel(1.0) * RBF(length_scale=1.0)\n&gt;&gt;&gt; gp_rbf = GaussianProcessRegressor(kernel=kernel_rbf, normalize_y=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Rational Quadratic kernel (mixture of RBF kernels)\n&gt;&gt;&gt; kernel_rq = ConstantKernel(1.0) * RationalQuadratic(length_scale=1.0, alpha=1.0)\n&gt;&gt;&gt; gp_rq = GaussianProcessRegressor(kernel=kernel_rq, normalize_y=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use any of these as surrogate\n&gt;&gt;&gt; optimizer_rbf = SpotOptim(fun=objective, bounds=[(-5, 5), (-5, 5)],\n...                           surrogate=gp_rbf, max_iter=30, n_initial=10)\n&gt;&gt;&gt; result = optimizer_rbf.optimize()\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>class SpotOptim(BaseEstimator):\n    \"\"\"SPOT optimizer compatible with scipy.optimize interface.\n\n    Args:\n        fun (callable): Objective function to minimize. Should accept array of shape (n_samples, n_features).\n        bounds (list of tuple): Bounds for each dimension as [(low, high), ...].\n        max_iter (int, optional): Maximum number of total function evaluations (including initial design).\n            For example, max_iter=30 with n_initial=10 will perform 10 initial evaluations plus\n            20 sequential optimization iterations. Defaults to 20.\n        n_initial (int, optional): Number of initial design points. Defaults to 10.\n        surrogate (object, optional): Surrogate model with scikit-learn interface (fit/predict methods).\n            If None, uses a Gaussian Process Regressor with Matern kernel. Default configuration::\n\n                from sklearn.gaussian_process import GaussianProcessRegressor\n                from sklearn.gaussian_process.kernels import Matern, ConstantKernel\n\n                kernel = ConstantKernel(1.0, (1e-2, 1e12)) * Matern(length_scale=1.0, length_scale_bounds=(1e-4, 1e2), nu=2.5)\n                surrogate = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=100)\n                surrogate = GaussianProcessRegressor(\n                    kernel=kernel,\n                    n_restarts_optimizer=10,\n                    normalize_y=True,\n                    random_state=self.seed,\n                )\n\n            Alternative surrogates can be provided, including SpotOptim's Kriging model,\n            Random Forests, or any scikit-learn compatible regressor. See Examples section.\n            Defaults to None (uses default Gaussian Process configuration).\n        acquisition (str, optional): Acquisition function ('ei', 'y', 'pi'). Defaults to 'y'.\n        var_type (list of str, optional): Variable types for each dimension. Supported types:\n            - 'float': Python floats, continuous optimization (no rounding)\n            - 'int': Python int, float values will be rounded to integers\n            - 'factor': Unordered categorical data, internally mapped to int values\n              (e.g., \"red\"-&gt;0, \"green\"-&gt;1, etc.)\n            Defaults to None (which sets all dimensions to 'float').\n        var_name (list of str, optional): Variable names for each dimension.\n            If None, uses default names ['x0', 'x1', 'x2', ...]. Defaults to None.\n        tolerance_x (float, optional): Minimum distance between points. Defaults to np.sqrt(np.spacing(1))\n        var_trans (list of str, optional): Variable transformations for each dimension. Supported:\n            - 'log': Logarithmic transformation, e.g. \"log10\" for base-10 log\n            - 'sqrt': Square root transformation, \"sqrt\" for square root\n            - None or 'id' or 'None': No transformation\n            Defaults to None (no transformations).\n        max_time (float, optional): Maximum runtime in minutes. If np.inf (default), no time limit.\n            The optimization terminates when either max_iter evaluations are reached OR max_time\n            minutes have elapsed, whichever comes first. Defaults to np.inf.\n        repeats_initial (int, optional): Number of times to evaluate each initial design point.\n            Useful for noisy objective functions. If &gt; 1, noise handling is activated and\n            statistics (mean, variance) are tracked. Defaults to 1.\n        repeats_surrogate (int, optional): Number of times to evaluate each surrogate-suggested point.\n            Useful for noisy objective functions. If &gt; 1, noise handling is activated and\n            statistics (mean, variance) are tracked. Defaults to 1.\n        ocba_delta (int, optional): Number of additional evaluations to allocate using Optimal Computing\n            Budget Allocation (OCBA) when noise handling is active. OCBA determines which existing\n            design points should be re-evaluated to best distinguish between alternatives. Only used\n            when noise=True (repeats &gt; 1) and ocba_delta &gt; 0. Requires at least 3 design points with\n            variance information. Defaults to 0 (no OCBA).\n        tensorboard_log (bool, optional): Enable TensorBoard logging. If True, optimization metrics\n            and hyperparameters are logged to TensorBoard. View logs by running:\n            `tensorboard --logdir=&lt;tensorboard_path&gt;` in a separate terminal. Defaults to False.\n        tensorboard_path (str, optional): Path for TensorBoard log files. If None and tensorboard_log\n            is True, creates a default path: runs/spotoptim_YYYYMMDD_HHMMSS. Defaults to None.\n        tensorboard_clean (bool, optional): If True, removes all old TensorBoard log directories from\n            the 'runs' folder before starting optimization. Use with caution as this permanently\n            deletes all subdirectories in 'runs'. Defaults to False.\n        fun_mo2so (callable, optional): Function to convert multi-objective values to single-objective.\n            Takes an array of shape (n_samples, n_objectives) and returns array of shape (n_samples,).\n            If None and objective function returns multi-objective values, uses first objective.\n            Defaults to None.\n        seed (int, optional): Random seed for reproducibility. Defaults to None.\n        verbose (bool, optional): Print progress information. Defaults to False.\n        warnings_filter (str, optional): Filter for warnings. One of \"error\", \"ignore\", \"always\", \"all\",\n            \"default\", \"module\", or \"once\". Defaults to \"ignore\".\n        n_infill_points (int, optional): Number of infill points to suggest at each iteration.\n            Defaults to 1. If &gt; 1, multiple distinct points are proposed using the optimizer\n            and fallback strategies.\n        max_surrogate_points (int, optional): Maximum number of points to use for surrogate model fitting.\n            If None, all points are used. If the number of evaluated points exceeds this limit,\n            a subset is selected using the selection method. Defaults to None.\n        selection_method (str, optional): Method for selecting points when max_surrogate_points is exceeded.\n            Options: 'distant' (Select points that are distant from each other via K-means clustering) or\n            'best' (Select all points from the cluster with the best mean objective value).\n            Defaults to 'distant'.\n        acquisition_failure_strategy (str, optional): Strategy for handling acquisition function failures.\n            Options: 'random' (space-filling design via Latin Hypercube Sampling)\n            Defaults to 'random'.\n        penalty (bool, optional): Whether to use penalty for handling NaN/inf values in objective function evaluations.\n            Defaults to False.\n        penalty_val (float, optional): Penalty value to replace NaN/inf values in objective function evaluations.\n            When the objective function returns NaN or inf, these values are replaced with penalty plus\n            a small random noise (sampled from N(0, 0.1)) to avoid identical penalty values.\n            This allows optimization to continue despite occasional function evaluation failures.\n            Defaults to None.\n        acquisition_fun_return_size (int, optional): Number of top candidates to return from acquisition function optimization.\n            Defaults to 3.\n        acquisition_optimizer (str or callable, optional): Optimizer to use for maximizing acquisition function.\n            Can be \"differential_evolution\" (default) or any method name supported by scipy.optimize.minimize\n            (e.g., \"Nelder-Mead\", \"L-BFGS-B\"). Can also be a callable with signature compatible with\n            scipy.optimize.minimize (fun, x0, bounds, ...). A specific version is \"de_tricands\", which combines DE with Tricands.\n            It can be parameterized with \"prob_de_tricands\" (probability of using DE).\n            Defaults to \"differential_evolution\".\n        acquisition_optimizer_kwargs (dict, optional): Kwargs passed to the acquisition function optimizer\n            and GPR surrogate optimizer. Defaults to {'maxiter': 10000, 'gtol': 1e-9}.\n        restart_after_n (int, optional): Number of consecutive iterations with zero success rate\n            before triggering a restart. Defaults to 100.\n        restart_inject_best (bool, optional): Whether to inject the best solution found so far\n            as a starting point for the next restart. Defaults to True.\n        x0 (array-like, optional): Starting point for optimization, shape (n_features,).\n            If provided, this point will be evaluated first and included in the initial design.\n            The point should be within the bounds and will be validated before use.\n            Defaults to None (no starting point, uses only LHS design).\n        de_x0_prob (float, optional): Probability of using the best point as starting point for differential evolution.\n            Defaults to 0.1.\n        tricands_fringe (bool, optional): Whether to use the fringe of the design space for the initial design.\n            Defaults to False.\n        prob_de_tricands (float, optional): Probability of using differential evolution as an optimizer\n            on the surrogate model. 1 - prob_de_tricands is the probability of using tricands. Defaults to 0.8.\n        window_size (int, optional): Window size for success rate calculation.\n        min_tol_metric (str, optional): Distance metric used when checking `tolerance_x` for\n            duplicate detection. Default is \"chebyshev\". Supports all metrics from\n            scipy.spatial.distance.cdist, including:\n            - \"chebyshev\": L-infinity distance (hypercube). Default. Matches previous behavior.\n            - \"euclidean\": L2 distance (hypersphere).\n            - \"minkowski\": Lp distance (default p=2).\n            - \"cityblock\": Manhattan/L1 distance.\n            - \"cosine\": Cosine distance.\n            - \"correlation\": Correlation distance.\n            - \"canberra\", \"braycurtis\", \"sqeuclidean\", etc.\n\n    Attributes:\n        X_ (ndarray): All evaluated points, shape (n_samples, n_features).\n        y_ (ndarray): Function values at X_, shape (n_samples,). For multi-objective problems,\n            these are the converted single-objective values.\n        y_mo (ndarray or None): Multi-objective function values, shape (n_samples, n_objectives).\n            None for single-objective problems.\n        best_x_ (ndarray): Best point found, shape (n_features,).\n        best_y_ (float): Best function value found.\n        n_iter_ (int): Number of iterations performed. This is not the same as counter. Provided for compatibility with scipy.optimize routines.\n        counter (int): Total number of function evaluations.\n        success_rate (float): Rolling success rate over the last window_size evaluations.\n            A success is counted when a new evaluation improves upon the best value found so far.\n        warnings_filter (str): Filter for warnings during optimization.\n        max_surrogate_points (int or None): Maximum number of points for surrogate fitting.\n        selection_method (str): Point selection method.\n        acquisition_failure_strategy (str): Strategy for handling acquisition failures ('random').\n        noise (bool): True if noise handling is active (repeats &gt; 1).\n        mean_X (ndarray or None): Aggregated unique design points (if noise=True).\n        mean_y (ndarray or None): Mean y values per design point (if noise=True).\n        var_y (ndarray or None): Variance of y values per design point (if noise=True).\n        min_mean_X (ndarray or None): X value of best mean y (if noise=True).\n        min_mean_y (float or None): Best mean y value (if noise=True).\n        min_var_y (float or None): Variance of best mean y (if noise=True).\n        de_x0_prob (float): Probability of using the best point as starting point for differential evolution.\n        tricands_fringe (bool): Whether to use the fringe of the design space for the initial design.\n        prob_de_tricands (float): Probability of using differential evolution as an optimizer on the surrogate model.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; def objective(X):\n        ...     return np.sum(X**2, axis=1)\n        ...\n        &gt;&gt;&gt; # Example 1: Basic usage (deterministic function)\n        &gt;&gt;&gt; bounds = [(-5, 5), (-5, 5)]\n        &gt;&gt;&gt; optimizer = SpotOptim(fun=objective, bounds=bounds, max_iter=10, n_initial=5, verbose=True)\n        &gt;&gt;&gt; result = optimizer.optimize()\n        &gt;&gt;&gt; print(\"Best x:\", result.x)\n        &gt;&gt;&gt; print(\"Best f(x):\", result.fun)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 2: With custom variable names\n        &gt;&gt;&gt; optimizer = SpotOptim(\n        ...     fun=objective,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     var_name=[\"param1\", \"param2\"],\n        ...     max_iter=10,\n        ...     n_initial=5\n        ... )\n        &gt;&gt;&gt; result = optimizer.optimize()\n        &gt;&gt;&gt; optimizer.plot_surrogate()  # Uses custom names in plot labels\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 3: Noisy function with repeated evaluations\n        &gt;&gt;&gt; def noisy_objective(X):\n        ...     import numpy as np\n        ...     base = np.sum(X**2, axis=1)\n        ...     noise = np.random.normal(0, 0.1, size=base.shape)\n        ...     return base + noise\n        ...\n        &gt;&gt;&gt; optimizer = SpotOptim(\n        ...     fun=noisy_objective,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     repeats_initial=3,      # Evaluate each initial point 3 times\n        ...     repeats_surrogate=2,    # Evaluate each new point 2 times\n        ...     seed=42,                # For reproducibility\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; result = optimizer.optimize()\n        &gt;&gt;&gt; # Access noise statistics\n        &gt;&gt;&gt; print(\"Unique design points:\", optimizer.mean_X.shape[0])\n        &gt;&gt;&gt; print(\"Best mean value:\", optimizer.min_mean_y)\n        &gt;&gt;&gt; print(\"Variance at best point:\", optimizer.min_var_y)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 4: Noisy function with OCBA (Optimal Computing Budget Allocation)\n        &gt;&gt;&gt; optimizer_ocba = SpotOptim(\n        ...     fun=noisy_objective,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     max_iter=50,\n        ...     n_initial=10,\n        ...     repeats_initial=2,      # Initial repeats\n        ...     repeats_surrogate=1,    # Surrogate repeats\n        ...     ocba_delta=3,           # Allocate 3 additional evaluations per iteration\n        ...     seed=42,\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; result = optimizer_ocba.optimize()\n        &gt;&gt;&gt; # OCBA intelligently re-evaluates promising points to reduce uncertainty\n        &gt;&gt;&gt; print(\"Total evaluations:\", result.nfev)\n        &gt;&gt;&gt; print(\"Unique design points:\", optimizer_ocba.mean_X.shape[0])\n        &gt;&gt;&gt; print(\"Best mean value:\", optimizer.min_mean_y)\n        &gt;&gt;&gt; print(\"Variance at best point:\", optimizer.min_var_y)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 5: With TensorBoard logging\n        &gt;&gt;&gt; optimizer_tb = SpotOptim(\n        ...     fun=objective,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     tensorboard_log=True,   # Enable TensorBoard\n        ...     tensorboard_path=\"runs/my_optimization\",  # Optional custom path\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; result = optimizer_tb.optimize()\n        &gt;&gt;&gt; # View logs in browser: tensorboard --logdir=runs/my_optimization\n        &gt;&gt;&gt; print(\"Logs saved to:\", optimizer_tb.tensorboard_path)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 6: Using SpotOptim's Kriging surrogate\n        &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n        &gt;&gt;&gt; kriging_model = Kriging(\n        ...     noise=1e-10,           # Regularization parameter\n        ...     kernel='gauss',         # Gaussian/RBF kernel\n        ...     min_theta=-3.0,         # Min log10(theta) bound\n        ...     max_theta=2.0,          # Max log10(theta) bound\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt; optimizer_kriging = SpotOptim(\n        ...     fun=objective,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     surrogate=kriging_model,\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     seed=42,\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; result = optimizer_kriging.optimize()\n        &gt;&gt;&gt; print(\"Best solution found:\", result.x)\n        &gt;&gt;&gt; print(\"Best value:\", result.fun)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 7: Using sklearn Gaussian Process with custom kernel\n        &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor\n        &gt;&gt;&gt; from sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\n        &gt;&gt;&gt; # Custom kernel: constant * RBF + white noise\n        &gt;&gt;&gt; custom_kernel = ConstantKernel(1.0, (1e-2, 1e2)) * RBF(\n        ...     length_scale=1.0, length_scale_bounds=(1e-1, 10.0)\n        ... ) + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-10, 1e-1))\n        &gt;&gt;&gt; gp_custom = GaussianProcessRegressor(\n        ...     kernel=custom_kernel,\n        ...     n_restarts_optimizer=15,\n        ...     normalize_y=True,\n        ...     random_state=42\n        ... )\n        &gt;&gt;&gt; optimizer_custom_gp = SpotOptim(\n        ...     fun=objective,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     surrogate=gp_custom,\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt; result = optimizer_custom_gp.optimize()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 8: Using Random Forest as surrogate\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; rf_model = RandomForestRegressor(\n        ...     n_estimators=100,\n        ...     max_depth=10,\n        ...     random_state=42\n        ... )\n        &gt;&gt;&gt; optimizer_rf = SpotOptim(\n        ...     fun=objective,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     surrogate=rf_model,\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt; result = optimizer_rf.optimize()\n        &gt;&gt;&gt; # Note: Random Forests don't provide uncertainty estimates,\n        &gt;&gt;&gt; # so Expected Improvement (EI) may be less effective.\n        &gt;&gt;&gt; # Consider using acquisition='y' for pure exploitation.\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 9: Comparing different kernels for Gaussian Process\n        &gt;&gt;&gt; from sklearn.gaussian_process.kernels import Matern, RationalQuadratic\n        &gt;&gt;&gt; # Matern kernel with nu=1.5 (once differentiable)\n        &gt;&gt;&gt; kernel_matern15 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=1.5)\n        &gt;&gt;&gt; gp_matern15 = GaussianProcessRegressor(kernel=kernel_matern15, normalize_y=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Matern kernel with nu=2.5 (twice differentiable, DEFAULT)\n        &gt;&gt;&gt; kernel_matern25 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)\n        &gt;&gt;&gt; gp_matern25 = GaussianProcessRegressor(kernel=kernel_matern25, normalize_y=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # RBF kernel (infinitely differentiable, smooth)\n        &gt;&gt;&gt; kernel_rbf = ConstantKernel(1.0) * RBF(length_scale=1.0)\n        &gt;&gt;&gt; gp_rbf = GaussianProcessRegressor(kernel=kernel_rbf, normalize_y=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Rational Quadratic kernel (mixture of RBF kernels)\n        &gt;&gt;&gt; kernel_rq = ConstantKernel(1.0) * RationalQuadratic(length_scale=1.0, alpha=1.0)\n        &gt;&gt;&gt; gp_rq = GaussianProcessRegressor(kernel=kernel_rq, normalize_y=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use any of these as surrogate\n        &gt;&gt;&gt; optimizer_rbf = SpotOptim(fun=objective, bounds=[(-5, 5), (-5, 5)],\n        ...                           surrogate=gp_rbf, max_iter=30, n_initial=10)\n        &gt;&gt;&gt; result = optimizer_rbf.optimize()\n    \"\"\"\n\n    # ====================\n    # Core\n    # ====================\n\n    def __init__(\n        self,\n        fun: Callable,\n        bounds: Optional[list] = None,\n        max_iter: int = 20,\n        n_initial: int = 10,\n        surrogate: Optional[object] = None,\n        acquisition: str = \"y\",\n        var_type: Optional[list] = None,\n        var_name: Optional[list] = None,\n        var_trans: Optional[list] = None,\n        tolerance_x: Optional[float] = None,\n        max_time: float = np.inf,\n        repeats_initial: int = 1,\n        repeats_surrogate: int = 1,\n        ocba_delta: int = 0,\n        tensorboard_log: bool = False,\n        tensorboard_path: Optional[str] = None,\n        tensorboard_clean: bool = False,\n        fun_mo2so: Optional[Callable] = None,\n        seed: Optional[int] = None,\n        verbose: bool = False,\n        warnings_filter: str = \"ignore\",\n        n_infill_points: int = 1,\n        max_surrogate_points: Optional[Union[int, List[int]]] = None,\n        selection_method: str = \"distant\",\n        acquisition_failure_strategy: str = \"random\",\n        penalty: bool = False,\n        penalty_val: Optional[float] = None,\n        acquisition_fun_return_size: int = 3,\n        acquisition_optimizer: Union[str, Callable] = \"differential_evolution\",\n        restart_after_n: int = 100,\n        restart_inject_best: bool = True,\n        x0: Optional[np.ndarray] = None,\n        de_x0_prob: float = 0.1,\n        tricands_fringe: bool = False,\n        prob_de_tricands: float = 0.8,\n        window_size: Optional[int] = None,\n        min_tol_metric: str = \"chebyshev\",\n        prob_surrogate: Optional[List[float]] = None,\n        n_jobs: int = 1,\n        acquisition_optimizer_kwargs: Optional[Dict[str, Any]] = None,\n        args: Tuple = (),\n        kwargs: Optional[Dict[str, Any]] = None,\n    ):\n        warnings.filterwarnings(warnings_filter)\n\n        self.eps = np.sqrt(np.spacing(1))\n\n        if tolerance_x is None:\n            tolerance_x = self.eps\n\n        # Infer parameters from objective function if not provided\n        if bounds is None:\n            bounds = getattr(fun, \"bounds\", None)\n            if bounds is None:\n                raise ValueError(\n                    \"Bounds must be provided either as an argument or via the objective function.\"\n                )\n\n        if var_type is None:\n            var_type = getattr(fun, \"var_type\", None)\n\n        if var_name is None:\n            var_name = getattr(fun, \"var_name\", None)\n\n        if var_trans is None:\n            var_trans = getattr(fun, \"var_trans\", None)\n\n        # Validate parameters\n        if max_iter &lt; n_initial:\n            raise ValueError(\n                f\"max_iter ({max_iter}) must be &gt;= n_initial ({n_initial}). \"\n                f\"max_iter represents the total function evaluation budget including initial design.\"\n            )\n\n        if acquisition_optimizer_kwargs is None:\n            acquisition_optimizer_kwargs = {\"maxiter\": 10000, \"gtol\": 1e-9}\n\n        # Initialize Configuration\n        self.config = SpotOptimConfig(\n            bounds=bounds,\n            max_iter=max_iter,\n            n_initial=n_initial,\n            surrogate=surrogate,\n            acquisition=acquisition.lower(),\n            var_type=var_type,\n            var_name=var_name,\n            var_trans=var_trans,\n            tolerance_x=tolerance_x,\n            max_time=max_time,\n            repeats_initial=repeats_initial,\n            repeats_surrogate=repeats_surrogate,\n            ocba_delta=ocba_delta,\n            tensorboard_log=tensorboard_log,\n            tensorboard_path=tensorboard_path,\n            tensorboard_clean=tensorboard_clean,\n            fun_mo2so=fun_mo2so,\n            seed=seed,\n            verbose=verbose,\n            warnings_filter=warnings_filter,\n            n_infill_points=n_infill_points,\n            max_surrogate_points=max_surrogate_points,\n            selection_method=selection_method,\n            acquisition_failure_strategy=acquisition_failure_strategy,\n            penalty=penalty,\n            penalty_val=penalty_val,\n            acquisition_fun_return_size=acquisition_fun_return_size,\n            acquisition_optimizer=acquisition_optimizer,\n            restart_after_n=restart_after_n,\n            restart_inject_best=restart_inject_best,\n            x0=x0,\n            de_x0_prob=de_x0_prob,\n            tricands_fringe=tricands_fringe,\n            prob_de_tricands=prob_de_tricands,\n            window_size=window_size,\n            min_tol_metric=min_tol_metric,\n            prob_surrogate=prob_surrogate,\n            n_jobs=n_jobs,\n            acquisition_optimizer_kwargs=acquisition_optimizer_kwargs,\n            args=args,\n            kwargs=kwargs,\n        )\n\n        # Initialize State\n        self.state = SpotOptimState()\n\n        # Other attributes\n        self.fun = fun\n        self.objective_names = getattr(\n            fun, \"objective_names\", getattr(fun, \"metrics\", None)\n        )\n\n        # Initialize persistent RNG\n        self.rng = np.random.RandomState(self.seed)\n        self._set_seed()\n\n        # Process bounds and factor variables\n        self._factor_maps = {}  # Maps dimension index to {int: str} mapping\n        self._original_bounds = self.bounds.copy()  # Store original bounds\n        self.process_factor_bounds()  # Maps factor bounds to integer indices (updates config.bounds)\n\n        # Derived attribute dimension n_dim\n        self.n_dim = len(self.bounds)\n\n        # Default variable types\n        if self.var_type is None:\n            self.var_type = self.detect_var_type()\n\n        # Modify bounds based on var_type\n        self.modify_bounds_based_on_var_type()\n\n        self.lower = np.array([b[0] for b in self.bounds])\n        self.upper = np.array([b[1] for b in self.bounds])\n\n        # Default variable names\n        if self.var_name is None:\n            self.var_name = [f\"x{i}\" for i in range(self.n_dim)]\n\n        # Handle default variable transformations\n        self.handle_default_var_trans()\n\n        # Apply transformations to bounds (internal representation)\n        self._original_lower = self.lower.copy()\n        self._original_upper = self.upper.copy()\n        self.transform_bounds()\n\n        # Dimension reduction: backup original bounds and identify fixed dimensions\n        self._setup_dimension_reduction()\n\n        # Validate and process starting point if provided\n        if self.x0 is not None:\n            self.x0 = self._validate_x0(self.x0)\n\n        # Initialize surrogate if not provided\n        # Initialize surrogate\n        self._surrogates_list = None\n        self._prob_surrogate = None\n\n        if isinstance(self.surrogate, list):\n            self._surrogates_list = self.surrogate\n            if not self._surrogates_list:\n                raise ValueError(\"Surrogate list cannot be empty.\")\n\n            # Handle probabilities\n            if self.config.prob_surrogate is None:\n                # Uniform probability\n                n = len(self._surrogates_list)\n                self._prob_surrogate = [1.0 / n] * n\n            else:\n                probs = self.config.prob_surrogate\n                if len(probs) != len(self._surrogates_list):\n                    raise ValueError(\n                        f\"Length of prob_surrogate ({len(probs)}) must match \"\n                        f\"number of surrogates ({len(self._surrogates_list)}).\"\n                    )\n                # Normalize probabilities\n                total = sum(probs)\n                if not np.isclose(total, 1.0) and total &gt; 0:\n                    self._prob_surrogate = [p / total for p in probs]\n                else:\n                    self._prob_surrogate = probs\n\n            # Handle max_surrogate_points list\n            self._max_surrogate_points_list = None\n            if isinstance(self.config.max_surrogate_points, list):\n                if len(self.config.max_surrogate_points) != len(self._surrogates_list):\n                    raise ValueError(\n                        f\"Length of max_surrogate_points ({len(self.config.max_surrogate_points)}) \"\n                        f\"must match number of surrogates ({len(self._surrogates_list)}).\"\n                    )\n                self._max_surrogate_points_list = self.config.max_surrogate_points\n            else:\n                # If int or None, broadcast to list for easier indexing\n                self._max_surrogate_points_list = [\n                    self.config.max_surrogate_points\n                ] * len(self._surrogates_list)\n\n            # Set initial surrogate and max points\n            self.surrogate = self._surrogates_list[0]\n            self._active_max_surrogate_points = self._max_surrogate_points_list[0]\n\n        elif self.surrogate is None:\n            # Default single surrogate case\n            self._max_surrogate_points_list = None\n            self._active_max_surrogate_points = self.config.max_surrogate_points\n\n            kernel = ConstantKernel(1.0, (1e-2, 1e12)) * Matern(\n                length_scale=1.0, length_scale_bounds=(1e-4, 1e2), nu=2.5\n            )\n\n            # Determine optimizer for GPR\n            optimizer = \"fmin_l_bfgs_b\"  # Default used by sklearn\n            if self.config.acquisition_optimizer_kwargs is not None:\n                optimizer = partial(\n                    _gpr_minimize_wrapper, **self.config.acquisition_optimizer_kwargs\n                )\n\n            self.surrogate = GaussianProcessRegressor(\n                kernel=kernel,\n                n_restarts_optimizer=100,\n                normalize_y=True,\n                random_state=self.seed,\n                optimizer=optimizer,\n            )\n\n        # Design generator\n        self.lhs_sampler = LatinHypercube(d=self.n_dim, seed=self.seed)\n\n        # Logic for window_size default based on restart_after_n\n        if self.window_size is None:\n            if self.restart_after_n is not None:\n                self.window_size = self.restart_after_n\n            else:\n                self.window_size = 100\n\n        # Clean old TensorBoard logs if requested\n        self._clean_tensorboard_logs()\n\n        # Initialize TensorBoard writer\n        self._init_tensorboard_writer()\n\n    def __getattr__(self, name):\n        \"\"\"Proxy attribute access to config and state.\"\"\"\n        try:\n            config = super().__getattribute__(\"config\")\n            if hasattr(config, name):\n                return getattr(config, name)\n        except AttributeError:\n            pass\n\n        try:\n            state = super().__getattribute__(\"state\")\n            if hasattr(state, name):\n                return getattr(state, name)\n        except AttributeError:\n            pass\n\n        raise AttributeError(\n            f\"'{type(self).__name__}' object has no attribute '{name}'\"\n        )\n\n    def __setattr__(self, name, value):\n        \"\"\"Proxy attribute assignment to config and state.\"\"\"\n        if name in (\"config\", \"state\"):\n            super().__setattr__(name, value)\n            return\n\n        try:\n            config = super().__getattribute__(\"config\")\n            if hasattr(config, name):\n                setattr(config, name, value)\n                return\n        except AttributeError:\n            pass\n\n        try:\n            state = super().__getattribute__(\"state\")\n            if hasattr(state, name):\n                setattr(state, name, value)\n                return\n        except AttributeError:\n            pass\n\n        super().__setattr__(name, value)\n\n    def __dir__(self):\n        \"\"\"Include config and state attributes in dir().\"\"\"\n        d = set(super().__dir__())\n        try:\n            config = super().__getattribute__(\"config\")\n            d.update(dir(config))\n        except AttributeError:\n            pass\n\n        try:\n            state = super().__getattribute__(\"state\")\n            # Filter internal methods/fields from dir if desired, but good for now\n            d.update(dir(state))\n        except AttributeError:\n            pass\n\n        return list(d)\n\n    # ====================\n    # Configuration &amp; Helpers\n    # ====================\n\n    def _set_seed(self) -&gt; None:\n        \"\"\"Set global random seeds for reproducibility.\n\n        Sets seeds for:\n        - random\n        - numpy.random\n        - torch (cpu and cuda)\n\n        Only performs actions if self.seed is not None.\n\n        Args:\n            None\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(0, 1)], seed=42)\n            &gt;&gt;&gt; spot._set_seed()\n            &gt;&gt;&gt; np.random.rand()  # Should be deterministic\n            0.3745401188473625\n        \"\"\"\n        if self.seed is not None:\n            random.seed(self.seed)\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(self.seed)\n\n    def detect_var_type(self) -&gt; list:\n        \"\"\"Auto-detect variable types based on factor mappings.\n\n        Returns:\n            list: List of variable types ('factor' or 'float') for each dimension.\n                  Dimensions with factor mappings are assigned 'factor', others 'float'.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[('red', 'green', 'blue'), (0, 10)])\n            &gt;&gt;&gt; spot.detect_var_type()\n            ['factor', 'float']\n        \"\"\"\n        return [\n            \"factor\" if i in self._factor_maps else \"float\" for i in range(self.n_dim)\n        ]\n\n    def modify_bounds_based_on_var_type(self) -&gt; None:\n        \"\"\"Modify bounds based on variable types.\n\n        Adjusts bounds for each dimension according to its var_type:\n        - 'int': Ensures bounds are integers (ceiling for lower, floor for upper)\n        - 'factor': Bounds already set to (0, n_levels-1) by process_factor_bounds\n        - 'float': Explicitly converts bounds to float\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If an unsupported var_type is encountered.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(0.5, 10.5)], var_type=['int'])\n            &gt;&gt;&gt; spot.bounds\n            [(1, 10)]\n            &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(0, 10)], var_type=['float'])\n            &gt;&gt;&gt; spot.bounds\n            [(0.0, 10.0)]\n        \"\"\"\n        for i, vtype in enumerate(self.var_type):\n            if vtype == \"int\":\n                # For integer variables, ensure bounds are integers\n                # Use Python's int() to convert numpy types to native Python int\n                lower = int(np.ceil(self.bounds[i][0]))\n                upper = int(np.floor(self.bounds[i][1]))\n                self.bounds[i] = (lower, upper)\n            elif vtype == \"factor\":\n                # For factor variables, bounds are already set to (0, n_levels-1)\n                # Ensure they are Python int, not numpy int64\n                lower = int(self.bounds[i][0])\n                upper = int(self.bounds[i][1])\n                self.bounds[i] = (lower, upper)\n            elif vtype == \"float\":\n                # Continuous variable, convert explicitly to float bounds\n                # Use Python's float() to convert numpy types to native Python float\n                lower = float(self.bounds[i][0])\n                upper = float(self.bounds[i][1])\n                self.bounds[i] = (lower, upper)\n            else:\n                raise ValueError(\n                    f\"Unsupported var_type '{vtype}' at dimension {i}. \"\n                    f\"Supported types are 'float', 'int', 'factor'.\"\n                )\n\n    def handle_default_var_trans(self) -&gt; None:\n        \"\"\"Handle default variable transformations.\n\n        Sets var_trans to a list of None values if not specified, or normalizes\n        transformation names by converting 'id', 'None', or None to None.\n\n        Also validates that var_trans length matches the number of dimensions.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If var_trans length doesn't match n_dim.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Default behavior - all None\n            &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(0, 10), (0, 10)])\n            &gt;&gt;&gt; spot.var_trans\n            [None, None]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Normalize transformation names\n            &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10), (1, 100)],\n            ...                  var_trans=['log10', 'id', None, 'None'])\n            &gt;&gt;&gt; spot.var_trans\n            ['log10', None, None, None]\n        \"\"\"\n        # Default variable transformations (None means no transformation)\n        if self.var_trans is None:\n            self.var_trans = [None] * self.n_dim\n        else:\n            # Normalize transformation names\n            self.var_trans = [\n                None if (t is None or t == \"id\" or t == \"None\") else t\n                for t in self.var_trans\n            ]\n\n        # Validate var_trans length\n        if len(self.var_trans) != self.n_dim:\n            raise ValueError(\n                f\"Length of var_trans ({len(self.var_trans)}) must match \"\n                f\"number of dimensions ({self.n_dim})\"\n            )\n\n    def process_factor_bounds(self) -&gt; None:\n        \"\"\"Process bounds to handle factor variables.\n\n        For dimensions with tuple bounds (factor variables), creates internal\n        integer mappings and replaces bounds with (0, n_levels-1).\n\n        Stores mappings in self._factor_maps: {dim_idx: {int_val: str_val}}\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If bounds are invalidly formatted.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[('red', 'green', 'blue'), (0, 10)])\n            &gt;&gt;&gt; spot.process_factor_bounds()\n            Factor variable at dimension 0:\n              Levels: ['red', 'green', 'blue']\n              Mapped to integers: 0 to 2\n            &gt;&gt;&gt; print(spot.bounds)\n            [(0, 2), (0, 10)]\n        \"\"\"\n        processed_bounds = []\n\n        for dim_idx, bound in enumerate(self.bounds):\n            if isinstance(bound, (tuple, list)) and len(bound) &gt;= 1:\n                # Check if this is a factor variable (contains strings)\n                if all(isinstance(v, str) for v in bound) and len(bound) &gt; 0:\n                    # Factor variable: create integer mapping\n                    factor_levels = list(bound)\n                    n_levels = len(factor_levels)\n\n                    # Create mapping: {0: \"level1\", 1: \"level2\", ...}\n                    self._factor_maps[dim_idx] = {\n                        i: level for i, level in enumerate(factor_levels)\n                    }\n\n                    # Replace with integer bounds (use Python int, not numpy types)\n                    processed_bounds.append((int(0), int(n_levels - 1)))\n\n                    if self.verbose:\n                        print(f\"Factor variable at dimension {dim_idx}:\")\n                        print(f\"  Levels: {factor_levels}\")\n                        print(f\"  Mapped to integers: 0 to {n_levels - 1}\")\n                elif len(bound) == 2 and all(\n                    isinstance(v, (int, float, np.integer, np.floating)) for v in bound\n                ):\n                    # Numeric bound tuple (accepts Python and numpy numeric types)\n                    # Always cast to Python float/int\n                    low, high = float(bound[0]), float(bound[1])\n\n                    # Convert to int if both are integer-valued\n                    if low.is_integer() and high.is_integer():\n                        low, high = int(low), int(high)\n\n                    processed_bounds.append((low, high))\n                else:\n                    raise ValueError(\n                        f\"Invalid bound at dimension {dim_idx}: {bound}. \"\n                        f\"Expected either (lower, upper) for numeric variables or \"\n                        f\"tuple of strings for factor variables.\"\n                    )\n            else:\n                raise ValueError(\n                    f\"Invalid bound at dimension {dim_idx}: {bound}. \"\n                    f\"Expected a tuple/list with at least 1 element.\"\n                )\n\n        # Update bounds with processed values\n        self.bounds = processed_bounds\n\n    def get_best_hyperparameters(\n        self, as_dict: bool = True\n    ) -&gt; Union[Dict[str, Any], np.ndarray, None]:\n        \"\"\"\n        Get the best hyperparameter configuration found during optimization.\n\n        If noise handling is active (repeats_initial &gt; 1 or OCBA), this returns the parameter\n        configuration associated with the best *mean* objective value. Otherwise, it returns\n        the configuration associated with the absolute best observed value.\n\n        Args:\n            as_dict (bool, optional): If True, returns a dictionary mapping parameter names\n                to their values. If False, returns the raw numpy array. Defaults to True.\n\n        Returns:\n            Union[Dict[str, Any], np.ndarray, None]: The best hyperparameter configuration.\n                Returns None if optimization hasn't started (no data).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda x: np.sum(x**2), bounds=[(-5, 5)], var_name=[\"x\"])\n            &gt;&gt;&gt; opt.optimize()\n            &gt;&gt;&gt; best_params = opt.get_best_hyperparameters()\n            &gt;&gt;&gt; print(best_params['x']) # Should be close to 0\n        \"\"\"\n        if self.X_ is None or len(self.X_) == 0:\n            return None\n\n        # Determine which \"best\" to use\n        if (self.repeats_initial &gt; 1 or self.repeats_surrogate &gt; 1) and hasattr(\n            self, \"min_mean_X\"\n        ):\n            best_x = self.min_mean_X\n        else:\n            best_x = self.best_x_\n\n        if not as_dict:\n            return best_x\n\n        # Map factors using existing method (handles 2D, returns 2D)\n        # We pass best_x as (1, D) and get (1, D) back\n        mapped_x = self._map_to_factor_values(best_x.reshape(1, -1))[0]\n\n        # Convert to dictionary with types\n        params = {}\n        names = (\n            self.var_name if self.var_name else [f\"p{i}\" for i in range(len(best_x))]\n        )\n\n        for i, name in enumerate(names):\n            val = mapped_x[i]\n\n            # Handle types if available (specifically int, as factors are already mapped)\n            if self.var_type:\n                v_type = self.var_type[i]\n                if v_type == \"int\":\n                    val = int(round(val))\n\n            params[name] = val\n\n        return params\n\n    def _repair_non_numeric(self, X: np.ndarray, var_type: List[str]) -&gt; np.ndarray:\n        \"\"\"Round non-numeric values to integers based on variable type.\n\n        This method applies rounding to variables that are not continuous:\n        - 'float': No rounding (continuous values)\n        - 'int': Rounded to integers\n        - 'factor': Rounded to integers (representing categorical values)\n\n        Args:\n            X (ndarray): X array with values to potentially round.\n            var_type (list of str): List with type information for each dimension.\n\n        Returns:\n            ndarray: X array with non-continuous values rounded to integers.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 var_type=['int', 'float'])\n            &gt;&gt;&gt; X = np.array([[1.2, 2.5], [3.7, 4.1], [5.9, 6.8]])\n            &gt;&gt;&gt; X_repaired = opt._repair_non_numeric(X, opt.var_type)\n            &gt;&gt;&gt; print(X_repaired)\n            [[1. 2.5]\n             [4. 4.1]\n             [6. 6.8]]\n        \"\"\"\n        # Don't round float or num types (continuous values)\n        mask = np.isin(var_type, [\"float\", \"float\"], invert=True)\n        X[:, mask] = np.around(X[:, mask])\n        return X\n\n    def _reinitialize_components(self) -&gt; None:\n        \"\"\"Reinitialize components that were excluded during pickling.\n\n        This method recreates the surrogate model and LHS sampler that were\n        excluded when saving an experiment or result.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Load experiment\n            &gt;&gt;&gt; opt = SpotOptim.load_experiment(\"sphere_opt_exp.pkl\")\n            &gt;&gt;&gt; # Reinitialize components\n            &gt;&gt;&gt; opt._reinitialize_components()\n        \"\"\"\n        # Reinitialize LHS sampler if needed\n        if not hasattr(self, \"lhs_sampler\") or self.lhs_sampler is None:\n            self.lhs_sampler = LatinHypercube(d=self.n_dim, seed=self.seed)\n\n        # Reinitialize surrogate if needed\n        if not hasattr(self, \"surrogate\") or self.surrogate is None:\n            kernel = ConstantKernel(1.0, (1e-2, 1e12)) * Matern(\n                length_scale=1.0, length_scale_bounds=(1e-4, 1e2), nu=2.5\n            )\n            self.surrogate = GaussianProcessRegressor(\n                kernel=kernel,\n                n_restarts_optimizer=100,\n                random_state=self.seed,\n                normalize_y=True,\n            )\n\n    def _get_pickle_safe_optimizer(\n        self, unpickleables: str = \"file_io\", verbosity: int = 0\n    ) -&gt; \"SpotOptim\":\n        \"\"\"Create a pickle-safe copy of the optimizer.\n\n        This method creates a copy of the optimizer instance with unpickleable components removed\n        or set to None to enable safe serialization.\n\n        Args:\n            unpickleables (str): Type of unpickleable components to exclude.\n                - \"file_io\": Excludes only file I/O components (tb_writer) and fun\n                - \"all\": Excludes file I/O, fun, surrogate, and lhs_sampler\n                Defaults to \"file_io\".\n            verbosity (int): Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.\n\n        Returns:\n            SpotOptim: A copy of the optimizer with unpickleable components removed.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Define optimizer\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=30,\n            ...     n_initial=10,\n            ...     seed=42\n            ... )\n            &gt;&gt;&gt; # Create pickle-safe copy excluding all unpickleables\n            &gt;&gt;&gt; opt_safe = opt._get_pickle_safe_optimizer(unpickleables=\"all\", verbosity=1)\n\n        \"\"\"\n        # Always exclude tb_writer (can't reliably pickle file handles)\n        # Determine which additional attributes to exclude\n        if unpickleables == \"file_io\":\n            unpickleable_attrs = [\"tb_writer\"]\n        else:\n            # \"all\" or specific exclusions\n            unpickleable_attrs = [\"tb_writer\", \"surrogate\", \"lhs_sampler\"]\n\n        # Prepare picklable state dictionary\n        picklable_state = {}\n\n        for key, value in self.__dict__.items():\n            if key not in unpickleable_attrs:\n                try:\n                    # Test if attribute can be pickled\n                    dill.dumps(value, protocol=dill.HIGHEST_PROTOCOL)\n                    picklable_state[key] = value\n                    if verbosity &gt; 1:\n                        print(f\"Attribute '{key}' is picklable and will be included.\")\n                except Exception as e:\n                    if verbosity &gt; 0:\n                        print(\n                            f\"Attribute '{key}' is not picklable and will be excluded: {e}\"\n                        )\n                    continue\n            else:\n                if verbosity &gt; 1:\n                    print(f\"Attribute '{key}' explicitly excluded from pickling.\")\n\n        # Create new instance with picklable state\n        picklable_instance = self.__class__.__new__(self.__class__)\n        picklable_instance.__dict__.update(picklable_state)\n\n        # Set excluded attributes to None\n        for attr in unpickleable_attrs:\n            if not hasattr(picklable_instance, attr):\n                setattr(picklable_instance, attr, None)\n\n        return picklable_instance\n\n    # ====================\n    # Dimension Reduction\n    # ====================\n\n    def _setup_dimension_reduction(self) -&gt; None:\n        \"\"\"Set up dimension reduction by identifying fixed dimensions.\n\n        identifies dimensions where lower and upper bounds are equal in **Transformed Space**.\n        Reduces `self.bounds`, `self.lower`, `self.upper`, etc., to the **Mapped Space**\n        (active variables only).\n\n        The resulting `self.bounds` defines the **Transformed and Mapped Space** used\n        for optimization.\n\n        This method identifies variables that are fixed (constant) and excludes them\n        from the optimization process. It stores:\n        - Original bounds and metadata in `all_*` attributes\n        - Boolean mask of fixed dimensions in `ident`\n        - Reduced bounds, types, and names for optimization\n        - `red_dim` flag indicating if reduction occurred\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10), (5, 5), (0, 1)])\n            &gt;&gt;&gt; spot._setup_dimension_reduction()\n            &gt;&gt;&gt; print(\"Original lower bounds:\", spot.all_lower)\n            Original lower bounds: [ 1  5  0]\n            &gt;&gt;&gt; print(\"Original upper bounds:\", spot.all_upper)\n            Original upper bounds: [10  5  1]\n            &gt;&gt;&gt; print(\"Fixed dimensions mask:\", spot.ident)\n            Fixed dimensions mask: [False  True False]\n            &gt;&gt;&gt; print(\"Reduced lower bounds:\", spot.lower)\n            Reduced lower bounds: [1 0]\n            &gt;&gt;&gt; print(\"Reduced upper bounds:\", spot.upper)\n            Reduced upper bounds: [10  1]\n            &gt;&gt;&gt; print(\"Reduced variable names:\", spot.var_name)\n            Reduced variable names: ['x0', 'x2']\n            &gt;&gt;&gt; print(\"Is dimension reduction active?\", spot.red_dim)\n            Is dimension reduction active? True\n        \"\"\"\n        # Backup original values\n        self.all_lower = self.lower.copy()\n        self.all_upper = self.upper.copy()\n        self.all_var_type = self.var_type.copy()\n        self.all_var_name = self.var_name.copy()\n        self.all_var_trans = self.var_trans.copy()\n\n        # Identify fixed dimensions (lower == upper)\n        self.ident = (self.upper - self.lower) == 0\n\n        # Check if any dimension is fixed\n        self.red_dim = self.ident.any()\n\n        if self.red_dim:\n            # Reduce bounds to only varying dimensions\n            self.lower = self.lower[~self.ident]\n            self.upper = self.upper[~self.ident]\n\n            # Update dimension count\n            self.n_dim = self.lower.size\n\n            # Reduce variable types and names\n            self.var_type = [\n                vtype\n                for vtype, fixed in zip(self.all_var_type, self.ident)\n                if not fixed\n            ]\n            self.var_name = [\n                vname\n                for vname, fixed in zip(self.all_var_name, self.ident)\n                if not fixed\n            ]\n\n            # Reduce transformations\n            self.var_trans = [\n                vtrans\n                for vtrans, fixed in zip(self.all_var_trans, self.ident)\n                if not fixed\n            ]\n\n            # Update bounds list for reduced dimensions\n            # Convert numpy types to Python native types (int or float based on var_type)\n            self.bounds = []\n            for i in range(self.n_dim):\n                # Check if var_type has this index (handle mismatched lengths)\n                if i &lt; len(self.var_type) and (\n                    self.var_type[i] == \"int\" or self.var_type[i] == \"factor\"\n                ):\n                    self.bounds.append((int(self.lower[i]), int(self.upper[i])))\n                else:\n                    self.bounds.append((float(self.lower[i]), float(self.upper[i])))\n\n            # Recreate LHS sampler with reduced dimensions\n            self.lhs_sampler = LatinHypercube(d=self.n_dim, seed=self.seed)\n\n    def to_red_dim(self, X_full: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Reduce full-dimensional points to optimization space.\n\n        This method removes fixed dimensions from full-dimensional points,\n        extracting only the varying dimensions used in optimization.\n\n        Args:\n            X_full (ndarray): Points in full space, shape (n_samples, n_original_dims).\n\n        Returns:\n            ndarray: Points in reduced space, shape (n_samples, n_reduced_dims).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Create problem with one fixed dimension\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (2, 2), (-5, 5)],  # x1 is fixed at 2\n            ...     max_iter=1,\n            ...     n_initial=3\n            ... )\n            &gt;&gt;&gt; X_full = np.array([[1.0, 2.0, 3.0], [4.0, 2.0, 5.0]])\n            &gt;&gt;&gt; X_red = opt.to_red_dim(X_full)\n            &gt;&gt;&gt; X_red.shape\n            (2, 2)\n            &gt;&gt;&gt; np.array_equal(X_red, np.array([[1.0, 3.0], [4.0, 5.0]]))\n            True\n        \"\"\"\n        if not self.red_dim:\n            # No reduction occurred, return as-is\n            return X_full\n\n        # Handle 1D array\n        if X_full.ndim == 1:\n            return X_full[~self.ident]\n\n        # Select only non-fixed dimensions (2D)\n        return X_full[:, ~self.ident]\n\n    def to_all_dim(self, X_red: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Expand reduced-dimensional points to full-dimensional representation.\n\n        This method restores points from the reduced optimization space to the\n        full-dimensional space by inserting fixed values for constant dimensions.\n\n        Args:\n            X_red (ndarray): Points in reduced space, shape (n_samples, n_reduced_dims).\n\n        Returns:\n            ndarray: Points in full space, shape (n_samples, n_original_dims).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Create problem with one fixed dimension\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (2, 2), (-5, 5)],  # x1 is fixed at 2\n            ...     max_iter=1,\n            ...     n_initial=3\n            ... )\n            &gt;&gt;&gt; X_red = np.array([[1.0, 3.0], [2.0, 4.0]])  # Only x0 and x2\n            &gt;&gt;&gt; X_full = opt.to_all_dim(X_red)\n            &gt;&gt;&gt; X_full.shape\n            (2, 3)\n            &gt;&gt;&gt; X_full[:, 1]  # Middle dimension should be 2.0\n            array([2., 2.])\n        \"\"\"\n        if not self.red_dim:\n            # No reduction occurred, return as-is\n            return X_red\n\n        # Number of samples and full dimensions\n        n_samples = X_red.shape[0]\n        n_full_dims = len(self.ident)\n\n        # Initialize full-dimensional array\n        X_full = np.zeros((n_samples, n_full_dims))\n\n        # Track index in reduced array\n        red_idx = 0\n\n        # Fill in values dimension by dimension\n        for i in range(n_full_dims):\n            if self.ident[i]:\n                # Fixed dimension: use stored value\n                X_full[:, i] = self.all_lower[i]\n            else:\n                # Varying dimension: use value from reduced array\n                X_full[:, i] = X_red[:, red_idx]\n                red_idx += 1\n\n        return X_full\n\n    # ====================\n    # Variable Transformation\n    # ====================\n\n    def transform_value(self, x: float, trans: Optional[str]) -&gt; float:\n        \"\"\"Apply transformation to a single float value.\n\n        Args:\n            x: Value to transform\n            trans: Transformation name. Can be one of 'id', 'log10', 'log', 'ln', 'sqrt',\n                   'exp', 'square', 'cube', 'inv', 'reciprocal', or None.\n                   Also supports dynamic strings like 'log(x)', 'sqrt(x)', 'pow(x, p)'.\n\n        Returns:\n            Transformed value\n\n        Raises:\n            TypeError: If x is not a float.\n            ValueError: If an unknown transformation is specified.\n\n        Notes:\n            See also inverse_transform_value.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10)])\n            &gt;&gt;&gt; spot.transform_value(10, 'log10')\n            1.0\n            &gt;&gt;&gt; spot.transform_value(100, 'log(x)')\n            4.605170185988092\n        \"\"\"\n        # Ensure x is a float\n        if not isinstance(x, float):\n            try:\n                x = float(x)\n            except (ValueError, TypeError):\n                raise TypeError(\n                    f\"transform_value expects a float, got {type(x).__name__} (value: {x})\"\n                )\n        if trans is None or trans == \"id\":\n            return x\n        elif trans == \"log10\":\n            return np.log10(x)\n        elif trans == \"log\" or trans == \"ln\":\n            return np.log(x)\n        elif trans == \"sqrt\":\n            return np.sqrt(x)\n        elif trans == \"exp\":\n            return np.exp(x)\n        elif trans == \"square\":\n            return x**2\n        elif trans == \"cube\":\n            return x**3\n        elif trans == \"inv\" or trans == \"reciprocal\":\n            return 1.0 / x\n\n        # Dynamic Transformations\n        import re\n\n        if trans == \"log(x)\":\n            return np.log(x)\n        if trans == \"sqrt(x)\":\n            return np.sqrt(x)\n\n        m = re.match(r\"pow\\(x,\\s*([0-9.]+)\\)\", trans)\n        if m:\n            p = float(m.group(1))\n            return x**p\n\n        m = re.match(r\"pow\\(([0-9.]+),\\s*x\\)\", trans)\n        if m:\n            base = float(m.group(1))\n            return base**x\n\n        m = re.match(r\"log\\(x,\\s*([0-9.]+)\\)\", trans)\n        if m:\n            base = float(m.group(1))\n            return np.log(x) / np.log(base)\n\n        raise ValueError(f\"Unknown transformation: {trans}\")\n\n    def inverse_transform_value(self, x: float, trans: Optional[str]) -&gt; float:\n        \"\"\"Apply inverse transformation to a single float value.\n\n        Args:\n            x: Transformed value\n            trans: Transformation name.\n\n        Returns:\n            Original value\n\n        Notes:\n            See also transform_value.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10)])\n            &gt;&gt;&gt; spot.inverse_transform_value(10, 'log10')\n            10.0\n            &gt;&gt;&gt; spot.inverse_transform_value(100, 'log(x)')\n            10.0\n        \"\"\"\n        # Ensure x is a float\n        if not isinstance(x, float):\n            try:\n                x = float(x)\n            except (ValueError, TypeError):\n                raise TypeError(\n                    f\"transform_value expects a float, got {type(x).__name__} (value: {x})\"\n                )\n        if trans is None or trans == \"id\":\n            return x\n        elif trans == \"log10\":\n            return 10**x\n        elif trans == \"log\" or trans == \"ln\":\n            return np.exp(x)\n        elif trans == \"sqrt\":\n            return x**2\n        elif trans == \"exp\":\n            return np.log(x)\n        elif trans == \"square\":\n            return np.sqrt(x)\n        elif trans == \"cube\":\n            return np.power(x, 1.0 / 3.0)\n        elif trans == \"inv\" or trans == \"reciprocal\":\n            return 1.0 / x\n\n        # Dynamic Transformations (Inverses)\n        if trans == \"log(x)\":\n            return np.exp(x)\n        if trans == \"sqrt(x)\":\n            return x**2\n\n        m = re.match(r\"pow\\(x,\\s*([0-9.]+)\\)\", trans)\n        if m:\n            p = float(m.group(1))\n            return x ** (1.0 / p)\n\n        m = re.match(r\"pow\\(([0-9.]+),\\s*x\\)\", trans)\n        if m:\n            base = float(m.group(1))\n            return np.log(x) / np.log(base)\n\n        m = re.match(r\"log\\(x,\\s*([0-9.]+)\\)\", trans)\n        if m:\n            base = float(m.group(1))\n            return base**x\n\n        raise ValueError(f\"Unknown transformation: {trans}\")\n\n    def _transform_X(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Transform parameter array from original to internal scale.\n\n        Converts from **Natural Space** (Original) to **Transformed Space** (Full Dimension).\n        Does NOT handle dimension reduction (mapping).\n\n        Args:\n            X (ndarray): Array in **Natural Space**, shape (n_samples, n_features)\n\n        Returns:\n            ndarray: Array in **Transformed Space** (Full Dimension)\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10)])\n            &gt;&gt;&gt; X_orig = np.array([[1], [10], [100]])\n            &gt;&gt;&gt; spot._transform_X(X_orig)\n            array([[0.        ],\n                   [1.        ],\n                   [2.        ]])\n        \"\"\"\n        X_transformed = X.copy()\n\n        # Handle 1D array\n        if X.ndim == 1:\n            for i, trans in enumerate(self.var_trans):\n                if trans is not None:\n                    X_transformed[i] = self.transform_value(X[i], trans)\n            return X_transformed\n\n        # Handle 2D array\n        for i, trans in enumerate(self.var_trans):\n            if trans is not None:\n                X_transformed[:, i] = np.array(\n                    [self.transform_value(x, trans) for x in X[:, i]]\n                )\n        return X_transformed\n\n    def _inverse_transform_X(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Transform parameter array from internal to original scale.\n\n        Converts from **Transformed Space** (Full Dimension) to **Natural Space** (Original).\n        Does NOT handle dimension expansion (un-mapping).\n\n        Args:\n            X (ndarray): Array in **Transformed Space**, shape (n_samples, n_features)\n\n        Returns:\n            ndarray: Array in **Natural Space**\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10)])\n            &gt;&gt;&gt; X_trans = np.array([[0], [1], [2]])\n            &gt;&gt;&gt; spot._inverse_transform_X(X_trans)\n            array([[  1.],\n                   [ 10.],\n                   [100.]])\n        \"\"\"\n        X_original = X.copy()\n\n        # Handle 1D array (single sample)\n        if X.ndim == 1:\n            for i, trans in enumerate(self.var_trans):\n                if trans is not None:\n                    # Element-wise transformation for 1D array\n                    X_original[i] = self.inverse_transform_value(X[i], trans)\n            return X_original\n\n        # Handle 2D array (multiple samples)\n        for i, trans in enumerate(self.var_trans):\n            if trans is not None:\n                X_original[:, i] = np.array(\n                    [self.inverse_transform_value(x, trans) for x in X[:, i]]\n                )\n        return X_original\n\n    def transform_bounds(self) -&gt; None:\n        \"\"\"Transform bounds from original to internal scale.\n\n        Updates `self.bounds` (and `self.lower`, `self.upper`) from **Natural Space**\n        to **Transformed Space**.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10), (0.1, 100)])\n            &gt;&gt;&gt; spot.var_trans = ['log10', 'sqrt']\n            &gt;&gt;&gt; spot.transform_bounds()\n            &gt;&gt;&gt; print(spot.bounds)\n            [(0.0, 1.0), (0.31622776601683794, 10.0)]\n        \"\"\"\n        for i, trans in enumerate(self.var_trans):\n            if trans is not None:\n                lower_t = self.transform_value(self.lower[i], trans)\n                upper_t = self.transform_value(self.upper[i], trans)\n\n                # Handle reversed bounds (e.g., reciprocal transformation)\n                if lower_t &gt; upper_t:\n                    self.lower[i], self.upper[i] = upper_t, lower_t\n                else:\n                    self.lower[i], self.upper[i] = lower_t, upper_t\n\n        # Update self.bounds to reflect transformed bounds\n        # Convert numpy types to Python native types (int or float based on var_type)\n        self.bounds = []\n        for i in range(len(self.lower)):\n            # Check if var_type has this index (handle mismatched lengths)\n            if i &lt; len(self.var_type) and (\n                self.var_type[i] == \"int\" or self.var_type[i] == \"factor\"\n            ):\n                self.bounds.append((int(self.lower[i]), int(self.upper[i])))\n            else:\n                self.bounds.append((float(self.lower[i]), float(self.upper[i])))\n\n    def _map_to_factor_values(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Map internal integer factor values back to string labels.\n\n        For factor variables, converts integer indices back to original string values.\n        Other variable types remain unchanged.\n\n        Args:\n            X (ndarray): Design points with integer values for factors,\n                shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Design points with factor integers replaced by string labels.\n                Dtype will be object or string if mixed types are present.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; spot = SpotOptim(\n            ...     fun=lambda x: x,\n            ...     bounds=[('red', 'blue'), (0, 10)]\n            ... )\n            &gt;&gt;&gt; spot.process_factor_bounds()\n            &gt;&gt;&gt; X_int = np.array([[0, 5.0], [1, 8.0]])\n            &gt;&gt;&gt; X_str = spot._map_to_factor_values(X_int)\n            &gt;&gt;&gt; print(X_str[0])\n            ['red' 5.0]\n        \"\"\"\n\n        if not self._factor_maps:\n            # No factor variables\n            return X\n\n        X = np.atleast_2d(X)\n        # Create object array to hold mixed types (strings and numbers)\n        X_mapped = np.empty(X.shape, dtype=object)\n        X_mapped[:] = X  # Copy numeric values\n\n        for dim_idx, mapping in self._factor_maps.items():\n            # Check if already mapped (strings) or needs mapping (numeric)\n            col_values = X[:, dim_idx]\n\n            # If already strings, keep them\n            if isinstance(col_values[0], str):\n                continue\n\n            # Round to nearest integer and map to string\n            int_values = np.round(col_values).astype(int)\n            # Clip to valid range\n            int_values = np.clip(int_values, 0, len(mapping) - 1)\n            # Map to strings\n            for i, val in enumerate(int_values):\n                X_mapped[i, dim_idx] = mapping[int(val)]\n\n        return X_mapped\n\n    # ====================\n    # Initial Design\n    # ====================\n\n    def get_initial_design(self, X0: Optional[np.ndarray] = None) -&gt; np.ndarray:\n        \"\"\"Generate or process initial design points.\n\n        Handles three scenarios:\n        1. X0 is None: Generate space-filling design using LHS\n        2. X0 is None but x0 is provided: Generate LHS and include x0 as first point\n        3. X0 is provided: Transform and prepare user-provided initial design\n\n        Args:\n            X0 (ndarray, optional): User-provided initial design points in original scale,\n                shape (n_initial, n_features). If None, generates space-filling design.\n                Defaults to None.\n\n        Returns:\n            ndarray: Initial design points in internal (transformed and reduced) scale,\n                shape (n_initial, n_features_reduced).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=10\n            ... )\n            &gt;&gt;&gt; # Generate default LHS design\n            &gt;&gt;&gt; X0 = opt.get_initial_design()\n            &gt;&gt;&gt; X0.shape\n            (10, 2)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Provide custom initial design\n            &gt;&gt;&gt; X0_custom = np.array([[0, 0], [1, 1], [2, 2]])\n            &gt;&gt;&gt; X0_processed = opt.get_initial_design(X0_custom)\n            &gt;&gt;&gt; X0_processed.shape\n            (3, 2)\n        \"\"\"\n        # Generate or use provided initial design\n        if X0 is None:\n            X0 = self._generate_initial_design()\n\n            # If starting point x0 was provided, include it in initial design\n            if self.x0 is not None:\n                # x0 is already validated and in internal scale\n                # Check if x0 is 1D or 2D\n                if self.x0.ndim == 1:\n                    x0_points = self.x0.reshape(1, -1)\n                else:\n                    x0_points = self.x0\n\n                n_x0 = x0_points.shape[0]\n\n                # If we have more x0 points than n_initial, use all x0 points\n                if n_x0 &gt;= self.n_initial:\n                    X0 = x0_points\n                    if self.verbose:\n                        print(f\"Using provided x0 points ({n_x0}) as initial design.\")\n                else:\n                    # Replace the first n_x0 points of LHS with x0 points\n                    X0 = np.vstack([x0_points, X0[:-n_x0]])\n                    if self.verbose:\n                        print(\n                            f\"Including {n_x0} starting points from x0 in initial design.\"\n                        )\n        else:\n            X0 = np.atleast_2d(X0)\n            # If user provided X0, it's in original scale - transform it\n            X0 = self._transform_X(X0)\n            # If X0 is in full dimensions and we have dimension reduction, reduce it\n            if self.red_dim and X0.shape[1] == len(self.ident):\n                X0 = self.to_red_dim(X0)\n            X0 = self._repair_non_numeric(X0, self.var_type)\n\n        return X0\n\n    def _generate_initial_design(self) -&gt; np.ndarray:\n        \"\"\"Generate initial space-filling design using Latin Hypercube Sampling.\n        Used in the optimize() method to create the initial set of design points.\n\n        Args:\n            None\n\n        Returns:\n            ndarray: Initial design points, shape (n_initial, n_features).\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 n_initial=10)\n            &gt;&gt;&gt; X0 = opt._generate_initial_design()\n            &gt;&gt;&gt; X0.shape\n            (10, 2)\n        \"\"\"\n        # Generate samples in [0, 1]^d\n        X0_unit = self.lhs_sampler.random(n=self.n_initial)\n\n        # Scale to [lower, upper]\n        X0 = self.lower + X0_unit * (self.upper - self.lower)\n\n        return self._repair_non_numeric(X0, self.var_type)\n\n    def _curate_initial_design(self, X0: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Remove duplicates and ensure sufficient unique points in initial design.\n\n        This method handles deduplication that can occur after rounding integer/factor\n        variables. If duplicates are found, it generates additional points to reach\n        the target n_initial unique points. Also handles repeating points when\n        repeats_initial &gt; 1.\n\n        Args:\n            X0 (ndarray): Initial design points in internal scale,\n                shape (n_samples, n_features).\n\n        Returns:\n            ndarray: Curated initial design with duplicates removed and repeated\n                if necessary, shape (n_unique * repeats_initial, n_features).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=10,\n            ...     var_type=['int', 'int']  # Integer variables may cause duplicates\n            ... )\n            &gt;&gt;&gt; X0 = opt.get_initial_design()\n            &gt;&gt;&gt; X0_curated = opt._curate_initial_design(X0)\n            &gt;&gt;&gt; X0_curated.shape[0] == 10  # Should have n_initial unique points\n            True\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With repeats\n            &gt;&gt;&gt; opt_repeat = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5,\n            ...     repeats_initial=3\n            ... )\n            &gt;&gt;&gt; X0 = opt_repeat.get_initial_design()\n            &gt;&gt;&gt; X0_curated = opt_repeat._curate_initial_design(X0)\n            &gt;&gt;&gt; X0_curated.shape[0] == 15  # 5 unique points * 3 repeats\n            True\n        \"\"\"\n        # Remove duplicates from initial design (can occur after rounding integers/factors)\n        # Keep only unique rows based on rounded values\n        X0_unique, unique_indices = np.unique(X0, axis=0, return_index=True)\n        if len(X0_unique) &lt; len(X0):\n            n_duplicates = len(X0) - len(X0_unique)\n            if self.verbose:\n                print(\n                    f\"Removed {n_duplicates} duplicate(s) from initial design after rounding\"\n                )\n\n            # Generate additional points to reach n_initial unique points\n            if len(X0_unique) &lt; self.n_initial:\n                n_additional = self.n_initial - len(X0_unique)\n                if self.verbose:\n                    print(\n                        f\"Generating {n_additional} additional point(s) to reach n_initial={self.n_initial}\"\n                    )\n\n                # Generate extra points and deduplicate again\n                max_gen_attempts = 10\n                for gen_attempt in range(max_gen_attempts):\n                    X_extra_unit = self.lhs_sampler.random(\n                        n=n_additional * 2\n                    )  # Generate extras\n                    X_extra = self.lower + X_extra_unit * (self.upper - self.lower)\n                    X_extra = self._repair_non_numeric(X_extra, self.var_type)\n\n                    # Combine and get unique\n                    X_combined = np.vstack([X0_unique, X_extra])\n                    X_combined_unique = np.unique(X_combined, axis=0)\n\n                    if len(X_combined_unique) &gt;= self.n_initial:\n                        X0 = X_combined_unique[: self.n_initial]\n                        break\n                else:\n                    # If still not enough unique points, just use what we have\n                    X0 = X_combined_unique\n                    if self.verbose:\n                        print(\n                            f\"Warning: Could only generate {len(X0)} unique initial points (target was {self.n_initial})\"\n                        )\n            else:\n                X0 = X0_unique\n\n        # Repeat initial design points if repeats_initial &gt; 1\n        if self.repeats_initial &gt; 1:\n            X0 = np.repeat(X0, self.repeats_initial, axis=0)\n\n        return X0\n\n    def _rm_NA_values(\n        self, X0: np.ndarray, y0: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, int]:\n        \"\"\"Remove NaN/inf values from initial design evaluations.\n\n        This method filters out design points that returned NaN or inf values\n        during initial evaluation. Unlike the sequential optimization phase where\n        penalties are applied, initial design points with invalid values are\n        simply removed.\n\n        Args:\n            X0 (ndarray): Initial design points in internal scale,\n                shape (n_samples, n_features).\n            y0 (ndarray): Function values at X0, shape (n_samples,).\n\n        Returns:\n            Tuple[ndarray, ndarray, int]: Filtered (X0, y0) with only finite values\n                and the original count before filtering. X0 has shape (n_valid, n_features),\n                y0 has shape (n_valid,), and the int is the original size.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=10\n            ... )\n            &gt;&gt;&gt; X0 = np.array([[1, 2], [3, 4], [5, 6]])\n            &gt;&gt;&gt; y0 = np.array([5.0, np.nan, np.inf])\n            &gt;&gt;&gt; X0_clean, y0_clean, n_eval = opt._rm_NA_values(X0, y0)\n            &gt;&gt;&gt; X0_clean.shape\n            (1, 2)\n            &gt;&gt;&gt; y0_clean\n            array([5.])\n            &gt;&gt;&gt; n_eval\n            3\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # All valid values - no filtering\n            &gt;&gt;&gt; X0 = np.array([[1, 2], [3, 4]])\n            &gt;&gt;&gt; y0 = np.array([5.0, 25.0])\n            &gt;&gt;&gt; X0_clean, y0_clean, n_eval = opt._rm_NA_values(X0, y0)\n            &gt;&gt;&gt; X0_clean.shape\n            (2, 2)\n            &gt;&gt;&gt; n_eval\n            2\n        \"\"\"\n        # Handle NaN/inf values in initial design - REMOVE them instead of applying penalty\n\n        # If y0 contains None (object dtype), convert None to NaN and ensure float dtype\n        if y0.dtype == object:\n            # Create a float array, replacing None with NaN\n            # Use list comprehension for safe conversion of None\n            y0 = np.array([np.nan if v is None else v for v in y0], dtype=float)\n\n        finite_mask = np.isfinite(y0)\n        n_non_finite = np.sum(~finite_mask)\n\n        if n_non_finite &gt; 0:\n            if self.verbose:\n                print(\n                    f\"Warning: {n_non_finite} initial design point(s) returned NaN/inf \"\n                    f\"and will be ignored (reduced from {len(y0)} to {np.sum(finite_mask)} points)\"\n                )\n            X0 = X0[finite_mask]\n            y0 = y0[finite_mask]\n\n            # Also filter y_mo if it exists (must match y0 size)\n            if self.y_mo is not None:\n                if len(self.y_mo) == len(finite_mask):  # Safety check\n                    self.y_mo = self.y_mo[finite_mask]\n                else:\n                    # Fallback or warning if sizes already mismatched (shouldn't happen here normally)\n                    if self.verbose:\n                        print(\n                            f\"Warning: y_mo size ({len(self.y_mo)}) != mask size ({len(finite_mask)}) in initial design filtering\"\n                        )\n                    # Try to filter only if sizes match, otherwise we might be in inconsistent state\n\n        return X0, y0, len(finite_mask)\n\n    def _validate_x0(self, x0: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Validate and process starting point x0.\n\n        This method checks that x0:\n        - Is a numpy array\n        - Has the correct number of dimensions\n        - Has values within bounds (in original scale)\n        - Is properly transformed to internal scale\n\n        Args:\n            x0 (array-like): Starting point in original scale\n\n        Returns:\n            ndarray: Validated and transformed x0 in internal scale, shape (n_features,)\n\n        Raises:\n            ValueError: If x0 is invalid\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     x0=np.array([1.0, 2.0])\n            ... )\n            &gt;&gt;&gt; # x0 is validated during initialization\n        \"\"\"\n        # Convert to numpy array\n        x0 = np.asarray(x0)\n\n        # Check if x0 is 1D or can be flattened to 1D\n        if x0.ndim == 0:\n            raise ValueError(\n                f\"x0 must be a 1D array-like, got scalar value. \"\n                f\"Expected shape: ({len(self.all_lower)},)\"\n            )\n        elif x0.ndim &gt; 2:\n            raise ValueError(\n                f\"x0 must be a 1D array-like, got {x0.ndim}D array. \"\n                f\"Expected shape: ({len(self.all_lower)},)\"\n            )\n        elif x0.ndim == 2:\n            pass  # 2D array is allowed now for x0\n            if x0.shape[0] == 1:\n                x0 = x0.ravel()\n\n        # Check number of dimensions (compare with original full dimensions before reduction)\n        # Check number of dimensions\n        expected_dim = len(self.all_lower)\n        if x0.ndim == 1:\n            if len(x0) != expected_dim:\n                raise ValueError(\n                    f\"x0 has {len(x0)} dimensions, but expected {expected_dim} dimensions. \"\n                    f\"Bounds specify {expected_dim} parameters: {self.all_var_name}\"\n                )\n        else:\n            if x0.shape[1] != expected_dim:\n                raise ValueError(\n                    f\"x0 has {x0.shape[1]} dimensions, but expected {expected_dim} dimensions. \"\n                    f\"Bounds specify {expected_dim} parameters: {self.all_var_name}\"\n                )\n\n        # Helper to validate a single point\n        def check_point(pt):\n            for i, (val, low, high, name) in enumerate(\n                zip(pt, self._original_lower, self._original_upper, self.all_var_name)\n            ):\n                # Ensure val is scalar for comparison (zip iterates elements, but be safe)\n                if self.red_dim and self.ident[i]:\n                    if not np.isclose(val, low, atol=self.eps):\n                        raise ValueError(\n                            f\"x0 ({name}) = {val} is a fixed dimension and must equal {low}. \"\n                        )\n                else:\n                    if not (low &lt;= val &lt;= high):\n                        raise ValueError(\n                            f\"x0 ({name}) = {val} is outside bounds [{low}, {high}]. \"\n                        )\n\n        if x0.ndim == 1:\n            check_point(x0)\n            # Apply transformations to x0 (from original to internal scale)\n            x0_transformed = self._transform_X(x0.reshape(1, -1)).ravel()\n        else:  # 2D case\n            for idx, pt in enumerate(x0):\n                check_point(pt)\n            x0_transformed = self._transform_X(x0)\n\n        # If dimension reduction is active, reduce x0 to non-fixed dimensions\n        if self.red_dim:\n            x0_transformed = x0_transformed[~self.ident]\n\n        if self.verbose:\n            print(\"Starting point x0 validated and processed successfully.\")\n            print(f\"  Original scale: {x0}\")\n            print(f\"  Internal scale: {x0_transformed}\")\n\n        return x0_transformed\n\n    def _check_size_initial_design(self, y0: np.ndarray, n_evaluated: int) -&gt; None:\n        \"\"\"Validate that initial design has sufficient points for surrogate fitting.\n\n        Checks if the number of valid initial design points meets the minimum\n        requirement for fitting a surrogate model. The minimum required is the\n        smaller of: (a) typical minimum for surrogate fitting (3 for multi-dimensional,\n        2 for 1D), or (b) what the user requested (n_initial).\n\n        Args:\n            y0 (ndarray): Function values at initial design points (after filtering),\n                shape (n_valid,).\n            n_evaluated (int): Original number of points evaluated before filtering.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If the number of valid points is less than the minimum required.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=10\n            ... )\n            &gt;&gt;&gt; # Sufficient points - no error\n            &gt;&gt;&gt; y0 = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n            &gt;&gt;&gt; opt._check_size_initial_design(y0, n_evaluated=10)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Insufficient points - raises ValueError\n            &gt;&gt;&gt; y0_small = np.array([1.0])\n            &gt;&gt;&gt; try:\n            ...     opt._check_size_initial_design(y0_small, n_evaluated=10)\n            ... except ValueError as e:\n            ...     print(f\"Error: {e}\")\n            Error: Insufficient valid initial design points: only 1 finite value(s) out of 10 evaluated...\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With verbose output\n            &gt;&gt;&gt; opt_verbose = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=10,\n            ...     verbose=True\n            ... )\n            &gt;&gt;&gt; y0_reduced = np.array([1.0, 2.0, 3.0])  # Less than n_initial but valid\n            &gt;&gt;&gt; opt_verbose._check_size_initial_design(y0_reduced, n_evaluated=10)\n            Note: Initial design size (3) is smaller than requested (10) due to NaN/inf values\n        \"\"\"\n        # Check if we have enough points to continue\n        # Use the smaller of: (a) typical minimum for surrogate fitting, or (b) what user requested\n        min_points_typical = 3 if self.n_dim &gt; 1 else 2\n        min_points_required = min(min_points_typical, self.n_initial)\n\n        if len(y0) &lt; min_points_required:\n            error_msg = (\n                f\"Insufficient valid initial design points: only {len(y0)} finite value(s) \"\n                f\"out of {n_evaluated} evaluated. Need at least {min_points_required} \"\n                f\"points to fit surrogate model. Please check your objective function or increase n_initial.\"\n            )\n            raise ValueError(error_msg)\n\n        if len(y0) &lt; self.n_initial and self.verbose:\n            print(\n                f\"Note: Initial design size ({len(y0)}) is smaller than requested \"\n                f\"({self.n_initial}) due to NaN/inf values\"\n            )\n\n    def _get_best_xy_initial_design(self) -&gt; None:\n        \"\"\"Determine and store the best point from initial design.\n\n        Finds the best (minimum) function value in the initial design,\n        stores the corresponding point and value in instance attributes,\n        and optionally prints the results if verbose mode is enabled.\n\n        For noisy functions, also reports the mean best value.\n\n        Note:\n            This method assumes self.X_ and self.y_ have been initialized\n            with the initial design evaluations.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5,\n            ...     verbose=True\n            ... )\n            &gt;&gt;&gt; # Simulate initial design (normally done in optimize())\n            &gt;&gt;&gt; opt.X_ = np.array([[1, 2], [0, 0], [2, 1]])\n            &gt;&gt;&gt; opt.y_ = np.array([5.0, 0.0, 5.0])\n            &gt;&gt;&gt; opt._get_best_xy_initial_design()\n            Initial best: f(x) = 0.000000\n            &gt;&gt;&gt; print(f\"Best x: {opt.best_x_}\")\n            Best x: [0 0]\n            &gt;&gt;&gt; print(f\"Best y: {opt.best_y_}\")\n            Best y: 0.0\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With noisy function\n            &gt;&gt;&gt; opt_noise = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5,\n            ...     noise=True,\n            ...     verbose=True\n            ... )\n            &gt;&gt;&gt; opt_noise.X_ = np.array([[1, 2], [0, 0], [2, 1]])\n            &gt;&gt;&gt; opt_noise.y_ = np.array([5.0, 0.0, 5.0])\n            &gt;&gt;&gt; opt_noise.min_mean_y = 0.5  # Simulated mean best\n            &gt;&gt;&gt; opt_noise._get_best_xy_initial_design()\n            Initial best: f(x) = 0.000000, mean best: f(x) = 0.500000\n        \"\"\"\n        # Initial best\n        best_idx = np.argmin(self.y_)\n        self.best_x_ = self.X_[best_idx].copy()\n        self.best_y_ = self.y_[best_idx]\n\n        if self.verbose:\n            if (self.repeats_initial &gt; 1) or (self.repeats_surrogate &gt; 1):\n                print(\n                    f\"Initial best: f(x) = {self.best_y_:.6f}, mean best: f(x) = {self.min_mean_y:.6f}\"\n                )\n            else:\n                print(f\"Initial best: f(x) = {self.best_y_:.6f}\")\n\n    def _update_repeats_infill_points(self, x_next: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Repeat infill point for noisy function evaluation.\n\n        For noisy objective functions (repeats_surrogate &gt; 1), creates multiple\n        copies of the suggested point for repeated evaluation. Otherwise, returns\n        the point in 2D array format.\n\n        Args:\n            x_next (ndarray): Next point to evaluate, shape (n_features,).\n\n        Returns:\n            ndarray: Points to evaluate, shape (repeats_surrogate, n_features)\n                or (1, n_features) if repeats_surrogate == 1.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Without repeats\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     repeats_surrogate=1\n            ... )\n            &gt;&gt;&gt; x_next = np.array([1.0, 2.0])\n            &gt;&gt;&gt; x_repeated = opt._update_repeats_infill_points(x_next)\n            &gt;&gt;&gt; x_repeated.shape\n            (1, 2)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With repeats for noisy function\n            &gt;&gt;&gt; opt_noisy = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     repeats_surrogate=3\n            ... )\n            &gt;&gt;&gt; x_next = np.array([1.0, 2.0])\n            &gt;&gt;&gt; x_repeated = opt_noisy._update_repeats_infill_points(x_next)\n            &gt;&gt;&gt; x_repeated.shape\n            (3, 2)\n            &gt;&gt;&gt; # All three copies should be identical\n            &gt;&gt;&gt; np.all(x_repeated[0] == x_repeated[1])\n            True\n        \"\"\"\n        if x_next.ndim == 1:\n            x_next = x_next.reshape(1, -1)\n\n        if self.repeats_surrogate &gt; 1:\n            # Repeat each row repeats_surrogate times\n            # Note: np.repeat with axis=0 repeats rows [r1, r1, r2, r2...]\n            x_next_repeated = np.repeat(x_next, self.repeats_surrogate, axis=0)\n        else:\n            x_next_repeated = x_next\n        return x_next_repeated\n\n    def _remove_nan(\n        self, X: np.ndarray, y: np.ndarray, stop_on_zero_return: bool = True\n    ) -&gt; tuple:\n        \"\"\"Remove rows where y contains NaN or inf values.\n        Used in the optimize() method after function evaluations.\n\n        Args:\n            X (ndarray): Design matrix, shape (n_samples, n_features).\n            y (ndarray): Objective values, shape (n_samples,).\n            stop_on_zero_return (bool): If True, raise error when all values are removed.\n\n        Returns:\n            tuple: (X_clean, y_clean) with NaN/inf rows removed.\n\n        Raises:\n            ValueError: If all values are NaN/inf and stop_on_zero_return is True.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1), bounds=[(-5, 5)])\n            &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n            &gt;&gt;&gt; y = np.array([1.0, np.nan, np.inf])\n            &gt;&gt;&gt; X_clean, y_clean = opt._remove_nan(X, y, stop_on_zero_return=False)\n            &gt;&gt;&gt; print(\"Clean X:\", X_clean)\n            Clean X: [[1 2]]\n            &gt;&gt;&gt; print(\"Clean y:\", y_clean)\n            Clean y: [1.]\n        \"\"\"\n        # Find finite values\n        finite_mask = np.isfinite(y)\n\n        if not np.any(finite_mask):\n            msg = \"All objective function values are NaN or inf.\"\n            if stop_on_zero_return:\n                raise ValueError(msg)\n            else:\n                if self.verbose:\n                    print(f\"Warning: {msg} Returning empty arrays.\")\n                return np.array([]).reshape(0, X.shape[1]), np.array([])\n\n        # Filter out non-finite values\n        n_removed = np.sum(~finite_mask)\n        if n_removed &gt; 0 and self.verbose:\n            print(f\"Warning: Removed {n_removed} sample(s) with NaN/inf values\")\n\n        return X[finite_mask], y[finite_mask]\n\n    # ====================\n    # Surrogate &amp; Acquisition\n    # ====================\n\n    def _fit_surrogate(self, X: np.ndarray, y: np.ndarray) -&gt; None:\n        \"\"\"Fit surrogate model to data.\n        Used in optimize() to fit the surrogate model.\n\n        If the number of points exceeds `self.max_surrogate_points`,\n        a subset of points is selected using the selection dispatcher.\n\n        Args:\n            X (ndarray): Design points, shape (n_samples, n_features).\n            y (ndarray): Function values at X, shape (n_samples,).\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 max_surrogate_points=10,\n            ...                 surrogate=GaussianProcessRegressor())\n            &gt;&gt;&gt; X = np.random.rand(50, 2)\n            &gt;&gt;&gt; y = np.random.rand(50)\n            &gt;&gt;&gt; opt._fit_surrogate(X, y)\n            &gt;&gt;&gt; # Surrogate is now fitted\n        \"\"\"\n        X_fit = X\n        y_fit = y\n\n        # Select subset if needed\n        # Resolve active max points\n        max_k = getattr(self, \"_active_max_surrogate_points\", self.max_surrogate_points)\n\n        if max_k is not None and X.shape[0] &gt; max_k:\n            if self.verbose:\n                print(\n                    f\"Selecting subset of {max_k} points \"\n                    f\"from {X.shape[0]} total points for surrogate fitting.\"\n                )\n            X_fit, y_fit = self._selection_dispatcher(X, y)\n\n        self.surrogate.fit(X_fit, y_fit)\n\n    def _fit_scheduler(self) -&gt; None:\n        \"\"\"Fit surrogate model using appropriate data based on noise handling.\n\n        This method selects the appropriate training data for surrogate fitting:\n        - For noisy functions (noise=True): Uses mean_X and mean_y (aggregated values)\n        - For deterministic functions: Uses X_ and y_ (all evaluated points)\n\n        The data is transformed to internal scale before fitting the surrogate.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor\n            &gt;&gt;&gt; # Deterministic function\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     surrogate=GaussianProcessRegressor(),\n            ...     n_initial=5\n            ... )\n            &gt;&gt;&gt; # Simulate optimization state\n            &gt;&gt;&gt; opt.X_ = np.array([[1, 2], [0, 0], [2, 1]])\n            &gt;&gt;&gt; opt.y_ = np.array([5.0, 0.0, 5.0])\n            &gt;&gt;&gt; opt._fit_scheduler()\n            &gt;&gt;&gt; # Surrogate fitted with X_ and y_\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Noisy function\n            &gt;&gt;&gt; opt_noise = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     surrogate=GaussianProcessRegressor(),\n            ...     n_initial=5,\n            ...     repeats_initial=3,\n            ...     noise=True  # Activates noise handling\n            ... )\n            &gt;&gt;&gt; # Simulate noisy optimization state\n            &gt;&gt;&gt; opt_noise.mean_X = np.array([[1, 2], [0, 0]])\n            &gt;&gt;&gt; opt_noise.mean_y = np.array([5.0, 0.0])\n            &gt;&gt;&gt; opt_noise._fit_scheduler()\n            &gt;&gt;&gt; # Surrogate fitted with mean_X and mean_y\n        \"\"\"\n        # Fit surrogate (use mean_y if noise, otherwise y_)\n        # Transform X to internal scale for surrogate fitting\n\n        # Handle multi-surrogate selection\n        if getattr(self, \"_surrogates_list\", None) is not None:\n            idx = self.rng.choice(len(self._surrogates_list), p=self._prob_surrogate)\n            self.surrogate = self._surrogates_list[idx]\n            # Update active max surrogate points\n            self._active_max_surrogate_points = self._max_surrogate_points_list[idx]\n            if self.verbose:\n                # Optional: print selected surrogate? separate from verbose maybe\n                pass\n\n        if (self.repeats_initial &gt; 1) or (self.repeats_surrogate &gt; 1):\n            X_for_surrogate = self._transform_X(self.mean_X)\n            self._fit_surrogate(X_for_surrogate, self.mean_y)\n        else:\n            X_for_surrogate = self._transform_X(self.X_)\n            self._fit_surrogate(X_for_surrogate, self.y_)\n\n    def _predict_with_uncertainty(self, X: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Predict with uncertainty estimates, handling surrogates without return_std.\n        Used in the _acquisition_function() method and in the plot_surrogate() method.\n\n        Args:\n            X: Input points, shape (n_samples, n_features)\n\n        Returns:\n            Tuple of (predictions, std_deviations). If surrogate doesn't support\n            return_std, returns predictions with zeros for std.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     surrogate=GaussianProcessRegressor()\n            ... )\n            &gt;&gt;&gt; X_train = np.array([[0, 0], [1, 1], [2, 2]])\n            &gt;&gt;&gt; y_train = np.array([0, 2, 8])\n            &gt;&gt;&gt; opt._fit_surrogate(X_train, y_train)\n            &gt;&gt;&gt; X_test = np.array([[1.5, 1.5], [3.0, 3.0]])\n            &gt;&gt;&gt; preds, stds = opt._predict_with_uncertainty(X_test)\n            &gt;&gt;&gt; print(\"Predictions:\", preds)\n            Predictions: [4.5 9. ]\n            &gt;&gt;&gt; print(\"Standard deviations:\", stds)\n            Standard deviations: [some values or zeros depending on surrogate]\n        \"\"\"\n        try:\n            # Try to get uncertainty estimates\n            y_pred, y_std = self.surrogate.predict(X, return_std=True)\n            return y_pred, y_std\n        except (TypeError, AttributeError):\n            # Surrogate doesn't support return_std (e.g., Random Forest, XGBoost)\n            y_pred = self.surrogate.predict(X)\n            y_std = np.zeros_like(y_pred)\n            return y_pred, y_std\n\n    def _acquisition_function(self, x: np.ndarray) -&gt; float:\n        \"\"\"Compute acquisition function value.\n        Used in the suggest_next_infill_point() method.\n\n        This implements \"Infill Criteria\" as described in Forrester et al. (2008),\n        Section 3 \"Exploring and Exploiting\".\n\n        Args:\n            x (ndarray): Point to evaluate, shape (n_features,).\n\n        Returns:\n            float: Acquisition function value (to be minimized).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     surrogate=GaussianProcessRegressor(),\n            ...     acquisition='ei'\n            ... )\n            &gt;&gt;&gt; X_train = np.array([[0, 0], [1, 1], [2, 2]])\n            &gt;&gt;&gt; y_train = np.array([0, 2, 8])\n            &gt;&gt;&gt; opt._fit_surrogate(X_train, y_train)\n            &gt;&gt;&gt; x_eval = np.array([1.5, 1.5])\n            &gt;&gt;&gt; acq_value = opt._acquisition_function(x_eval)\n            &gt;&gt;&gt; print(\"Acquisition function value:\", acq_value)\n            Acquisition function value: [some float value]\n        \"\"\"\n        x = x.reshape(1, -1)\n\n        if self.acquisition == \"y\":\n            # Predicted mean\n            return self.surrogate.predict(x)[0]\n\n        elif self.acquisition == \"ei\":\n            # Expected Improvement\n            mu, sigma = self._predict_with_uncertainty(x)\n            mu = mu[0]\n            sigma = sigma[0]\n\n            if sigma &lt; 1e-10:\n                return 0.0\n\n            y_best = np.min(self.y_)\n            improvement = y_best - mu\n            Z = improvement / sigma\n            ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n            return -ei  # Minimize negative EI\n\n        elif self.acquisition == \"pi\":\n            # Probability of Improvement\n            mu, sigma = self._predict_with_uncertainty(x)\n            mu = mu[0]\n            sigma = sigma[0]\n\n            if sigma &lt; 1e-10:\n                return 0.0\n\n            y_best = np.min(self.y_)\n            Z = (y_best - mu) / sigma\n            pi = norm.cdf(Z)\n            return -pi  # Minimize negative PI\n\n            raise ValueError(f\"Unknown acquisition function: {self.acquisition}\")\n\n    def _optimize_acquisition_tricands(self) -&gt; np.ndarray:\n        \"\"\"Optimize using geometric infill strategy via triangulation candidates.\n\n        Returns:\n            ndarray: The optimized point(s).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5\n            ... )\n            &gt;&gt;&gt; # Requires points to work\n            &gt;&gt;&gt; opt.X_ = np.random.rand(10, 2)\n            &gt;&gt;&gt; x_next = opt._optimize_acquisition_tricands()\n            &gt;&gt;&gt; x_next.shape[1] == 2\n            True\n        \"\"\"\n        # Use X_ (all evaluated points) as basis for triangulation\n        # If no points yet (e.g. before initial design), fallback to LHS or random\n        if not hasattr(self, \"X_\") or self.X_ is None or len(self.X_) &lt; self.n_dim + 1:\n            # Not enough points for valid triangulation (need n &gt;= m + 1)\n            # Fallback to random search using existing logic logic in 'else' block or explicit call pass\n            # Will fall through to 'else' block which handles generic minimize/random x0\n            # BUT 'tricands' isn't a valid minimize method, so we should handle this fallback specifically.\n            # Actually, let's just use random sampling here for fallback.\n\n            # Fallback to random search using generate_uniform_design\n            # Return size defaults to 1 unless specified\n            n_design = max(1, self.acquisition_fun_return_size)\n            x0 = generate_uniform_design(self.bounds, n_design, seed=self.rng)\n\n            # If we requested ONLY 1 point, return expected shape (n_dim,) for flatten behavior\n            if self.acquisition_fun_return_size &lt;= 1:\n                return x0.flatten()\n            return x0\n\n        # Generate candidates\n        # Default nmax to a reasonable multiple of desired return size, or just large enough\n        # tricands handles nmax internally (default 100*m).\n        # We pass nmax as max(100*m, acquisition_fun_return_size * 10) to ensure we have enough.\n        nmax = max(100 * self.n_dim, self.acquisition_fun_return_size * 50)\n\n        # Wrapper for tricands: Normalize -&gt; Gen Candidates -&gt; Denormalize\n        # This handles non-hypercube bounds correctly as tricands assumes [lower, upper]^m box.\n\n        # Normalize X_ to [0, 1] relative to bounds\n        X_norm = (self.X_ - self.lower) / (self.upper - self.lower)\n\n        # Generate candidates in [0, 1] space\n        X_cands_norm = tricands(\n            X_norm, nmax=nmax, lower=0.0, upper=1.0, fringe=self.tricands_fringe\n        )\n\n        # Denormalize candidates back to original space\n        X_cands = X_cands_norm * (self.upper - self.lower) + self.lower\n\n        # Evaluate acquisition function on all candidates\n        # _acquisition_function returns NEGATIVE acquisition values (minimization)\n        # We iterate to ensure correct handling of 1D/2D shapes by _acquisition_function\n        acq_values = np.array([self._acquisition_function(x) for x in X_cands])\n\n        # Sort indices (smallest is best because of negation)\n        sorted_indices = np.argsort(acq_values)\n\n        # Select top n\n        top_n = min(self.acquisition_fun_return_size, len(sorted_indices))\n        best_indices = sorted_indices[:top_n]\n        return X_cands[best_indices]\n\n    def _prepare_de_kwargs(self, x0=None):\n        \"\"\"Prepare kwargs for differential_evolution, extracting options if necessary.\"\"\"\n        kwargs = (self.config.acquisition_optimizer_kwargs or {}).copy()\n\n        # Extract 'options' if present (compatibility with minimize structure)\n        if \"options\" in kwargs and isinstance(kwargs[\"options\"], dict):\n            options = kwargs.pop(\"options\")\n            kwargs.update(options)\n\n        # Define valid arguments for differential_evolution\n        valid_de_args = {\n            \"strategy\",\n            \"maxiter\",\n            \"popsize\",\n            \"tol\",\n            \"mutation\",\n            \"recombination\",\n            \"seed\",\n            \"callback\",\n            \"disp\",\n            \"polish\",\n            \"init\",\n            \"atol\",\n            \"updating\",\n            \"workers\",\n            \"constraints\",\n            \"x0\",\n        }\n\n        # Filter kwargs to only include valid DE arguments\n        # This prevents errors when passing shared kwargs that contain\n        # optimizer-specific options for the surrogate (e.g. 'gtol' for L-BFGS-B)\n        filtered_kwargs = {k: v for k, v in kwargs.items() if k in valid_de_args}\n\n        # Set defaults if not provided\n        if \"maxiter\" not in filtered_kwargs:\n            filtered_kwargs[\"maxiter\"] = 1000\n\n        return filtered_kwargs\n\n    def _optimize_acquisition_de(self) -&gt; np.ndarray:\n        \"\"\"Optimize using differential evolution.\n\n        Returns:\n            ndarray: The optimized point(s).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5\n            ... )\n            &gt;&gt;&gt; # Requires surrogate model and points to work effectively\n            &gt;&gt;&gt; # but can run without them (will return randomish or best_x if set)\n            &gt;&gt;&gt; x_next = opt._optimize_acquisition_de()\n            &gt;&gt;&gt; x_next.shape[0] &gt;= 0\n            True\n        \"\"\"\n        # Variables to capture population from callback\n        population = None\n        population_energies = None\n        # with probability .5 select best_x_ as x0 or None\n        # Determine which \"best\" to use\n        if (self.repeats_initial &gt; 1 or self.repeats_surrogate &gt; 1) and hasattr(\n            self, \"min_mean_X\"\n        ):\n            best_x = self.min_mean_X\n        else:\n            best_x = self.best_x_\n\n        if best_x is not None:\n            best_x = self._transform_X(best_x)\n            best_X = best_x if self.rng.rand() &lt; self.de_x0_prob else None\n        else:\n            best_X = None\n\n        def callback(intermediate_result: OptimizeResult):\n            nonlocal population, population_energies\n            # Capture population if available (requires scipy &gt;= 1.10.0)\n            if hasattr(intermediate_result, \"population\"):\n                population = intermediate_result.population\n                population_energies = intermediate_result.population_energies\n\n        result = differential_evolution(\n            func=self._acquisition_function,\n            bounds=self.bounds,\n            seed=self.rng,\n            callback=callback,\n            x0=best_X,\n            **self._prepare_de_kwargs(best_X),\n        )\n\n        if self.acquisition_fun_return_size &gt; 1:\n            if population is not None and population_energies is not None:\n                # Sort by energy (ascending, since DE minimizes)\n                sorted_indices = np.argsort(population_energies)\n\n                # Determine how many to take\n                top_n = min(self.acquisition_fun_return_size, len(sorted_indices))\n\n                # First candidate is always the polished result (best)\n                candidates = [result.x]\n\n                # Add remaining candidates from population (skipping the best unpolished one which corresponds to result.x)\n                if top_n &gt; 1:\n                    # Take next (top_n - 1) indices\n                    # Start from 1 because 0 is the best unpolished\n                    next_indices = sorted_indices[1:top_n]\n                    candidates.extend(population[next_indices])\n\n                return np.array(candidates)\n            else:\n                # Fallback if population not available (e.g. very fast convergence or old scipy)\n                # Just return the best point as 2D array\n                return result.x.reshape(1, -1)\n\n        return result.x\n\n    def _optimize_acquisition_scipy(self) -&gt; np.ndarray:\n        \"\"\"Optimize using scipy.optimize.minimize interface (default).\n\n        Args:\n            None\n\n        Returns:\n            np.ndarray: The optimized acquisition function values.\n\n        Raises:\n            ValueError: If acquisition optimizer is not a string or callable.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Define objective function\n            &gt;&gt;&gt; def fun(x): return np.sum(x**2, axis=1)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Initialize optimizer with a scipy-compatible acquisition optimizer\n            &gt;&gt;&gt; # Note: default is 'differential_evolution' which uses a different method\n            &gt;&gt;&gt; optimizer = SpotOptim(\n            ...     fun=fun,\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     acquisition_optimizer=\"L-BFGS-B\"\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Create some dummy data to fit the surrogate model\n            &gt;&gt;&gt; X = np.array([[0.0, 0.0], [1.0, 1.0], [2.0, 2.0]])\n            &gt;&gt;&gt; y = fun(X)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Fit the surrogate model manually\n            &gt;&gt;&gt; # Note: this is normally handled inside optimize()\n            &gt;&gt;&gt; optimizer._fit_surrogate(X, y)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Optimize the acquisition function using scipy's minimize\n            &gt;&gt;&gt; x_next = optimizer._optimize_acquisition_scipy()\n            &gt;&gt;&gt; x_next.shape\n            (2,)\n\n        \"\"\"\n        # Use scipy.optimize.minimize interface\n        # Generate random x0 within bounds\n        low = np.array([b[0] for b in self.bounds])\n        high = np.array([b[1] for b in self.bounds])\n        # Use persistent RNG\n        x0 = self.rng.uniform(low, high)\n\n        if isinstance(self.acquisition_optimizer, str):\n            kwargs = self.config.acquisition_optimizer_kwargs or {}\n\n            # Avoid duplicate method args if kwargs has it, but self.acquisition_optimizer is the primary source\n            # However, if user passes kwargs={'method': '...'}, it might conflict if acquisition_optimizer is also a string.\n            # Strategy: self.acquisition_optimizer takes precedence if it's a specific string like \"L-BFGS-B\"\n            # But wait, acquisition_optimizer default is \"differential_evolution\".\n            # If user sets acquisition_optimizer=\"Nelder-Mead\", we use that.\n\n            run_kwargs = kwargs.copy()\n            if \"method\" not in run_kwargs:\n                run_kwargs[\"method\"] = self.acquisition_optimizer\n\n            # Define valid arguments for minimize() (excluding fun, x0, bounds which we pass explicitly)\n            valid_minimize_args = {\n                \"args\",\n                \"method\",\n                \"jac\",\n                \"hess\",\n                \"hessp\",\n                \"constraints\",\n                \"tol\",\n                \"callback\",\n                \"options\",\n            }\n\n            # Move any argument that is NOT a valid minimize() argument into 'options'\n            if \"options\" not in run_kwargs:\n                run_kwargs[\"options\"] = {}\n\n            keys_to_move = [k for k in run_kwargs if k not in valid_minimize_args]\n            for k in keys_to_move:\n                run_kwargs[\"options\"][k] = run_kwargs.pop(k)\n\n            # It's a method name for minimize (e.g. \"Nelder-Mead\")\n            result = minimize(\n                fun=self._acquisition_function, x0=x0, bounds=self.bounds, **run_kwargs\n            )\n        elif callable(self.acquisition_optimizer):\n            # It's a custom callable compatible with minimize\n            # We pass kwargs if the callable supports it?\n            # Safest is to just call it as before, or assume it handles kwargs if documented.\n            # Let's pass kwargs if we can, but the standard protocol might not expect it.\n            # We'll assume the callable is configured or wraps kwargs itself.\n            result = self.acquisition_optimizer(\n                fun=self._acquisition_function, x0=x0, bounds=self.bounds\n            )\n        else:\n            raise ValueError(\n                f\"Unknown acquisition optimizer type: {type(self.acquisition_optimizer)}\"\n            )\n\n        # Minimize-based optimizers typically return a single point\n        # If acquisition_fun_return_size &gt; 1, we map to 2D array but only 1 unique point\n        if self.acquisition_fun_return_size &gt; 1:\n            return result.x.reshape(1, -1)\n\n        return result.x\n\n    def _try_optimizer_candidates(\n        self, n_needed: int = 1, current_batch: Optional[List[np.ndarray]] = None\n    ) -&gt; List[np.ndarray]:\n        \"\"\"Try candidates proposed by the acquisition result optimizer.\n\n        Args:\n            n_needed (int): Number of candidates needed.\n            current_batch (list): Points already selected in current iteration (to check distance against).\n\n        Returns:\n            List[ndarray]: List of unique valid candidate points found.\n        \"\"\"\n        valid_candidates = []\n        if current_batch is None:\n            current_batch = []\n\n        # Phase 1: Try candidates from acquisition function optimizer\n        # These can be multiple if acquisition_fun_return_size &gt; 1\n        x_next_candidates = self.optimize_acquisition_func()\n\n        # Ensure iterable of 1D arrays\n        if x_next_candidates.ndim == 1:\n            obs_candidates = [x_next_candidates]\n        else:\n            obs_candidates = [\n                x_next_candidates[i] for i in range(x_next_candidates.shape[0])\n            ]\n\n        # Combine existing points and previously found candidates for distance check\n        # We need to check against (X_ + current_batch + valid_candidates)\n        # _select_new checks against X passed to it.\n        # We should construct the reference set once or update it.\n\n        X_transformed = self._transform_X(self.X_)\n\n        # Helper to check if a point is valid\n        def is_valid(p, reference_set):\n            p_rounded = self._repair_non_numeric(p.reshape(1, -1), self.var_type)[0]\n            # Check distance\n            p_2d = p_rounded.reshape(1, -1)\n            # select_new returns subset of A that is distant from X\n            x_new, _ = self.select_new(\n                A=p_2d, X=reference_set, tolerance=self.tolerance_x\n            )\n            return p_rounded if x_new.shape[0] &gt; 0 else None\n\n        for i, x_next in enumerate(obs_candidates):\n            if len(valid_candidates) &gt;= n_needed:\n                break\n\n            # Build current reference set: X_ + current_batch + valid_candidates_so_far\n            # Note: This can be expensive if X_ is large, but n_infill is usually small.\n            ref_list = [X_transformed]\n            if current_batch:\n                ref_list.append(np.array(current_batch))\n            if valid_candidates:\n                ref_list.append(np.array(valid_candidates))\n\n            if len(ref_list) &gt; 1:\n                reference_set = np.vstack(ref_list)\n            else:\n                reference_set = ref_list[0]\n\n            candidate = is_valid(x_next, reference_set)\n\n            if candidate is not None:\n                valid_candidates.append(candidate)\n            elif self.verbose:\n                print(\n                    f\"Optimizer candidate {i+1}/{len(obs_candidates)} was duplicate/invalid.\"\n                )\n\n        return valid_candidates\n\n    def _handle_acquisition_failure(self) -&gt; np.ndarray:\n        \"\"\"Handle acquisition failure by proposing new design points.\n        Used in the suggest_next_infill_point() method.\n\n        This method is called when no new design points can be suggested\n        by the surrogate model (e.g., when the proposed point is too close\n        to existing points). It proposes a random space-filling design as a fallback.\n\n        Returns:\n            ndarray: New design point as a fallback, shape (n_features,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     acquisition_failure_strategy='random'\n            ... )\n            &gt;&gt;&gt; opt.X_ = np.array([[0, 0], [1, 1]])\n            &gt;&gt;&gt; opt.y_ = np.array([0, 2])\n            &gt;&gt;&gt; x_fallback = opt._handle_acquisition_failure()\n            &gt;&gt;&gt; x_fallback.shape\n            (2,)\n            &gt;&gt;&gt; print(x_fallback)\n            [some new point within bounds]\n        \"\"\"\n        if self.acquisition_failure_strategy == \"random\":\n            # Default: random space-filling design (Latin Hypercube Sampling)\n            if self.verbose:\n                print(\n                    \"Acquisition failure: Using random space-filling design as fallback.\"\n                )\n            x_new_unit = self.lhs_sampler.random(n=1)[0]\n            x_new = self.lower + x_new_unit * (self.upper - self.lower)\n\n        return self._repair_non_numeric(x_new.reshape(1, -1), self.var_type)[0]\n\n    def _try_fallback_strategy(\n        self, max_attempts: int = 10, current_batch: Optional[List[np.ndarray]] = None\n    ) -&gt; Tuple[Optional[np.ndarray], np.ndarray]:\n        \"\"\"Try fallback strategy (e.g. random search) to find a unique point.\n        Calls _handle_acquisition_failure.\n\n        Args:\n            max_attempts (int): Maximum number of fallback attempts.\n            current_batch (list): Points already selected in current iteration.\n\n        Returns:\n            Tuple[Optional[ndarray], ndarray]:\n                - The first element is the unique valid candidate point if found, else None.\n                - The second element is the last attempted point.\n        \"\"\"\n        x_last = None\n        if current_batch is None:\n            current_batch = []\n\n        X_transformed = self._transform_X(self.X_)\n        # Build reference set once if possible or dynamically\n        # Since fallback is random, we check against X + current_batch\n\n        for attempt in range(max_attempts):\n            if self.verbose:\n                print(\n                    f\"Fallback attempt {attempt + 1}/{max_attempts}: Using fallback strategy\"\n                )\n            x_next = self._handle_acquisition_failure()\n\n            x_next_rounded = self._repair_non_numeric(\n                x_next.reshape(1, -1), self.var_type\n            )[0]\n            x_last = x_next_rounded\n\n            x_next_2d = x_next_rounded.reshape(1, -1)\n\n            # Reference set\n            ref_list = [X_transformed]\n            if len(current_batch) &gt; 0:\n                ref_list.append(np.array(current_batch))\n\n            if len(ref_list) &gt; 1:\n                reference_set = np.vstack(ref_list)\n            else:\n                reference_set = ref_list[0]\n\n            x_new, _ = self.select_new(\n                A=x_next_2d, X=reference_set, tolerance=self.tolerance_x\n            )\n\n            if x_new.shape[0] &gt; 0:\n                return x_next_rounded, x_last\n\n        return None, x_last\n\n    def _get_shape(self, y: np.ndarray) -&gt; Tuple[int, Optional[int]]:\n        \"\"\"Get the shape of the objective function output.\n\n        Args:\n            y (ndarray): Objective function output, shape (n_samples,) or (n_samples, n_objectives).\n\n        Returns:\n            tuple: (n_samples, n_objectives) where n_objectives is None for single-objective.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=10,\n            ...     n_initial=5\n            ... )\n            &gt;&gt;&gt; y_single = np.array([1.0, 2.0, 3.0])\n            &gt;&gt;&gt; n, m = opt._get_shape(y_single)\n            &gt;&gt;&gt; print(f\"n={n}, m={m}\")\n            n=3, m=None\n            &gt;&gt;&gt; y_multi = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n            &gt;&gt;&gt; n, m = opt._get_shape(y_multi)\n            &gt;&gt;&gt; print(f\"n={n}, m={m}\")\n            n=3, m=2\n        \"\"\"\n        if y.ndim == 1:\n            return y.shape[0], None\n        elif y.ndim == 2:\n            return y.shape[0], y.shape[1]\n        else:\n            # For higher dimensions, flatten to 1D\n            return y.size, None\n\n    def _store_mo(self, y_mo: np.ndarray) -&gt; None:\n        \"\"\"Store multi-objective values in self.y_mo.\n\n        If multi-objective values are present (ndim==2), they are stored in self.y_mo.\n        New values are appended to existing ones. For single-objective problems,\n        self.y_mo remains None.\n\n        Args:\n            y_mo (ndarray): If multi-objective, shape (n_samples, n_objectives).\n                           If single-objective, shape (n_samples,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.column_stack([\n            ...         np.sum(X**2, axis=1),\n            ...         np.sum((X-1)**2, axis=1)\n            ...     ]),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=10,\n            ...     n_initial=5\n            ... )\n            &gt;&gt;&gt; y_mo_1 = np.array([[1.0, 2.0], [3.0, 4.0]])\n            &gt;&gt;&gt; opt._store_mo(y_mo_1)\n            &gt;&gt;&gt; print(f\"y_mo after first call: {opt.y_mo}\")\n            y_mo after first call: [[1. 2.]\n             [3. 4.]]\n            &gt;&gt;&gt; y_mo_2 = np.array([[5.0, 6.0], [7.0, 8.0]])\n            &gt;&gt;&gt; opt._store_mo(y_mo_2)\n            &gt;&gt;&gt; print(f\"y_mo after second call: {opt.y_mo}\")\n            y_mo after second call: [[1. 2.]\n             [3. 4.]\n             [5. 6.]\n             [7. 8.]]\n        \"\"\"\n        # Store y_mo in self.y_mo (append new values) if multi-objective\n        if self.y_mo is None and y_mo.ndim == 2:\n            self.y_mo = y_mo\n        elif y_mo.ndim == 2:\n            self.y_mo = np.vstack([self.y_mo, y_mo])\n\n    def _mo2so(self, y_mo: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Convert multi-objective values to single-objective.\n\n        Converts multi-objective values to a single-objective value by applying a user-defined\n        function from `fun_mo2so`. If no user-defined function is given, the\n        values in the first objective column are used.\n\n        This method is called after the objective function evaluation. It returns a 1D array\n        with the single-objective values.\n\n        Args:\n            y_mo (ndarray): If multi-objective, shape (n_samples, n_objectives).\n                           If single-objective, shape (n_samples,).\n\n        Returns:\n            ndarray: Single-objective values, shape (n_samples,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Multi-objective function\n            &gt;&gt;&gt; def mo_fun(X):\n            ...     return np.column_stack([\n            ...         np.sum(X**2, axis=1),\n            ...         np.sum((X-1)**2, axis=1)\n            ...     ])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 1: Default behavior (use first objective)\n            &gt;&gt;&gt; opt1 = SpotOptim(\n            ...     fun=mo_fun,\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=10,\n            ...     n_initial=5\n            ... )\n            &gt;&gt;&gt; y_mo = np.array([[1.0, 2.0], [3.0, 4.0]])\n            &gt;&gt;&gt; y_so = opt1._mo2so(y_mo)\n            &gt;&gt;&gt; print(f\"Single-objective (default): {y_so}\")\n            Single-objective (default): [1. 3.]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 2: Custom conversion function (sum of objectives)\n            &gt;&gt;&gt; def custom_mo2so(y_mo):\n            ...     return y_mo[:, 0] + y_mo[:, 1]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; opt2 = SpotOptim(\n            ...     fun=mo_fun,\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=10,\n            ...     n_initial=5,\n            ...     fun_mo2so=custom_mo2so\n            ... )\n            &gt;&gt;&gt; y_so_custom = opt2._mo2so(y_mo)\n            &gt;&gt;&gt; print(f\"Single-objective (custom): {y_so_custom}\")\n            Single-objective (custom): [3. 7.]\n        \"\"\"\n        n, m = self._get_shape(y_mo)\n        self._store_mo(y_mo)\n\n        # Use ndim to check if multi-objective\n        if y_mo.ndim == 2:\n            if self.fun_mo2so is not None:\n                # Apply user-defined conversion function\n                y0 = self.fun_mo2so(y_mo)\n            else:\n                # Default: use first column\n                if y_mo.size &gt; 0:\n                    y0 = y_mo[:, 0]\n                else:\n                    y0 = y_mo\n        else:\n            # Single-objective, return as-is\n            y0 = y_mo\n\n        return y0\n\n    def _get_ranks(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Returns ranks of numbers within input array x.\n\n        Args:\n            x (ndarray): Input array.\n\n        Returns:\n            ndarray: Ranks array where ranks[i] is the rank of x[i].\n\n        Examples:\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1), bounds=[(-5, 5)])\n            &gt;&gt;&gt; opt._get_ranks(np.array([2, 1]))\n            array([1, 0])\n            &gt;&gt;&gt; opt._get_ranks(np.array([20, 10, 100]))\n            array([1, 0, 2])\n        \"\"\"\n        ts = x.argsort()\n        ranks = np.empty_like(ts)\n        ranks[ts] = np.arange(len(x))\n        return ranks\n\n    def _get_ocba(\n        self, means: np.ndarray, vars: np.ndarray, delta: int, verbose: bool = False\n    ) -&gt; np.ndarray:\n        \"\"\"Optimal Computing Budget Allocation (OCBA).\n\n        Calculates budget recommendations for given means, variances, and incremental\n        budget using the OCBA algorithm.\n\n        References:\n            [1] Chun-Hung Chen and Loo Hay Lee: Stochastic Simulation Optimization:\n                An Optimal Computer Budget Allocation, pp. 49 and pp. 215\n\n        Args:\n            means (ndarray): Array of means.\n            vars (ndarray): Array of variances.\n            delta (int): Incremental budget.\n            verbose (bool): If True, print debug information. Defaults to False.\n\n        Returns:\n            ndarray: Array of budget recommendations, or None if conditions not met.\n\n        Examples:\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1), bounds=[(-5, 5)])\n            &gt;&gt;&gt; means = np.array([1, 2, 3, 4, 5])\n            &gt;&gt;&gt; vars = np.array([1, 1, 9, 9, 4])\n            &gt;&gt;&gt; allocations = opt._get_ocba(means, vars, 50)\n            &gt;&gt;&gt; allocations\n            array([11,  9, 19,  9,  2])\n        \"\"\"\n        if np.all(vars &gt; 0) and (means.shape[0] &gt; 2):\n            n_designs = means.shape[0]\n            allocations = np.zeros(n_designs, np.int32)\n            ratios = np.zeros(n_designs, np.float64)\n            budget = delta\n            ranks = self._get_ranks(means)\n            best, second_best = np.argpartition(ranks, 2)[:2]\n            ratios[second_best] = 1.0\n            select = [i for i in range(n_designs) if i not in [best, second_best]]\n            temp = (means[best] - means[second_best]) / (means[best] - means[select])\n            ratios[select] = np.square(temp) * (vars[select] / vars[second_best])\n            select = [i for i in range(n_designs) if i not in [best]]\n            temp = (np.square(ratios[select]) / vars[select]).sum()\n            ratios[best] = np.sqrt(vars[best] * temp)\n            more_runs = np.full(n_designs, True, dtype=bool)\n            add_budget = np.zeros(n_designs, dtype=float)\n            more_alloc = True\n\n            if verbose:\n                print(\"\\nIn _get_ocba():\")\n                print(f\"means: {means}\")\n                print(f\"vars: {vars}\")\n                print(f\"delta: {delta}\")\n                print(f\"n_designs: {n_designs}\")\n                print(f\"Ratios: {ratios}\")\n                print(f\"Best: {best}, Second best: {second_best}\")\n\n            while more_alloc:\n                more_alloc = False\n                ratio_s = (more_runs * ratios).sum()\n                add_budget[more_runs] = (budget / ratio_s) * ratios[more_runs]\n                add_budget = np.around(add_budget).astype(int)\n                mask = add_budget &lt; allocations\n                add_budget[mask] = allocations[mask]\n                more_runs[mask] = 0\n\n                if mask.sum() &gt; 0:\n                    more_alloc = True\n                if more_alloc:\n                    budget = allocations.sum() + delta\n                    budget -= (add_budget * ~more_runs).sum()\n\n            t_budget = add_budget.sum()\n\n            # Adjust the best design to match the exact delta\n            # Ensure we don't go below current allocations\n            adjustment = allocations.sum() + delta - t_budget\n            add_budget[best] = max(allocations[best], add_budget[best] + adjustment)\n\n            return add_budget - allocations\n        else:\n            return None\n\n    def _get_ocba_X(\n        self,\n        X: np.ndarray,\n        means: np.ndarray,\n        vars: np.ndarray,\n        delta: int,\n        verbose: bool = False,\n    ) -&gt; np.ndarray:\n        \"\"\"Calculate OCBA allocation and repeat input array X.\n        Used in the optimize() method to generate new design points based on OCBA.\n\n        Args:\n            X (ndarray): Input array to be repeated, shape (n_designs, n_features).\n            means (ndarray): Array of means for each design.\n            vars (ndarray): Array of variances for each design.\n            delta (int): Incremental budget.\n            verbose (bool): If True, print debug information. Defaults to False.\n\n        Returns:\n            ndarray: Repeated array of X based on OCBA allocation, or None if\n                     conditions not met.\n\n        Examples:\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1), bounds=[(-5, 5)])\n            &gt;&gt;&gt; X = np.array([[1, 2], [4, 5], [7, 8]])\n            &gt;&gt;&gt; means = np.array([1.5, 35, 550])\n            &gt;&gt;&gt; vars = np.array([0.5, 50, 5000])\n            &gt;&gt;&gt; X_new = opt._get_ocba_X(X, means, vars, delta=5, verbose=False)\n            &gt;&gt;&gt; X_new.shape[0] == 5  # Should have 5 additional evaluations\n            True\n        \"\"\"\n        if np.all(vars &gt; 0) and (means.shape[0] &gt; 2):\n            o = self._get_ocba(means=means, vars=vars, delta=delta, verbose=verbose)\n            return np.repeat(X, o, axis=0)\n        else:\n            return None\n\n    def _evaluate_function(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Evaluate objective function at points X.\n        Used in the optimize() method to evaluate the objective function.\n\n        **Input Space**: `X` is expected in **Transformed and Mapped Space** (Internal scale, Reduced dimensions).\n        **Process**:\n        1. Expands `X` to **Transformed Space** (Full dimensions) if dimension reduction is active.\n        2. Inverse transforms `X` to **Natural Space** (Original scale).\n        3. Evaluates the user function with points in **Natural Space**.\n\n        If dimension reduction is active, expands X to full dimensions before evaluation.\n        Supports both single-objective and multi-objective functions. For multi-objective\n        functions, converts to single-objective using _mo2so method.\n\n        Args:\n            X (ndarray): Points to evaluate in **Transformed and Mapped Space**, shape (n_samples, n_reduced_features).\n\n        Returns:\n            ndarray: Function values, shape (n_samples,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Single-objective function\n            &gt;&gt;&gt; opt_so = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=10,\n            ...     n_initial=5\n            ... )\n            &gt;&gt;&gt; X = np.array([[1.0, 2.0], [3.0, 4.0]])\n            &gt;&gt;&gt; y = opt_so._evaluate_function(X)\n            &gt;&gt;&gt; print(f\"Single-objective output: {y}\")\n            Single-objective output: [ 5. 25.]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Multi-objective function (default: use first objective)\n            &gt;&gt;&gt; opt_mo = SpotOptim(\n            ...     fun=lambda X: np.column_stack([\n            ...         np.sum(X**2, axis=1),\n            ...         np.sum((X-1)**2, axis=1)\n            ...     ]),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=10,\n            ...     n_initial=5\n            ... )\n            &gt;&gt;&gt; y_mo = opt_mo._evaluate_function(X)\n            &gt;&gt;&gt; print(f\"Multi-objective output (first obj): {y_mo}\")\n            Multi-objective output (first obj): [ 5. 25.]\n        \"\"\"\n        # Ensure X is 2D\n        X = np.atleast_2d(X)\n\n        # Expand to full dimensions if needed\n        if self.red_dim:\n            X = self.to_all_dim(X)\n\n        # Apply inverse transformations to get original scale for function evaluation\n        X_original = self._inverse_transform_X(X)\n\n        # Map factor variables to original string values\n        X_for_eval = self._map_to_factor_values(X_original)\n\n        # Evaluate function\n        y_raw = self.fun(X_for_eval, *self.args, **self.kwargs)\n\n        # Convert to numpy array if needed\n        if not isinstance(y_raw, np.ndarray):\n            y_raw = np.array([y_raw])\n\n        # Handle multi-objective case\n        y = self._mo2so(y_raw)\n\n        # Ensure y is 1D\n        if y.ndim &gt; 1:\n            y = y.ravel()\n\n        return y\n\n    def _select_distant_points(\n        self, X: np.ndarray, y: np.ndarray, k: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Selects k points that are distant from each other using K-means clustering.\n\n        This method performs K-means clustering to find k clusters, then selects\n        the point closest to each cluster center. This ensures a space-filling\n        subset of points for surrogate model training.\n\n        Args:\n            X (ndarray): Design points, shape (n_samples, n_features).\n            y (ndarray): Function values at X, shape (n_samples,).\n            k (int): Number of points to select.\n\n        Returns:\n            tuple: A tuple containing:\n                - selected_X (ndarray): Selected design points, shape (k, n_features).\n                - selected_y (ndarray): Function values at selected points, shape (k,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 max_surrogate_points=5)\n            &gt;&gt;&gt; X = np.random.rand(100, 2)\n            &gt;&gt;&gt; y = np.random.rand(100)\n            &gt;&gt;&gt; X_sel, y_sel = opt._select_distant_points(X, y, 5)\n            &gt;&gt;&gt; X_sel.shape\n            (5, 2)\n        \"\"\"\n        # Perform k-means clustering\n        kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n\n        # Find the closest point to each cluster center\n        selected_indices = []\n        for center in kmeans.cluster_centers_:\n            distances = np.linalg.norm(X - center, axis=1)\n            closest_idx = np.argmin(distances)\n            selected_indices.append(closest_idx)\n\n        selected_indices = np.array(selected_indices)\n        return X[selected_indices], y[selected_indices]\n\n    def _select_best_cluster(\n        self, X: np.ndarray, y: np.ndarray, k: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Selects all points from the cluster with the smallest mean y value.\n\n        This method performs K-means clustering and selects all points from the\n        cluster whose center corresponds to the best (smallest) mean objective\n        function value.\n\n        Args:\n            X (ndarray): Design points, shape (n_samples, n_features).\n            y (ndarray): Function values at X, shape (n_samples,).\n            k (int): Number of clusters.\n\n        Returns:\n            tuple: A tuple containing:\n                - selected_X (ndarray): Selected design points from best cluster, shape (m, n_features).\n                - selected_y (ndarray): Function values at selected points, shape (m,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 max_surrogate_points=5,\n            ...                 selection_method='best')\n            &gt;&gt;&gt; X = np.random.rand(100, 2)\n            &gt;&gt;&gt; y = np.random.rand(100)\n            &gt;&gt;&gt; X_sel, y_sel = opt._select_best_cluster(X, y, 5)\n        \"\"\"\n        # Perform k-means clustering\n        kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n        labels = kmeans.labels_\n\n        # Compute mean y for each cluster\n        cluster_means = []\n        for cluster_idx in range(k):\n            cluster_y = y[labels == cluster_idx]\n            if len(cluster_y) == 0:\n                cluster_means.append(np.inf)\n            else:\n                cluster_means.append(np.mean(cluster_y))\n\n        # Find cluster with smallest mean y\n        best_cluster = np.argmin(cluster_means)\n\n        # Select all points from the best cluster\n        mask = labels == best_cluster\n        return X[mask], y[mask]\n\n    def _selection_dispatcher(\n        self, X: np.ndarray, y: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Dispatcher for selection methods.\n\n        Depending on the value of `self.selection_method`, this method calls\n        the appropriate selection function to choose a subset of points for\n        surrogate model training when the total number of points exceeds\n        `self.max_surrogate_points`.\n\n        Args:\n            X (ndarray): Design points, shape (n_samples, n_features).\n            y (ndarray): Function values at X, shape (n_samples,).\n\n        Returns:\n            tuple: A tuple containing:\n                - selected_X (ndarray): Selected design points.\n                - selected_y (ndarray): Function values at selected points.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 max_surrogate_points=5)\n            &gt;&gt;&gt; X = np.random.rand(100, 2)\n            &gt;&gt;&gt; y = np.random.rand(100)\n            &gt;&gt;&gt; X_sel, y_sel = opt._selection_dispatcher(X, y)\n            &gt;&gt;&gt; X_sel.shape[0] &lt;= 5\n            True\n        \"\"\"\n        # Resolve active max points\n        max_k = getattr(self, \"_active_max_surrogate_points\", self.max_surrogate_points)\n\n        if max_k is None:\n            return X, y\n\n        if self.selection_method == \"distant\":\n            return self._select_distant_points(X=X, y=y, k=max_k)\n        elif self.selection_method == \"best\":\n            return self._select_best_cluster(X=X, y=y, k=max_k)\n        else:\n            # If no valid selection method, return all points\n            return X, y\n\n    def select_new(\n        self, A: np.ndarray, X: np.ndarray, tolerance: float = 0\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Select rows from A that are not in X.\n        Used in suggest_next_infill_point() to avoid duplicate evaluations.\n\n        Args:\n            A (ndarray): Array with new values.\n            X (ndarray): Array with known values.\n            tolerance (float, optional): Tolerance value for comparison. Defaults to 0.\n\n        Returns:\n            tuple: A tuple containing:\n                - ndarray: Array with unknown (new) values.\n                - ndarray: Array with True if value is new, otherwise False.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1), bounds=[(-5, 5)])\n            &gt;&gt;&gt; A = np.array([[1, 2], [3, 4], [5, 6]])\n            &gt;&gt;&gt; X = np.array([[3, 4], [7, 8]])\n            &gt;&gt;&gt; new_A, is_new = opt.select_new(A, X)\n            &gt;&gt;&gt; print(\"New A:\", new_A)\n            New A: [[1 2]\n             [5 6]]\n            &gt;&gt;&gt; print(\"Is new:\", is_new)\n            Is new: [ True False  True]\n        \"\"\"\n        if len(X) == 0:\n            return A, np.ones(len(A), dtype=bool)\n\n        # Calculate distances using the configured metric\n        # cdist supports 'euclidean', 'minkowski', 'chebyshev', etc.\n        # Note: 'chebyshev' is closest to the previous logic but checks absolute difference on all coords\n        # Previous logic: np.all(np.abs(diff) &lt;= tolerance) -&gt; Chebyshev &lt;= tolerance\n        dists = cdist(A, X, metric=self.min_tol_metric)\n\n        # Check if min distance to any existing point is &lt;= tolerance (duplicate)\n        # Duplicate if ANY existing point is within tolerance\n        # is_duplicate[i] is True if A[i] is close to at least one point in X\n        is_duplicate = np.any(dists &lt;= tolerance, axis=1)\n\n        ind = is_duplicate\n        return A[~ind], ~ind\n\n    def optimize_acquisition_func(self) -&gt; np.ndarray:\n        \"\"\"Optimize the acquisition function to find the next point to evaluate.\n\n        Returns:\n            ndarray: The optimized point(s).\n                If acquisition_fun_return_size == 1, returns 1D array of shape (n_features,).\n                If acquisition_fun_return_size &gt; 1, returns 2D array of shape (N, n_features),\n                where N is min(acquisition_fun_return_size, population_size).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     acquisition='ei'\n            ... )\n            &gt;&gt;&gt; X_train = np.array([[0, 0], [1, 1], [2, 2]])\n            &gt;&gt;&gt; y_train = np.array([0, 2, 8])\n            &gt;&gt;&gt; opt._fit_surrogate(X_train, y_train)\n            &gt;&gt;&gt; x_next = opt.optimize_acquisition_func()\n            &gt;&gt;&gt; print(\"Next point to evaluate:\", x_next)\n            Next point to evaluate: [some float values]\n        \"\"\"\n        if self.acquisition_optimizer == \"tricands\":\n            return self._optimize_acquisition_tricands()\n        elif self.acquisition_optimizer == \"differential_evolution\":\n            return self._optimize_acquisition_de()\n        elif self.acquisition_optimizer == \"de_tricands\":\n            val = self.rng.rand()\n            if val &lt; self.prob_de_tricands:\n                return self._optimize_acquisition_de()\n            else:\n                return self._optimize_acquisition_tricands()\n        else:\n            return self._optimize_acquisition_scipy()\n\n    # ====================\n    # Optimization Loop\n    # ====================\n\n    def _optimize_run_task(\n        self,\n        seed: int,\n        timeout_start: float,\n        X0: Optional[np.ndarray],\n        y0_known_val: Optional[float],\n        max_iter_override: Optional[int],\n        shared_best_y=None,  # Accept shared value\n        shared_lock=None,  # Accept shared lock\n    ) -&gt; Tuple[str, OptimizeResult]:\n        \"\"\"Helper to run a single optimization task with a specific seed. Calls _optimize_single_run.\n\n        Args:\n            seed (int): Seed for this run.\n            timeout_start (float): Start time for timeout.\n            X0 (Optional[np.ndarray]): Initial design points in Natural Space, shape (n_initial, n_features).\n            y0_known_val (Optional[float]): Known best value for initial design.\n            max_iter_override (Optional[int]): Override for maximum number of iterations.\n            shared_best_y (Optional[float]): Shared best value for parallel runs.\n            shared_lock (Optional[Lock]): Shared lock for parallel runs.\n\n        Returns:\n            Tuple[str, OptimizeResult]: Tuple containing status and optimization result.\n        \"\"\"\n        # Set the seed for this run\n        self.seed = seed\n        self._set_seed()\n\n        # Re-initialize LHS sampler with new seed to ensure diversity in initial design\n        if hasattr(self, \"n_dim\"):\n            self.lhs_sampler = LatinHypercube(d=self.n_dim, seed=self.seed)\n\n        return self._optimize_single_run(\n            timeout_start,\n            X0,\n            y0_known=y0_known_val,\n            max_iter_override=max_iter_override,\n            shared_best_y=shared_best_y,\n            shared_lock=shared_lock,\n        )\n\n    def optimize(self, X0: Optional[np.ndarray] = None) -&gt; OptimizeResult:\n        \"\"\"Run the optimization process.\n\n        The optimization terminates when either:\n        - Total function evaluations reach max_iter (including initial design), OR\n        - Runtime exceeds max_time minutes\n\n        Input/Output Spaces:\n        - Input X0: Expected in Natural Space (original scale, physical units).\n        - Output result.x: Returned in Natural Space.\n        - Output result.X: Returned in Natural Space.\n        - Internal Optimization: Performed in Transformed and Mapped Space.\n\n        Args:\n            X0 (ndarray, optional): Initial design points in Natural Space, shape (n_initial, n_features).\n                If None, generates space-filling design. Defaults to None.\n\n        Returns:\n            OptimizeResult: Optimization result with fields:\n                - x: best point found in Natural Space\n                - fun: best function value\n                - nfev: number of function evaluations (including initial design)\n                - nit: number of sequential optimization iterations (after initial design)\n                - success: whether optimization succeeded\n                - message: termination message indicating reason for stopping, including\n                  statistics (function value, iterations, evaluations)\n                - X: all evaluated points in Natural Space\n                - y: all function values\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5,\n            ...     max_iter=20,\n            ...     seed=0,\n            ...     x0=np.array([0.0, 0.0]),\n            ...     verbose=True\n            ... )\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt; print(result.message.splitlines()[0])\n            Optimization terminated: maximum evaluations (20) reached\n            &gt;&gt;&gt; print(\"Best point:\", result.x)\n            Best point: [0. 0.]\n            &gt;&gt;&gt; print(\"Best value:\", result.fun)\n            Best value: 0.0\n        \"\"\"\n        # Track results across restarts for final aggregation.\n        self.restarts_results_ = []\n        # Capture start time for timeout enforcement.\n        timeout_start = time.time()\n\n        # Initial run state.\n        current_X0 = X0\n        status = \"START\"\n\n        while True:\n            # Get best result so far if we have results\n            best_res = (\n                min(self.restarts_results_, key=lambda x: x.fun)\n                if self.restarts_results_\n                else None\n            )\n\n            # Compute injected best value for restarts, then run one optimization cycle.\n            # y0_known_val carries the current global best objective so the next\n            # run can skip re-evaluating that known point when restart injection is on.\n            y0_known_val = (\n                best_res.fun\n                if (\n                    status == \"RESTART\"\n                    and self.restart_inject_best\n                    and self.restarts_results_\n                )\n                else None\n            )\n\n            # Calculate remaining budget\n            total_evals_so_far = sum(len(r.y) for r in self.restarts_results_)\n            remaining_iter = self.max_iter - total_evals_so_far\n\n            # If we don't have enough budget for at least initial design (or some minimal amount), stop\n            if remaining_iter &lt; self.n_initial:\n                if self.verbose:\n                    print(\"Global budget exhausted. Stopping restarts.\")\n                break\n\n            # Execute one optimization run using the remaining budget; dispatcher\n            # selects sequential vs parallel based on `n_jobs` and returns status/result.\n            status, result = self._execute_optimization_run(\n                timeout_start,\n                current_X0,\n                y0_known=y0_known_val,\n                max_iter_override=remaining_iter,\n            )\n            self.restarts_results_.append(result)\n\n            if status == \"FINISHED\":\n                break\n            elif status == \"RESTART\":\n                # Prepare for a clean restart: let get_initial_design() regenerate the full design.\n                current_X0 = None\n\n                # Find the global best result across completed restarts.\n                if self.restarts_results_:\n                    best_res = min(self.restarts_results_, key=lambda r: r.fun)\n\n                    if self.restart_inject_best:\n                        # Inject the current global best into the next run's initial design.\n                        # best_res.x is in natural scale; _validate_x0 converts to internal scale\n                        # so the injected point can be mixed with LHS samples.\n                        self.x0 = self._validate_x0(best_res.x)\n                        # Keep current_X0 unset so the initial design is rebuilt around the injected x0.\n                        current_X0 = None\n\n                        if self.verbose:\n                            print(\n                                f\"Restart injection: Using best found point so far as starting point (f(x)={best_res.fun:.6f}).\"\n                            )\n\n                if self.seed is not None and self.n_jobs == 1:\n                    # In sequential mode we advance the seed between restarts to diversify the LHS.\n                    # Parallel mode increments seeds per worker during dispatch.\n                    self.seed += 1\n                # Continue loop\n            else:\n                # Should not happen\n                break\n\n        # Return best result\n        if not self.restarts_results_:\n            return result  # Fallback\n\n        # Find best result based on 'fun'\n        best_result = min(self.restarts_results_, key=lambda r: r.fun)\n\n        # Merge results from all parallel runs (and sequential runs if any)\n        X_all_list = [res.X for res in self.restarts_results_]\n        y_all_list = [res.y for res in self.restarts_results_]\n\n        # Concatenate all evaluations\n        self.X_ = np.vstack(X_all_list)\n        self.y_ = np.concatenate(y_all_list)\n        self.counter = len(self.y_)\n\n        # Aggregated iterations (sum of all runs)\n        self.n_iter_ = sum(getattr(res, \"nit\", 0) for res in self.restarts_results_)\n\n        # Update best solution found\n        self.best_x_ = best_result.x\n        self.best_y_ = best_result.fun\n\n        return best_result\n\n    def _execute_optimization_run(\n        self,\n        timeout_start: float,\n        X0: Optional[np.ndarray] = None,\n        y0_known: Optional[float] = None,\n        max_iter_override: Optional[int] = None,\n        shared_best_y=None,  # New arg\n        shared_lock=None,  # New arg\n    ) -&gt; Tuple[str, OptimizeResult]:\n        \"\"\"Dispatcher for optimization run (Sequential vs Steady-State Parallel).\n        Depending on n_jobs, calls _optimize_steady_state (n_jobs &gt; 1) or _optimize_sequential_run (n_jobs == 1).\n\n        Args:\n            timeout_start (float): Start time for timeout.\n            X0 (Optional[np.ndarray]): Initial design points in Natural Space, shape (n_initial, n_features).\n            y0_known (Optional[float]): Known best value for initial design.\n            max_iter_override (Optional[int]): Override for maximum number of iterations.\n            shared_best_y (Optional[float]): Shared best value for parallel runs.\n            shared_lock (Optional[Lock]): Shared lock for parallel runs.\n\n        Returns:\n            Tuple[str, OptimizeResult]: Tuple containing status and optimization result.\n\n        Examples:\n            &gt;&gt;&gt; import time\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5,\n            ...     max_iter=20,\n            ...     seed=0,\n            ...     n_jobs=1,  # Use sequential optimization for deterministic output\n            ...     verbose=True\n            ... )\n            &gt;&gt;&gt; status, result = opt._execute_optimization_run(timeout_start=time.time())\n            &gt;&gt;&gt; print(status)\n            FINISHED\n            &gt;&gt;&gt; print(result.message.splitlines()[0])\n            Optimization terminated: maximum evaluations (20) reached\n        \"\"\"\n\n        # Dispatch to steady-state optimizer if proper parallelization is requested\n        if self.n_jobs &gt; 1:\n            return self._optimize_steady_state(\n                timeout_start,\n                X0,\n                y0_known=y0_known,\n                max_iter_override=max_iter_override,\n            )\n        else:\n            return self._optimize_sequential_run(\n                timeout_start,\n                X0,\n                y0_known=y0_known,\n                max_iter_override=max_iter_override,\n                shared_best_y=shared_best_y,\n                shared_lock=shared_lock,\n            )\n\n    def _optimize_sequential_run(\n        self,\n        timeout_start: float,\n        X0: Optional[np.ndarray] = None,\n        y0_known: Optional[float] = None,\n        max_iter_override: Optional[int] = None,\n        shared_best_y=None,\n        shared_lock=None,\n    ) -&gt; Tuple[str, OptimizeResult]:\n        \"\"\"Perform a single sequential optimization run.\n        Calls _initialize_run, _rm_NA_values, _check_size_initial_design, _init_storage, _get_best_xy_initial_design, and _run_sequential_loop.\n\n\n        Args:\n            timeout_start (float): Start time for timeout.\n            X0 (Optional[np.ndarray]): Initial design points in Natural Space, shape (n_initial, n_features).\n            y0_known (Optional[float]): Known best value for initial design.\n            max_iter_override (Optional[int]): Override for maximum number of iterations.\n            shared_best_y (Optional[float]): Shared best value for parallel runs.\n            shared_lock (Optional[Lock]): Shared lock for parallel runs.\n\n        Returns:\n            Tuple[str, OptimizeResult]: Tuple containing status and optimization result.\n\n        Raises:\n            ValueError: If the initial design has no valid points after removing NaN/inf values, or if the initial design is too small to proceed.\n\n        Examples:\n            &gt;&gt;&gt; import time\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5,\n            ...     max_iter=20,\n            ...     seed=0,\n            ...     n_jobs=1,  # Use sequential optimization for deterministic output\n            ...     verbose=True\n            ... )\n            &gt;&gt;&gt; status, result = opt._optimize_sequential_run(timeout_start=time.time())\n            &gt;&gt;&gt; print(status)\n            FINISHED\n            &gt;&gt;&gt; print(result.message.splitlines()[0])\n            Optimization terminated: maximum evaluations (20) reached\n        \"\"\"\n\n        # Store shared variable if provided\n        self.shared_best_y = shared_best_y\n        self.shared_lock = shared_lock\n\n        # Initialize: Set seed, Design, Evaluate Initial Design, Init Storage &amp; TensorBoard\n        X0, y0 = self._initialize_run(X0, y0_known)\n\n        # Handle NaN/inf values in initial design (remove invalid points)\n        X0, y0, n_evaluated = self._rm_NA_values(X0, y0)\n\n        # Check if we have enough valid points to continue\n        self._check_size_initial_design(y0, n_evaluated)\n\n        # Initialize storage and statistics\n        self._init_storage(X0, y0)\n        self._zero_success_count = 0\n        self._success_history = []  # Clear success history for new run\n\n        # Update stats after initial design\n        self.update_stats()\n\n        # Log initial design to TensorBoard\n        self._init_tensorboard()\n\n        # Determine and report initial best\n        self._get_best_xy_initial_design()\n\n        # Run the main sequential optimization loop\n        effective_max_iter = (\n            max_iter_override if max_iter_override is not None else self.max_iter\n        )\n        return self._run_sequential_loop(timeout_start, effective_max_iter)\n\n    def _initialize_run(\n        self, X0: Optional[np.ndarray], y0_known: Optional[float]\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Initialize optimization run: seed, design generation, initial evaluation.\n        Called from _optimize_sequential_run.\n\n        Args:\n            X0 (Optional[np.ndarray]): Initial design points in Natural Space, shape (n_initial, n_features).\n            y0_known (Optional[float]): Known best value for initial design.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Tuple containing initial design and corresponding objective values.\n\n        Raises:\n            ValueError: If the initial design has no valid points after removing NaN/inf values, or if the initial design is too small to proceed.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5,\n            ...     seed=0,\n            ...     x0=np.array([0.0, 0.0]),\n            ...     verbose=True\n            ... )\n            &gt;&gt;&gt; X0, y0 = opt._initialize_run(X0=None, y0_known=None)\n            &gt;&gt;&gt; X0.shape\n            (5, 2)\n            &gt;&gt;&gt; np.allclose(y0, np.sum(X0**2, axis=1))\n            True\n        \"\"\"\n        # Set seed for reproducibility\n        self._set_seed()\n\n        # Set initial design (generate or process user-provided points)\n        X0 = self.get_initial_design(X0)\n\n        # Curate initial design (remove duplicates, generate additional points if needed, repeat if necessary)\n        X0 = self._curate_initial_design(X0)\n\n        # Evaluate initial design\n        if y0_known is not None and self.x0 is not None:\n            # Identify injected point to skip evaluation\n            dists = np.linalg.norm(X0 - self.x0, axis=1)\n            # Use a small tolerance for matching\n            matches = dists &lt; 1e-9\n\n            if np.any(matches):\n                if self.verbose:\n                    print(\"Skipping re-evaluation of injected best point.\")\n\n                # Initialize y0\n                y0 = np.empty(len(X0))\n                y0[:] = np.nan\n\n                # Set known values\n                y0[matches] = y0_known\n\n                # Evaluate others\n                not_matches = ~matches\n                if np.any(not_matches):\n                    y0_others = self._evaluate_function(X0[not_matches])\n                    y0[not_matches] = y0_others\n            else:\n                # Injected point lost during curation? Should not happen if it was unique\n                y0 = self._evaluate_function(X0)\n        else:\n            y0 = self._evaluate_function(X0)\n\n        return X0, y0\n\n    def _run_sequential_loop(\n        self, timeout_start: float, effective_max_iter: int\n    ) -&gt; Tuple[str, OptimizeResult]:\n        \"\"\"Execute the main sequential optimization loop.\n\n        Args:\n             timeout_start (float): Start time for timeout.\n             effective_max_iter (int): Maximum number of iterations for this run (may be overridden for restarts).\n\n         Returns:\n             Tuple[str, OptimizeResult]: Tuple containing status and optimization result.\n\n         Raises:\n             ValueError:\n                If excessive consecutive failures occur (e.g., due to NaN/inf values in evaluations), indicating a potential issue with the objective function.\n\n         Examples:\n             &gt;&gt;&gt; import time\n             &gt;&gt;&gt; import numpy as np\n             &gt;&gt;&gt; from spotoptim import SpotOptim\n             &gt;&gt;&gt; opt = SpotOptim(\n             ...     fun=lambda X: np.sum(X**2, axis=1),\n             ...     bounds=[(-5, 5), (-5, 5)],\n             ...     n_initial=5,\n             ...     max_iter=20,\n             ...     seed=0,\n             ...     n_jobs=1,  # Use sequential optimization for deterministic output\n             ...     verbose=True\n             ... )\n             &gt;&gt;&gt; X0, y0 = opt._initialize_run(X0=None, y0_known=None)\n             &gt;&gt;&gt; X0, y0, n_evaluated = opt._rm_NA_values(X0, y0)\n             &gt;&gt;&gt; opt._check_size_initial_design(y0, n_evaluated)\n             &gt;&gt;&gt; opt._init_storage(X0, y0)\n             &gt;&gt;&gt; opt._zero_success_count = 0\n             &gt;&gt;&gt; opt._success_history = []\n             &gt;&gt;&gt; opt.update_stats()\n             &gt;&gt;&gt; opt._get_best_xy_initial_design()\n             &gt;&gt;&gt; status, result = opt._run_sequential_loop(timeout_start=time.time(), effective_max_iter=20)\n             &gt;&gt;&gt; print(status)\n             FINISHED\n             &gt;&gt;&gt; print(result.message.splitlines()[0])\n             Optimization terminated: maximum evaluations (20) reached\n        \"\"\"\n        consecutive_failures = 0\n\n        while (len(self.y_) &lt; effective_max_iter) and (\n            time.time() &lt; timeout_start + self.max_time * 60\n        ):\n            # Check for excessive consecutive failures (infinite loop prevention)\n            if consecutive_failures &gt; self.max_iter:\n                msg = (\n                    f\"Optimization stopped due to {consecutive_failures} consecutive \"\n                    \"invalid evaluations (NaN/inf). Check your objective function.\"\n                )\n                if self.verbose:\n                    print(f\"Warning: {msg}\")\n                return \"FINISHED\", OptimizeResult(\n                    x=self.best_x_,\n                    fun=self.best_y_,\n                    nfev=len(self.y_),\n                    nit=self.n_iter_,\n                    success=False,\n                    message=msg,\n                    X=self.X_,\n                    y=self.y_,\n                )\n\n            # Increment iteration counter. This is not the same as number of function evaluations.\n            self.n_iter_ += 1\n\n            # Fit surrogate (use mean_y if noise, otherwise y_)\n            self._fit_scheduler()\n\n            # Apply OCBA for noisy functions\n            X_ocba = self._apply_ocba()\n\n            # Suggest next point\n            x_next = self.suggest_next_infill_point()\n\n            # Repeat next point if repeats_surrogate &gt; 1\n            x_next_repeated = self._update_repeats_infill_points(x_next)\n\n            # Append OCBA points to new design points (if applicable)\n            if X_ocba is not None:\n                x_next_repeated = append(X_ocba, x_next_repeated, axis=0)\n\n            # Evaluate next point(s) including OCBA points\n            y_next = self._evaluate_function(x_next_repeated)\n\n            # Handle NaN/inf values in new evaluations\n            x_next_repeated, y_next = self._handle_NA_new_points(\n                x_next_repeated, y_next\n            )\n            if x_next_repeated is None:\n                consecutive_failures += 1\n                continue  # Skip iteration if all evaluations were invalid\n\n            # Reset failure counter if we got valid points\n            consecutive_failures = 0\n\n            # Update success rate BEFORE updating storage (so it compares against previous best)\n            self._update_success_rate(y_next)\n\n            # Check for restart\n            if self.success_rate == 0.0:\n                self._zero_success_count += 1\n            else:\n                self._zero_success_count = 0\n\n            if self._zero_success_count &gt;= self.restart_after_n:\n                if self.verbose:\n                    print(\n                        f\"Restarting optimization: success_rate 0 for {self._zero_success_count} iterations.\"\n                    )\n\n                status_message = \"Restart triggered due to lack of improvement.\"\n\n                # Expand results to full dimensions if needed\n                best_x_full = (\n                    self.to_all_dim(self.best_x_.reshape(1, -1))[0]\n                    if self.red_dim\n                    else self.best_x_\n                )\n                X_full = self.to_all_dim(self.X_) if self.red_dim else self.X_\n\n                # Map factor variables back to original strings\n                best_x_result = self._map_to_factor_values(best_x_full.reshape(1, -1))[\n                    0\n                ]\n                X_result = (\n                    self._map_to_factor_values(X_full) if self._factor_maps else X_full\n                )\n\n                res = OptimizeResult(\n                    x=best_x_result,\n                    fun=self.best_y_,\n                    nfev=len(self.y_),\n                    nit=self.n_iter_,\n                    success=False,\n                    message=status_message,\n                    X=X_result,\n                    y=self.y_,\n                )\n                return \"RESTART\", res\n\n            # Update storage\n            self._update_storage(x_next_repeated, y_next)\n\n            # Update stats\n            self.update_stats()\n\n            # Log to TensorBoard\n            if self.tb_writer is not None:\n                # Log each new evaluation\n                for i in range(len(y_next)):\n                    self._write_tensorboard_hparams(x_next_repeated[i], y_next[i])\n                self._write_tensorboard_scalars()\n\n            # Update best solution\n            self._update_best_main_loop(\n                x_next_repeated, y_next, start_time=timeout_start\n            )\n\n        # Expand results to full dimensions if needed\n        # Note: best_x_ and X_ are already in original scale (stored that way)\n        best_x_full = (\n            self.to_all_dim(self.best_x_.reshape(1, -1))[0]\n            if self.red_dim\n            else self.best_x_\n        )\n        X_full = self.to_all_dim(self.X_) if self.red_dim else self.X_\n\n        # Determine termination reason\n        status_message = self._determine_termination(timeout_start)\n\n        # Append statistics to match scipy.optimize.minimize format\n        message = (\n            f\"{status_message}\\n\"\n            f\"         Current function value: {float(self.best_y_):.6f}\\n\"\n            f\"         Iterations: {self.n_iter_}\\n\"\n            f\"         Function evaluations: {len(self.y_)}\"\n        )\n\n        # Close TensorBoard writer\n        self._close_tensorboard_writer()\n\n        # Map factor variables back to original strings for results\n        best_x_result = self._map_to_factor_values(best_x_full.reshape(1, -1))[0]\n        X_result = self._map_to_factor_values(X_full) if self._factor_maps else X_full\n\n        # Return scipy-style result\n        return \"FINISHED\", OptimizeResult(\n            x=best_x_result,\n            fun=self.best_y_,\n            nfev=len(self.y_),\n            nit=self.n_iter_,\n            success=True,\n            message=message,\n            X=X_result,\n            y=self.y_,\n        )\n\n    def _update_storage_steady(self, x, y):\n        \"\"\"Helper to safely append single point (for steady state).\n\n            This method is designed for the steady-state parallel optimization scenario, where new points are evaluated and returned asynchronously.\n            It safely appends new points to the existing storage of evaluated points and their function values,\n            while also updating the current best solution if the new point is better.\n\n        Args:\n            x (ndarray):\n                New point(s) in original scale, shape (n_features,) or (N, n_features).\n            y (float or ndarray):\n                Corresponding function value(s).\n\n        Returns:\n            None. This method updates the internal state of the optimizer.\n\n        Note:\n           - This method assumes that the caller handles any necessary synchronization if used in a parallel context\n           (e.g., using locks when updating shared state).\n\n        Raises:\n            ValueError: If the input shapes are inconsistent or if y is not a scalar when x is a single point.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_jobs=2\n            ... )\n            &gt;&gt;&gt; opt._update_storage_steady(np.array([1.0, 2.0]), 5.0)\n            &gt;&gt;&gt; print(opt.X_)\n            [[1. 2.]]\n            &gt;&gt;&gt; print(opt.y_)\n            [5.]\n            &gt;&gt;&gt; print(opt.best_x_)\n            [1. 2.]\n            &gt;&gt;&gt; print(opt.best_y_)\n            5.0\n\n        \"\"\"\n        x = np.atleast_2d(x)\n        if self.X_ is None:\n            self.X_ = x\n            self.y_ = np.array([y])\n        else:\n            self.X_ = np.vstack([self.X_, x])\n            self.y_ = np.append(self.y_, y)\n\n        # Update best\n        if self.best_y_ is None or y &lt; self.best_y_:\n            self.best_y_ = y\n            self.best_x_ = x.flatten()\n\n        self.min_y = self.best_y_\n        self.min_X = self.best_x_\n\n    def _optimize_steady_state(\n        self,\n        timeout_start: float,\n        X0: Optional[np.ndarray],\n        y0_known: Optional[float] = None,\n        max_iter_override: Optional[int] = None,\n    ) -&gt; Tuple[str, OptimizeResult]:\n        \"\"\"Perform steady-state asynchronous optimization (n_jobs &gt; 1).\n\n        This method implements a steady-state parallelization strategy:\n\n        1.  Parallel Initial Design:\n            The first class of `n_initial * repeats_initial` runs are managed by the executor.\n            The first `n_jobs` are sent to separate processors. If the first job is ready,\n            its result is returned and the next of the initial design runs is dispatched.\n            This is continued until all `n_initial * repeats_initial` runs have returned their values.\n\n        2.  First Surrogate Fit:\n            The first surrogate model is built (fitted) using the `n_initial * repeats_initial` evaluations\n            collected in step 1.\n\n        3.  Parallel Search:\n            `n_jobs` searches (optimizations) on this surrogate are initially performed in parallel.\n\n        4.  Steady-State Loop:\n            - If a Search task is ready, the candidate point `x_cand` is immediately sent to the evaluation function to get `y_new`.\n            - As soon as `y_new` is available, the surrogate is fitted again (including the new `x_cand`, `y_new`).\n            - A new Search task is dispatched with this continuously updated model.\n            - This cycle ensures the surrogate is continuously updated as new points are available.\n\n        The optimization terminates when either:\n        - Total function evaluations reach `max_iter` (including initial design), OR\n        - Runtime exceeds `max_time` minutes\n\n        Args:\n            timeout_start (float): Start time for timeout.\n            X0 (Optional[np.ndarray]): Initial design points in Natural Space, shape (n_initial, n_features).\n            y0_known (Optional[float]): Known best value for initial design.\n            max_iter_override (Optional[int]): Override for maximum number of iterations.\n\n        Raises:\n            RuntimeError: If all initial design evaluations fail, likely due to pickling issues or missing imports in the worker process.\n            The error message provides guidance on how to address this issue.\n\n        Returns:\n            Tuple[str, OptimizeResult]: Tuple containing status and optimization result.\n\n        Examples:\n            &gt;&gt;&gt; import time\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5,\n            ...     max_iter=10,\n            ...     seed=0,\n            ...     n_jobs=2,  # Use parallel optimization\n            ...     verbose=True\n            ... )\n            &gt;&gt;&gt; status, result = opt._optimize_steady_state(timeout_start=time.time(), X0=None)\n            &gt;&gt;&gt; print(status)\n            FINISHED\n            &gt;&gt;&gt; print(result.message.splitlines()[0])\n            Optimization finished (Steady State)\n        \"\"\"\n        # Setup similar to _optimize_single_run\n        self._set_seed()\n        X0 = self.get_initial_design(X0)\n        X0 = self._curate_initial_design(X0)\n\n        # We need to know how many evaluations to do\n        effective_max_iter = (\n            max_iter_override if max_iter_override is not None else self.max_iter\n        )\n\n        # Import dill locally (assuming installed)\n        import dill\n\n        from concurrent.futures import ProcessPoolExecutor, wait, FIRST_COMPLETED\n\n        with ProcessPoolExecutor(max_workers=self.n_jobs) as executor:\n            futures = {}  # map future -&gt; type ('eval', 'search')\n\n            # --- Phase 1: Initial Design Evaluation ---\n            if self.verbose:\n                print(f\"Submitted {len(X0)} initial points for parallel evaluation...\")\n\n            for i, x in enumerate(X0):\n                # Dump args using dill\n                # Temporarily remove tb_writer (not picklable)\n                _tb_writer_temp = self.tb_writer\n                self.tb_writer = None\n                try:\n                    pickled_args = dill.dumps((self, x))\n                finally:\n                    self.tb_writer = _tb_writer_temp\n\n                fut = executor.submit(_remote_eval_wrapper, pickled_args)\n                futures[fut] = \"eval\"\n\n            # Wait for all initial to complete\n            initial_done_count = 0\n            while initial_done_count &lt; len(X0):\n                done, _ = wait(futures.keys(), return_when=FIRST_COMPLETED)\n                for fut in done:\n                    # Clean up\n                    ftype = futures.pop(fut)\n                    if ftype != \"eval\":\n                        continue\n\n                    try:\n                        x_done, y_done = fut.result()\n                        if isinstance(y_done, Exception):\n                            if self.verbose:\n                                print(f\"Eval failed: {y_done}\")\n                        else:\n                            self._update_storage_steady(x_done, y_done)\n                    except Exception as e:\n                        if self.verbose:\n                            print(f\"Task failed: {e}\")\n\n                    initial_done_count += 1\n\n            # Init tensorboard and stats\n            self._init_tensorboard()\n\n            if self.y_ is None or len(self.y_) == 0:\n                raise RuntimeError(\n                    \"All initial design evaluations failed. \"\n                    \"Check your objective function for pickling issues or missing imports (e.g. numpy) in the worker process. \"\n                    \"If defining functions in a notebook/script, ensure imports are inside the function.\"\n                )\n\n            self.update_stats()\n            self._get_best_xy_initial_design()\n\n            # Fit first surrogate\n            if self.verbose:\n                print(\n                    f\"Initial design evaluated. Fitting surrogate... (Data size: {len(self.y_)})\"\n                )\n            self._fit_scheduler()\n\n            # --- Phase 2: Steady State Loop ---\n            if self.verbose:\n                print(\"Starting steady-state optimization loop...\")\n\n            while (len(self.y_) &lt; effective_max_iter) and (\n                time.time() &lt; timeout_start + self.max_time * 60\n            ):\n                # 1. Fill slots - launch/dispatch\n                n_active = len(futures)\n                n_slots = self.n_jobs - n_active\n\n                if n_slots &gt; 0:\n                    for _ in range(n_slots):\n                        n_pending_evals = sum(\n                            1 for t in futures.values() if t == \"eval\"\n                        )\n                        if len(self.y_) + n_pending_evals &lt; effective_max_iter:\n                            # Dumpt optimizer (self) using dill\n                            # Note: self can be large, but it's the only way to send state reliably with dill\n                            # We must temporarily remove tb_writer as it is not picklable\n                            _tb_writer_temp = self.tb_writer\n                            self.tb_writer = None\n                            try:\n                                pickled_opt = dill.dumps(self)\n                            finally:\n                                self.tb_writer = _tb_writer_temp\n\n                            fut = executor.submit(_remote_search_task, pickled_opt)\n                            futures[fut] = \"search\"\n                        else:\n                            break\n\n                if not futures:\n                    break  # Done\n\n                # 2. Wait for completion\n                done, _ = wait(futures.keys(), return_when=FIRST_COMPLETED)\n                for fut in done:\n                    ftype = futures.pop(fut)\n                    try:\n                        res = fut.result()\n                        if isinstance(res, Exception):\n                            if self.verbose:\n                                print(f\"Remote {ftype} failed: {res}\")\n                            # If search failed, we just retry loop (next iter will check slots)\n                            # If eval failed, we unfortunately lost a budget count unless we don't count it?\n                            # Current logic counts evaluations in self.y_ only if successful.\n                            # So failed eval means budget not consumed, will retry.\n                            continue\n\n                        if ftype == \"search\":\n                            x_cand = res\n                            # Submit Eval immediately\n                            _tb_writer_temp = self.tb_writer\n                            self.tb_writer = None\n                            try:\n                                pickled_args = dill.dumps((self, x_cand))\n                            finally:\n                                self.tb_writer = _tb_writer_temp\n\n                            fut_eval = executor.submit(\n                                _remote_eval_wrapper, pickled_args\n                            )\n                            futures[fut_eval] = \"eval\"\n\n                        elif ftype == \"eval\":\n                            x_new, y_new = res\n                            # Update\n                            self._update_success_rate(np.array([y_new]))\n                            self._update_storage_steady(x_new, y_new)\n                            self.n_iter_ += 1\n\n                            if self.verbose:\n                                # Calculate progress\n                                if self.max_time != np.inf:\n                                    prog_val = (\n                                        (time.time() - timeout_start)\n                                        / (self.max_time * 60)\n                                        * 100\n                                    )\n                                    progress_str = f\"Time: {prog_val:.1f}%\"\n                                else:\n                                    prog_val = len(self.y_) / effective_max_iter * 100\n                                    progress_str = f\"Evals: {prog_val:.1f}%\"\n\n                                print(\n                                    f\"Iter {len(self.y_)}/{effective_max_iter} | \"\n                                    f\"Best: {self.best_y_:.6f} | \"\n                                    f\"Curr: {y_new:.6f} | \"\n                                    f\"Rate: {self.success_rate:.2f} | \"\n                                    f\"{progress_str}\"\n                                )\n\n                            # Refit surrogate\n                            self._fit_scheduler()\n\n                    except Exception as e:\n                        if self.verbose:\n                            print(f\"Error processing future: {e}\")\n\n        return \"FINISHED\", OptimizeResult(\n            x=self.best_x_,\n            fun=self.best_y_,\n            nfev=len(self.y_),\n            nit=self.n_iter_,\n            success=True,\n            message=\"Optimization finished (Steady State)\",\n            X=self.X_,\n            y=self.y_,\n        )\n\n    def suggest_next_infill_point(self) -&gt; np.ndarray:\n        \"\"\"Suggest next point to evaluate (dispatcher).\n\n        The returned point is in the **Transformed and Mapped Space** (Internal Optimization Space).\n        This means:\n        1. Transformations (e.g., log, sqrt) have been applied.\n        2. Dimension reduction has been applied (fixed variables removed).\n\n        Process:\n        1. Try candidates from acquisition function optimizer.\n        2. Handle acquisition failure (fallback).\n        3. Return last attempt if all fails.\n\n\n        Returns:\n            ndarray: Next point(s) to evaluate in **Transformed and Mapped Space**.\n            Shape is (n_infill_points, n_features).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5,\n            ...     n_infill_points=2\n            ... )\n            &gt;&gt;&gt; # Need to initialize optimization state (X_, y_, surrogate)\n            &gt;&gt;&gt; # Normally done inside optimize()\n            &gt;&gt;&gt; np.random.seed(0)\n            &gt;&gt;&gt; opt.X_ = np.random.rand(10, 2)\n            &gt;&gt;&gt; opt.y_ = np.random.rand(10)\n            &gt;&gt;&gt; opt._fit_surrogate(opt.X_, opt.y_)\n            &gt;&gt;&gt; x_next = opt.suggest_next_infill_point()\n            &gt;&gt;&gt; x_next.shape\n            (2, 2)\n        \"\"\"\n        # 1. Optimizer candidates\n        candidates = []\n        opt_candidates = self._try_optimizer_candidates(\n            n_needed=self.n_infill_points, current_batch=candidates\n        )\n        candidates.extend(opt_candidates)\n\n        if len(candidates) &gt;= self.n_infill_points:\n            return np.vstack(candidates)\n\n        # 2. Try fallback strategy to fill remaining slots\n        while len(candidates) &lt; self.n_infill_points:\n            # Just try one attempt at a time but loop\n            # We pass current batch to avoid dups\n            cand, x_last = self._try_fallback_strategy(\n                max_attempts=10, current_batch=candidates\n            )\n            if cand is not None:\n                candidates.append(cand)\n            else:\n                # Fallback failed to find unique point even after retries\n                # Break and fill with last attempts or just return what we have?\n                # If we return partial batch, we might fail downstream if code expects n points?\n                # Actually code should handle any number of points returned by this method?\n                # Or duplicate valid points?\n                # Warn and use duplicate if absolutely necessary?\n                if self.verbose:\n                    print(\n                        \"Warning: Could not fill all infill points with unique candidates.\"\n                    )\n                break\n\n        if len(candidates) &gt; 0:\n            return np.vstack(candidates)\n\n        # 3. Return last attempt (duplicate) if absolutely nothing found\n        # This returns a single point (1, d).\n        # Should we return n copies?\n        # If n_infill_points &gt; 1, we should probably output (n, d)\n\n        if self.verbose:\n            print(\n                \"Warning: Could not find unique point after optimization candidates and fallback attempts. \"\n                \"Returning last candidate (duplicate).\"\n            )\n\n        # Verify x_last is not None\n        if x_last is None:\n            # Should practically not happen\n            x_next = self._handle_acquisition_failure()\n            return x_next.reshape(1, -1)\n\n        # Return duplicated x_last to fill n_infill_points? OR just 1?\n        # Let's return 1 and let loop repeat it?\n        # But loop repeats based on x_next logic.\n        # If we return 1 point, it is treated as 1 point.\n        # If user asked for n_infill_points, maybe we should just return what we have (1 duplicated).\n\n        return x_last.reshape(1, -1)\n\n    def _handle_NA_new_points(\n        self, x_next: np.ndarray, y_next: np.ndarray\n    ) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"Handle NaN/inf values in new evaluation points.\n\n        Applies penalties to NaN/inf values and removes any remaining invalid points.\n        If all evaluations are invalid, returns None for both arrays to signal that\n        the iteration should be skipped.\n\n        Args:\n            x_next (ndarray): Design points that were evaluated, shape (n_eval, n_features).\n            y_next (ndarray): Function values at x_next, shape (n_eval,).\n\n        Returns:\n            Tuple[Optional[ndarray], Optional[ndarray]]: Tuple of (x_clean, y_clean).\n                Both are None if all evaluations were NaN/inf (iteration should be skipped).\n                Otherwise returns filtered arrays with only finite values.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5,\n            ...     verbose=True\n            ... )\n            &gt;&gt;&gt; # Simulate optimization state\n            &gt;&gt;&gt; opt.y_ = np.array([1.0, 2.0, 3.0])  # Historical values\n            &gt;&gt;&gt; opt.n_iter_ = 1\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Case 1: Some valid values\n            &gt;&gt;&gt; x_next = np.array([[1, 2], [3, 4], [5, 6]])\n            &gt;&gt;&gt; y_next = np.array([5.0, np.nan, 10.0])\n            &gt;&gt;&gt; x_clean, y_clean = opt._handle_NA_new_points(x_next, y_next)\n            &gt;&gt;&gt; x_clean.shape\n            (2, 2)\n            &gt;&gt;&gt; y_clean.shape\n            (2,)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Case 2: All NaN/inf - should skip iteration\n            &gt;&gt;&gt; x_all_bad = np.array([[1, 2], [3, 4]])\n            &gt;&gt;&gt; y_all_bad = np.array([np.nan, np.inf])\n            &gt;&gt;&gt; x_clean, y_clean = opt._handle_NA_new_points(x_all_bad, y_all_bad)\n            Warning: All new evaluations were NaN/inf, skipping iteration 1\n            &gt;&gt;&gt; x_clean is None\n            True\n            &gt;&gt;&gt; y_clean is None\n            True\n        \"\"\"\n        # Handle NaN/inf values in new evaluations\n        # Use historical y values (self.y_) for computing penalty statistics\n        if self.penalty:\n            y_next = self._apply_penalty_NA(y_next, y_history=self.y_)\n\n        # Identify which points are valid (finite) BEFORE removing them\n        # Note: _remove_nan filters based on y_next finite values\n\n        # Ensure y_next is a float array (maps non-convertible values like \"error\" or None to nan)\n        # This is critical if the objective function returns non-numeric values and penalty=False\n        if y_next.dtype == object:\n            # Use safe float conversion similar to _apply_penalty_NA\n            def _safe_float(v):\n                try:\n                    return float(v)\n                except (ValueError, TypeError):\n                    return np.nan\n\n            # Reconstruct as float array\n            y_flat = np.array(y_next).flatten()\n            y_next = np.array([_safe_float(v) for v in y_flat])\n\n        finite_mask = np.isfinite(y_next)\n\n        X_next_clean, y_next_clean = self._remove_nan(\n            x_next, y_next, stop_on_zero_return=False\n        )\n\n        # If we have multi-objective values, we need to filter them too\n        # The new MO values were appended to self.y_mo in _evaluate_function -&gt; _mo2so -&gt; _store_mo\n        # So self.y_mo currently contains the INVALID points at the end.\n        if self.y_mo is not None:\n            n_new = len(y_next)\n            # Check if y_mo has the new points appended\n            if len(self.y_mo) &gt;= n_new:\n                # The new points are at the end of y_mo\n                y_mo_new = self.y_mo[-n_new:]\n                y_mo_old = self.y_mo[:-n_new]\n\n                # Filter the new MO points using the mask from y_next\n                y_mo_new_clean = y_mo_new[finite_mask]\n\n                # Reconstruct y_mo\n                if len(y_mo_old) &gt; 0:\n                    self.y_mo = (\n                        np.vstack([y_mo_old, y_mo_new_clean])\n                        if len(y_mo_new_clean) &gt; 0\n                        else y_mo_old\n                    )\n                else:\n                    self.y_mo = y_mo_new_clean\n            else:\n                if self.verbose:\n                    print(\n                        \"Warning: y_mo size inconsistent with new points in _handle_NA_new_points\"\n                    )\n\n        # Skip this iteration if all new points were NaN/inf\n        if len(y_next_clean) == 0:\n            if self.verbose:\n                print(\n                    f\"Warning: All new evaluations were NaN/inf, skipping iteration {self.n_iter_}\"\n                )\n            return None, None\n\n        return X_next_clean, y_next_clean\n\n    def _update_best_main_loop(\n        self,\n        x_next_repeated: np.ndarray,\n        y_next: np.ndarray,\n        start_time: Optional[float] = None,\n    ) -&gt; None:\n        \"\"\"Update best solution found during main optimization loop.\n\n        Checks if any new evaluations improve upon the current best solution.\n        If improvement is found, updates best_x_ and best_y_ attributes and\n        prints progress if verbose mode is enabled.\n\n        Args:\n            x_next_repeated (ndarray): Design points that were evaluated in transformed space,\n                shape (n_eval, n_features).\n            y_next (ndarray): Function values at x_next_repeated, shape (n_eval,).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5,\n            ...     verbose=True\n            ... )\n            &gt;&gt;&gt; # Simulate optimization state\n            &gt;&gt;&gt; opt.n_iter_ = 1\n            &gt;&gt;&gt; opt.best_x_ = np.array([1.0, 1.0])\n            &gt;&gt;&gt; opt.best_y_ = 2.0\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Case 1: New best found\n            &gt;&gt;&gt; x_new = np.array([[0.1, 0.1], [0.5, 0.5]])\n            &gt;&gt;&gt; y_new = np.array([0.02, 0.5])\n            &gt;&gt;&gt; opt._update_best_main_loop(x_new, y_new)\n            Iteration 1: New best f(x) = 0.020000\n            &gt;&gt;&gt; opt.best_y_\n            0.02\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Case 2: No improvement\n            &gt;&gt;&gt; opt.n_iter_ = 2\n            &gt;&gt;&gt; x_no_improve = np.array([[1.5, 1.5]])\n            &gt;&gt;&gt; y_no_improve = np.array([4.5])\n            &gt;&gt;&gt; opt._update_best_main_loop(x_no_improve, y_no_improve)\n            Iteration 2: f(x) = 4.500000\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Case 3: With noisy function\n            &gt;&gt;&gt; opt_noise = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     noise=True,\n            ...     verbose=True\n            ... )\n            &gt;&gt;&gt; opt_noise.n_iter_ = 1\n            &gt;&gt;&gt; opt_noise.best_y_ = 2.0\n            &gt;&gt;&gt; opt_noise.min_mean_y = 1.5\n            &gt;&gt;&gt; y_noise = np.array([0.5])\n            &gt;&gt;&gt; x_noise = np.array([[0.5, 0.5]])\n            &gt;&gt;&gt; opt_noise._update_best_main_loop(x_noise, y_noise)\n            Iteration 1: New best f(x) = 0.500000, mean best: f(x) = 1.500000\n        \"\"\"\n        # Update best\n        # Determine global best value for printing if shared variable exists\n        global_best_val = None\n        if hasattr(self, \"shared_best_y\") and self.shared_best_y is not None:\n            # Sync with global shared value\n            lock_obj = getattr(self, \"shared_lock\", None)\n            if lock_obj is not None:\n                with lock_obj:\n                    if (\n                        self.best_y_ is not None\n                        and self.best_y_ &lt; self.shared_best_y.value\n                    ):\n                        self.shared_best_y.value = self.best_y_\n\n                    min_y_next = np.min(y_next)\n                    if min_y_next &lt; self.shared_best_y.value:\n                        self.shared_best_y.value = min_y_next\n\n                    global_best_val = self.shared_best_y.value\n\n        current_best = np.min(y_next)\n        if current_best &lt; self.best_y_:\n            best_idx_in_new = np.argmin(y_next)\n            # x_next_repeated is in transformed space, convert to original for storage\n            self.best_x_ = self._inverse_transform_X(\n                x_next_repeated[best_idx_in_new].reshape(1, -1)\n            )[0]\n            self.best_y_ = current_best\n\n            if self.verbose:\n                # Calculate progress\n                if self.max_time != np.inf and start_time is not None:\n                    progress = (time.time() - start_time) / (self.max_time * 60) * 100\n                    progress_str = f\"Time: {progress:.1f}%\"\n                else:\n                    prev_evals = sum(res.nfev for res in self.restarts_results_)\n                    progress = (prev_evals + self.counter) / self.max_iter * 100\n                    progress_str = f\"Evals: {progress:.1f}%\"\n\n                msg = f\"Iter {self.n_iter_}\"\n                if global_best_val is not None:\n                    msg += f\" | GlobalBest: {global_best_val:.6f}\"\n                msg += f\" | Best: {self.best_y_:.6f} | Rate: {self.success_rate:.2f} | {progress_str}\"\n\n                if (self.repeats_initial &gt; 1) or (self.repeats_surrogate &gt; 1):\n                    msg += f\" | Mean Best: {self.min_mean_y:.6f}\"\n\n                print(msg)\n        elif self.verbose:\n            if self.max_time != np.inf and start_time is not None:\n                progress = (time.time() - start_time) / (self.max_time * 60) * 100\n                progress_str = f\"Time: {progress:.1f}%\"\n            else:\n                prev_evals = sum(res.nfev for res in self.restarts_results_)\n                progress = (prev_evals + self.counter) / self.max_iter * 100\n                progress_str = f\"Evals: {progress:.1f}%\"\n\n            current_val = np.min(y_next)\n            msg = f\"Iter {self.n_iter_}\"\n            if global_best_val is not None:\n                msg += f\" | GlobalBest: {global_best_val:.6f}\"\n            msg += f\" | Best: {self.best_y_:.6f} | Curr: {current_val:.6f} | Rate: {self.success_rate:.2f} | {progress_str}\"\n\n            if (self.repeats_initial &gt; 1) or (self.repeats_surrogate &gt; 1):\n                mean_y_new = np.mean(y_next)\n                msg += f\" | Mean Curr: {mean_y_new:.6f}\"\n            pass\n\n    def _determine_termination(self, timeout_start: float) -&gt; str:\n        \"\"\"Determine termination reason for optimization.\n\n        Checks the termination conditions and returns an appropriate message\n        indicating why the optimization stopped. Three possible termination\n        conditions are checked in order of priority:\n        1. Maximum number of evaluations reached\n        2. Maximum time limit exceeded\n        3. Successful completion (neither limit reached)\n\n        Args:\n            timeout_start (float): Start time of optimization (from time.time()).\n\n        Returns:\n            str: Message describing the termination reason.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import time\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=20,\n            ...     max_time=10.0\n            ... )\n            &gt;&gt;&gt; # Case 1: Maximum evaluations reached\n            &gt;&gt;&gt; opt.y_ = np.zeros(20)  # Simulate 20 evaluations\n            &gt;&gt;&gt; start_time = time.time()\n            &gt;&gt;&gt; msg = opt._determine_termination(start_time)\n            &gt;&gt;&gt; print(msg)\n            Optimization terminated: maximum evaluations (20) reached\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Case 2: Time limit exceeded\n            &gt;&gt;&gt; opt.y_ = np.zeros(10)  # Only 10 evaluations\n            &gt;&gt;&gt; start_time = time.time() - 700  # Simulate 11.67 minutes elapsed\n            &gt;&gt;&gt; msg = opt._determine_termination(start_time)\n            &gt;&gt;&gt; print(msg)\n            Optimization terminated: time limit (10.00 min) reached\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Case 3: Successful completion\n            &gt;&gt;&gt; opt.y_ = np.zeros(10)  # Under max_iter\n            &gt;&gt;&gt; start_time = time.time()  # Just started\n            &gt;&gt;&gt; msg = opt._determine_termination(start_time)\n            &gt;&gt;&gt; print(msg)\n            Optimization finished successfully\n        \"\"\"\n        # Determine termination reason\n        elapsed_time = time.time() - timeout_start\n        if len(self.y_) &gt;= self.max_iter:\n            message = f\"Optimization terminated: maximum evaluations ({self.max_iter}) reached\"\n        elif elapsed_time &gt;= self.max_time * 60:\n            message = (\n                f\"Optimization terminated: time limit ({self.max_time:.2f} min) reached\"\n            )\n        else:\n            message = \"Optimization finished successfully\"\n\n        return message\n\n    def _apply_ocba(self) -&gt; Optional[np.ndarray]:\n        \"\"\"Apply Optimal Computing Budget Allocation for noisy functions.\n\n        Determines which existing design points should be re-evaluated based on\n        OCBA algorithm. This method computes optimal budget allocation to improve\n        the quality of the estimated best design.\n\n        Returns:\n            Optional[ndarray]: Array of design points to re-evaluate, shape (n_re_eval, n_features).\n                Returns None if OCBA conditions are not met or OCBA is disabled.\n\n        Note:\n            OCBA is only applied when:\n            - (self.repeats_initial &gt; 1) or (self.repeats_surrogate &gt; 1)\n            - self.ocba_delta &gt; 0\n            - All variances are &gt; 0\n            - At least 3 design points exist\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1) + np.random.normal(0, 0.1, X.shape[0]),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5,\n            ...     noise=True,\n            ...     ocba_delta=5,\n            ...     verbose=True\n            ... )\n            &gt;&gt;&gt; # Simulate optimization state (normally done in optimize())\n            &gt;&gt;&gt; opt.mean_X = np.array([[1, 2], [0, 0], [2, 1]])\n            &gt;&gt;&gt; opt.mean_y = np.array([5.0, 0.1, 5.0])\n            &gt;&gt;&gt; opt.var_y = np.array([0.1, 0.05, 0.15])\n            &gt;&gt;&gt; X_ocba = opt._apply_ocba()\n              OCBA: Adding 5 re-evaluation(s)\n            &gt;&gt;&gt; X_ocba.shape[0] == 5\n            True\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # OCBA skipped - insufficient points\n            &gt;&gt;&gt; opt2 = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     noise=True,\n            ...     ocba_delta=5,\n            ...     verbose=True\n            ... )\n            &gt;&gt;&gt; opt2.mean_X = np.array([[1, 2], [0, 0]])\n            &gt;&gt;&gt; opt2.mean_y = np.array([5.0, 0.1])\n            &gt;&gt;&gt; opt2.var_y = np.array([0.1, 0.05])\n            &gt;&gt;&gt; X_ocba = opt2._apply_ocba()\n            Warning: OCBA skipped (need &gt;2 points with variance &gt; 0)\n            &gt;&gt;&gt; X_ocba is None\n            True\n        \"\"\"\n        # OCBA: Compute optimal budget allocation for noisy functions\n        # This determines which existing design points should be re-evaluated\n        X_ocba = None\n        if (\n            self.repeats_initial &gt; 1 or self.repeats_surrogate &gt; 1\n        ) and self.ocba_delta &gt; 0:\n            # Check conditions for OCBA (need variance &gt; 0 and at least 3 points)\n            if not np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &lt;= 2):\n                if self.verbose:\n                    print(\"Warning: OCBA skipped (need &gt;2 points with variance &gt; 0)\")\n            elif np.all(self.var_y &gt; 0) and (self.mean_X.shape[0] &gt; 2):\n                # Get OCBA allocation\n                X_ocba = self._get_ocba_X(\n                    self.mean_X,\n                    self.mean_y,\n                    self.var_y,\n                    self.ocba_delta,\n                    verbose=self.verbose,\n                )\n                if self.verbose and X_ocba is not None:\n                    print(f\"  OCBA: Adding {X_ocba.shape[0]} re-evaluation(s)\")\n\n        return X_ocba\n\n    def _apply_penalty_NA(\n        self,\n        y: np.ndarray,\n        y_history: Optional[np.ndarray] = None,\n        penalty_value: Optional[float] = None,\n        sd: float = 0.1,\n    ) -&gt; np.ndarray:\n        \"\"\"Replace NaN and infinite values with penalty plus random noise.\n        Used in the optimize() method after function evaluations.\n\n        This method follows the approach from spotpython.utils.repair.apply_penalty_NA,\n        replacing NaN/inf values with a penalty value plus random noise to avoid\n        identical penalty values.\n\n        Args:\n            y (ndarray): Array of objective function values to be repaired.\n            y_history (ndarray, optional): Historical objective function values used for\n                computing penalty statistics. If None, uses y itself. Default is None.\n            penalty_value (float, optional): Value to replace NaN/inf with.\n                If None, computes penalty as: max(finite_y_history) + 3 * std(finite_y_history).\n                If all values are NaN/inf or only one finite value exists, falls back\n                to self.penalty_val. Default is None.\n            sd (float): Standard deviation for random noise added to penalty.\n                Default is 0.1.\n\n        Returns:\n            ndarray: Array with NaN/inf replaced by penalty_value + random noise.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1), bounds=[(-5, 5)])\n            &gt;&gt;&gt; y_hist = np.array([1.0, 2.0, 3.0, 5.0])\n            &gt;&gt;&gt; y_new = np.array([4.0, np.nan, np.inf])\n            &gt;&gt;&gt; y_clean = opt._apply_penalty_NA(y_new, y_history=y_hist)\n            &gt;&gt;&gt; np.all(np.isfinite(y_clean))\n            True\n            &gt;&gt;&gt; # NaN/inf replaced with worst value from history + 3*std + noise\n            &gt;&gt;&gt; y_clean[1] &gt; 5.0  # Should be larger than max finite value in history\n            True\n        \"\"\"\n\n        # Ensure y is a float array (maps non-convertible values like \"error\" or None to nan)\n        def _safe_float(v):\n            try:\n                return float(v)\n            except (ValueError, TypeError):\n                return np.nan\n\n        y_flat = np.array(y).flatten()\n        y = np.array([_safe_float(v) for v in y_flat])\n        # Identify NaN and inf values in y\n        mask = ~np.isfinite(y)\n\n        if np.any(mask):\n            n_bad = np.sum(mask)\n\n            # Compute penalty_value if not provided\n            if penalty_value is None:\n                # Get finite values from history for statistics\n                # Use y_history if provided, otherwise fall back to y itself\n                if y_history is not None:\n                    finite_values = y_history[np.isfinite(y_history)]\n                else:\n                    # Use current y values\n                    finite_values = y[~mask]\n\n                # If we have at least 2 finite values, compute adaptive penalty\n                if len(finite_values) &gt;= 2:\n                    max_y = np.max(finite_values)\n                    std_y = np.std(finite_values, ddof=1)\n                    penalty_value = max_y + 3.0 * std_y\n\n                    if self.verbose:\n                        print(\n                            f\"Warning: Found {n_bad} NaN/inf value(s), replacing with \"\n                            f\"adaptive penalty (max + 3*std = {penalty_value:.4f})\"\n                        )\n                else:\n                    # Fallback to self.penalty if insufficient finite values\n                    if self.penalty_val is not None:\n                        penalty_value = self.penalty_val\n                    elif len(finite_values) == 1:\n                        # Use the single finite value + a large constant\n                        penalty_value = finite_values[0] + 1000.0\n                    else:\n                        # All values are NaN/inf, use a large default\n                        penalty_value = 1e10\n\n                    if self.verbose:\n                        print(\n                            f\"Warning: Found {n_bad} NaN/inf value(s), insufficient finite values \"\n                            f\"for adaptive penalty. Using penalty_value = {penalty_value}\"\n                        )\n            else:\n                if self.verbose:\n                    print(\n                        f\"Warning: Found {n_bad} NaN/inf value(s), replacing with {penalty_value} + noise\"\n                    )\n\n            # Generate random noise and add to penalty\n            random_noise = self.rng.normal(0, sd, y.shape)\n            penalty_values = penalty_value + random_noise\n\n            # Replace NaN/inf with penalty + noise\n            y[mask] = penalty_values[mask]\n\n        return y\n\n    # ====================\n    # Storage &amp; Statistics\n    # ====================\n\n    def _init_storage(self, X0: np.ndarray, y0: np.ndarray) -&gt; None:\n        \"\"\"Initialize storage for optimization.\n\n        Sets up the initial data structures needed for optimization tracking:\n        - X_: Evaluated design points (in original scale)\n        - y_: Function values at evaluated points\n        - n_iter_: Iteration counter\n\n        Then updates statistics by calling update_stats().\n\n        Args:\n            X0 (ndarray): Initial design points in internal scale, shape (n_samples, n_features).\n            y0 (ndarray): Function values at X0, shape (n_samples,).\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 n_initial=5)\n            &gt;&gt;&gt; X0 = np.array([[1, 2], [3, 4], [0, 1]])\n            &gt;&gt;&gt; y0 = np.array([5.0, 25.0, 1.0])\n            &gt;&gt;&gt; opt._init_storage(X0, y0)\n            &gt;&gt;&gt; opt.X_.shape\n            (3, 2)\n            &gt;&gt;&gt; opt.y_.shape\n            (3,)\n            &gt;&gt;&gt; opt.n_iter_\n            0\n            &gt;&gt;&gt; opt.counter\n            3\n        \"\"\"\n        # Initialize storage (convert to original scale for user-facing storage)\n        self.X_ = self._inverse_transform_X(X0.copy())\n        self.y_ = y0.copy()\n        self.n_iter_ = 0\n\n    def _update_storage(self, X_new: np.ndarray, y_new: np.ndarray) -&gt; None:\n        \"\"\"Update storage with new evaluation points.\n\n        Appends new design points and their function values to the storage arrays.\n        Points are converted from internal scale to original scale before storage.\n\n        Args:\n            X_new (ndarray): New design points in internal scale, shape (n_new, n_features).\n            y_new (ndarray): Function values at X_new, shape (n_new,).\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 n_initial=5)\n            &gt;&gt;&gt; # Initialize with some data\n            &gt;&gt;&gt; opt.X_ = np.array([[1, 2], [3, 4]])\n            &gt;&gt;&gt; opt.y_ = np.array([5.0, 25.0])\n            &gt;&gt;&gt; # Add new points\n            &gt;&gt;&gt; X_new = np.array([[0, 1], [2, 3]])\n            &gt;&gt;&gt; y_new = np.array([1.0, 13.0])\n            &gt;&gt;&gt; opt._update_storage(X_new, y_new)\n            &gt;&gt;&gt; opt.X_.shape\n            (4, 2)\n            &gt;&gt;&gt; opt.y_.shape\n            (4,)\n        \"\"\"\n        # Update storage (convert to original scale for user-facing storage)\n        self.X_ = np.vstack([self.X_, self._inverse_transform_X(X_new)])\n        self.y_ = np.append(self.y_, y_new)\n\n    def update_stats(self) -&gt; None:\n        \"\"\"Update optimization statistics.\n\n        Updates various statistics related to the optimization progress:\n        - `min_y`: Minimum y value found so far\n        - `min_X`: X value corresponding to minimum y\n        - `counter`: Total number of function evaluations\n\n        Note: `success_rate` is updated separately via `_update_success_rate()` method,\n        which is called after each batch of function evaluations.\n\n        If `noise` is True (repeats &gt; 1), additionally computes:\n        1. `mean_X`: Unique design points (aggregated from repeated evaluations)\n        2. `mean_y`: Mean y values per design point\n        3. `var_y`: Variance of y values per design point\n        4. `min_mean_X`: X value of the best mean y value\n        5. `min_mean_y`: Best mean y value\n        6. `min_var_y`: Variance of the best mean y value\n\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; # Without noise\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 max_iter=10, n_initial=5)\n            &gt;&gt;&gt; opt.X_ = np.array([[1, 2], [3, 4], [0, 1]])\n            &gt;&gt;&gt; opt.y_ = np.array([5.0, 25.0, 1.0])\n            &gt;&gt;&gt; opt.update_stats()\n            &gt;&gt;&gt; opt.min_y\n            1.0\n            &gt;&gt;&gt; opt.min_X\n            array([0, 1])\n            &gt;&gt;&gt; opt.counter\n            3\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # With noise\n            &gt;&gt;&gt; opt_noise = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                       bounds=[(-5, 5), (-5, 5)],\n            ...                       n_initial=5,\n            ...                       repeats_initial=2)\n            &gt;&gt;&gt; opt_noise.noise = True\n            &gt;&gt;&gt; opt_noise.X_ = np.array([[1, 2], [1, 2], [3, 4]])\n            &gt;&gt;&gt; opt_noise.y_ = np.array([5.0, 5.0, 25.0])\n            &gt;&gt;&gt; opt_noise.update_stats()\n            &gt;&gt;&gt; opt_noise.mean_y.shape\n            (2,)\n        \"\"\"\n        if self.y_ is None or len(self.y_) == 0:\n            return\n\n        # Basic stats\n        self.min_y = np.min(self.y_)\n        self.min_X = self.X_[np.argmin(self.y_)]\n        self.counter = len(self.y_)\n\n        # Aggregated stats for noisy functions\n        if (self.repeats_initial &gt; 1) or (self.repeats_surrogate &gt; 1):\n            self.mean_X, self.mean_y, self.var_y = self._aggregate_mean_var(\n                self.X_, self.y_\n            )\n            # X value of the best mean y value so far\n            best_mean_idx = np.argmin(self.mean_y)\n            self.min_mean_X = self.mean_X[best_mean_idx]\n            # Best mean y value so far\n            self.min_mean_y = self.mean_y[best_mean_idx]\n            # Variance of the best mean y value so far\n            self.min_var_y = self.var_y[best_mean_idx]\n\n    def _update_success_rate(self, y_new: np.ndarray) -&gt; None:\n        \"\"\"Update the rolling success rate of the optimization process.\n\n        A success is counted only if the new value is better (smaller) than the best\n        found y value so far. The success rate is calculated based on the last\n        `window_size` successes.\n\n        Important: This method should be called BEFORE updating self.y_ to correctly\n        track improvements against the previous best value.\n\n        Args:\n            y_new (ndarray): The new function values to consider for the success rate update.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 max_iter=10, n_initial=5)\n            &gt;&gt;&gt; opt.X_ = np.array([[1, 2], [3, 4], [0, 1]])\n            &gt;&gt;&gt; opt.y_ = np.array([5.0, 3.0, 2.0])\n            &gt;&gt;&gt; opt._update_success_rate(np.array([1.5, 2.5]))\n            &gt;&gt;&gt; opt.success_rate &gt; 0\n            True\n        \"\"\"\n        # Initialize or update the rolling history of successes (1 for success, 0 for failure)\n        if not hasattr(self, \"_success_history\") or self._success_history is None:\n            self._success_history = []\n\n        # Get the best y value so far (before adding new evaluations)\n        # Since this is called BEFORE updating self.y_, we can safely use min(self.y_)\n        if self.y_ is not None and len(self.y_) &gt; 0:\n            best_y_before = min(self.y_)\n        else:\n            # This is the initial design, no previous best\n            best_y_before = float(\"inf\")\n\n        successes = []\n        current_best = best_y_before\n\n        for val in y_new:\n            if val &lt; current_best:\n                successes.append(1)\n                current_best = val  # Update for next comparison within this batch\n            else:\n                successes.append(0)\n\n        # Add new successes to the history\n        self._success_history.extend(successes)\n        # Keep only the last window_size successes\n        self._success_history = self._success_history[-self.window_size :]\n\n        # Calculate the rolling success rate\n        window_size = len(self._success_history)\n        num_successes = sum(self._success_history)\n        self.success_rate = num_successes / window_size if window_size &gt; 0 else 0.0\n\n    def _get_success_rate(self) -&gt; float:\n        \"\"\"Get the current success rate of the optimization process.\n\n        Returns:\n            float: The current success rate.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda x: x,\n            ...                 bounds=[(-5, 5), (-5, 5)])\n            &gt;&gt;&gt; print(opt._get_success_rate())\n            0.0\n        \"\"\"\n        return float(getattr(self, \"success_rate\", 0.0) or 0.0)\n\n    def _aggregate_mean_var(\n        self, X: np.ndarray, y: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Aggregate X and y values to compute mean and variance per group.\n\n        For repeated evaluations at the same design point, this method computes\n        the mean function value and variance (using population variance, ddof=0).\n\n        Args:\n            X (ndarray): Design points, shape (n_samples, n_features).\n            y (ndarray): Function values, shape (n_samples,).\n\n        Returns:\n            tuple: A tuple containing:\n                - X_agg (ndarray): Unique design points, shape (n_groups, n_features)\n                - y_mean (ndarray): Mean y values per group, shape (n_groups,)\n                - y_var (ndarray): Variance of y values per group, shape (n_groups,)\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n            ...                 bounds=[(-5, 5), (-5, 5)],\n            ...                 repeats_initial=2)\n            &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [1, 2]])\n            &gt;&gt;&gt; y = np.array([1, 2, 3])\n            &gt;&gt;&gt; X_agg, y_mean, y_var = opt._aggregate_mean_var(X, y)\n            &gt;&gt;&gt; X_agg.shape\n            (2, 2)\n            &gt;&gt;&gt; y_mean\n            array([2., 2.])\n            &gt;&gt;&gt; y_var\n            array([1., 0.])\n        \"\"\"\n        # Input validation\n        X = np.asarray(X)\n        y = np.asarray(y)\n\n        if X.ndim != 2 or y.ndim != 1 or X.shape[0] != y.shape[0]:\n            raise ValueError(\"Invalid input shapes for _aggregate_mean_var\")\n\n        if X.shape[0] == 0:\n            return np.empty((0, X.shape[1])), np.array([]), np.array([])\n\n        # Find unique rows and group indices\n        _, unique_idx, inverse_idx = np.unique(\n            X, axis=0, return_index=True, return_inverse=True\n        )\n\n        X_agg = X[unique_idx]\n\n        # Calculate mean and variance for each group\n        n_groups = len(unique_idx)\n        y_mean = np.zeros(n_groups)\n        y_var = np.zeros(n_groups)\n\n        for i in range(n_groups):\n            group_mask = inverse_idx == i\n            group_y = y[group_mask]\n            y_mean[i] = np.mean(group_y)\n            # Use population variance (ddof=0) for consistency with Spot\n            y_var[i] = np.var(group_y, ddof=0)\n\n        return X_agg, y_mean, y_var\n\n    # ====================\n    # Results &amp; Analysis\n    # ====================\n\n    def save_result(\n        self,\n        filename: Optional[str] = None,\n        prefix: str = \"result\",\n        path: Optional[str] = None,\n        overwrite: bool = True,\n        verbosity: int = 0,\n    ) -&gt; None:\n        \"\"\"Save the complete optimization results to a pickle file.\n\n        A result contains all information from a completed optimization run, including\n        the experiment configuration and all evaluation results. This is useful for\n        saving completed runs for later analysis.\n\n        The result includes everything in an experiment plus:\n        - All evaluated points (X_)\n        - All function values (y_)\n        - Best point and best value\n        - Iteration count\n        - Success rate statistics\n        - Noise statistics (if applicable)\n\n        Args:\n            filename (str, optional): Filename for the result file. If None, generates\n                from prefix. Defaults to None.\n            prefix (str): Prefix for auto-generated filename. Defaults to \"result\".\n            path (str, optional): Directory path to save the file. If None, saves in current\n                directory. Creates directory if it doesn't exist. Defaults to None.\n            overwrite (bool): If True, overwrites existing file. If False, raises error if\n                file exists. Defaults to True.\n            verbosity (int): Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Run optimization\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=30,\n            ...     n_initial=10,\n            ...     seed=42\n            ... )\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Save complete results\n            &gt;&gt;&gt; opt.save_result(prefix=\"sphere_opt\")\n            Result saved to sphere_opt_res.pkl\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Later: load and analyze\n            &gt;&gt;&gt; # opt_loaded = SpotOptim.load_result(\"sphere_opt_res.pkl\")\n            &gt;&gt;&gt; # print(\"Best value:\", opt_loaded.best_y_)\n            &gt;&gt;&gt; # opt_loaded.plot_surrogate()\n        \"\"\"\n        # Use save_experiment with file_io unpickleables to preserve results\n        if filename is None:\n            filename = self._get_result_filename(prefix)\n\n        self.save_experiment(\n            filename=filename,\n            path=path,\n            overwrite=overwrite,\n            unpickleables=\"file_io\",\n            verbosity=verbosity,\n        )\n\n        # Update message\n        if path is not None:\n            full_path = os.path.join(path, filename)\n        else:\n            full_path = filename\n        print(f\"Result saved to {full_path}\")\n\n    @staticmethod\n    def load_result(filename: str) -&gt; \"SpotOptim\":\n        \"\"\"Load complete optimization results from a pickle file.\n\n        Loads results that were saved with save_result(). The loaded optimizer\n        will have both configuration and all optimization results.\n\n        Args:\n            filename (str): Path to the result pickle file.\n\n        Returns:\n            SpotOptim: Loaded optimizer instance with complete results.\n\n        Raises:\n            FileNotFoundError: If the specified file doesn't exist.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Load results\n            &gt;&gt;&gt; opt = SpotOptim.load_result(\"sphere_opt_res.pkl\")\n            Loaded result from sphere_opt_res.pkl\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Analyze results\n            &gt;&gt;&gt; print(\"Best point:\", opt.best_x_)\n            &gt;&gt;&gt; print(\"Best value:\", opt.best_y_)\n            &gt;&gt;&gt; print(\"Total evaluations:\", opt.counter)\n            &gt;&gt;&gt; print(\"Success rate:\", opt.success_rate)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Continue optimization if needed\n            &gt;&gt;&gt; # opt.fun = lambda X: np.sum(X**2, axis=1)  # Re-attach if continuing\n            &gt;&gt;&gt; # opt.max_iter = 50  # Increase budget\n            &gt;&gt;&gt; # result = opt.optimize()\n        \"\"\"\n        if not os.path.exists(filename):\n            raise FileNotFoundError(f\"Result file not found: {filename}\")\n\n        try:\n            with open(filename, \"rb\") as handle:\n                optimizer = dill.load(handle)\n            print(f\"Loaded result from {filename}\")\n\n            # Reinitialize components that were excluded\n            optimizer._reinitialize_components()\n\n            return optimizer\n        except Exception as e:\n            print(f\"Error loading result: {e}\")\n            raise\n\n    def save_experiment(\n        self,\n        filename: Optional[str] = None,\n        prefix: str = \"experiment\",\n        path: Optional[str] = None,\n        overwrite: bool = True,\n        unpickleables: str = \"all\",\n        verbosity: int = 0,\n    ) -&gt; None:\n        \"\"\"Save the experiment configuration to a pickle file.\n\n        An experiment contains the optimizer configuration needed to run optimization,\n        but excludes the results. This is useful for defining experiments locally and\n        executing them on remote machines.\n\n        The experiment includes:\n        - Bounds, variable types, variable names\n        - Optimization parameters (max_iter, n_initial, etc.)\n        - Surrogate and acquisition settings\n        - Random seed\n\n        The experiment excludes:\n        - Function evaluations (X_, y_)\n        - Optimization results\n\n\n        Args:\n            filename (str, optional): Filename for the experiment file. If None, generates\n                from prefix. Defaults to None.\n            prefix (str): Prefix for auto-generated filename. Defaults to \"experiment\".\n            path (str, optional): Directory path to save the file. If None, saves in current\n                directory. Creates directory if it doesn't exist. Defaults to None.\n            overwrite (bool): If True, overwrites existing file. If False, raises error if\n                file exists. Defaults to True.\n            unpickleables (str): Components to exclude for pickling:\n                - \"all\": Excludes surrogate, lhs_sampler, tb_writer (experiment only)\n                - \"file_io\": Excludes only tb_writer (lighter exclusion)\n                Defaults to \"all\".\n            verbosity (int): Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Define experiment locally\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     max_iter=30,\n            ...     n_initial=10,\n            ...     seed=42\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Save experiment (without results)\n            &gt;&gt;&gt; opt.save_experiment(prefix=\"sphere_opt\")\n            Experiment saved to sphere_opt_exp.pkl\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # On remote machine: load and run\n            &gt;&gt;&gt; # opt_remote = SpotOptim.load_experiment(\"sphere_opt_exp.pkl\")\n            &gt;&gt;&gt; # result = opt_remote.optimize()\n            &gt;&gt;&gt; # opt_remote.save_result(prefix=\"sphere_opt\")  # Save results\n        \"\"\"\n        # Close TensorBoard writer before pickling\n        self._close_and_del_tensorboard_writer()\n\n        # Create pickle-safe copy\n        optimizer_copy = self._get_pickle_safe_optimizer(\n            unpickleables=unpickleables, verbosity=verbosity\n        )\n\n        # Determine filename\n        if filename is None:\n            filename = self._get_experiment_filename(prefix)\n\n        # Add path if provided\n        if path is not None:\n            if not os.path.exists(path):\n                os.makedirs(path)\n            filename = os.path.join(path, filename)\n\n        # Check for existing file\n        if os.path.exists(filename) and not overwrite:\n            raise FileExistsError(\n                f\"File {filename} already exists. Use overwrite=True to overwrite.\"\n            )\n\n        # Save to pickle file\n        try:\n            with open(filename, \"wb\") as handle:\n                dill.dump(optimizer_copy, handle, protocol=dill.HIGHEST_PROTOCOL)\n            print(f\"Experiment saved to {filename}\")\n        except Exception as e:\n            print(f\"Error during pickling: {e}\")\n            raise\n\n    @staticmethod\n    def load_experiment(filename: str) -&gt; \"SpotOptim\":\n        \"\"\"Load an experiment configuration from a pickle file.\n\n        Loads an experiment that was saved with save_experiment(). The loaded optimizer\n        will have the configuration and the objective function (thanks to dill).\n\n\n        Args:\n            filename (str): Path to the experiment pickle file.\n\n        Returns:\n            SpotOptim: Loaded optimizer instance (without fun attached).\n\n        Raises:\n            FileNotFoundError: If the specified file doesn't exist.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Load experiment\n            &gt;&gt;&gt; opt = SpotOptim.load_experiment(\"sphere_opt_exp.pkl\")\n            Loaded experiment from sphere_opt_exp.pkl\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Re-attach objective function\n            &gt;&gt;&gt; opt.fun = lambda X: np.sum(X**2, axis=1)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Run optimization\n            &gt;&gt;&gt; result = opt.optimize()\n        \"\"\"\n        if not os.path.exists(filename):\n            raise FileNotFoundError(f\"Experiment file not found: {filename}\")\n\n        try:\n            with open(filename, \"rb\") as handle:\n                optimizer = dill.load(handle)\n            print(f\"Loaded experiment from {filename}\")\n\n            # Reinitialize components that were excluded\n            optimizer._reinitialize_components()\n\n            return optimizer\n        except Exception as e:\n            print(f\"Error loading experiment: {e}\")\n            raise\n\n    def _get_result_filename(self, prefix: str) -&gt; str:\n        \"\"\"Generate result filename from prefix.\n\n        Args:\n            prefix (str): Prefix for the filename.\n\n        Returns:\n            str: Filename with '_res.pkl' suffix.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda x: x, bounds=[(0, 1)])\n            &gt;&gt;&gt; res_filename = opt._get_result_filename(prefix=\"my_experiment\")\n            &gt;&gt;&gt; print(res_filename)\n            my_experiment_res.pkl\n        \"\"\"\n        if prefix is None:\n            return \"result_res.pkl\"\n        return f\"{prefix}_res.pkl\"\n\n    def _get_experiment_filename(self, prefix: str) -&gt; str:\n        \"\"\"Generate experiment filename from prefix.\n\n        Args:\n            prefix (str): Prefix for the filename.\n\n        Returns:\n            str: Filename with '_exp.pkl' suffix.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda x: x, bounds=[(0, 1)])\n            &gt;&gt;&gt; exp_filename = opt._get_experiment_filename(prefix=\"my_experiment\")\n            &gt;&gt;&gt; print(exp_filename)\n            my_experiment_exp.pkl\n        \"\"\"\n        if prefix is None:\n            return \"experiment_exp.pkl\"\n        return f\"{prefix}_exp.pkl\"\n\n    def print_results(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Alias for print_results_table for compatibility.\n        Prints the table.\n        \"\"\"\n        self.print_results_table(*args, **kwargs)\n\n    def print_best(\n        self,\n        result: Optional[OptimizeResult] = None,\n        transformations: Optional[List[Optional[Callable]]] = None,\n        show_name: bool = True,\n        precision: int = 4,\n    ) -&gt; None:\n        \"\"\"Print the best solution found during optimization.\n\n        This method displays the best hyperparameters and objective value in a\n        formatted table. It supports custom transformations for parameters\n        (e.g., converting log-scale values back to original scale).\n\n        Args:\n            result (OptimizeResult, optional): Optimization result object from optimize().\n                If None, uses the stored best values from the optimizer. Defaults to None.\n            transformations (list of callable, optional): List of transformation functions\n                to apply to each parameter. Each function takes a single value and returns\n                the transformed value. Use None for parameters that don't need transformation.\n                Length must match number of dimensions. Example: [None, None, lambda x: 10**x]\n                to convert the 3rd parameter from log10 scale. Defaults to None.\n            show_name (bool, optional): Whether to display variable names. If False,\n                uses generic names like 'x0', 'x1', etc. Defaults to True.\n            precision (int, optional): Number of decimal places for floating point values.\n                Defaults to 4.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 1: Basic usage\n            &gt;&gt;&gt; def sphere(X):\n            ...     return np.sum(X**2, axis=1)\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=sphere,\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     var_name=[\"x1\", \"x2\"],\n            ...     max_iter=20,\n            ...     n_initial=10\n            ... )\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt; opt.print_best(result)\n            &lt;BLANKLINE&gt;\n            Best Solution Found:\n            --------------------------------------------------\n              x1: 0.0123\n              x2: -0.0045\n              Objective Value: 0.000173\n              Total Evaluations: 20\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 2: With log-scale transformations (e.g., for learning rates)\n            &gt;&gt;&gt; def objective(X):\n            ...     # X[:, 0]: neurons (int), X[:, 1]: layers (int),\n            ...     # X[:, 2]: log10(lr), X[:, 3]: log10(alpha)\n            ...     return np.sum(X**2, axis=1)  # Placeholder\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=objective,\n            ...     bounds=[(16, 128), (1, 4), (-3, 0), (-2, 1)],\n            ...     var_type=[\"int\", \"int\", \"float\", \"float\"],\n            ...     var_name=[\"neurons\", \"layers\", \"log10_lr\", \"log10_alpha\"],\n            ...     max_iter=30,\n            ...     n_initial=10\n            ... )\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt; # Transform log-scale parameters back to original scale\n            &gt;&gt;&gt; transformations = [\n            ...     int,              # neurons -&gt; int\n            ...     int,              # layers -&gt; int\n            ...     lambda x: 10**x,  # log10_lr -&gt; lr\n            ...     lambda x: 10**x   # log10_alpha -&gt; alpha\n            ... ]\n            &gt;&gt;&gt; opt.print_best(result, transformations=transformations)\n            &lt;BLANKLINE&gt;\n            Best Solution Found:\n            --------------------------------------------------\n              neurons: 64\n              layers: 2\n              log10_lr: 0.0012\n              log10_alpha: 0.0345\n              Objective Value: 1.2345\n              Total Evaluations: 30\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 3: Without result object (using stored values)\n            &gt;&gt;&gt; opt.print_best()  # Uses opt.best_x_ and opt.best_y_\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 4: Hide variable names\n            &gt;&gt;&gt; opt.print_best(result, show_name=False)\n            &lt;BLANKLINE&gt;\n            Best Solution Found:\n            --------------------------------------------------\n              x0: 0.0123\n              x1: -0.0045\n              Objective Value: 0.000173\n              Total Evaluations: 20\n        \"\"\"\n        # Get values from result or stored attributes\n        if result is not None:\n            best_x = result.x\n            best_y = result.fun\n            n_evals = result.nfev\n        else:\n            if self.best_x_ is None or self.best_y_ is None:\n                print(\"No optimization results available. Run optimize() first.\")\n                return\n            best_x = self.best_x_\n            best_y = self.best_y_\n            n_evals = self.counter\n\n        # Expand to full dimensions if dimension reduction was applied\n        if self.red_dim:\n            best_x_full = self.to_all_dim(best_x.reshape(1, -1))[0]\n        else:\n            best_x_full = best_x\n\n        # Map factor variables back to original string values\n        best_x_full = self._map_to_factor_values(best_x_full.reshape(1, -1))[0]\n\n        # Determine variable names to use\n        if show_name and self.all_var_name is not None:\n            var_names = self.all_var_name\n        else:\n            var_names = [f\"x{i}\" for i in range(len(best_x_full))]\n\n        # Validate transformations length\n        if transformations is not None:\n            if len(transformations) != len(best_x_full):\n                raise ValueError(\n                    f\"Length of transformations ({len(transformations)}) must match \"\n                    f\"number of dimensions ({len(best_x_full)})\"\n                )\n        else:\n            transformations = [None] * len(best_x_full)\n\n        # Print header\n        print(\"\\nBest Solution Found:\")\n        print(\"-\" * 50)\n\n        # Print each parameter\n        for i, (name, value, transform) in enumerate(\n            zip(var_names, best_x_full, transformations)\n        ):\n            # Apply transformation if provided\n            if transform is not None:\n                try:\n                    display_value = transform(value)\n                except Exception as e:\n                    print(f\"Warning: Transformation failed for {name}: {e}\")\n                    display_value = value\n            else:\n                display_value = value\n\n            # Format based on variable type\n            var_type = self.all_var_type[i] if i &lt; len(self.all_var_type) else \"float\"\n\n            if var_type == \"int\" or isinstance(display_value, (int, np.integer)):\n                print(f\"  {name}: {int(display_value)}\")\n            elif var_type == \"factor\" or isinstance(display_value, str):\n                print(f\"  {name}: {display_value}\")\n            else:\n                print(f\"  {name}: {display_value:.{precision}f}\")\n\n        # Print objective value and evaluations\n        print(f\"  Objective Value: {best_y:.{precision}f}\")\n        print(f\"  Total Evaluations: {n_evals}\")\n\n    def print_results_table(\n        self,\n        tablefmt: str = \"github\",\n        precision: int = 4,\n        show_importance: bool = False,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Print (and return) a comprehensive table of optimization results.\n\n        This method calls `get_results_table` to generate the table string, prints it,\n        and then returns it.\n\n        Args:\n            tablefmt (str, optional): Table format. Defaults to 'github'.\n            precision (int, optional): Decimal precision. Defaults to 4.\n            show_importance (bool, optional): Show importance column. Defaults to False.\n            *args: Arguments passed to get_results_table.\n            **kwargs: Keyword arguments passed to get_results_table.\n\n        Returns:\n            str: Formatted table string.\n        \"\"\"\n        table = self.get_results_table(\n            tablefmt=tablefmt,\n            precision=precision,\n            show_importance=show_importance,\n            *args,\n            **kwargs,\n        )\n        print(table)\n        return table\n\n    def print_design_table(\n        self,\n        tablefmt: str = \"github\",\n        precision: int = 4,\n    ) -&gt; str:\n        \"\"\"Print (and return) a table showing the search space design before optimization.\n\n        This method calls `get_design_table` to generate the table string, prints it,\n        and then returns it.\n\n        Args:\n            tablefmt (str, optional): Table format for tabulate library.\n                Defaults to 'github'.\n            precision (int, optional): Number of decimal places for float values.\n                Defaults to 4.\n\n        Returns:\n            str: Formatted table string.\n        \"\"\"\n        table = self.get_design_table(tablefmt=tablefmt, precision=precision)\n        print(table)\n        return table\n\n    def get_results_table(\n        self,\n        tablefmt: str = \"github\",\n        precision: int = 4,\n        show_importance: bool = False,\n    ) -&gt; str:\n        \"\"\"Get a comprehensive table string of optimization results.\n\n        This method generates a formatted table of the search space configuration,\n        best values found, and optionally variable importance scores.\n\n        Args:\n            tablefmt (str, optional): Table format for tabulate library. Options include:\n                'github', 'grid', 'simple', 'plain', 'html', 'latex', etc.\n                Defaults to 'github'.\n            precision (int, optional): Number of decimal places for float values.\n                Defaults to 4.\n            show_importance (bool, optional): Whether to include importance scores.\n                Importance is calculated as the normalized standard deviation of each\n                parameter's effect on the objective. Requires multiple evaluations.\n                Defaults to False.\n\n        Returns:\n            str: Formatted table string that can be printed or saved.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 1: Basic usage after optimization\n            &gt;&gt;&gt; def sphere(X):\n            ...     return np.sum(X**2, axis=1)\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=sphere,\n            ...     bounds=[(-5, 5), (-5, 5), (-5, 5)],\n            ...     var_name=[\"x1\", \"x2\", \"x3\"],\n            ...     var_type=[\"float\", \"float\", \"float\"],\n            ...     max_iter=30,\n            ...     n_initial=10\n            ... )\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt; table = opt.get_results_table()\n            &gt;&gt;&gt; print(table)\n            | name   | type   |   lower |   upper |   tuned |\n            |--------|--------|---------|---------|---------|\n            | x1     | num    |    -5.0 |     5.0 |  0.0123 |\n            | x2     | num    |    -5.0 |     5.0 | -0.0234 |\n            | x3     | num    |    -5.0 |     5.0 |  0.0345 |\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 2: With importance scores\n            &gt;&gt;&gt; table = opt.get_results_table(show_importance=True)\n            &gt;&gt;&gt; print(table)\n            | name   | type   |   lower |   upper |   tuned |   importance | stars   |\n            |--------|--------|---------|---------|---------|--------------|---------|\n            | x1     | num    |    -5.0 |     5.0 |  0.0123 |        45.23 | **      |\n            | x2     | num    |    -5.0 |     5.0 | -0.0234 |        32.17 | *       |\n            | x3     | num    |    -5.0 |     5.0 |  0.0345 |        22.60 | *       |\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 3: Different table format\n            &gt;&gt;&gt; table = opt.get_results_table(tablefmt=\"grid\")\n            &gt;&gt;&gt; print(table)\n            +--------+--------+---------+---------+---------+\n            | name   | type   |   lower |   upper |   tuned |\n            +========+========+=========+=========+=========+\n            | x1     | num    |    -5.0 |     5.0 |  0.0123 |\n            +--------+--------+---------+---------+---------+\n            ...\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 4: With factor variables\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (\"red\", \"green\", \"blue\")],\n            ...     var_name=[\"size\", \"color\"],\n            ...     var_type=[\"float\", \"factor\"],\n            ...     max_iter=20,\n            ...     n_initial=10\n            ... )\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt; table = opt.get_results_table()\n            &gt;&gt;&gt; print(table)\n            | name   | type   | lower   | upper   | tuned   |\n            |--------|--------|---------|---------|---------|\n            | size   | num    | -5.0    | 5.0     | 0.0123  |\n            | color  | factor | red     | blue    | green   |\n        \"\"\"\n        if self.best_x_ is None or self.best_y_ is None:\n            return \"No optimization results available. Run optimize() first.\"\n\n        # Get best solution in full dimensions\n        # Note: best_x_ is already in original scale\n        if self.red_dim:\n            best_x_full = self.to_all_dim(self.best_x_.reshape(1, -1))[0]\n        else:\n            best_x_full = self.best_x_\n\n        # Map factor variables back to original string values\n        best_x_display = self._map_to_factor_values(best_x_full.reshape(1, -1))[0]\n\n        # Prepare all variable transformations (use all_var_trans if dimension reduction occurred)\n        if self.red_dim and hasattr(self, \"all_var_trans\"):\n            all_var_trans = self.all_var_trans\n        else:\n            all_var_trans = self.var_trans\n\n        # Prepare table data\n        table_data = {\n            \"name\": (\n                self.all_var_name\n                if self.all_var_name\n                else [f\"x{i}\" for i in range(len(best_x_display))]\n            ),\n            \"type\": (\n                self.all_var_type\n                if self.all_var_type\n                else [\"float\"] * len(best_x_display)\n            ),\n            \"default\": [],\n            \"lower\": [],\n            \"upper\": [],\n            \"tuned\": [],\n            \"transform\": [t if t is not None else \"-\" for t in all_var_trans],\n        }\n\n        # Helper to format values\n        def fmt_val(v):\n            if isinstance(v, (float, np.floating)):\n                return f\"{v:.{precision}f}\"\n            return v\n\n        # Process bounds, defaults, and tuned values\n        for i in range(len(best_x_display)):\n            var_type = table_data[\"type\"][i]\n\n            # Handle bounds and defaults based on variable type\n            if var_type == \"factor\":\n                # For factors, show original string values\n                if i in self._factor_maps:\n                    factor_map = self._factor_maps[i]\n                    # Default is middle level logic (matching get_design_table)\n                    mid_idx = len(factor_map) // 2\n                    default_str = factor_map[mid_idx]\n\n                    table_data[\"lower\"].append(\"-\")\n                    table_data[\"upper\"].append(\"-\")\n                    table_data[\"default\"].append(default_str)\n                else:\n                    table_data[\"lower\"].append(\"-\")\n                    table_data[\"upper\"].append(\"-\")\n                    table_data[\"default\"].append(\"N/A\")\n            else:\n                table_data[\"lower\"].append(fmt_val(self._original_lower[i]))\n                table_data[\"upper\"].append(fmt_val(self._original_upper[i]))\n                # Default is midpoint logic\n                default_val = (self._original_lower[i] + self._original_upper[i]) / 2\n                if var_type == \"int\":\n                    table_data[\"default\"].append(int(default_val))\n                else:\n                    table_data[\"default\"].append(fmt_val(default_val))\n\n            # Format tuned value\n            tuned_val = best_x_display[i]\n            if var_type == \"int\":\n                table_data[\"tuned\"].append(int(tuned_val))\n            elif var_type == \"factor\":\n                table_data[\"tuned\"].append(str(tuned_val))\n            else:\n                table_data[\"tuned\"].append(fmt_val(tuned_val))\n\n        # Add importance if requested\n        if show_importance:\n            importance = self.get_importance()\n            table_data[\"importance\"] = [f\"{x:.2f}\" for x in importance]\n            table_data[\"stars\"] = self.get_stars(importance)\n\n        # Generate table\n        table = tabulate(\n            table_data,\n            headers=\"keys\",\n            tablefmt=tablefmt,\n            numalign=\"right\",\n            stralign=\"right\",\n        )\n\n        # Add interpretation if importance is shown\n        if show_importance:\n            table += \"\\n\\nInterpretation: ***: &gt;99%, **: &gt;75%, *: &gt;50%, .: &gt;10%\"\n\n        return table\n\n    def get_design_table(\n        self,\n        tablefmt: str = \"github\",\n        precision: int = 4,\n    ) -&gt; str:\n        \"\"\"Get a table string showing the search space design before optimization.\n\n        This method generates a table displaying the variable names, types, bounds,\n        and defaults without requiring an optimization run. Useful for inspecting\n        and documenting the search space configuration.\n\n        Args:\n            tablefmt (str, optional): Table format for tabulate library.\n                Defaults to 'github'.\n            precision (int, optional): Number of decimal places for float values.\n                Defaults to 4.\n\n        Returns:\n            str: Formatted table string.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 1: Numeric parameters\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-10, 10), (0, 1)],\n            ...     var_name=[\"x1\", \"x2\", \"x3\"],\n            ...     var_type=[\"float\", \"int\", \"float\"],\n            ...     max_iter=20,\n            ...     n_initial=10\n            ... )\n            &gt;&gt;&gt; table = opt.get_design_table()\n            &gt;&gt;&gt; print(table)\n            | name   | type   |   lower |   upper |   default |\n            |--------|--------|---------|---------|-----------|\n            | x1     | num    |    -5.0 |     5.0 |       0.0 |\n            | x2     | int    |   -10.0 |    10.0 |       0.0 |\n            | x3     | num    |     0.0 |     1.0 |       0.5 |\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 2: With factor variables\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(10, 100), (\"SGD\", \"Adam\", \"RMSprop\"), (0.001, 0.1)],\n            ...     var_name=[\"neurons\", \"optimizer\", \"lr\"],\n            ...     var_type=[\"int\", \"factor\", \"float\"],\n            ...     max_iter=30,\n            ...     n_initial=10\n            ... )\n            &gt;&gt;&gt; table = opt.get_design_table()\n            &gt;&gt;&gt; print(table)\n            | name      | type   | lower   | upper   | default   |\n            |-----------|--------|---------|---------|-----------|\n            | neurons   | int    | 10.0    | 100.0   | 55.0      |\n            | optimizer | factor | SGD     | RMSprop | Adam      |\n            | lr        | num    | 0.001   | 0.1     | 0.0505    |\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 3: Before running optimization\n            &gt;&gt;&gt; def hyperparameter_objective(X):\n            ...     # X[:, 0]: layers, X[:, 1]: neurons, X[:, 2]: dropout\n            ...     return np.sum(X**2, axis=1)  # Placeholder\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=hyperparameter_objective,\n            ...     bounds=[(1, 5), (16, 256), (0.0, 0.5)],\n            ...     var_name=[\"layers\", \"neurons\", \"dropout\"],\n            ...     var_type=[\"int\", \"int\", \"float\"],\n            ...     max_iter=50,\n            ...     n_initial=15\n            ... )\n            &gt;&gt;&gt; # Get design table before optimization\n            &gt;&gt;&gt; print(\"Search Space Configuration:\")\n            &gt;&gt;&gt; table = opt.get_design_table()\n            &gt;&gt;&gt; print(table)\n            Search Space Configuration:\n            | name    | type   |   lower |   upper |   default |\n            |---------|--------|---------|---------|-----------|\n            | layers  | int    |     1.0 |     5.0 |       3.0 |\n            | neurons | int    |    16.0 |   256.0 |     136.0 |\n            | dropout | num    |     0.0 |     0.5 |      0.25 |\n        \"\"\"\n        # Prepare all variable transformations (use all_var_trans if dimension reduction occurred)\n        if self.red_dim and hasattr(self, \"all_var_trans\"):\n            all_var_trans = self.all_var_trans\n        else:\n            all_var_trans = self.var_trans\n\n        # Prepare table data\n        table_data = {\n            \"name\": (\n                self.all_var_name\n                if self.all_var_name\n                else [f\"x{i}\" for i in range(len(self.all_lower))]\n            ),\n            \"type\": (\n                self.all_var_type\n                if self.all_var_type\n                else [\"float\"] * len(self.all_lower)\n            ),\n            \"lower\": [],\n            \"upper\": [],\n            \"default\": [],\n            \"transform\": [t if t is not None else \"-\" for t in all_var_trans],\n        }\n\n        # Helper to format values\n        def fmt_val(v):\n            if isinstance(v, (float, np.floating)):\n                return f\"{v:.{precision}f}\"\n            return v\n\n        # Process bounds and compute defaults (use original bounds for display)\n        for i in range(len(self._original_lower)):\n            var_type = table_data[\"type\"][i]\n\n            if var_type == \"factor\":\n                # For factors, show original string values\n                if i in self._factor_maps:\n                    factor_map = self._factor_maps[i]\n                    # Default is middle level\n                    mid_idx = len(factor_map) // 2\n                    default_str = factor_map[mid_idx]\n                    table_data[\"lower\"].append(\"-\")\n                    table_data[\"upper\"].append(\"-\")\n                    table_data[\"default\"].append(default_str)\n                else:\n                    table_data[\"lower\"].append(\"-\")\n                    table_data[\"upper\"].append(\"-\")\n                    table_data[\"default\"].append(\"N/A\")\n            else:\n                table_data[\"lower\"].append(fmt_val(self._original_lower[i]))\n                table_data[\"upper\"].append(fmt_val(self._original_upper[i]))\n                # Default is midpoint\n                default_val = (self._original_lower[i] + self._original_upper[i]) / 2\n                if var_type == \"int\":\n                    table_data[\"default\"].append(int(default_val))\n                else:\n                    table_data[\"default\"].append(fmt_val(default_val))\n\n        # Generate table\n        table = tabulate(\n            table_data,\n            headers=\"keys\",\n            tablefmt=tablefmt,\n            numalign=\"right\",\n            stralign=\"right\",\n        )\n\n        return table\n\n    def gen_design_table(self, precision: int = 4, tablefmt: str = \"github\") -&gt; str:\n        \"\"\"Generate a table of the design or results.\n\n        If optimization has been run (results available), returns the results table.\n        Otherwise, returns the design table (search space configuration).\n\n        Args:\n            tablefmt (str, optional): Table format. Defaults to 'github'.\n            precision (int, optional): Number of decimal places for float values.\n                Defaults to 4.\n\n        Returns:\n            str: Formatted table string.\n        \"\"\"\n        if self.best_x_ is not None:\n            return self.get_results_table(precision=precision, tablefmt=tablefmt)\n        else:\n            return self.get_design_table(precision=precision, tablefmt=tablefmt)\n\n    def get_importance(self) -&gt; List[float]:\n        \"\"\"Calculate variable importance scores.\n\n        Importance is computed as the normalized sensitivity of each parameter\n        based on the variation in objective values across the evaluated points.\n        Higher scores indicate parameters that have more influence on the objective.\n\n        The importance is calculated as:\n        1. For each dimension, compute the correlation between parameter values\n           and objective values\n        2. Normalize to percentage scale (0-100)\n        3. Higher values indicate more important parameters\n\n        Returns:\n            List[float]: Importance scores for each dimension (0-100 scale).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 1: Identify important parameters\n            &gt;&gt;&gt; def test_func(X):\n            ...     # x0 has strong effect, x1 has weak effect\n            ...     return 10 * X[:, 0]**2 + 0.1 * X[:, 1]**2\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=test_func,\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     var_name=[\"x0\", \"x1\"],\n            ...     max_iter=30,\n            ...     n_initial=10,\n            ...     seed=42\n            ... )\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt; importance = opt.get_importance()\n            &gt;&gt;&gt; print(f\"x0 importance: {importance[0]:.2f}\")\n            &gt;&gt;&gt; print(f\"x1 importance: {importance[1]:.2f}\")\n            x0 importance: 89.23\n            x1 importance: 10.77\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 2: With more dimensions\n            &gt;&gt;&gt; def rosenbrock(X):\n            ...     return np.sum(100*(X[:, 1:] - X[:, :-1]**2)**2 + (1 - X[:, :-1])**2, axis=1)\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=rosenbrock,\n            ...     bounds=[(-2, 2)] * 4,\n            ...     var_name=[\"x0\", \"x1\", \"x2\", \"x3\"],\n            ...     max_iter=50,\n            ...     n_initial=20,\n            ...     seed=42\n            ... )\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt; importance = opt.get_importance()\n            &gt;&gt;&gt; for i, imp in enumerate(importance):\n            ...     print(f\"x{i}: {imp:.2f}%\")\n            x0: 32.15%\n            x1: 28.43%\n            x2: 25.67%\n            x3: 13.75%\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Example 3: Use in results table\n            &gt;&gt;&gt; table = opt.print_results_table(show_importance=True)\n            &gt;&gt;&gt; print(table)\n        \"\"\"\n        if self.X_ is None or self.y_ is None or len(self.y_) &lt; 3:\n            # Not enough data to compute importance\n            return [0.0] * len(self.all_lower)\n\n        # Use full-dimensional data\n        X_full = self.X_\n        if self.red_dim:\n            X_full = np.array([self.to_all_dim(x.reshape(1, -1))[0] for x in self.X_])\n\n        # Calculate sensitivity for each dimension\n        sensitivities = []\n        for i in range(X_full.shape[1]):\n            x_i = X_full[:, i]\n\n            # Handle factor variables: map strings to integers\n            if hasattr(self, \"_factor_maps\") and i in self._factor_maps:\n                # _factor_maps[i] is {int: str}, we need {str: int}\n                str_to_int = {v: k for k, v in self._factor_maps[i].items()}\n                try:\n                    # Map values, handle potential missing values if any (though shouldn't simplify be there)\n                    x_i = np.array([str_to_int.get(val, -1) for val in x_i])\n                except Exception:\n                    # Fallback if mapping fails\n                    sensitivities.append(0.0)\n                    continue\n            else:\n                # Ensure numeric type for non-factors\n                try:\n                    x_i = x_i.astype(float)\n                except ValueError:\n                    # If conversion fails, likely a string column without factor map?\n                    sensitivities.append(0.0)\n                    continue\n\n            # Skip if no variation in this dimension\n            if np.std(x_i) &lt; 1e-10:\n                sensitivities.append(0.0)\n                continue\n\n            # Compute correlation with objective\n            try:\n                correlation = np.abs(np.corrcoef(x_i, self.y_)[0, 1])\n                if np.isnan(correlation):\n                    correlation = 0.0\n            except Exception:\n                correlation = 0.0\n\n            sensitivities.append(correlation)\n\n        # Normalize to percentage\n        total = sum(sensitivities)\n        if total &gt; 0:\n            importance = [(s / total) * 100 for s in sensitivities]\n        else:\n            importance = [0.0] * len(sensitivities)\n\n        return importance\n\n    def sensitivity_spearman(self) -&gt; None:\n        \"\"\"Compute and print Spearman correlation between parameters and objective values.\n\n        This method analyzes the sensitivity of the objective function to each\n        hyperparameter by computing Spearman rank correlations. For categorical\n        (factor) variables, correlation is not computed as they require visual\n        inspection instead.\n\n        The method automatically handles different parameter types:\n        - Integer/float parameters: Direct correlation with objective values\n        - Log-transformed parameters (log10, log, ln): Correlation in log-space\n        - Factor (categorical) parameters: Skipped with informative message\n\n        Significance levels:\n        - ***: p &lt; 0.001 (highly significant)\n        - **: p &lt; 0.01 (significant)\n        - *: p &lt; 0.05 (marginally significant)\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # After running optimization\n            &gt;&gt;&gt; opt = SpotOptim(...)\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt; opt.sensitivity_spearman()\n            Sensitivity Analysis (Spearman Correlation):\n            --------------------------------------------------\n              l1 (neurons)        : +0.005 (p=0.959)\n              num_layers          : -0.192 (p=0.056)\n              activation          : (categorical variable, use visual inspection)\n              lr_unified          : -0.040 (p=0.689)\n              alpha               : -0.233 (p=0.020) *\n\n        Note:\n            Requires scipy to be installed. If not available, raises ImportError.\n            Only meaningful after optimize() has been called with sufficient evaluations.\n        \"\"\"\n        try:\n            from scipy.stats import spearmanr\n        except ImportError:\n            raise ImportError(\n                \"scipy is required for sensitivity_spearman(). \"\n                \"Install it with: pip install scipy\"\n            )\n\n        if self.X_ is None or self.y_ is None:\n            raise ValueError(\"No optimization data available. Run optimize() first.\")\n\n        # Get optimization history and parameters\n        history = self.y_\n        all_params = self.X_\n\n        # Get parameter names\n        param_names = (\n            self.var_name if self.var_name else [f\"x{i}\" for i in range(self.n_dim)]\n        )\n\n        print(\"\\nSensitivity Analysis (Spearman Correlation):\")\n        print(\"-\" * 50)\n\n        for param_idx in range(self.n_dim):\n            name = param_names[param_idx]\n            param_values = all_params[:, param_idx]\n\n            # Check if it's a factor variable\n            var_type = self.var_type[param_idx] if self.var_type else \"float\"\n\n            if var_type == \"factor\":\n                # For categorical variables, skip correlation\n                print(f\"  {name:20s}: (categorical variable, use visual inspection)\")\n                continue\n\n            # Check if parameter has log transformation\n            var_trans = self.var_trans[param_idx] if self.var_trans else None\n\n            # Compute correlation based on transformation\n            if var_trans in [\"log10\", \"log\", \"ln\"]:\n                # For log-transformed parameters, use log-space correlation\n                try:\n                    param_values_numeric = param_values.astype(float)\n                    # Filter out non-positive values\n                    valid_mask = (param_values_numeric &gt; 0) &amp; (history &gt; 0)\n                    if valid_mask.sum() &lt; 3:\n                        print(\n                            f\"  {name:20s}: (insufficient valid data for log correlation)\"\n                        )\n                        continue\n\n                    corr, p_value = spearmanr(\n                        np.log10(param_values_numeric[valid_mask]),\n                        np.log10(history[valid_mask]),\n                    )\n                except (ValueError, TypeError):\n                    print(f\"  {name:20s}: (error computing log correlation)\")\n                    continue\n            else:\n                # For integer/float parameters, direct correlation\n                try:\n                    param_values_numeric = param_values.astype(float)\n                    corr, p_value = spearmanr(param_values_numeric, history)\n                except (ValueError, TypeError):\n                    print(f\"  {name:20s}: (error computing correlation)\")\n                    continue\n\n            # Determine significance level\n            if p_value &lt; 0.001:\n                significance = \" ***\"\n            elif p_value &lt; 0.01:\n                significance = \" **\"\n            elif p_value &lt; 0.05:\n                significance = \" *\"\n            else:\n                significance = \"\"\n\n            print(f\"  {name:20s}: {corr:+.3f} (p={p_value:.3f}){significance}\")\n\n    def get_stars(self, input_list: list) -&gt; list:\n        \"\"\"Converts a list of values to a list of stars.\n\n        Used to visualize the importance of a variable.\n        Thresholds: &gt;99: ***, &gt;75: **, &gt;50: *, &gt;10: .\n\n        Args:\n            input_list (list): A list of importance scores (0-100).\n\n        Returns:\n            list: A list of star strings.\n        \"\"\"\n        output_list = []\n        for value in input_list:\n            if value &gt; 99:\n                output_list.append(\"***\")\n            elif value &gt; 75:\n                output_list.append(\"**\")\n            elif value &gt; 50:\n                output_list.append(\"*\")\n            elif value &gt; 10:\n                output_list.append(\".\")\n            else:\n                output_list.append(\"\")\n        return output_list\n\n    # ====================\n    # TensorBoard Integration\n    # ====================\n\n    def _clean_tensorboard_logs(self) -&gt; None:\n        \"\"\"Clean old TensorBoard log directories from the runs folder.\n\n        Removes all subdirectories in the 'runs' directory if tensorboard_clean is True.\n        This is useful for removing old logs before starting a new optimization run.\n\n        Warning:\n            This will permanently delete all subdirectories in the 'runs' folder.\n            Use with caution.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     tensorboard_log=True,\n            ...     tensorboard_clean=True\n            ... )\n            &gt;&gt;&gt; # Old logs in 'runs' will be removed before optimization starts\n        \"\"\"\n        if self.tensorboard_clean:\n            runs_dir = \"runs\"\n            if os.path.exists(runs_dir) and os.path.isdir(runs_dir):\n                # Get all subdirectories in runs\n                subdirs = [\n                    os.path.join(runs_dir, d)\n                    for d in os.listdir(runs_dir)\n                    if os.path.isdir(os.path.join(runs_dir, d))\n                ]\n\n                if subdirs:\n                    removed_count = 0\n                    for subdir in subdirs:\n                        try:\n                            shutil.rmtree(subdir)\n                            removed_count += 1\n                            if self.verbose:\n                                print(f\"Removed old TensorBoard logs: {subdir}\")\n                        except Exception as e:\n                            if self.verbose:\n                                print(f\"Warning: Could not remove {subdir}: {e}\")\n\n                    if self.verbose and removed_count &gt; 0:\n                        print(\n                            f\"Cleaned {removed_count} old TensorBoard log director{'y' if removed_count == 1 else 'ies'}\"\n                        )\n                elif self.verbose:\n                    print(\"No old TensorBoard logs to clean in 'runs' directory\")\n            elif self.verbose:\n                print(\"'runs' directory does not exist, nothing to clean\")\n\n    def _init_tensorboard_writer(self) -&gt; None:\n        \"\"\"Initialize TensorBoard SummaryWriter if logging is enabled.\n\n        Creates a unique log directory based on timestamp if tensorboard_log is True.\n        The log directory will be in the format: runs/spotoptim_YYYYMMDD_HHMMSS\n\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     tensorboard_log=True\n            ... )\n            &gt;&gt;&gt; hasattr(opt, 'tb_writer')\n            True\n        \"\"\"\n        if self.tensorboard_log:\n\n            if self.tensorboard_path is None:\n                # Create default path with timestamp\n                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                self.tensorboard_path = f\"runs/spotoptim_{timestamp}\"\n\n            # Create directory if it doesn't exist\n            os.makedirs(self.tensorboard_path, exist_ok=True)\n\n            self.tb_writer = SummaryWriter(log_dir=self.tensorboard_path)\n            if self.verbose:\n                print(f\"TensorBoard logging enabled: {self.tensorboard_path}\")\n        else:\n            self.tb_writer = None\n            if self.verbose:\n                print(\"TensorBoard logging disabled\")\n\n    def _write_tensorboard_scalars(self) -&gt; None:\n        \"\"\"Write scalar metrics to TensorBoard.\n\n        Logs the following metrics:\n        - Best y value found so far (min_y)\n        - Last y value evaluated\n        - Best X coordinates (for each dimension)\n        - If noise=True: also logs mean values and variance\n        \"\"\"\n        if self.tb_writer is None or self.y_ is None or len(self.y_) == 0:\n            return\n\n        step = self.counter\n        y_last = self.y_[-1]\n\n        if not (self.repeats_initial &gt; 1) or (self.repeats_surrogate &gt; 1):\n            # Non-noisy optimization\n            self.tb_writer.add_scalars(\n                \"y_values\", {\"min\": self.min_y, \"last\": y_last}, step\n            )\n            # Log success rate\n            self.tb_writer.add_scalar(\"success_rate\", self.success_rate, step)\n            # Log best X coordinates using var_name if available\n            for i in range(self.n_dim):\n                param_name = self.var_name[i] if self.var_name else f\"x{i}\"\n                self.tb_writer.add_scalar(f\"X_best/{param_name}\", self.min_X[i], step)\n        else:\n            # Noisy optimization\n            self.tb_writer.add_scalars(\n                \"y_values\",\n                {\"min\": self.min_y, \"mean_best\": self.min_mean_y, \"last\": y_last},\n                step,\n            )\n            # Log variance of best mean\n            self.tb_writer.add_scalar(\"y_variance_at_best\", self.min_var_y, step)\n            # Log success rate\n            self.tb_writer.add_scalar(\"success_rate\", self.success_rate, step)\n\n            # Log best X coordinates (by mean) using var_name if available\n            for i in range(self.n_dim):\n                param_name = self.var_name[i] if self.var_name else f\"x{i}\"\n                self.tb_writer.add_scalar(\n                    f\"X_mean_best/{param_name}\", self.min_mean_X[i], step\n                )\n\n        self.tb_writer.flush()\n\n    def _write_tensorboard_hparams(self, X: np.ndarray, y: float) -&gt; None:\n        \"\"\"Write hyperparameters and metric to TensorBoard.\n\n        Args:\n            X (ndarray): Design point coordinates, shape (n_features,)\n            y (float): Function value at X\n        \"\"\"\n        if self.tb_writer is None:\n            return\n\n        # Create hyperparameter dict with variable names\n        hparam_dict = {self.var_name[i]: float(X[i]) for i in range(self.n_dim)}\n        metric_dict = {\"hp_metric\": float(y)}\n\n        self.tb_writer.add_hparams(hparam_dict, metric_dict)\n        self.tb_writer.flush()\n\n    def _close_tensorboard_writer(self) -&gt; None:\n        \"\"\"Close TensorBoard writer and cleanup.\"\"\"\n        if hasattr(self, \"tb_writer\") and self.tb_writer is not None:\n            self.tb_writer.flush()\n            self.tb_writer.close()\n            if self.verbose:\n                print(\n                    f\"TensorBoard writer closed. View logs with: tensorboard --logdir={self.tensorboard_path}\"\n                )\n            del self.tb_writer\n\n    def _init_tensorboard(self) -&gt; None:\n        \"\"\"Log initial design to TensorBoard.\n\n        Logs all initial design points (hyperparameters and function values)\n        and scalar metrics to TensorBoard. Only executes if TensorBoard logging\n        is enabled (tb_writer is not None).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=lambda X: np.sum(X**2, axis=1),\n            ...     bounds=[(-5, 5), (-5, 5)],\n            ...     n_initial=5,\n            ...     tensorboard_log=True,\n            ...     verbose=False\n            ... )\n            &gt;&gt;&gt; # Simulate initial design (normally done in optimize())\n            &gt;&gt;&gt; opt.X_ = np.array([[1, 2], [0, 0], [2, 1]])\n            &gt;&gt;&gt; opt.y_ = np.array([5.0, 0.0, 5.0])\n            &gt;&gt;&gt; opt._init_tensorboard()\n            &gt;&gt;&gt; # TensorBoard logs created for all initial points\n        \"\"\"\n        # Create writer if not exists\n        if self.tensorboard_log and self.tb_writer is None:\n            # Determine log directory\n            if self.tensorboard_path:\n                log_dir = self.tensorboard_path\n            else:\n                import datetime\n\n                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                log_dir = f\"runs/spotoptim_{timestamp}\"\n                self.config.tensorboard_path = log_dir\n                self.tensorboard_path = log_dir\n\n            try:\n                from torch.utils.tensorboard import SummaryWriter\n\n                self.tb_writer = SummaryWriter(log_dir=log_dir)\n                if self.verbose:\n                    print(f\"TensorBoard logging enabled: {log_dir}\")\n            except ImportError:\n                print(\"Warning: torch or tensorboard not installed. Logging disabled.\")\n                self.tb_writer = None\n                self.config.tensorboard_log = False\n                self.tensorboard_log = False\n\n        if self.tb_writer is not None:\n            for i in range(len(self.y_)):\n                self._write_tensorboard_hparams(self.X_[i], self.y_[i])\n            self._write_tensorboard_scalars()\n\n    def _close_and_del_tensorboard_writer(self) -&gt; None:\n        \"\"\"Close and delete TensorBoard writer to prepare for pickling.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; opt = SpotOptim(fun=lambda x: x, bounds=[(0, 1)])\n            &gt;&gt;&gt; # Assume tb_writer is initialized\n            &gt;&gt;&gt; opt.tb_writer = SomeTensorBoardWriter()\n            &gt;&gt;&gt; # Close and delete tb_writer before pickling\n            &gt;&gt;&gt; opt._close_and_del_tensorboard_writer()\n        \"\"\"\n        if hasattr(self, \"tb_writer\") and self.tb_writer is not None:\n            try:\n                self.tb_writer.flush()\n                self.tb_writer.close()\n            except Exception:\n                pass\n            self.tb_writer = None\n\n    # ====================\n    # Plotting\n    # ====================\n\n    def plot_progress(\n        self,\n        show: bool = True,\n        log_y: bool = False,\n        figsize: Tuple[int, int] = (10, 6),\n        ylabel: str = \"Objective Value\",\n        mo: bool = False,\n    ) -&gt; None:\n        \"\"\"Plot optimization progress using spotoptim.plot.visualization.plot_progress.\"\"\"\n        plot_progress(\n            self, show=show, log_y=log_y, figsize=figsize, ylabel=ylabel, mo=mo\n        )\n\n    def plot_surrogate(\n        self,\n        i: int = 0,\n        j: int = 1,\n        show: bool = True,\n        alpha: float = 0.8,\n        var_name: Optional[List[str]] = None,\n        cmap: str = \"jet\",\n        num: int = 100,\n        vmin: Optional[float] = None,\n        vmax: Optional[float] = None,\n        add_points: bool = True,\n        grid_visible: bool = True,\n        contour_levels: int = 30,\n        figsize: Tuple[int, int] = (12, 10),\n    ) -&gt; None:\n        \"\"\"Plot the surrogate model for two dimensions.\n\n        Delegates to spotoptim.plot.visualization.plot_surrogate.\n        \"\"\"\n        plot_surrogate(\n            self,\n            i=i,\n            j=j,\n            show=show,\n            alpha=alpha,\n            var_name=var_name,\n            cmap=cmap,\n            num=num,\n            vmin=vmin,\n            vmax=vmax,\n            add_points=add_points,\n            grid_visible=grid_visible,\n            contour_levels=contour_levels,\n            figsize=figsize,\n        )\n\n    def plot_important_hyperparameter_contour(\n        self,\n        max_imp: int = 3,\n        show: bool = True,\n        alpha: float = 0.8,\n        cmap: str = \"jet\",\n        num: int = 100,\n        add_points: bool = True,\n        grid_visible: bool = True,\n        contour_levels: int = 30,\n        figsize: Tuple[int, int] = (12, 10),\n    ) -&gt; None:\n        \"\"\"Plot surrogate contours using spotoptim.plot.visualization.plot_important_hyperparameter_contour.\"\"\"\n        plot_important_hyperparameter_contour(\n            self,\n            max_imp=max_imp,\n            show=show,\n            alpha=alpha,\n            cmap=cmap,\n            num=num,\n            add_points=add_points,\n            grid_visible=grid_visible,\n            contour_levels=contour_levels,\n            figsize=figsize,\n        )\n\n    def _plot_surrogate_with_factors(\n        self,\n        i: int,\n        j: int,\n        show: bool = True,\n        alpha: float = 0.8,\n        cmap: str = \"jet\",\n        num: int = 100,\n        add_points: bool = True,\n        grid_visible: bool = True,\n        contour_levels: int = 30,\n        figsize: Tuple[int, int] = (12, 10),\n    ) -&gt; None:\n        \"\"\"Delegates to spotoptim.plot.visualization._plot_surrogate_with_factors.\"\"\"\n        _plot_surrogate_with_factors(\n            self,\n            i=i,\n            j=j,\n            show=show,\n            alpha=alpha,\n            cmap=cmap,\n            num=num,\n            add_points=add_points,\n            grid_visible=grid_visible,\n            contour_levels=contour_levels,\n            figsize=figsize,\n        )\n\n    def plot_importance(\n        self, threshold: float = 0.0, figsize: Tuple[int, int] = (10, 6)\n    ) -&gt; None:\n        \"\"\"Plot variable importance.\n\n        Args:\n            threshold (float): Minimum importance percentage to include in plot.\n            figsize (tuple): Figure size.\n        \"\"\"\n        importance = self.get_importance()\n        names = (\n            self.all_var_name\n            if self.all_var_name\n            else [f\"x{i}\" for i in range(len(importance))]\n        )\n\n        # Filter by threshold\n        filtered_data = [(n, i) for n, i in zip(names, importance) if i &gt;= threshold]\n        filtered_data.sort(key=lambda x: x[1], reverse=True)\n\n        if not filtered_data:\n            print(\"No variables met the importance threshold.\")\n            return\n\n        names, values = zip(*filtered_data)\n\n        plt.figure(figsize=figsize)\n        y_pos = np.arange(len(names))\n        plt.barh(y_pos, values, align=\"center\")\n        plt.yticks(y_pos, names)\n        plt.xlabel(\"Importance (%)\")\n        plt.title(\"Variable Importance\")\n        plt.gca().invert_yaxis()  # Best on top\n        plt.show()\n\n    def plot_parameter_scatter(\n        self,\n        result: Optional[OptimizeResult] = None,\n        show: bool = True,\n        figsize: Tuple[int, int] = (12, 10),\n        ylabel: str = \"Objective Value\",\n        cmap: str = \"viridis_r\",\n        show_correlation: bool = False,\n        log_y: bool = False,\n    ) -&gt; None:\n        \"\"\"Plot parameter distributions showing relationship between each parameter and objective.\n\n        Creates a grid of scatter plots, one for each parameter dimension, showing how\n        the objective function value varies with each parameter. The best configuration\n        is marked with a red star. Parameters with log-scale transformations (var_trans)\n        are automatically displayed on a log x-axis.\n\n        Optionally displays Spearman correlation coefficients in plot titles for\n        sensitivity analysis. For factor (categorical) variables, correlation is not\n        computed and they are displayed with discrete positions on the x-axis.\n\n        Args:\n            result (OptimizeResult, optional): Optimization result containing best parameters.\n                If None, uses the best found values from self.best_x_ and self.best_y_.\n            show (bool, optional): Whether to display the plot. Defaults to True.\n            figsize (tuple, optional): Figure size as (width, height). Defaults to (12, 10).\n            ylabel (str, optional): Label for y-axis. Defaults to \"Objective Value\".\n            cmap (str, optional): Colormap for scatter plot. Defaults to \"viridis_r\".\n            show_correlation (bool, optional): Whether to compute and display Spearman\n                correlation coefficients in plot titles. Requires scipy. Defaults to False.\n            log_y (bool, optional): Whether to use logarithmic scale for y-axis.\n                Defaults to False.\n\n        Raises:\n            ValueError: If no optimization data is available.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; def objective(X):\n            ...     return np.sum(X**2, axis=1)\n            &gt;&gt;&gt; opt = SpotOptim(\n            ...     fun=objective,\n            ...     bounds=[(-5, 5), (-5, 5), (-5, 5), (-5, 5)],\n            ...     var_name=[\"x0\", \"x1\", \"x2\", \"x3\"],\n            ...     max_iter=30,\n            ...     n_initial=10,\n            ...     seed=42\n            ... )\n            &gt;&gt;&gt; result = opt.optimize()\n            &gt;&gt;&gt; # Plot parameter distributions\n            &gt;&gt;&gt; opt.plot_parameter_scatter(result)\n            &gt;&gt;&gt; # Plot with custom settings\n            &gt;&gt;&gt; opt.plot_parameter_scatter(result, cmap=\"plasma\", ylabel=\"Error\")\n        \"\"\"\n        try:\n            import matplotlib.pyplot as plt\n        except ImportError:\n            raise ImportError(\n                \"matplotlib is required for plot_parameter_scatter(). \"\n                \"Install it with: pip install matplotlib\"\n            )\n\n        # Import scipy if correlation is requested\n        if show_correlation:\n            try:\n                from scipy.stats import spearmanr\n            except ImportError:\n                raise ImportError(\n                    \"scipy is required for show_correlation=True. \"\n                    \"Install it with: pip install scipy\"\n                )\n\n        if self.X_ is None or self.y_ is None or len(self.y_) == 0:\n            raise ValueError(\"No optimization data available. Run optimize() first.\")\n\n        # Get best values\n        if result is not None:\n            best_x = result.x\n            best_y = result.fun\n        elif self.best_x_ is not None and self.best_y_ is not None:\n            best_x = self.best_x_\n            best_y = self.best_y_\n        else:\n            raise ValueError(\"No best solution available.\")\n\n        all_params = self.X_\n        history = self.y_\n\n        # Determine grid dimensions\n        n_params = all_params.shape[1]\n        n_cols = min(4, n_params)\n        n_rows = int(np.ceil(n_params / n_cols))\n\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n\n        # Make axes always iterable\n        if n_params == 1:\n            axes = np.array([axes])\n        axes_flat = axes.flatten() if n_params &gt; 1 else axes\n\n        for idx in range(n_params):\n            ax = axes_flat[idx]\n            param_values = all_params[:, idx]\n            param_name = self.var_name[idx] if self.var_name else f\"x{idx}\"\n            var_type = self.var_type[idx] if self.var_type else \"float\"\n            var_trans = self.var_trans[idx] if self.var_trans else None\n\n            # Check if this is a factor variable\n            is_factor = var_type == \"factor\"\n\n            # Compute correlation if requested\n            corr, p_value = np.nan, np.nan\n            if show_correlation and not is_factor:\n                try:\n                    if var_trans in [\"log10\", \"log\", \"ln\"]:\n                        # For log-transformed parameters, correlate in log-space\n                        param_values_numeric = param_values.astype(float)\n                        valid_mask = (param_values_numeric &gt; 0) &amp; (history &gt; 0)\n                        if valid_mask.sum() &gt;= 3:\n                            corr, p_value = spearmanr(\n                                np.log10(param_values_numeric[valid_mask]),\n                                np.log10(history[valid_mask]),\n                            )\n                    else:\n                        # Direct correlation for non-transformed parameters\n                        param_values_numeric = param_values.astype(float)\n                        corr, p_value = spearmanr(param_values_numeric, history)\n                except (ValueError, TypeError):\n                    pass  # Keep corr as nan\n\n            # Handle factor variables differently\n            if is_factor:\n                # Map factor levels to integer positions\n                unique_vals = np.unique(param_values)\n                positions = {val: i for i, val in enumerate(unique_vals)}\n                numeric_vals = np.array([positions[val] for val in param_values])\n\n                # Scatter plot with discrete x positions\n                ax.scatter(\n                    numeric_vals,\n                    history,\n                    c=history,\n                    cmap=cmap,\n                    s=50,\n                    alpha=0.7,\n                    edgecolors=\"black\",\n                    linewidth=0.5,\n                )\n\n                # Mark best configuration\n                best_val = best_x[idx]\n                if best_val not in positions:\n                    positions[best_val] = len(positions)\n                    unique_vals = np.append(unique_vals, best_val)\n\n                best_pos = positions[best_val]\n                ax.scatter(\n                    [best_pos],\n                    [best_y],\n                    color=\"red\",\n                    s=200,\n                    marker=\"*\",\n                    edgecolors=\"black\",\n                    linewidth=1.5,\n                    label=\"Best\",\n                    zorder=5,\n                )\n\n                # Set categorical x-axis labels\n                ax.set_xticks(range(len(unique_vals)))\n                ax.set_xticklabels(unique_vals, rotation=45, ha=\"right\")\n            else:\n                # Standard scatter plot for numeric variables\n                ax.scatter(\n                    param_values,\n                    history,\n                    c=history,\n                    cmap=cmap,\n                    s=50,\n                    alpha=0.7,\n                    edgecolors=\"black\",\n                    linewidth=0.5,\n                )\n\n                # Mark best configuration\n                ax.scatter(\n                    [best_x[idx]],\n                    [best_y],\n                    color=\"red\",\n                    s=200,\n                    marker=\"*\",\n                    edgecolors=\"black\",\n                    linewidth=1.5,\n                    label=\"Best\",\n                    zorder=5,\n                )\n\n                # Use log scale for parameters with log transformations\n                if var_trans in [\"log10\", \"log\", \"ln\"]:\n                    ax.set_xscale(\"log\")\n\n            # Set labels\n            ax.set_xlabel(param_name, fontsize=11)\n            ax.set_ylabel(ylabel, fontsize=11)\n\n            # Set title with optional correlation\n            if show_correlation and not np.isnan(corr):\n                ax.set_title(\n                    f\"{param_name}\\nCorr: {corr:.3f} (p={p_value:.3f})\", fontsize=11\n                )\n            elif show_correlation and is_factor:\n                ax.set_title(f\"{param_name}\\n(categorical)\", fontsize=11)\n            else:\n                ax.set_title(f\"{param_name} vs {ylabel}\", fontsize=12)\n\n            ax.legend(fontsize=9)\n            ax.grid(True, alpha=0.3)\n\n            # Use log scale for y-axis if requested\n            if log_y:\n                ax.set_yscale(\"log\")\n\n        # Hide unused subplots\n        for idx in range(n_params, len(axes_flat)):\n            axes_flat[idx].set_visible(False)\n\n        plt.tight_layout()\n\n        if show:\n            plt.show()\n\n    def _generate_mesh_grid(self, i: int, j: int, num: int = 100):\n        # Wrapper for _generate_mesh_grid from visualization module.\n        return _generate_mesh_grid(self, i, j, num)\n\n    def _generate_mesh_grid_with_factors(\n        self, i: int, j: int, num: int, is_factor_i: bool, is_factor_j: bool\n    ):\n        # Wrapper for _generate_mesh_grid_with_factors from visualization module.\n        return _generate_mesh_grid_with_factors(\n            self, i, j, num, is_factor_i, is_factor_j\n        )\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.__dir__","title":"<code>__dir__()</code>","text":"<p>Include config and state attributes in dir().</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def __dir__(self):\n    \"\"\"Include config and state attributes in dir().\"\"\"\n    d = set(super().__dir__())\n    try:\n        config = super().__getattribute__(\"config\")\n        d.update(dir(config))\n    except AttributeError:\n        pass\n\n    try:\n        state = super().__getattribute__(\"state\")\n        # Filter internal methods/fields from dir if desired, but good for now\n        d.update(dir(state))\n    except AttributeError:\n        pass\n\n    return list(d)\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Proxy attribute access to config and state.</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"Proxy attribute access to config and state.\"\"\"\n    try:\n        config = super().__getattribute__(\"config\")\n        if hasattr(config, name):\n            return getattr(config, name)\n    except AttributeError:\n        pass\n\n    try:\n        state = super().__getattribute__(\"state\")\n        if hasattr(state, name):\n            return getattr(state, name)\n    except AttributeError:\n        pass\n\n    raise AttributeError(\n        f\"'{type(self).__name__}' object has no attribute '{name}'\"\n    )\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.__setattr__","title":"<code>__setattr__(name, value)</code>","text":"<p>Proxy attribute assignment to config and state.</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def __setattr__(self, name, value):\n    \"\"\"Proxy attribute assignment to config and state.\"\"\"\n    if name in (\"config\", \"state\"):\n        super().__setattr__(name, value)\n        return\n\n    try:\n        config = super().__getattribute__(\"config\")\n        if hasattr(config, name):\n            setattr(config, name, value)\n            return\n    except AttributeError:\n        pass\n\n    try:\n        state = super().__getattribute__(\"state\")\n        if hasattr(state, name):\n            setattr(state, name, value)\n            return\n    except AttributeError:\n        pass\n\n    super().__setattr__(name, value)\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.detect_var_type","title":"<code>detect_var_type()</code>","text":"<p>Auto-detect variable types based on factor mappings.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of variable types (\u2018factor\u2019 or \u2018float\u2019) for each dimension.   Dimensions with factor mappings are assigned \u2018factor\u2019, others \u2018float\u2019.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[('red', 'green', 'blue'), (0, 10)])\n&gt;&gt;&gt; spot.detect_var_type()\n['factor', 'float']\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def detect_var_type(self) -&gt; list:\n    \"\"\"Auto-detect variable types based on factor mappings.\n\n    Returns:\n        list: List of variable types ('factor' or 'float') for each dimension.\n              Dimensions with factor mappings are assigned 'factor', others 'float'.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[('red', 'green', 'blue'), (0, 10)])\n        &gt;&gt;&gt; spot.detect_var_type()\n        ['factor', 'float']\n    \"\"\"\n    return [\n        \"factor\" if i in self._factor_maps else \"float\" for i in range(self.n_dim)\n    ]\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.gen_design_table","title":"<code>gen_design_table(precision=4, tablefmt='github')</code>","text":"<p>Generate a table of the design or results.</p> <p>If optimization has been run (results available), returns the results table. Otherwise, returns the design table (search space configuration).</p> <p>Parameters:</p> Name Type Description Default <code>tablefmt</code> <code>str</code> <p>Table format. Defaults to \u2018github\u2019.</p> <code>'github'</code> <code>precision</code> <code>int</code> <p>Number of decimal places for float values. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted table string.</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def gen_design_table(self, precision: int = 4, tablefmt: str = \"github\") -&gt; str:\n    \"\"\"Generate a table of the design or results.\n\n    If optimization has been run (results available), returns the results table.\n    Otherwise, returns the design table (search space configuration).\n\n    Args:\n        tablefmt (str, optional): Table format. Defaults to 'github'.\n        precision (int, optional): Number of decimal places for float values.\n            Defaults to 4.\n\n    Returns:\n        str: Formatted table string.\n    \"\"\"\n    if self.best_x_ is not None:\n        return self.get_results_table(precision=precision, tablefmt=tablefmt)\n    else:\n        return self.get_design_table(precision=precision, tablefmt=tablefmt)\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.get_best_hyperparameters","title":"<code>get_best_hyperparameters(as_dict=True)</code>","text":"<p>Get the best hyperparameter configuration found during optimization.</p> <p>If noise handling is active (repeats_initial &gt; 1 or OCBA), this returns the parameter configuration associated with the best mean objective value. Otherwise, it returns the configuration associated with the absolute best observed value.</p> <p>Parameters:</p> Name Type Description Default <code>as_dict</code> <code>bool</code> <p>If True, returns a dictionary mapping parameter names to their values. If False, returns the raw numpy array. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[Dict[str, Any], ndarray, None]</code> <p>Union[Dict[str, Any], np.ndarray, None]: The best hyperparameter configuration. Returns None if optimization hasn\u2019t started (no data).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; opt = SpotOptim(fun=lambda x: np.sum(x**2), bounds=[(-5, 5)], var_name=[\"x\"])\n&gt;&gt;&gt; opt.optimize()\n&gt;&gt;&gt; best_params = opt.get_best_hyperparameters()\n&gt;&gt;&gt; print(best_params['x']) # Should be close to 0\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def get_best_hyperparameters(\n    self, as_dict: bool = True\n) -&gt; Union[Dict[str, Any], np.ndarray, None]:\n    \"\"\"\n    Get the best hyperparameter configuration found during optimization.\n\n    If noise handling is active (repeats_initial &gt; 1 or OCBA), this returns the parameter\n    configuration associated with the best *mean* objective value. Otherwise, it returns\n    the configuration associated with the absolute best observed value.\n\n    Args:\n        as_dict (bool, optional): If True, returns a dictionary mapping parameter names\n            to their values. If False, returns the raw numpy array. Defaults to True.\n\n    Returns:\n        Union[Dict[str, Any], np.ndarray, None]: The best hyperparameter configuration.\n            Returns None if optimization hasn't started (no data).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; opt = SpotOptim(fun=lambda x: np.sum(x**2), bounds=[(-5, 5)], var_name=[\"x\"])\n        &gt;&gt;&gt; opt.optimize()\n        &gt;&gt;&gt; best_params = opt.get_best_hyperparameters()\n        &gt;&gt;&gt; print(best_params['x']) # Should be close to 0\n    \"\"\"\n    if self.X_ is None or len(self.X_) == 0:\n        return None\n\n    # Determine which \"best\" to use\n    if (self.repeats_initial &gt; 1 or self.repeats_surrogate &gt; 1) and hasattr(\n        self, \"min_mean_X\"\n    ):\n        best_x = self.min_mean_X\n    else:\n        best_x = self.best_x_\n\n    if not as_dict:\n        return best_x\n\n    # Map factors using existing method (handles 2D, returns 2D)\n    # We pass best_x as (1, D) and get (1, D) back\n    mapped_x = self._map_to_factor_values(best_x.reshape(1, -1))[0]\n\n    # Convert to dictionary with types\n    params = {}\n    names = (\n        self.var_name if self.var_name else [f\"p{i}\" for i in range(len(best_x))]\n    )\n\n    for i, name in enumerate(names):\n        val = mapped_x[i]\n\n        # Handle types if available (specifically int, as factors are already mapped)\n        if self.var_type:\n            v_type = self.var_type[i]\n            if v_type == \"int\":\n                val = int(round(val))\n\n        params[name] = val\n\n    return params\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.get_design_table","title":"<code>get_design_table(tablefmt='github', precision=4)</code>","text":"<p>Get a table string showing the search space design before optimization.</p> <p>This method generates a table displaying the variable names, types, bounds, and defaults without requiring an optimization run. Useful for inspecting and documenting the search space configuration.</p> <p>Parameters:</p> Name Type Description Default <code>tablefmt</code> <code>str</code> <p>Table format for tabulate library. Defaults to \u2018github\u2019.</p> <code>'github'</code> <code>precision</code> <code>int</code> <p>Number of decimal places for float values. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted table string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 1: Numeric parameters\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (-10, 10), (0, 1)],\n...     var_name=[\"x1\", \"x2\", \"x3\"],\n...     var_type=[\"float\", \"int\", \"float\"],\n...     max_iter=20,\n...     n_initial=10\n... )\n&gt;&gt;&gt; table = opt.get_design_table()\n&gt;&gt;&gt; print(table)\n| name   | type   |   lower |   upper |   default |\n|--------|--------|---------|---------|-----------|\n| x1     | num    |    -5.0 |     5.0 |       0.0 |\n| x2     | int    |   -10.0 |    10.0 |       0.0 |\n| x3     | num    |     0.0 |     1.0 |       0.5 |\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 2: With factor variables\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(10, 100), (\"SGD\", \"Adam\", \"RMSprop\"), (0.001, 0.1)],\n...     var_name=[\"neurons\", \"optimizer\", \"lr\"],\n...     var_type=[\"int\", \"factor\", \"float\"],\n...     max_iter=30,\n...     n_initial=10\n... )\n&gt;&gt;&gt; table = opt.get_design_table()\n&gt;&gt;&gt; print(table)\n| name      | type   | lower   | upper   | default   |\n|-----------|--------|---------|---------|-----------|\n| neurons   | int    | 10.0    | 100.0   | 55.0      |\n| optimizer | factor | SGD     | RMSprop | Adam      |\n| lr        | num    | 0.001   | 0.1     | 0.0505    |\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 3: Before running optimization\n&gt;&gt;&gt; def hyperparameter_objective(X):\n...     # X[:, 0]: layers, X[:, 1]: neurons, X[:, 2]: dropout\n...     return np.sum(X**2, axis=1)  # Placeholder\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=hyperparameter_objective,\n...     bounds=[(1, 5), (16, 256), (0.0, 0.5)],\n...     var_name=[\"layers\", \"neurons\", \"dropout\"],\n...     var_type=[\"int\", \"int\", \"float\"],\n...     max_iter=50,\n...     n_initial=15\n... )\n&gt;&gt;&gt; # Get design table before optimization\n&gt;&gt;&gt; print(\"Search Space Configuration:\")\n&gt;&gt;&gt; table = opt.get_design_table()\n&gt;&gt;&gt; print(table)\nSearch Space Configuration:\n| name    | type   |   lower |   upper |   default |\n|---------|--------|---------|---------|-----------|\n| layers  | int    |     1.0 |     5.0 |       3.0 |\n| neurons | int    |    16.0 |   256.0 |     136.0 |\n| dropout | num    |     0.0 |     0.5 |      0.25 |\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def get_design_table(\n    self,\n    tablefmt: str = \"github\",\n    precision: int = 4,\n) -&gt; str:\n    \"\"\"Get a table string showing the search space design before optimization.\n\n    This method generates a table displaying the variable names, types, bounds,\n    and defaults without requiring an optimization run. Useful for inspecting\n    and documenting the search space configuration.\n\n    Args:\n        tablefmt (str, optional): Table format for tabulate library.\n            Defaults to 'github'.\n        precision (int, optional): Number of decimal places for float values.\n            Defaults to 4.\n\n    Returns:\n        str: Formatted table string.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 1: Numeric parameters\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (-10, 10), (0, 1)],\n        ...     var_name=[\"x1\", \"x2\", \"x3\"],\n        ...     var_type=[\"float\", \"int\", \"float\"],\n        ...     max_iter=20,\n        ...     n_initial=10\n        ... )\n        &gt;&gt;&gt; table = opt.get_design_table()\n        &gt;&gt;&gt; print(table)\n        | name   | type   |   lower |   upper |   default |\n        |--------|--------|---------|---------|-----------|\n        | x1     | num    |    -5.0 |     5.0 |       0.0 |\n        | x2     | int    |   -10.0 |    10.0 |       0.0 |\n        | x3     | num    |     0.0 |     1.0 |       0.5 |\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 2: With factor variables\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(10, 100), (\"SGD\", \"Adam\", \"RMSprop\"), (0.001, 0.1)],\n        ...     var_name=[\"neurons\", \"optimizer\", \"lr\"],\n        ...     var_type=[\"int\", \"factor\", \"float\"],\n        ...     max_iter=30,\n        ...     n_initial=10\n        ... )\n        &gt;&gt;&gt; table = opt.get_design_table()\n        &gt;&gt;&gt; print(table)\n        | name      | type   | lower   | upper   | default   |\n        |-----------|--------|---------|---------|-----------|\n        | neurons   | int    | 10.0    | 100.0   | 55.0      |\n        | optimizer | factor | SGD     | RMSprop | Adam      |\n        | lr        | num    | 0.001   | 0.1     | 0.0505    |\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 3: Before running optimization\n        &gt;&gt;&gt; def hyperparameter_objective(X):\n        ...     # X[:, 0]: layers, X[:, 1]: neurons, X[:, 2]: dropout\n        ...     return np.sum(X**2, axis=1)  # Placeholder\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=hyperparameter_objective,\n        ...     bounds=[(1, 5), (16, 256), (0.0, 0.5)],\n        ...     var_name=[\"layers\", \"neurons\", \"dropout\"],\n        ...     var_type=[\"int\", \"int\", \"float\"],\n        ...     max_iter=50,\n        ...     n_initial=15\n        ... )\n        &gt;&gt;&gt; # Get design table before optimization\n        &gt;&gt;&gt; print(\"Search Space Configuration:\")\n        &gt;&gt;&gt; table = opt.get_design_table()\n        &gt;&gt;&gt; print(table)\n        Search Space Configuration:\n        | name    | type   |   lower |   upper |   default |\n        |---------|--------|---------|---------|-----------|\n        | layers  | int    |     1.0 |     5.0 |       3.0 |\n        | neurons | int    |    16.0 |   256.0 |     136.0 |\n        | dropout | num    |     0.0 |     0.5 |      0.25 |\n    \"\"\"\n    # Prepare all variable transformations (use all_var_trans if dimension reduction occurred)\n    if self.red_dim and hasattr(self, \"all_var_trans\"):\n        all_var_trans = self.all_var_trans\n    else:\n        all_var_trans = self.var_trans\n\n    # Prepare table data\n    table_data = {\n        \"name\": (\n            self.all_var_name\n            if self.all_var_name\n            else [f\"x{i}\" for i in range(len(self.all_lower))]\n        ),\n        \"type\": (\n            self.all_var_type\n            if self.all_var_type\n            else [\"float\"] * len(self.all_lower)\n        ),\n        \"lower\": [],\n        \"upper\": [],\n        \"default\": [],\n        \"transform\": [t if t is not None else \"-\" for t in all_var_trans],\n    }\n\n    # Helper to format values\n    def fmt_val(v):\n        if isinstance(v, (float, np.floating)):\n            return f\"{v:.{precision}f}\"\n        return v\n\n    # Process bounds and compute defaults (use original bounds for display)\n    for i in range(len(self._original_lower)):\n        var_type = table_data[\"type\"][i]\n\n        if var_type == \"factor\":\n            # For factors, show original string values\n            if i in self._factor_maps:\n                factor_map = self._factor_maps[i]\n                # Default is middle level\n                mid_idx = len(factor_map) // 2\n                default_str = factor_map[mid_idx]\n                table_data[\"lower\"].append(\"-\")\n                table_data[\"upper\"].append(\"-\")\n                table_data[\"default\"].append(default_str)\n            else:\n                table_data[\"lower\"].append(\"-\")\n                table_data[\"upper\"].append(\"-\")\n                table_data[\"default\"].append(\"N/A\")\n        else:\n            table_data[\"lower\"].append(fmt_val(self._original_lower[i]))\n            table_data[\"upper\"].append(fmt_val(self._original_upper[i]))\n            # Default is midpoint\n            default_val = (self._original_lower[i] + self._original_upper[i]) / 2\n            if var_type == \"int\":\n                table_data[\"default\"].append(int(default_val))\n            else:\n                table_data[\"default\"].append(fmt_val(default_val))\n\n    # Generate table\n    table = tabulate(\n        table_data,\n        headers=\"keys\",\n        tablefmt=tablefmt,\n        numalign=\"right\",\n        stralign=\"right\",\n    )\n\n    return table\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.get_importance","title":"<code>get_importance()</code>","text":"<p>Calculate variable importance scores.</p> <p>Importance is computed as the normalized sensitivity of each parameter based on the variation in objective values across the evaluated points. Higher scores indicate parameters that have more influence on the objective.</p> <p>The importance is calculated as: 1. For each dimension, compute the correlation between parameter values    and objective values 2. Normalize to percentage scale (0-100) 3. Higher values indicate more important parameters</p> <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: Importance scores for each dimension (0-100 scale).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 1: Identify important parameters\n&gt;&gt;&gt; def test_func(X):\n...     # x0 has strong effect, x1 has weak effect\n...     return 10 * X[:, 0]**2 + 0.1 * X[:, 1]**2\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=test_func,\n...     bounds=[(-5, 5), (-5, 5)],\n...     var_name=[\"x0\", \"x1\"],\n...     max_iter=30,\n...     n_initial=10,\n...     seed=42\n... )\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt; importance = opt.get_importance()\n&gt;&gt;&gt; print(f\"x0 importance: {importance[0]:.2f}\")\n&gt;&gt;&gt; print(f\"x1 importance: {importance[1]:.2f}\")\nx0 importance: 89.23\nx1 importance: 10.77\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 2: With more dimensions\n&gt;&gt;&gt; def rosenbrock(X):\n...     return np.sum(100*(X[:, 1:] - X[:, :-1]**2)**2 + (1 - X[:, :-1])**2, axis=1)\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=rosenbrock,\n...     bounds=[(-2, 2)] * 4,\n...     var_name=[\"x0\", \"x1\", \"x2\", \"x3\"],\n...     max_iter=50,\n...     n_initial=20,\n...     seed=42\n... )\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt; importance = opt.get_importance()\n&gt;&gt;&gt; for i, imp in enumerate(importance):\n...     print(f\"x{i}: {imp:.2f}%\")\nx0: 32.15%\nx1: 28.43%\nx2: 25.67%\nx3: 13.75%\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 3: Use in results table\n&gt;&gt;&gt; table = opt.print_results_table(show_importance=True)\n&gt;&gt;&gt; print(table)\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def get_importance(self) -&gt; List[float]:\n    \"\"\"Calculate variable importance scores.\n\n    Importance is computed as the normalized sensitivity of each parameter\n    based on the variation in objective values across the evaluated points.\n    Higher scores indicate parameters that have more influence on the objective.\n\n    The importance is calculated as:\n    1. For each dimension, compute the correlation between parameter values\n       and objective values\n    2. Normalize to percentage scale (0-100)\n    3. Higher values indicate more important parameters\n\n    Returns:\n        List[float]: Importance scores for each dimension (0-100 scale).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 1: Identify important parameters\n        &gt;&gt;&gt; def test_func(X):\n        ...     # x0 has strong effect, x1 has weak effect\n        ...     return 10 * X[:, 0]**2 + 0.1 * X[:, 1]**2\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=test_func,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     var_name=[\"x0\", \"x1\"],\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt; importance = opt.get_importance()\n        &gt;&gt;&gt; print(f\"x0 importance: {importance[0]:.2f}\")\n        &gt;&gt;&gt; print(f\"x1 importance: {importance[1]:.2f}\")\n        x0 importance: 89.23\n        x1 importance: 10.77\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 2: With more dimensions\n        &gt;&gt;&gt; def rosenbrock(X):\n        ...     return np.sum(100*(X[:, 1:] - X[:, :-1]**2)**2 + (1 - X[:, :-1])**2, axis=1)\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=rosenbrock,\n        ...     bounds=[(-2, 2)] * 4,\n        ...     var_name=[\"x0\", \"x1\", \"x2\", \"x3\"],\n        ...     max_iter=50,\n        ...     n_initial=20,\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt; importance = opt.get_importance()\n        &gt;&gt;&gt; for i, imp in enumerate(importance):\n        ...     print(f\"x{i}: {imp:.2f}%\")\n        x0: 32.15%\n        x1: 28.43%\n        x2: 25.67%\n        x3: 13.75%\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 3: Use in results table\n        &gt;&gt;&gt; table = opt.print_results_table(show_importance=True)\n        &gt;&gt;&gt; print(table)\n    \"\"\"\n    if self.X_ is None or self.y_ is None or len(self.y_) &lt; 3:\n        # Not enough data to compute importance\n        return [0.0] * len(self.all_lower)\n\n    # Use full-dimensional data\n    X_full = self.X_\n    if self.red_dim:\n        X_full = np.array([self.to_all_dim(x.reshape(1, -1))[0] for x in self.X_])\n\n    # Calculate sensitivity for each dimension\n    sensitivities = []\n    for i in range(X_full.shape[1]):\n        x_i = X_full[:, i]\n\n        # Handle factor variables: map strings to integers\n        if hasattr(self, \"_factor_maps\") and i in self._factor_maps:\n            # _factor_maps[i] is {int: str}, we need {str: int}\n            str_to_int = {v: k for k, v in self._factor_maps[i].items()}\n            try:\n                # Map values, handle potential missing values if any (though shouldn't simplify be there)\n                x_i = np.array([str_to_int.get(val, -1) for val in x_i])\n            except Exception:\n                # Fallback if mapping fails\n                sensitivities.append(0.0)\n                continue\n        else:\n            # Ensure numeric type for non-factors\n            try:\n                x_i = x_i.astype(float)\n            except ValueError:\n                # If conversion fails, likely a string column without factor map?\n                sensitivities.append(0.0)\n                continue\n\n        # Skip if no variation in this dimension\n        if np.std(x_i) &lt; 1e-10:\n            sensitivities.append(0.0)\n            continue\n\n        # Compute correlation with objective\n        try:\n            correlation = np.abs(np.corrcoef(x_i, self.y_)[0, 1])\n            if np.isnan(correlation):\n                correlation = 0.0\n        except Exception:\n            correlation = 0.0\n\n        sensitivities.append(correlation)\n\n    # Normalize to percentage\n    total = sum(sensitivities)\n    if total &gt; 0:\n        importance = [(s / total) * 100 for s in sensitivities]\n    else:\n        importance = [0.0] * len(sensitivities)\n\n    return importance\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.get_initial_design","title":"<code>get_initial_design(X0=None)</code>","text":"<p>Generate or process initial design points.</p> <p>Handles three scenarios: 1. X0 is None: Generate space-filling design using LHS 2. X0 is None but x0 is provided: Generate LHS and include x0 as first point 3. X0 is provided: Transform and prepare user-provided initial design</p> <p>Parameters:</p> Name Type Description Default <code>X0</code> <code>ndarray</code> <p>User-provided initial design points in original scale, shape (n_initial, n_features). If None, generates space-filling design. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>Initial design points in internal (transformed and reduced) scale, shape (n_initial, n_features_reduced).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (-5, 5)],\n...     n_initial=10\n... )\n&gt;&gt;&gt; # Generate default LHS design\n&gt;&gt;&gt; X0 = opt.get_initial_design()\n&gt;&gt;&gt; X0.shape\n(10, 2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Provide custom initial design\n&gt;&gt;&gt; X0_custom = np.array([[0, 0], [1, 1], [2, 2]])\n&gt;&gt;&gt; X0_processed = opt.get_initial_design(X0_custom)\n&gt;&gt;&gt; X0_processed.shape\n(3, 2)\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def get_initial_design(self, X0: Optional[np.ndarray] = None) -&gt; np.ndarray:\n    \"\"\"Generate or process initial design points.\n\n    Handles three scenarios:\n    1. X0 is None: Generate space-filling design using LHS\n    2. X0 is None but x0 is provided: Generate LHS and include x0 as first point\n    3. X0 is provided: Transform and prepare user-provided initial design\n\n    Args:\n        X0 (ndarray, optional): User-provided initial design points in original scale,\n            shape (n_initial, n_features). If None, generates space-filling design.\n            Defaults to None.\n\n    Returns:\n        ndarray: Initial design points in internal (transformed and reduced) scale,\n            shape (n_initial, n_features_reduced).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     n_initial=10\n        ... )\n        &gt;&gt;&gt; # Generate default LHS design\n        &gt;&gt;&gt; X0 = opt.get_initial_design()\n        &gt;&gt;&gt; X0.shape\n        (10, 2)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Provide custom initial design\n        &gt;&gt;&gt; X0_custom = np.array([[0, 0], [1, 1], [2, 2]])\n        &gt;&gt;&gt; X0_processed = opt.get_initial_design(X0_custom)\n        &gt;&gt;&gt; X0_processed.shape\n        (3, 2)\n    \"\"\"\n    # Generate or use provided initial design\n    if X0 is None:\n        X0 = self._generate_initial_design()\n\n        # If starting point x0 was provided, include it in initial design\n        if self.x0 is not None:\n            # x0 is already validated and in internal scale\n            # Check if x0 is 1D or 2D\n            if self.x0.ndim == 1:\n                x0_points = self.x0.reshape(1, -1)\n            else:\n                x0_points = self.x0\n\n            n_x0 = x0_points.shape[0]\n\n            # If we have more x0 points than n_initial, use all x0 points\n            if n_x0 &gt;= self.n_initial:\n                X0 = x0_points\n                if self.verbose:\n                    print(f\"Using provided x0 points ({n_x0}) as initial design.\")\n            else:\n                # Replace the first n_x0 points of LHS with x0 points\n                X0 = np.vstack([x0_points, X0[:-n_x0]])\n                if self.verbose:\n                    print(\n                        f\"Including {n_x0} starting points from x0 in initial design.\"\n                    )\n    else:\n        X0 = np.atleast_2d(X0)\n        # If user provided X0, it's in original scale - transform it\n        X0 = self._transform_X(X0)\n        # If X0 is in full dimensions and we have dimension reduction, reduce it\n        if self.red_dim and X0.shape[1] == len(self.ident):\n            X0 = self.to_red_dim(X0)\n        X0 = self._repair_non_numeric(X0, self.var_type)\n\n    return X0\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.get_results_table","title":"<code>get_results_table(tablefmt='github', precision=4, show_importance=False)</code>","text":"<p>Get a comprehensive table string of optimization results.</p> <p>This method generates a formatted table of the search space configuration, best values found, and optionally variable importance scores.</p> <p>Parameters:</p> Name Type Description Default <code>tablefmt</code> <code>str</code> <p>Table format for tabulate library. Options include: \u2018github\u2019, \u2018grid\u2019, \u2018simple\u2019, \u2018plain\u2019, \u2018html\u2019, \u2018latex\u2019, etc. Defaults to \u2018github\u2019.</p> <code>'github'</code> <code>precision</code> <code>int</code> <p>Number of decimal places for float values. Defaults to 4.</p> <code>4</code> <code>show_importance</code> <code>bool</code> <p>Whether to include importance scores. Importance is calculated as the normalized standard deviation of each parameter\u2019s effect on the objective. Requires multiple evaluations. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted table string that can be printed or saved.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 1: Basic usage after optimization\n&gt;&gt;&gt; def sphere(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=sphere,\n...     bounds=[(-5, 5), (-5, 5), (-5, 5)],\n...     var_name=[\"x1\", \"x2\", \"x3\"],\n...     var_type=[\"float\", \"float\", \"float\"],\n...     max_iter=30,\n...     n_initial=10\n... )\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt; table = opt.get_results_table()\n&gt;&gt;&gt; print(table)\n| name   | type   |   lower |   upper |   tuned |\n|--------|--------|---------|---------|---------|\n| x1     | num    |    -5.0 |     5.0 |  0.0123 |\n| x2     | num    |    -5.0 |     5.0 | -0.0234 |\n| x3     | num    |    -5.0 |     5.0 |  0.0345 |\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 2: With importance scores\n&gt;&gt;&gt; table = opt.get_results_table(show_importance=True)\n&gt;&gt;&gt; print(table)\n| name   | type   |   lower |   upper |   tuned |   importance | stars   |\n|--------|--------|---------|---------|---------|--------------|---------|\n| x1     | num    |    -5.0 |     5.0 |  0.0123 |        45.23 | **      |\n| x2     | num    |    -5.0 |     5.0 | -0.0234 |        32.17 | *       |\n| x3     | num    |    -5.0 |     5.0 |  0.0345 |        22.60 | *       |\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 3: Different table format\n&gt;&gt;&gt; table = opt.get_results_table(tablefmt=\"grid\")\n&gt;&gt;&gt; print(table)\n+--------+--------+---------+---------+---------+\n| name   | type   |   lower |   upper |   tuned |\n+========+========+=========+=========+=========+\n| x1     | num    |    -5.0 |     5.0 |  0.0123 |\n+--------+--------+---------+---------+---------+\n...\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 4: With factor variables\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (\"red\", \"green\", \"blue\")],\n...     var_name=[\"size\", \"color\"],\n...     var_type=[\"float\", \"factor\"],\n...     max_iter=20,\n...     n_initial=10\n... )\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt; table = opt.get_results_table()\n&gt;&gt;&gt; print(table)\n| name   | type   | lower   | upper   | tuned   |\n|--------|--------|---------|---------|---------|\n| size   | num    | -5.0    | 5.0     | 0.0123  |\n| color  | factor | red     | blue    | green   |\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def get_results_table(\n    self,\n    tablefmt: str = \"github\",\n    precision: int = 4,\n    show_importance: bool = False,\n) -&gt; str:\n    \"\"\"Get a comprehensive table string of optimization results.\n\n    This method generates a formatted table of the search space configuration,\n    best values found, and optionally variable importance scores.\n\n    Args:\n        tablefmt (str, optional): Table format for tabulate library. Options include:\n            'github', 'grid', 'simple', 'plain', 'html', 'latex', etc.\n            Defaults to 'github'.\n        precision (int, optional): Number of decimal places for float values.\n            Defaults to 4.\n        show_importance (bool, optional): Whether to include importance scores.\n            Importance is calculated as the normalized standard deviation of each\n            parameter's effect on the objective. Requires multiple evaluations.\n            Defaults to False.\n\n    Returns:\n        str: Formatted table string that can be printed or saved.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 1: Basic usage after optimization\n        &gt;&gt;&gt; def sphere(X):\n        ...     return np.sum(X**2, axis=1)\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=sphere,\n        ...     bounds=[(-5, 5), (-5, 5), (-5, 5)],\n        ...     var_name=[\"x1\", \"x2\", \"x3\"],\n        ...     var_type=[\"float\", \"float\", \"float\"],\n        ...     max_iter=30,\n        ...     n_initial=10\n        ... )\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt; table = opt.get_results_table()\n        &gt;&gt;&gt; print(table)\n        | name   | type   |   lower |   upper |   tuned |\n        |--------|--------|---------|---------|---------|\n        | x1     | num    |    -5.0 |     5.0 |  0.0123 |\n        | x2     | num    |    -5.0 |     5.0 | -0.0234 |\n        | x3     | num    |    -5.0 |     5.0 |  0.0345 |\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 2: With importance scores\n        &gt;&gt;&gt; table = opt.get_results_table(show_importance=True)\n        &gt;&gt;&gt; print(table)\n        | name   | type   |   lower |   upper |   tuned |   importance | stars   |\n        |--------|--------|---------|---------|---------|--------------|---------|\n        | x1     | num    |    -5.0 |     5.0 |  0.0123 |        45.23 | **      |\n        | x2     | num    |    -5.0 |     5.0 | -0.0234 |        32.17 | *       |\n        | x3     | num    |    -5.0 |     5.0 |  0.0345 |        22.60 | *       |\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 3: Different table format\n        &gt;&gt;&gt; table = opt.get_results_table(tablefmt=\"grid\")\n        &gt;&gt;&gt; print(table)\n        +--------+--------+---------+---------+---------+\n        | name   | type   |   lower |   upper |   tuned |\n        +========+========+=========+=========+=========+\n        | x1     | num    |    -5.0 |     5.0 |  0.0123 |\n        +--------+--------+---------+---------+---------+\n        ...\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 4: With factor variables\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (\"red\", \"green\", \"blue\")],\n        ...     var_name=[\"size\", \"color\"],\n        ...     var_type=[\"float\", \"factor\"],\n        ...     max_iter=20,\n        ...     n_initial=10\n        ... )\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt; table = opt.get_results_table()\n        &gt;&gt;&gt; print(table)\n        | name   | type   | lower   | upper   | tuned   |\n        |--------|--------|---------|---------|---------|\n        | size   | num    | -5.0    | 5.0     | 0.0123  |\n        | color  | factor | red     | blue    | green   |\n    \"\"\"\n    if self.best_x_ is None or self.best_y_ is None:\n        return \"No optimization results available. Run optimize() first.\"\n\n    # Get best solution in full dimensions\n    # Note: best_x_ is already in original scale\n    if self.red_dim:\n        best_x_full = self.to_all_dim(self.best_x_.reshape(1, -1))[0]\n    else:\n        best_x_full = self.best_x_\n\n    # Map factor variables back to original string values\n    best_x_display = self._map_to_factor_values(best_x_full.reshape(1, -1))[0]\n\n    # Prepare all variable transformations (use all_var_trans if dimension reduction occurred)\n    if self.red_dim and hasattr(self, \"all_var_trans\"):\n        all_var_trans = self.all_var_trans\n    else:\n        all_var_trans = self.var_trans\n\n    # Prepare table data\n    table_data = {\n        \"name\": (\n            self.all_var_name\n            if self.all_var_name\n            else [f\"x{i}\" for i in range(len(best_x_display))]\n        ),\n        \"type\": (\n            self.all_var_type\n            if self.all_var_type\n            else [\"float\"] * len(best_x_display)\n        ),\n        \"default\": [],\n        \"lower\": [],\n        \"upper\": [],\n        \"tuned\": [],\n        \"transform\": [t if t is not None else \"-\" for t in all_var_trans],\n    }\n\n    # Helper to format values\n    def fmt_val(v):\n        if isinstance(v, (float, np.floating)):\n            return f\"{v:.{precision}f}\"\n        return v\n\n    # Process bounds, defaults, and tuned values\n    for i in range(len(best_x_display)):\n        var_type = table_data[\"type\"][i]\n\n        # Handle bounds and defaults based on variable type\n        if var_type == \"factor\":\n            # For factors, show original string values\n            if i in self._factor_maps:\n                factor_map = self._factor_maps[i]\n                # Default is middle level logic (matching get_design_table)\n                mid_idx = len(factor_map) // 2\n                default_str = factor_map[mid_idx]\n\n                table_data[\"lower\"].append(\"-\")\n                table_data[\"upper\"].append(\"-\")\n                table_data[\"default\"].append(default_str)\n            else:\n                table_data[\"lower\"].append(\"-\")\n                table_data[\"upper\"].append(\"-\")\n                table_data[\"default\"].append(\"N/A\")\n        else:\n            table_data[\"lower\"].append(fmt_val(self._original_lower[i]))\n            table_data[\"upper\"].append(fmt_val(self._original_upper[i]))\n            # Default is midpoint logic\n            default_val = (self._original_lower[i] + self._original_upper[i]) / 2\n            if var_type == \"int\":\n                table_data[\"default\"].append(int(default_val))\n            else:\n                table_data[\"default\"].append(fmt_val(default_val))\n\n        # Format tuned value\n        tuned_val = best_x_display[i]\n        if var_type == \"int\":\n            table_data[\"tuned\"].append(int(tuned_val))\n        elif var_type == \"factor\":\n            table_data[\"tuned\"].append(str(tuned_val))\n        else:\n            table_data[\"tuned\"].append(fmt_val(tuned_val))\n\n    # Add importance if requested\n    if show_importance:\n        importance = self.get_importance()\n        table_data[\"importance\"] = [f\"{x:.2f}\" for x in importance]\n        table_data[\"stars\"] = self.get_stars(importance)\n\n    # Generate table\n    table = tabulate(\n        table_data,\n        headers=\"keys\",\n        tablefmt=tablefmt,\n        numalign=\"right\",\n        stralign=\"right\",\n    )\n\n    # Add interpretation if importance is shown\n    if show_importance:\n        table += \"\\n\\nInterpretation: ***: &gt;99%, **: &gt;75%, *: &gt;50%, .: &gt;10%\"\n\n    return table\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.get_stars","title":"<code>get_stars(input_list)</code>","text":"<p>Converts a list of values to a list of stars.</p> <p>Used to visualize the importance of a variable. Thresholds: &gt;99: , &gt;75: , &gt;50: , &gt;10: .</p> <p>Parameters:</p> Name Type Description Default <code>input_list</code> <code>list</code> <p>A list of importance scores (0-100).</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of star strings.</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def get_stars(self, input_list: list) -&gt; list:\n    \"\"\"Converts a list of values to a list of stars.\n\n    Used to visualize the importance of a variable.\n    Thresholds: &gt;99: ***, &gt;75: **, &gt;50: *, &gt;10: .\n\n    Args:\n        input_list (list): A list of importance scores (0-100).\n\n    Returns:\n        list: A list of star strings.\n    \"\"\"\n    output_list = []\n    for value in input_list:\n        if value &gt; 99:\n            output_list.append(\"***\")\n        elif value &gt; 75:\n            output_list.append(\"**\")\n        elif value &gt; 50:\n            output_list.append(\"*\")\n        elif value &gt; 10:\n            output_list.append(\".\")\n        else:\n            output_list.append(\"\")\n    return output_list\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.handle_default_var_trans","title":"<code>handle_default_var_trans()</code>","text":"<p>Handle default variable transformations.</p> <p>Sets var_trans to a list of None values if not specified, or normalizes transformation names by converting \u2018id\u2019, \u2018None\u2019, or None to None.</p> <p>Also validates that var_trans length matches the number of dimensions.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If var_trans length doesn\u2019t match n_dim.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; # Default behavior - all None\n&gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(0, 10), (0, 10)])\n&gt;&gt;&gt; spot.var_trans\n[None, None]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Normalize transformation names\n&gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10), (1, 100)],\n...                  var_trans=['log10', 'id', None, 'None'])\n&gt;&gt;&gt; spot.var_trans\n['log10', None, None, None]\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def handle_default_var_trans(self) -&gt; None:\n    \"\"\"Handle default variable transformations.\n\n    Sets var_trans to a list of None values if not specified, or normalizes\n    transformation names by converting 'id', 'None', or None to None.\n\n    Also validates that var_trans length matches the number of dimensions.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If var_trans length doesn't match n_dim.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; # Default behavior - all None\n        &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(0, 10), (0, 10)])\n        &gt;&gt;&gt; spot.var_trans\n        [None, None]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Normalize transformation names\n        &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10), (1, 100)],\n        ...                  var_trans=['log10', 'id', None, 'None'])\n        &gt;&gt;&gt; spot.var_trans\n        ['log10', None, None, None]\n    \"\"\"\n    # Default variable transformations (None means no transformation)\n    if self.var_trans is None:\n        self.var_trans = [None] * self.n_dim\n    else:\n        # Normalize transformation names\n        self.var_trans = [\n            None if (t is None or t == \"id\" or t == \"None\") else t\n            for t in self.var_trans\n        ]\n\n    # Validate var_trans length\n    if len(self.var_trans) != self.n_dim:\n        raise ValueError(\n            f\"Length of var_trans ({len(self.var_trans)}) must match \"\n            f\"number of dimensions ({self.n_dim})\"\n        )\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.inverse_transform_value","title":"<code>inverse_transform_value(x, trans)</code>","text":"<p>Apply inverse transformation to a single float value.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>Transformed value</p> required <code>trans</code> <code>Optional[str]</code> <p>Transformation name.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Original value</p> Notes <p>See also transform_value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10)])\n&gt;&gt;&gt; spot.inverse_transform_value(10, 'log10')\n10.0\n&gt;&gt;&gt; spot.inverse_transform_value(100, 'log(x)')\n10.0\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def inverse_transform_value(self, x: float, trans: Optional[str]) -&gt; float:\n    \"\"\"Apply inverse transformation to a single float value.\n\n    Args:\n        x: Transformed value\n        trans: Transformation name.\n\n    Returns:\n        Original value\n\n    Notes:\n        See also transform_value.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10)])\n        &gt;&gt;&gt; spot.inverse_transform_value(10, 'log10')\n        10.0\n        &gt;&gt;&gt; spot.inverse_transform_value(100, 'log(x)')\n        10.0\n    \"\"\"\n    # Ensure x is a float\n    if not isinstance(x, float):\n        try:\n            x = float(x)\n        except (ValueError, TypeError):\n            raise TypeError(\n                f\"transform_value expects a float, got {type(x).__name__} (value: {x})\"\n            )\n    if trans is None or trans == \"id\":\n        return x\n    elif trans == \"log10\":\n        return 10**x\n    elif trans == \"log\" or trans == \"ln\":\n        return np.exp(x)\n    elif trans == \"sqrt\":\n        return x**2\n    elif trans == \"exp\":\n        return np.log(x)\n    elif trans == \"square\":\n        return np.sqrt(x)\n    elif trans == \"cube\":\n        return np.power(x, 1.0 / 3.0)\n    elif trans == \"inv\" or trans == \"reciprocal\":\n        return 1.0 / x\n\n    # Dynamic Transformations (Inverses)\n    if trans == \"log(x)\":\n        return np.exp(x)\n    if trans == \"sqrt(x)\":\n        return x**2\n\n    m = re.match(r\"pow\\(x,\\s*([0-9.]+)\\)\", trans)\n    if m:\n        p = float(m.group(1))\n        return x ** (1.0 / p)\n\n    m = re.match(r\"pow\\(([0-9.]+),\\s*x\\)\", trans)\n    if m:\n        base = float(m.group(1))\n        return np.log(x) / np.log(base)\n\n    m = re.match(r\"log\\(x,\\s*([0-9.]+)\\)\", trans)\n    if m:\n        base = float(m.group(1))\n        return base**x\n\n    raise ValueError(f\"Unknown transformation: {trans}\")\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.load_experiment","title":"<code>load_experiment(filename)</code>  <code>staticmethod</code>","text":"<p>Load an experiment configuration from a pickle file.</p> <p>Loads an experiment that was saved with save_experiment(). The loaded optimizer will have the configuration and the objective function (thanks to dill).</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the experiment pickle file.</p> required <p>Returns:</p> Name Type Description <code>SpotOptim</code> <code>SpotOptim</code> <p>Loaded optimizer instance (without fun attached).</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file doesn\u2019t exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load experiment\n&gt;&gt;&gt; opt = SpotOptim.load_experiment(\"sphere_opt_exp.pkl\")\nLoaded experiment from sphere_opt_exp.pkl\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Re-attach objective function\n&gt;&gt;&gt; opt.fun = lambda X: np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Run optimization\n&gt;&gt;&gt; result = opt.optimize()\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>@staticmethod\ndef load_experiment(filename: str) -&gt; \"SpotOptim\":\n    \"\"\"Load an experiment configuration from a pickle file.\n\n    Loads an experiment that was saved with save_experiment(). The loaded optimizer\n    will have the configuration and the objective function (thanks to dill).\n\n\n    Args:\n        filename (str): Path to the experiment pickle file.\n\n    Returns:\n        SpotOptim: Loaded optimizer instance (without fun attached).\n\n    Raises:\n        FileNotFoundError: If the specified file doesn't exist.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load experiment\n        &gt;&gt;&gt; opt = SpotOptim.load_experiment(\"sphere_opt_exp.pkl\")\n        Loaded experiment from sphere_opt_exp.pkl\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Re-attach objective function\n        &gt;&gt;&gt; opt.fun = lambda X: np.sum(X**2, axis=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Run optimization\n        &gt;&gt;&gt; result = opt.optimize()\n    \"\"\"\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"Experiment file not found: {filename}\")\n\n    try:\n        with open(filename, \"rb\") as handle:\n            optimizer = dill.load(handle)\n        print(f\"Loaded experiment from {filename}\")\n\n        # Reinitialize components that were excluded\n        optimizer._reinitialize_components()\n\n        return optimizer\n    except Exception as e:\n        print(f\"Error loading experiment: {e}\")\n        raise\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.load_result","title":"<code>load_result(filename)</code>  <code>staticmethod</code>","text":"<p>Load complete optimization results from a pickle file.</p> <p>Loads results that were saved with save_result(). The loaded optimizer will have both configuration and all optimization results.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the result pickle file.</p> required <p>Returns:</p> Name Type Description <code>SpotOptim</code> <code>SpotOptim</code> <p>Loaded optimizer instance with complete results.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file doesn\u2019t exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load results\n&gt;&gt;&gt; opt = SpotOptim.load_result(\"sphere_opt_res.pkl\")\nLoaded result from sphere_opt_res.pkl\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Analyze results\n&gt;&gt;&gt; print(\"Best point:\", opt.best_x_)\n&gt;&gt;&gt; print(\"Best value:\", opt.best_y_)\n&gt;&gt;&gt; print(\"Total evaluations:\", opt.counter)\n&gt;&gt;&gt; print(\"Success rate:\", opt.success_rate)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Continue optimization if needed\n&gt;&gt;&gt; # opt.fun = lambda X: np.sum(X**2, axis=1)  # Re-attach if continuing\n&gt;&gt;&gt; # opt.max_iter = 50  # Increase budget\n&gt;&gt;&gt; # result = opt.optimize()\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>@staticmethod\ndef load_result(filename: str) -&gt; \"SpotOptim\":\n    \"\"\"Load complete optimization results from a pickle file.\n\n    Loads results that were saved with save_result(). The loaded optimizer\n    will have both configuration and all optimization results.\n\n    Args:\n        filename (str): Path to the result pickle file.\n\n    Returns:\n        SpotOptim: Loaded optimizer instance with complete results.\n\n    Raises:\n        FileNotFoundError: If the specified file doesn't exist.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load results\n        &gt;&gt;&gt; opt = SpotOptim.load_result(\"sphere_opt_res.pkl\")\n        Loaded result from sphere_opt_res.pkl\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Analyze results\n        &gt;&gt;&gt; print(\"Best point:\", opt.best_x_)\n        &gt;&gt;&gt; print(\"Best value:\", opt.best_y_)\n        &gt;&gt;&gt; print(\"Total evaluations:\", opt.counter)\n        &gt;&gt;&gt; print(\"Success rate:\", opt.success_rate)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Continue optimization if needed\n        &gt;&gt;&gt; # opt.fun = lambda X: np.sum(X**2, axis=1)  # Re-attach if continuing\n        &gt;&gt;&gt; # opt.max_iter = 50  # Increase budget\n        &gt;&gt;&gt; # result = opt.optimize()\n    \"\"\"\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"Result file not found: {filename}\")\n\n    try:\n        with open(filename, \"rb\") as handle:\n            optimizer = dill.load(handle)\n        print(f\"Loaded result from {filename}\")\n\n        # Reinitialize components that were excluded\n        optimizer._reinitialize_components()\n\n        return optimizer\n    except Exception as e:\n        print(f\"Error loading result: {e}\")\n        raise\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.modify_bounds_based_on_var_type","title":"<code>modify_bounds_based_on_var_type()</code>","text":"<p>Modify bounds based on variable types.</p> <p>Adjusts bounds for each dimension according to its var_type: - \u2018int\u2019: Ensures bounds are integers (ceiling for lower, floor for upper) - \u2018factor\u2019: Bounds already set to (0, n_levels-1) by process_factor_bounds - \u2018float\u2019: Explicitly converts bounds to float</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported var_type is encountered.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(0.5, 10.5)], var_type=['int'])\n&gt;&gt;&gt; spot.bounds\n[(1, 10)]\n&gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(0, 10)], var_type=['float'])\n&gt;&gt;&gt; spot.bounds\n[(0.0, 10.0)]\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def modify_bounds_based_on_var_type(self) -&gt; None:\n    \"\"\"Modify bounds based on variable types.\n\n    Adjusts bounds for each dimension according to its var_type:\n    - 'int': Ensures bounds are integers (ceiling for lower, floor for upper)\n    - 'factor': Bounds already set to (0, n_levels-1) by process_factor_bounds\n    - 'float': Explicitly converts bounds to float\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If an unsupported var_type is encountered.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(0.5, 10.5)], var_type=['int'])\n        &gt;&gt;&gt; spot.bounds\n        [(1, 10)]\n        &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(0, 10)], var_type=['float'])\n        &gt;&gt;&gt; spot.bounds\n        [(0.0, 10.0)]\n    \"\"\"\n    for i, vtype in enumerate(self.var_type):\n        if vtype == \"int\":\n            # For integer variables, ensure bounds are integers\n            # Use Python's int() to convert numpy types to native Python int\n            lower = int(np.ceil(self.bounds[i][0]))\n            upper = int(np.floor(self.bounds[i][1]))\n            self.bounds[i] = (lower, upper)\n        elif vtype == \"factor\":\n            # For factor variables, bounds are already set to (0, n_levels-1)\n            # Ensure they are Python int, not numpy int64\n            lower = int(self.bounds[i][0])\n            upper = int(self.bounds[i][1])\n            self.bounds[i] = (lower, upper)\n        elif vtype == \"float\":\n            # Continuous variable, convert explicitly to float bounds\n            # Use Python's float() to convert numpy types to native Python float\n            lower = float(self.bounds[i][0])\n            upper = float(self.bounds[i][1])\n            self.bounds[i] = (lower, upper)\n        else:\n            raise ValueError(\n                f\"Unsupported var_type '{vtype}' at dimension {i}. \"\n                f\"Supported types are 'float', 'int', 'factor'.\"\n            )\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.optimize","title":"<code>optimize(X0=None)</code>","text":"<p>Run the optimization process.</p> <p>The optimization terminates when either: - Total function evaluations reach max_iter (including initial design), OR - Runtime exceeds max_time minutes</p> <p>Input/Output Spaces: - Input X0: Expected in Natural Space (original scale, physical units). - Output result.x: Returned in Natural Space. - Output result.X: Returned in Natural Space. - Internal Optimization: Performed in Transformed and Mapped Space.</p> <p>Parameters:</p> Name Type Description Default <code>X0</code> <code>ndarray</code> <p>Initial design points in Natural Space, shape (n_initial, n_features). If None, generates space-filling design. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>OptimizeResult</code> <code>OptimizeResult</code> <p>Optimization result with fields: - x: best point found in Natural Space - fun: best function value - nfev: number of function evaluations (including initial design) - nit: number of sequential optimization iterations (after initial design) - success: whether optimization succeeded - message: termination message indicating reason for stopping, including   statistics (function value, iterations, evaluations) - X: all evaluated points in Natural Space - y: all function values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (-5, 5)],\n...     n_initial=5,\n...     max_iter=20,\n...     seed=0,\n...     x0=np.array([0.0, 0.0]),\n...     verbose=True\n... )\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt; print(result.message.splitlines()[0])\nOptimization terminated: maximum evaluations (20) reached\n&gt;&gt;&gt; print(\"Best point:\", result.x)\nBest point: [0. 0.]\n&gt;&gt;&gt; print(\"Best value:\", result.fun)\nBest value: 0.0\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def optimize(self, X0: Optional[np.ndarray] = None) -&gt; OptimizeResult:\n    \"\"\"Run the optimization process.\n\n    The optimization terminates when either:\n    - Total function evaluations reach max_iter (including initial design), OR\n    - Runtime exceeds max_time minutes\n\n    Input/Output Spaces:\n    - Input X0: Expected in Natural Space (original scale, physical units).\n    - Output result.x: Returned in Natural Space.\n    - Output result.X: Returned in Natural Space.\n    - Internal Optimization: Performed in Transformed and Mapped Space.\n\n    Args:\n        X0 (ndarray, optional): Initial design points in Natural Space, shape (n_initial, n_features).\n            If None, generates space-filling design. Defaults to None.\n\n    Returns:\n        OptimizeResult: Optimization result with fields:\n            - x: best point found in Natural Space\n            - fun: best function value\n            - nfev: number of function evaluations (including initial design)\n            - nit: number of sequential optimization iterations (after initial design)\n            - success: whether optimization succeeded\n            - message: termination message indicating reason for stopping, including\n              statistics (function value, iterations, evaluations)\n            - X: all evaluated points in Natural Space\n            - y: all function values\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     n_initial=5,\n        ...     max_iter=20,\n        ...     seed=0,\n        ...     x0=np.array([0.0, 0.0]),\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt; print(result.message.splitlines()[0])\n        Optimization terminated: maximum evaluations (20) reached\n        &gt;&gt;&gt; print(\"Best point:\", result.x)\n        Best point: [0. 0.]\n        &gt;&gt;&gt; print(\"Best value:\", result.fun)\n        Best value: 0.0\n    \"\"\"\n    # Track results across restarts for final aggregation.\n    self.restarts_results_ = []\n    # Capture start time for timeout enforcement.\n    timeout_start = time.time()\n\n    # Initial run state.\n    current_X0 = X0\n    status = \"START\"\n\n    while True:\n        # Get best result so far if we have results\n        best_res = (\n            min(self.restarts_results_, key=lambda x: x.fun)\n            if self.restarts_results_\n            else None\n        )\n\n        # Compute injected best value for restarts, then run one optimization cycle.\n        # y0_known_val carries the current global best objective so the next\n        # run can skip re-evaluating that known point when restart injection is on.\n        y0_known_val = (\n            best_res.fun\n            if (\n                status == \"RESTART\"\n                and self.restart_inject_best\n                and self.restarts_results_\n            )\n            else None\n        )\n\n        # Calculate remaining budget\n        total_evals_so_far = sum(len(r.y) for r in self.restarts_results_)\n        remaining_iter = self.max_iter - total_evals_so_far\n\n        # If we don't have enough budget for at least initial design (or some minimal amount), stop\n        if remaining_iter &lt; self.n_initial:\n            if self.verbose:\n                print(\"Global budget exhausted. Stopping restarts.\")\n            break\n\n        # Execute one optimization run using the remaining budget; dispatcher\n        # selects sequential vs parallel based on `n_jobs` and returns status/result.\n        status, result = self._execute_optimization_run(\n            timeout_start,\n            current_X0,\n            y0_known=y0_known_val,\n            max_iter_override=remaining_iter,\n        )\n        self.restarts_results_.append(result)\n\n        if status == \"FINISHED\":\n            break\n        elif status == \"RESTART\":\n            # Prepare for a clean restart: let get_initial_design() regenerate the full design.\n            current_X0 = None\n\n            # Find the global best result across completed restarts.\n            if self.restarts_results_:\n                best_res = min(self.restarts_results_, key=lambda r: r.fun)\n\n                if self.restart_inject_best:\n                    # Inject the current global best into the next run's initial design.\n                    # best_res.x is in natural scale; _validate_x0 converts to internal scale\n                    # so the injected point can be mixed with LHS samples.\n                    self.x0 = self._validate_x0(best_res.x)\n                    # Keep current_X0 unset so the initial design is rebuilt around the injected x0.\n                    current_X0 = None\n\n                    if self.verbose:\n                        print(\n                            f\"Restart injection: Using best found point so far as starting point (f(x)={best_res.fun:.6f}).\"\n                        )\n\n            if self.seed is not None and self.n_jobs == 1:\n                # In sequential mode we advance the seed between restarts to diversify the LHS.\n                # Parallel mode increments seeds per worker during dispatch.\n                self.seed += 1\n            # Continue loop\n        else:\n            # Should not happen\n            break\n\n    # Return best result\n    if not self.restarts_results_:\n        return result  # Fallback\n\n    # Find best result based on 'fun'\n    best_result = min(self.restarts_results_, key=lambda r: r.fun)\n\n    # Merge results from all parallel runs (and sequential runs if any)\n    X_all_list = [res.X for res in self.restarts_results_]\n    y_all_list = [res.y for res in self.restarts_results_]\n\n    # Concatenate all evaluations\n    self.X_ = np.vstack(X_all_list)\n    self.y_ = np.concatenate(y_all_list)\n    self.counter = len(self.y_)\n\n    # Aggregated iterations (sum of all runs)\n    self.n_iter_ = sum(getattr(res, \"nit\", 0) for res in self.restarts_results_)\n\n    # Update best solution found\n    self.best_x_ = best_result.x\n    self.best_y_ = best_result.fun\n\n    return best_result\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.optimize_acquisition_func","title":"<code>optimize_acquisition_func()</code>","text":"<p>Optimize the acquisition function to find the next point to evaluate.</p> <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>The optimized point(s). If acquisition_fun_return_size == 1, returns 1D array of shape (n_features,). If acquisition_fun_return_size &gt; 1, returns 2D array of shape (N, n_features), where N is min(acquisition_fun_return_size, population_size).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (-5, 5)],\n...     acquisition='ei'\n... )\n&gt;&gt;&gt; X_train = np.array([[0, 0], [1, 1], [2, 2]])\n&gt;&gt;&gt; y_train = np.array([0, 2, 8])\n&gt;&gt;&gt; opt._fit_surrogate(X_train, y_train)\n&gt;&gt;&gt; x_next = opt.optimize_acquisition_func()\n&gt;&gt;&gt; print(\"Next point to evaluate:\", x_next)\nNext point to evaluate: [some float values]\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def optimize_acquisition_func(self) -&gt; np.ndarray:\n    \"\"\"Optimize the acquisition function to find the next point to evaluate.\n\n    Returns:\n        ndarray: The optimized point(s).\n            If acquisition_fun_return_size == 1, returns 1D array of shape (n_features,).\n            If acquisition_fun_return_size &gt; 1, returns 2D array of shape (N, n_features),\n            where N is min(acquisition_fun_return_size, population_size).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     acquisition='ei'\n        ... )\n        &gt;&gt;&gt; X_train = np.array([[0, 0], [1, 1], [2, 2]])\n        &gt;&gt;&gt; y_train = np.array([0, 2, 8])\n        &gt;&gt;&gt; opt._fit_surrogate(X_train, y_train)\n        &gt;&gt;&gt; x_next = opt.optimize_acquisition_func()\n        &gt;&gt;&gt; print(\"Next point to evaluate:\", x_next)\n        Next point to evaluate: [some float values]\n    \"\"\"\n    if self.acquisition_optimizer == \"tricands\":\n        return self._optimize_acquisition_tricands()\n    elif self.acquisition_optimizer == \"differential_evolution\":\n        return self._optimize_acquisition_de()\n    elif self.acquisition_optimizer == \"de_tricands\":\n        val = self.rng.rand()\n        if val &lt; self.prob_de_tricands:\n            return self._optimize_acquisition_de()\n        else:\n            return self._optimize_acquisition_tricands()\n    else:\n        return self._optimize_acquisition_scipy()\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.plot_importance","title":"<code>plot_importance(threshold=0.0, figsize=(10, 6))</code>","text":"<p>Plot variable importance.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Minimum importance percentage to include in plot.</p> <code>0.0</code> <code>figsize</code> <code>tuple</code> <p>Figure size.</p> <code>(10, 6)</code> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def plot_importance(\n    self, threshold: float = 0.0, figsize: Tuple[int, int] = (10, 6)\n) -&gt; None:\n    \"\"\"Plot variable importance.\n\n    Args:\n        threshold (float): Minimum importance percentage to include in plot.\n        figsize (tuple): Figure size.\n    \"\"\"\n    importance = self.get_importance()\n    names = (\n        self.all_var_name\n        if self.all_var_name\n        else [f\"x{i}\" for i in range(len(importance))]\n    )\n\n    # Filter by threshold\n    filtered_data = [(n, i) for n, i in zip(names, importance) if i &gt;= threshold]\n    filtered_data.sort(key=lambda x: x[1], reverse=True)\n\n    if not filtered_data:\n        print(\"No variables met the importance threshold.\")\n        return\n\n    names, values = zip(*filtered_data)\n\n    plt.figure(figsize=figsize)\n    y_pos = np.arange(len(names))\n    plt.barh(y_pos, values, align=\"center\")\n    plt.yticks(y_pos, names)\n    plt.xlabel(\"Importance (%)\")\n    plt.title(\"Variable Importance\")\n    plt.gca().invert_yaxis()  # Best on top\n    plt.show()\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.plot_important_hyperparameter_contour","title":"<code>plot_important_hyperparameter_contour(max_imp=3, show=True, alpha=0.8, cmap='jet', num=100, add_points=True, grid_visible=True, contour_levels=30, figsize=(12, 10))</code>","text":"<p>Plot surrogate contours using spotoptim.plot.visualization.plot_important_hyperparameter_contour.</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def plot_important_hyperparameter_contour(\n    self,\n    max_imp: int = 3,\n    show: bool = True,\n    alpha: float = 0.8,\n    cmap: str = \"jet\",\n    num: int = 100,\n    add_points: bool = True,\n    grid_visible: bool = True,\n    contour_levels: int = 30,\n    figsize: Tuple[int, int] = (12, 10),\n) -&gt; None:\n    \"\"\"Plot surrogate contours using spotoptim.plot.visualization.plot_important_hyperparameter_contour.\"\"\"\n    plot_important_hyperparameter_contour(\n        self,\n        max_imp=max_imp,\n        show=show,\n        alpha=alpha,\n        cmap=cmap,\n        num=num,\n        add_points=add_points,\n        grid_visible=grid_visible,\n        contour_levels=contour_levels,\n        figsize=figsize,\n    )\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.plot_parameter_scatter","title":"<code>plot_parameter_scatter(result=None, show=True, figsize=(12, 10), ylabel='Objective Value', cmap='viridis_r', show_correlation=False, log_y=False)</code>","text":"<p>Plot parameter distributions showing relationship between each parameter and objective.</p> <p>Creates a grid of scatter plots, one for each parameter dimension, showing how the objective function value varies with each parameter. The best configuration is marked with a red star. Parameters with log-scale transformations (var_trans) are automatically displayed on a log x-axis.</p> <p>Optionally displays Spearman correlation coefficients in plot titles for sensitivity analysis. For factor (categorical) variables, correlation is not computed and they are displayed with discrete positions on the x-axis.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>OptimizeResult</code> <p>Optimization result containing best parameters. If None, uses the best found values from self.best_x_ and self.best_y_.</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (12, 10).</p> <code>(12, 10)</code> <code>ylabel</code> <code>str</code> <p>Label for y-axis. Defaults to \u201cObjective Value\u201d.</p> <code>'Objective Value'</code> <code>cmap</code> <code>str</code> <p>Colormap for scatter plot. Defaults to \u201cviridis_r\u201d.</p> <code>'viridis_r'</code> <code>show_correlation</code> <code>bool</code> <p>Whether to compute and display Spearman correlation coefficients in plot titles. Requires scipy. Defaults to False.</p> <code>False</code> <code>log_y</code> <code>bool</code> <p>Whether to use logarithmic scale for y-axis. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no optimization data is available.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; def objective(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=objective,\n...     bounds=[(-5, 5), (-5, 5), (-5, 5), (-5, 5)],\n...     var_name=[\"x0\", \"x1\", \"x2\", \"x3\"],\n...     max_iter=30,\n...     n_initial=10,\n...     seed=42\n... )\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt; # Plot parameter distributions\n&gt;&gt;&gt; opt.plot_parameter_scatter(result)\n&gt;&gt;&gt; # Plot with custom settings\n&gt;&gt;&gt; opt.plot_parameter_scatter(result, cmap=\"plasma\", ylabel=\"Error\")\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def plot_parameter_scatter(\n    self,\n    result: Optional[OptimizeResult] = None,\n    show: bool = True,\n    figsize: Tuple[int, int] = (12, 10),\n    ylabel: str = \"Objective Value\",\n    cmap: str = \"viridis_r\",\n    show_correlation: bool = False,\n    log_y: bool = False,\n) -&gt; None:\n    \"\"\"Plot parameter distributions showing relationship between each parameter and objective.\n\n    Creates a grid of scatter plots, one for each parameter dimension, showing how\n    the objective function value varies with each parameter. The best configuration\n    is marked with a red star. Parameters with log-scale transformations (var_trans)\n    are automatically displayed on a log x-axis.\n\n    Optionally displays Spearman correlation coefficients in plot titles for\n    sensitivity analysis. For factor (categorical) variables, correlation is not\n    computed and they are displayed with discrete positions on the x-axis.\n\n    Args:\n        result (OptimizeResult, optional): Optimization result containing best parameters.\n            If None, uses the best found values from self.best_x_ and self.best_y_.\n        show (bool, optional): Whether to display the plot. Defaults to True.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (12, 10).\n        ylabel (str, optional): Label for y-axis. Defaults to \"Objective Value\".\n        cmap (str, optional): Colormap for scatter plot. Defaults to \"viridis_r\".\n        show_correlation (bool, optional): Whether to compute and display Spearman\n            correlation coefficients in plot titles. Requires scipy. Defaults to False.\n        log_y (bool, optional): Whether to use logarithmic scale for y-axis.\n            Defaults to False.\n\n    Raises:\n        ValueError: If no optimization data is available.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; def objective(X):\n        ...     return np.sum(X**2, axis=1)\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=objective,\n        ...     bounds=[(-5, 5), (-5, 5), (-5, 5), (-5, 5)],\n        ...     var_name=[\"x0\", \"x1\", \"x2\", \"x3\"],\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt; # Plot parameter distributions\n        &gt;&gt;&gt; opt.plot_parameter_scatter(result)\n        &gt;&gt;&gt; # Plot with custom settings\n        &gt;&gt;&gt; opt.plot_parameter_scatter(result, cmap=\"plasma\", ylabel=\"Error\")\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        raise ImportError(\n            \"matplotlib is required for plot_parameter_scatter(). \"\n            \"Install it with: pip install matplotlib\"\n        )\n\n    # Import scipy if correlation is requested\n    if show_correlation:\n        try:\n            from scipy.stats import spearmanr\n        except ImportError:\n            raise ImportError(\n                \"scipy is required for show_correlation=True. \"\n                \"Install it with: pip install scipy\"\n            )\n\n    if self.X_ is None or self.y_ is None or len(self.y_) == 0:\n        raise ValueError(\"No optimization data available. Run optimize() first.\")\n\n    # Get best values\n    if result is not None:\n        best_x = result.x\n        best_y = result.fun\n    elif self.best_x_ is not None and self.best_y_ is not None:\n        best_x = self.best_x_\n        best_y = self.best_y_\n    else:\n        raise ValueError(\"No best solution available.\")\n\n    all_params = self.X_\n    history = self.y_\n\n    # Determine grid dimensions\n    n_params = all_params.shape[1]\n    n_cols = min(4, n_params)\n    n_rows = int(np.ceil(n_params / n_cols))\n\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n\n    # Make axes always iterable\n    if n_params == 1:\n        axes = np.array([axes])\n    axes_flat = axes.flatten() if n_params &gt; 1 else axes\n\n    for idx in range(n_params):\n        ax = axes_flat[idx]\n        param_values = all_params[:, idx]\n        param_name = self.var_name[idx] if self.var_name else f\"x{idx}\"\n        var_type = self.var_type[idx] if self.var_type else \"float\"\n        var_trans = self.var_trans[idx] if self.var_trans else None\n\n        # Check if this is a factor variable\n        is_factor = var_type == \"factor\"\n\n        # Compute correlation if requested\n        corr, p_value = np.nan, np.nan\n        if show_correlation and not is_factor:\n            try:\n                if var_trans in [\"log10\", \"log\", \"ln\"]:\n                    # For log-transformed parameters, correlate in log-space\n                    param_values_numeric = param_values.astype(float)\n                    valid_mask = (param_values_numeric &gt; 0) &amp; (history &gt; 0)\n                    if valid_mask.sum() &gt;= 3:\n                        corr, p_value = spearmanr(\n                            np.log10(param_values_numeric[valid_mask]),\n                            np.log10(history[valid_mask]),\n                        )\n                else:\n                    # Direct correlation for non-transformed parameters\n                    param_values_numeric = param_values.astype(float)\n                    corr, p_value = spearmanr(param_values_numeric, history)\n            except (ValueError, TypeError):\n                pass  # Keep corr as nan\n\n        # Handle factor variables differently\n        if is_factor:\n            # Map factor levels to integer positions\n            unique_vals = np.unique(param_values)\n            positions = {val: i for i, val in enumerate(unique_vals)}\n            numeric_vals = np.array([positions[val] for val in param_values])\n\n            # Scatter plot with discrete x positions\n            ax.scatter(\n                numeric_vals,\n                history,\n                c=history,\n                cmap=cmap,\n                s=50,\n                alpha=0.7,\n                edgecolors=\"black\",\n                linewidth=0.5,\n            )\n\n            # Mark best configuration\n            best_val = best_x[idx]\n            if best_val not in positions:\n                positions[best_val] = len(positions)\n                unique_vals = np.append(unique_vals, best_val)\n\n            best_pos = positions[best_val]\n            ax.scatter(\n                [best_pos],\n                [best_y],\n                color=\"red\",\n                s=200,\n                marker=\"*\",\n                edgecolors=\"black\",\n                linewidth=1.5,\n                label=\"Best\",\n                zorder=5,\n            )\n\n            # Set categorical x-axis labels\n            ax.set_xticks(range(len(unique_vals)))\n            ax.set_xticklabels(unique_vals, rotation=45, ha=\"right\")\n        else:\n            # Standard scatter plot for numeric variables\n            ax.scatter(\n                param_values,\n                history,\n                c=history,\n                cmap=cmap,\n                s=50,\n                alpha=0.7,\n                edgecolors=\"black\",\n                linewidth=0.5,\n            )\n\n            # Mark best configuration\n            ax.scatter(\n                [best_x[idx]],\n                [best_y],\n                color=\"red\",\n                s=200,\n                marker=\"*\",\n                edgecolors=\"black\",\n                linewidth=1.5,\n                label=\"Best\",\n                zorder=5,\n            )\n\n            # Use log scale for parameters with log transformations\n            if var_trans in [\"log10\", \"log\", \"ln\"]:\n                ax.set_xscale(\"log\")\n\n        # Set labels\n        ax.set_xlabel(param_name, fontsize=11)\n        ax.set_ylabel(ylabel, fontsize=11)\n\n        # Set title with optional correlation\n        if show_correlation and not np.isnan(corr):\n            ax.set_title(\n                f\"{param_name}\\nCorr: {corr:.3f} (p={p_value:.3f})\", fontsize=11\n            )\n        elif show_correlation and is_factor:\n            ax.set_title(f\"{param_name}\\n(categorical)\", fontsize=11)\n        else:\n            ax.set_title(f\"{param_name} vs {ylabel}\", fontsize=12)\n\n        ax.legend(fontsize=9)\n        ax.grid(True, alpha=0.3)\n\n        # Use log scale for y-axis if requested\n        if log_y:\n            ax.set_yscale(\"log\")\n\n    # Hide unused subplots\n    for idx in range(n_params, len(axes_flat)):\n        axes_flat[idx].set_visible(False)\n\n    plt.tight_layout()\n\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.plot_progress","title":"<code>plot_progress(show=True, log_y=False, figsize=(10, 6), ylabel='Objective Value', mo=False)</code>","text":"<p>Plot optimization progress using spotoptim.plot.visualization.plot_progress.</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def plot_progress(\n    self,\n    show: bool = True,\n    log_y: bool = False,\n    figsize: Tuple[int, int] = (10, 6),\n    ylabel: str = \"Objective Value\",\n    mo: bool = False,\n) -&gt; None:\n    \"\"\"Plot optimization progress using spotoptim.plot.visualization.plot_progress.\"\"\"\n    plot_progress(\n        self, show=show, log_y=log_y, figsize=figsize, ylabel=ylabel, mo=mo\n    )\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.plot_surrogate","title":"<code>plot_surrogate(i=0, j=1, show=True, alpha=0.8, var_name=None, cmap='jet', num=100, vmin=None, vmax=None, add_points=True, grid_visible=True, contour_levels=30, figsize=(12, 10))</code>","text":"<p>Plot the surrogate model for two dimensions.</p> <p>Delegates to spotoptim.plot.visualization.plot_surrogate.</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def plot_surrogate(\n    self,\n    i: int = 0,\n    j: int = 1,\n    show: bool = True,\n    alpha: float = 0.8,\n    var_name: Optional[List[str]] = None,\n    cmap: str = \"jet\",\n    num: int = 100,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    add_points: bool = True,\n    grid_visible: bool = True,\n    contour_levels: int = 30,\n    figsize: Tuple[int, int] = (12, 10),\n) -&gt; None:\n    \"\"\"Plot the surrogate model for two dimensions.\n\n    Delegates to spotoptim.plot.visualization.plot_surrogate.\n    \"\"\"\n    plot_surrogate(\n        self,\n        i=i,\n        j=j,\n        show=show,\n        alpha=alpha,\n        var_name=var_name,\n        cmap=cmap,\n        num=num,\n        vmin=vmin,\n        vmax=vmax,\n        add_points=add_points,\n        grid_visible=grid_visible,\n        contour_levels=contour_levels,\n        figsize=figsize,\n    )\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.print_best","title":"<code>print_best(result=None, transformations=None, show_name=True, precision=4)</code>","text":"<p>Print the best solution found during optimization.</p> <p>This method displays the best hyperparameters and objective value in a formatted table. It supports custom transformations for parameters (e.g., converting log-scale values back to original scale).</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>OptimizeResult</code> <p>Optimization result object from optimize(). If None, uses the stored best values from the optimizer. Defaults to None.</p> <code>None</code> <code>transformations</code> <code>list of callable</code> <p>List of transformation functions to apply to each parameter. Each function takes a single value and returns the transformed value. Use None for parameters that don\u2019t need transformation. Length must match number of dimensions. Example: [None, None, lambda x: 10**x] to convert the 3rd parameter from log10 scale. Defaults to None.</p> <code>None</code> <code>show_name</code> <code>bool</code> <p>Whether to display variable names. If False, uses generic names like \u2018x0\u2019, \u2018x1\u2019, etc. Defaults to True.</p> <code>True</code> <code>precision</code> <code>int</code> <p>Number of decimal places for floating point values. Defaults to 4.</p> <code>4</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 1: Basic usage\n&gt;&gt;&gt; def sphere(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=sphere,\n...     bounds=[(-5, 5), (-5, 5)],\n...     var_name=[\"x1\", \"x2\"],\n...     max_iter=20,\n...     n_initial=10\n... )\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt; opt.print_best(result)\n\nBest Solution Found:\n--------------------------------------------------\n  x1: 0.0123\n  x2: -0.0045\n  Objective Value: 0.000173\n  Total Evaluations: 20\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 2: With log-scale transformations (e.g., for learning rates)\n&gt;&gt;&gt; def objective(X):\n...     # X[:, 0]: neurons (int), X[:, 1]: layers (int),\n...     # X[:, 2]: log10(lr), X[:, 3]: log10(alpha)\n...     return np.sum(X**2, axis=1)  # Placeholder\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=objective,\n...     bounds=[(16, 128), (1, 4), (-3, 0), (-2, 1)],\n...     var_type=[\"int\", \"int\", \"float\", \"float\"],\n...     var_name=[\"neurons\", \"layers\", \"log10_lr\", \"log10_alpha\"],\n...     max_iter=30,\n...     n_initial=10\n... )\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt; # Transform log-scale parameters back to original scale\n&gt;&gt;&gt; transformations = [\n...     int,              # neurons -&gt; int\n...     int,              # layers -&gt; int\n...     lambda x: 10**x,  # log10_lr -&gt; lr\n...     lambda x: 10**x   # log10_alpha -&gt; alpha\n... ]\n&gt;&gt;&gt; opt.print_best(result, transformations=transformations)\n\nBest Solution Found:\n--------------------------------------------------\n  neurons: 64\n  layers: 2\n  log10_lr: 0.0012\n  log10_alpha: 0.0345\n  Objective Value: 1.2345\n  Total Evaluations: 30\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 3: Without result object (using stored values)\n&gt;&gt;&gt; opt.print_best()  # Uses opt.best_x_ and opt.best_y_\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 4: Hide variable names\n&gt;&gt;&gt; opt.print_best(result, show_name=False)\n\nBest Solution Found:\n--------------------------------------------------\n  x0: 0.0123\n  x1: -0.0045\n  Objective Value: 0.000173\n  Total Evaluations: 20\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def print_best(\n    self,\n    result: Optional[OptimizeResult] = None,\n    transformations: Optional[List[Optional[Callable]]] = None,\n    show_name: bool = True,\n    precision: int = 4,\n) -&gt; None:\n    \"\"\"Print the best solution found during optimization.\n\n    This method displays the best hyperparameters and objective value in a\n    formatted table. It supports custom transformations for parameters\n    (e.g., converting log-scale values back to original scale).\n\n    Args:\n        result (OptimizeResult, optional): Optimization result object from optimize().\n            If None, uses the stored best values from the optimizer. Defaults to None.\n        transformations (list of callable, optional): List of transformation functions\n            to apply to each parameter. Each function takes a single value and returns\n            the transformed value. Use None for parameters that don't need transformation.\n            Length must match number of dimensions. Example: [None, None, lambda x: 10**x]\n            to convert the 3rd parameter from log10 scale. Defaults to None.\n        show_name (bool, optional): Whether to display variable names. If False,\n            uses generic names like 'x0', 'x1', etc. Defaults to True.\n        precision (int, optional): Number of decimal places for floating point values.\n            Defaults to 4.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 1: Basic usage\n        &gt;&gt;&gt; def sphere(X):\n        ...     return np.sum(X**2, axis=1)\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=sphere,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     var_name=[\"x1\", \"x2\"],\n        ...     max_iter=20,\n        ...     n_initial=10\n        ... )\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt; opt.print_best(result)\n        &lt;BLANKLINE&gt;\n        Best Solution Found:\n        --------------------------------------------------\n          x1: 0.0123\n          x2: -0.0045\n          Objective Value: 0.000173\n          Total Evaluations: 20\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 2: With log-scale transformations (e.g., for learning rates)\n        &gt;&gt;&gt; def objective(X):\n        ...     # X[:, 0]: neurons (int), X[:, 1]: layers (int),\n        ...     # X[:, 2]: log10(lr), X[:, 3]: log10(alpha)\n        ...     return np.sum(X**2, axis=1)  # Placeholder\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=objective,\n        ...     bounds=[(16, 128), (1, 4), (-3, 0), (-2, 1)],\n        ...     var_type=[\"int\", \"int\", \"float\", \"float\"],\n        ...     var_name=[\"neurons\", \"layers\", \"log10_lr\", \"log10_alpha\"],\n        ...     max_iter=30,\n        ...     n_initial=10\n        ... )\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt; # Transform log-scale parameters back to original scale\n        &gt;&gt;&gt; transformations = [\n        ...     int,              # neurons -&gt; int\n        ...     int,              # layers -&gt; int\n        ...     lambda x: 10**x,  # log10_lr -&gt; lr\n        ...     lambda x: 10**x   # log10_alpha -&gt; alpha\n        ... ]\n        &gt;&gt;&gt; opt.print_best(result, transformations=transformations)\n        &lt;BLANKLINE&gt;\n        Best Solution Found:\n        --------------------------------------------------\n          neurons: 64\n          layers: 2\n          log10_lr: 0.0012\n          log10_alpha: 0.0345\n          Objective Value: 1.2345\n          Total Evaluations: 30\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 3: Without result object (using stored values)\n        &gt;&gt;&gt; opt.print_best()  # Uses opt.best_x_ and opt.best_y_\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 4: Hide variable names\n        &gt;&gt;&gt; opt.print_best(result, show_name=False)\n        &lt;BLANKLINE&gt;\n        Best Solution Found:\n        --------------------------------------------------\n          x0: 0.0123\n          x1: -0.0045\n          Objective Value: 0.000173\n          Total Evaluations: 20\n    \"\"\"\n    # Get values from result or stored attributes\n    if result is not None:\n        best_x = result.x\n        best_y = result.fun\n        n_evals = result.nfev\n    else:\n        if self.best_x_ is None or self.best_y_ is None:\n            print(\"No optimization results available. Run optimize() first.\")\n            return\n        best_x = self.best_x_\n        best_y = self.best_y_\n        n_evals = self.counter\n\n    # Expand to full dimensions if dimension reduction was applied\n    if self.red_dim:\n        best_x_full = self.to_all_dim(best_x.reshape(1, -1))[0]\n    else:\n        best_x_full = best_x\n\n    # Map factor variables back to original string values\n    best_x_full = self._map_to_factor_values(best_x_full.reshape(1, -1))[0]\n\n    # Determine variable names to use\n    if show_name and self.all_var_name is not None:\n        var_names = self.all_var_name\n    else:\n        var_names = [f\"x{i}\" for i in range(len(best_x_full))]\n\n    # Validate transformations length\n    if transformations is not None:\n        if len(transformations) != len(best_x_full):\n            raise ValueError(\n                f\"Length of transformations ({len(transformations)}) must match \"\n                f\"number of dimensions ({len(best_x_full)})\"\n            )\n    else:\n        transformations = [None] * len(best_x_full)\n\n    # Print header\n    print(\"\\nBest Solution Found:\")\n    print(\"-\" * 50)\n\n    # Print each parameter\n    for i, (name, value, transform) in enumerate(\n        zip(var_names, best_x_full, transformations)\n    ):\n        # Apply transformation if provided\n        if transform is not None:\n            try:\n                display_value = transform(value)\n            except Exception as e:\n                print(f\"Warning: Transformation failed for {name}: {e}\")\n                display_value = value\n        else:\n            display_value = value\n\n        # Format based on variable type\n        var_type = self.all_var_type[i] if i &lt; len(self.all_var_type) else \"float\"\n\n        if var_type == \"int\" or isinstance(display_value, (int, np.integer)):\n            print(f\"  {name}: {int(display_value)}\")\n        elif var_type == \"factor\" or isinstance(display_value, str):\n            print(f\"  {name}: {display_value}\")\n        else:\n            print(f\"  {name}: {display_value:.{precision}f}\")\n\n    # Print objective value and evaluations\n    print(f\"  Objective Value: {best_y:.{precision}f}\")\n    print(f\"  Total Evaluations: {n_evals}\")\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.print_design_table","title":"<code>print_design_table(tablefmt='github', precision=4)</code>","text":"<p>Print (and return) a table showing the search space design before optimization.</p> <p>This method calls <code>get_design_table</code> to generate the table string, prints it, and then returns it.</p> <p>Parameters:</p> Name Type Description Default <code>tablefmt</code> <code>str</code> <p>Table format for tabulate library. Defaults to \u2018github\u2019.</p> <code>'github'</code> <code>precision</code> <code>int</code> <p>Number of decimal places for float values. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted table string.</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def print_design_table(\n    self,\n    tablefmt: str = \"github\",\n    precision: int = 4,\n) -&gt; str:\n    \"\"\"Print (and return) a table showing the search space design before optimization.\n\n    This method calls `get_design_table` to generate the table string, prints it,\n    and then returns it.\n\n    Args:\n        tablefmt (str, optional): Table format for tabulate library.\n            Defaults to 'github'.\n        precision (int, optional): Number of decimal places for float values.\n            Defaults to 4.\n\n    Returns:\n        str: Formatted table string.\n    \"\"\"\n    table = self.get_design_table(tablefmt=tablefmt, precision=precision)\n    print(table)\n    return table\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.print_results","title":"<code>print_results(*args, **kwargs)</code>","text":"<p>Alias for print_results_table for compatibility. Prints the table.</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def print_results(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Alias for print_results_table for compatibility.\n    Prints the table.\n    \"\"\"\n    self.print_results_table(*args, **kwargs)\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.print_results_table","title":"<code>print_results_table(tablefmt='github', precision=4, show_importance=False, *args, **kwargs)</code>","text":"<p>Print (and return) a comprehensive table of optimization results.</p> <p>This method calls <code>get_results_table</code> to generate the table string, prints it, and then returns it.</p> <p>Parameters:</p> Name Type Description Default <code>tablefmt</code> <code>str</code> <p>Table format. Defaults to \u2018github\u2019.</p> <code>'github'</code> <code>precision</code> <code>int</code> <p>Decimal precision. Defaults to 4.</p> <code>4</code> <code>show_importance</code> <code>bool</code> <p>Show importance column. Defaults to False.</p> <code>False</code> <code>*args</code> <code>Any</code> <p>Arguments passed to get_results_table.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed to get_results_table.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted table string.</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def print_results_table(\n    self,\n    tablefmt: str = \"github\",\n    precision: int = 4,\n    show_importance: bool = False,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Print (and return) a comprehensive table of optimization results.\n\n    This method calls `get_results_table` to generate the table string, prints it,\n    and then returns it.\n\n    Args:\n        tablefmt (str, optional): Table format. Defaults to 'github'.\n        precision (int, optional): Decimal precision. Defaults to 4.\n        show_importance (bool, optional): Show importance column. Defaults to False.\n        *args: Arguments passed to get_results_table.\n        **kwargs: Keyword arguments passed to get_results_table.\n\n    Returns:\n        str: Formatted table string.\n    \"\"\"\n    table = self.get_results_table(\n        tablefmt=tablefmt,\n        precision=precision,\n        show_importance=show_importance,\n        *args,\n        **kwargs,\n    )\n    print(table)\n    return table\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.process_factor_bounds","title":"<code>process_factor_bounds()</code>","text":"<p>Process bounds to handle factor variables.</p> <p>For dimensions with tuple bounds (factor variables), creates internal integer mappings and replaces bounds with (0, n_levels-1).</p> <p>Stores mappings in self._factor_maps: {dim_idx: {int_val: str_val}}</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If bounds are invalidly formatted.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[('red', 'green', 'blue'), (0, 10)])\n&gt;&gt;&gt; spot.process_factor_bounds()\nFactor variable at dimension 0:\n  Levels: ['red', 'green', 'blue']\n  Mapped to integers: 0 to 2\n&gt;&gt;&gt; print(spot.bounds)\n[(0, 2), (0, 10)]\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def process_factor_bounds(self) -&gt; None:\n    \"\"\"Process bounds to handle factor variables.\n\n    For dimensions with tuple bounds (factor variables), creates internal\n    integer mappings and replaces bounds with (0, n_levels-1).\n\n    Stores mappings in self._factor_maps: {dim_idx: {int_val: str_val}}\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If bounds are invalidly formatted.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[('red', 'green', 'blue'), (0, 10)])\n        &gt;&gt;&gt; spot.process_factor_bounds()\n        Factor variable at dimension 0:\n          Levels: ['red', 'green', 'blue']\n          Mapped to integers: 0 to 2\n        &gt;&gt;&gt; print(spot.bounds)\n        [(0, 2), (0, 10)]\n    \"\"\"\n    processed_bounds = []\n\n    for dim_idx, bound in enumerate(self.bounds):\n        if isinstance(bound, (tuple, list)) and len(bound) &gt;= 1:\n            # Check if this is a factor variable (contains strings)\n            if all(isinstance(v, str) for v in bound) and len(bound) &gt; 0:\n                # Factor variable: create integer mapping\n                factor_levels = list(bound)\n                n_levels = len(factor_levels)\n\n                # Create mapping: {0: \"level1\", 1: \"level2\", ...}\n                self._factor_maps[dim_idx] = {\n                    i: level for i, level in enumerate(factor_levels)\n                }\n\n                # Replace with integer bounds (use Python int, not numpy types)\n                processed_bounds.append((int(0), int(n_levels - 1)))\n\n                if self.verbose:\n                    print(f\"Factor variable at dimension {dim_idx}:\")\n                    print(f\"  Levels: {factor_levels}\")\n                    print(f\"  Mapped to integers: 0 to {n_levels - 1}\")\n            elif len(bound) == 2 and all(\n                isinstance(v, (int, float, np.integer, np.floating)) for v in bound\n            ):\n                # Numeric bound tuple (accepts Python and numpy numeric types)\n                # Always cast to Python float/int\n                low, high = float(bound[0]), float(bound[1])\n\n                # Convert to int if both are integer-valued\n                if low.is_integer() and high.is_integer():\n                    low, high = int(low), int(high)\n\n                processed_bounds.append((low, high))\n            else:\n                raise ValueError(\n                    f\"Invalid bound at dimension {dim_idx}: {bound}. \"\n                    f\"Expected either (lower, upper) for numeric variables or \"\n                    f\"tuple of strings for factor variables.\"\n                )\n        else:\n            raise ValueError(\n                f\"Invalid bound at dimension {dim_idx}: {bound}. \"\n                f\"Expected a tuple/list with at least 1 element.\"\n            )\n\n    # Update bounds with processed values\n    self.bounds = processed_bounds\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.save_experiment","title":"<code>save_experiment(filename=None, prefix='experiment', path=None, overwrite=True, unpickleables='all', verbosity=0)</code>","text":"<p>Save the experiment configuration to a pickle file.</p> <p>An experiment contains the optimizer configuration needed to run optimization, but excludes the results. This is useful for defining experiments locally and executing them on remote machines.</p> <p>The experiment includes: - Bounds, variable types, variable names - Optimization parameters (max_iter, n_initial, etc.) - Surrogate and acquisition settings - Random seed</p> <p>The experiment excludes: - Function evaluations (X_, y_) - Optimization results</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Filename for the experiment file. If None, generates from prefix. Defaults to None.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Prefix for auto-generated filename. Defaults to \u201cexperiment\u201d.</p> <code>'experiment'</code> <code>path</code> <code>str</code> <p>Directory path to save the file. If None, saves in current directory. Creates directory if it doesn\u2019t exist. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If True, overwrites existing file. If False, raises error if file exists. Defaults to True.</p> <code>True</code> <code>unpickleables</code> <code>str</code> <p>Components to exclude for pickling: - \u201call\u201d: Excludes surrogate, lhs_sampler, tb_writer (experiment only) - \u201cfile_io\u201d: Excludes only tb_writer (lighter exclusion) Defaults to \u201call\u201d.</p> <code>'all'</code> <code>verbosity</code> <code>int</code> <p>Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define experiment locally\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (-5, 5)],\n...     max_iter=30,\n...     n_initial=10,\n...     seed=42\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Save experiment (without results)\n&gt;&gt;&gt; opt.save_experiment(prefix=\"sphere_opt\")\nExperiment saved to sphere_opt_exp.pkl\n&gt;&gt;&gt;\n&gt;&gt;&gt; # On remote machine: load and run\n&gt;&gt;&gt; # opt_remote = SpotOptim.load_experiment(\"sphere_opt_exp.pkl\")\n&gt;&gt;&gt; # result = opt_remote.optimize()\n&gt;&gt;&gt; # opt_remote.save_result(prefix=\"sphere_opt\")  # Save results\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def save_experiment(\n    self,\n    filename: Optional[str] = None,\n    prefix: str = \"experiment\",\n    path: Optional[str] = None,\n    overwrite: bool = True,\n    unpickleables: str = \"all\",\n    verbosity: int = 0,\n) -&gt; None:\n    \"\"\"Save the experiment configuration to a pickle file.\n\n    An experiment contains the optimizer configuration needed to run optimization,\n    but excludes the results. This is useful for defining experiments locally and\n    executing them on remote machines.\n\n    The experiment includes:\n    - Bounds, variable types, variable names\n    - Optimization parameters (max_iter, n_initial, etc.)\n    - Surrogate and acquisition settings\n    - Random seed\n\n    The experiment excludes:\n    - Function evaluations (X_, y_)\n    - Optimization results\n\n\n    Args:\n        filename (str, optional): Filename for the experiment file. If None, generates\n            from prefix. Defaults to None.\n        prefix (str): Prefix for auto-generated filename. Defaults to \"experiment\".\n        path (str, optional): Directory path to save the file. If None, saves in current\n            directory. Creates directory if it doesn't exist. Defaults to None.\n        overwrite (bool): If True, overwrites existing file. If False, raises error if\n            file exists. Defaults to True.\n        unpickleables (str): Components to exclude for pickling:\n            - \"all\": Excludes surrogate, lhs_sampler, tb_writer (experiment only)\n            - \"file_io\": Excludes only tb_writer (lighter exclusion)\n            Defaults to \"all\".\n        verbosity (int): Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Define experiment locally\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Save experiment (without results)\n        &gt;&gt;&gt; opt.save_experiment(prefix=\"sphere_opt\")\n        Experiment saved to sphere_opt_exp.pkl\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # On remote machine: load and run\n        &gt;&gt;&gt; # opt_remote = SpotOptim.load_experiment(\"sphere_opt_exp.pkl\")\n        &gt;&gt;&gt; # result = opt_remote.optimize()\n        &gt;&gt;&gt; # opt_remote.save_result(prefix=\"sphere_opt\")  # Save results\n    \"\"\"\n    # Close TensorBoard writer before pickling\n    self._close_and_del_tensorboard_writer()\n\n    # Create pickle-safe copy\n    optimizer_copy = self._get_pickle_safe_optimizer(\n        unpickleables=unpickleables, verbosity=verbosity\n    )\n\n    # Determine filename\n    if filename is None:\n        filename = self._get_experiment_filename(prefix)\n\n    # Add path if provided\n    if path is not None:\n        if not os.path.exists(path):\n            os.makedirs(path)\n        filename = os.path.join(path, filename)\n\n    # Check for existing file\n    if os.path.exists(filename) and not overwrite:\n        raise FileExistsError(\n            f\"File {filename} already exists. Use overwrite=True to overwrite.\"\n        )\n\n    # Save to pickle file\n    try:\n        with open(filename, \"wb\") as handle:\n            dill.dump(optimizer_copy, handle, protocol=dill.HIGHEST_PROTOCOL)\n        print(f\"Experiment saved to {filename}\")\n    except Exception as e:\n        print(f\"Error during pickling: {e}\")\n        raise\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.save_result","title":"<code>save_result(filename=None, prefix='result', path=None, overwrite=True, verbosity=0)</code>","text":"<p>Save the complete optimization results to a pickle file.</p> <p>A result contains all information from a completed optimization run, including the experiment configuration and all evaluation results. This is useful for saving completed runs for later analysis.</p> <p>The result includes everything in an experiment plus: - All evaluated points (X_) - All function values (y_) - Best point and best value - Iteration count - Success rate statistics - Noise statistics (if applicable)</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Filename for the result file. If None, generates from prefix. Defaults to None.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Prefix for auto-generated filename. Defaults to \u201cresult\u201d.</p> <code>'result'</code> <code>path</code> <code>str</code> <p>Directory path to save the file. If None, saves in current directory. Creates directory if it doesn\u2019t exist. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If True, overwrites existing file. If False, raises error if file exists. Defaults to True.</p> <code>True</code> <code>verbosity</code> <code>int</code> <p>Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Run optimization\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (-5, 5)],\n...     max_iter=30,\n...     n_initial=10,\n...     seed=42\n... )\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Save complete results\n&gt;&gt;&gt; opt.save_result(prefix=\"sphere_opt\")\nResult saved to sphere_opt_res.pkl\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Later: load and analyze\n&gt;&gt;&gt; # opt_loaded = SpotOptim.load_result(\"sphere_opt_res.pkl\")\n&gt;&gt;&gt; # print(\"Best value:\", opt_loaded.best_y_)\n&gt;&gt;&gt; # opt_loaded.plot_surrogate()\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def save_result(\n    self,\n    filename: Optional[str] = None,\n    prefix: str = \"result\",\n    path: Optional[str] = None,\n    overwrite: bool = True,\n    verbosity: int = 0,\n) -&gt; None:\n    \"\"\"Save the complete optimization results to a pickle file.\n\n    A result contains all information from a completed optimization run, including\n    the experiment configuration and all evaluation results. This is useful for\n    saving completed runs for later analysis.\n\n    The result includes everything in an experiment plus:\n    - All evaluated points (X_)\n    - All function values (y_)\n    - Best point and best value\n    - Iteration count\n    - Success rate statistics\n    - Noise statistics (if applicable)\n\n    Args:\n        filename (str, optional): Filename for the result file. If None, generates\n            from prefix. Defaults to None.\n        prefix (str): Prefix for auto-generated filename. Defaults to \"result\".\n        path (str, optional): Directory path to save the file. If None, saves in current\n            directory. Creates directory if it doesn't exist. Defaults to None.\n        overwrite (bool): If True, overwrites existing file. If False, raises error if\n            file exists. Defaults to True.\n        verbosity (int): Verbosity level (0=silent, 1=basic, 2=detailed). Defaults to 0.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Run optimization\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Save complete results\n        &gt;&gt;&gt; opt.save_result(prefix=\"sphere_opt\")\n        Result saved to sphere_opt_res.pkl\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Later: load and analyze\n        &gt;&gt;&gt; # opt_loaded = SpotOptim.load_result(\"sphere_opt_res.pkl\")\n        &gt;&gt;&gt; # print(\"Best value:\", opt_loaded.best_y_)\n        &gt;&gt;&gt; # opt_loaded.plot_surrogate()\n    \"\"\"\n    # Use save_experiment with file_io unpickleables to preserve results\n    if filename is None:\n        filename = self._get_result_filename(prefix)\n\n    self.save_experiment(\n        filename=filename,\n        path=path,\n        overwrite=overwrite,\n        unpickleables=\"file_io\",\n        verbosity=verbosity,\n    )\n\n    # Update message\n    if path is not None:\n        full_path = os.path.join(path, filename)\n    else:\n        full_path = filename\n    print(f\"Result saved to {full_path}\")\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.select_new","title":"<code>select_new(A, X, tolerance=0)</code>","text":"<p>Select rows from A that are not in X. Used in suggest_next_infill_point() to avoid duplicate evaluations.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>Array with new values.</p> required <code>X</code> <code>ndarray</code> <p>Array with known values.</p> required <code>tolerance</code> <code>float</code> <p>Tolerance value for comparison. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[ndarray, ndarray]</code> <p>A tuple containing: - ndarray: Array with unknown (new) values. - ndarray: Array with True if value is new, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1), bounds=[(-5, 5)])\n&gt;&gt;&gt; A = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; X = np.array([[3, 4], [7, 8]])\n&gt;&gt;&gt; new_A, is_new = opt.select_new(A, X)\n&gt;&gt;&gt; print(\"New A:\", new_A)\nNew A: [[1 2]\n [5 6]]\n&gt;&gt;&gt; print(\"Is new:\", is_new)\nIs new: [ True False  True]\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def select_new(\n    self, A: np.ndarray, X: np.ndarray, tolerance: float = 0\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Select rows from A that are not in X.\n    Used in suggest_next_infill_point() to avoid duplicate evaluations.\n\n    Args:\n        A (ndarray): Array with new values.\n        X (ndarray): Array with known values.\n        tolerance (float, optional): Tolerance value for comparison. Defaults to 0.\n\n    Returns:\n        tuple: A tuple containing:\n            - ndarray: Array with unknown (new) values.\n            - ndarray: Array with True if value is new, otherwise False.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1), bounds=[(-5, 5)])\n        &gt;&gt;&gt; A = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; X = np.array([[3, 4], [7, 8]])\n        &gt;&gt;&gt; new_A, is_new = opt.select_new(A, X)\n        &gt;&gt;&gt; print(\"New A:\", new_A)\n        New A: [[1 2]\n         [5 6]]\n        &gt;&gt;&gt; print(\"Is new:\", is_new)\n        Is new: [ True False  True]\n    \"\"\"\n    if len(X) == 0:\n        return A, np.ones(len(A), dtype=bool)\n\n    # Calculate distances using the configured metric\n    # cdist supports 'euclidean', 'minkowski', 'chebyshev', etc.\n    # Note: 'chebyshev' is closest to the previous logic but checks absolute difference on all coords\n    # Previous logic: np.all(np.abs(diff) &lt;= tolerance) -&gt; Chebyshev &lt;= tolerance\n    dists = cdist(A, X, metric=self.min_tol_metric)\n\n    # Check if min distance to any existing point is &lt;= tolerance (duplicate)\n    # Duplicate if ANY existing point is within tolerance\n    # is_duplicate[i] is True if A[i] is close to at least one point in X\n    is_duplicate = np.any(dists &lt;= tolerance, axis=1)\n\n    ind = is_duplicate\n    return A[~ind], ~ind\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.sensitivity_spearman","title":"<code>sensitivity_spearman()</code>","text":"<p>Compute and print Spearman correlation between parameters and objective values.</p> <p>This method analyzes the sensitivity of the objective function to each hyperparameter by computing Spearman rank correlations. For categorical (factor) variables, correlation is not computed as they require visual inspection instead.</p> <p>The method automatically handles different parameter types: - Integer/float parameters: Direct correlation with objective values - Log-transformed parameters (log10, log, ln): Correlation in log-space - Factor (categorical) parameters: Skipped with informative message</p> <p>Significance levels: - : p &lt; 0.001 (highly significant) - : p &lt; 0.01 (significant) - : p &lt; 0.05 (marginally significant)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # After running optimization\n&gt;&gt;&gt; opt = SpotOptim(...)\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt; opt.sensitivity_spearman()\nSensitivity Analysis (Spearman Correlation):\n--------------------------------------------------\n  l1 (neurons)        : +0.005 (p=0.959)\n  num_layers          : -0.192 (p=0.056)\n  activation          : (categorical variable, use visual inspection)\n  lr_unified          : -0.040 (p=0.689)\n  alpha               : -0.233 (p=0.020) *\n</code></pre> Note <p>Requires scipy to be installed. If not available, raises ImportError. Only meaningful after optimize() has been called with sufficient evaluations.</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def sensitivity_spearman(self) -&gt; None:\n    \"\"\"Compute and print Spearman correlation between parameters and objective values.\n\n    This method analyzes the sensitivity of the objective function to each\n    hyperparameter by computing Spearman rank correlations. For categorical\n    (factor) variables, correlation is not computed as they require visual\n    inspection instead.\n\n    The method automatically handles different parameter types:\n    - Integer/float parameters: Direct correlation with objective values\n    - Log-transformed parameters (log10, log, ln): Correlation in log-space\n    - Factor (categorical) parameters: Skipped with informative message\n\n    Significance levels:\n    - ***: p &lt; 0.001 (highly significant)\n    - **: p &lt; 0.01 (significant)\n    - *: p &lt; 0.05 (marginally significant)\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # After running optimization\n        &gt;&gt;&gt; opt = SpotOptim(...)\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt; opt.sensitivity_spearman()\n        Sensitivity Analysis (Spearman Correlation):\n        --------------------------------------------------\n          l1 (neurons)        : +0.005 (p=0.959)\n          num_layers          : -0.192 (p=0.056)\n          activation          : (categorical variable, use visual inspection)\n          lr_unified          : -0.040 (p=0.689)\n          alpha               : -0.233 (p=0.020) *\n\n    Note:\n        Requires scipy to be installed. If not available, raises ImportError.\n        Only meaningful after optimize() has been called with sufficient evaluations.\n    \"\"\"\n    try:\n        from scipy.stats import spearmanr\n    except ImportError:\n        raise ImportError(\n            \"scipy is required for sensitivity_spearman(). \"\n            \"Install it with: pip install scipy\"\n        )\n\n    if self.X_ is None or self.y_ is None:\n        raise ValueError(\"No optimization data available. Run optimize() first.\")\n\n    # Get optimization history and parameters\n    history = self.y_\n    all_params = self.X_\n\n    # Get parameter names\n    param_names = (\n        self.var_name if self.var_name else [f\"x{i}\" for i in range(self.n_dim)]\n    )\n\n    print(\"\\nSensitivity Analysis (Spearman Correlation):\")\n    print(\"-\" * 50)\n\n    for param_idx in range(self.n_dim):\n        name = param_names[param_idx]\n        param_values = all_params[:, param_idx]\n\n        # Check if it's a factor variable\n        var_type = self.var_type[param_idx] if self.var_type else \"float\"\n\n        if var_type == \"factor\":\n            # For categorical variables, skip correlation\n            print(f\"  {name:20s}: (categorical variable, use visual inspection)\")\n            continue\n\n        # Check if parameter has log transformation\n        var_trans = self.var_trans[param_idx] if self.var_trans else None\n\n        # Compute correlation based on transformation\n        if var_trans in [\"log10\", \"log\", \"ln\"]:\n            # For log-transformed parameters, use log-space correlation\n            try:\n                param_values_numeric = param_values.astype(float)\n                # Filter out non-positive values\n                valid_mask = (param_values_numeric &gt; 0) &amp; (history &gt; 0)\n                if valid_mask.sum() &lt; 3:\n                    print(\n                        f\"  {name:20s}: (insufficient valid data for log correlation)\"\n                    )\n                    continue\n\n                corr, p_value = spearmanr(\n                    np.log10(param_values_numeric[valid_mask]),\n                    np.log10(history[valid_mask]),\n                )\n            except (ValueError, TypeError):\n                print(f\"  {name:20s}: (error computing log correlation)\")\n                continue\n        else:\n            # For integer/float parameters, direct correlation\n            try:\n                param_values_numeric = param_values.astype(float)\n                corr, p_value = spearmanr(param_values_numeric, history)\n            except (ValueError, TypeError):\n                print(f\"  {name:20s}: (error computing correlation)\")\n                continue\n\n        # Determine significance level\n        if p_value &lt; 0.001:\n            significance = \" ***\"\n        elif p_value &lt; 0.01:\n            significance = \" **\"\n        elif p_value &lt; 0.05:\n            significance = \" *\"\n        else:\n            significance = \"\"\n\n        print(f\"  {name:20s}: {corr:+.3f} (p={p_value:.3f}){significance}\")\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.suggest_next_infill_point","title":"<code>suggest_next_infill_point()</code>","text":"<p>Suggest next point to evaluate (dispatcher).</p> <p>The returned point is in the Transformed and Mapped Space (Internal Optimization Space). This means: 1. Transformations (e.g., log, sqrt) have been applied. 2. Dimension reduction has been applied (fixed variables removed).</p> <p>Process: 1. Try candidates from acquisition function optimizer. 2. Handle acquisition failure (fallback). 3. Return last attempt if all fails.</p> <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>Next point(s) to evaluate in Transformed and Mapped Space.</p> <code>ndarray</code> <p>Shape is (n_infill_points, n_features).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (-5, 5)],\n...     n_initial=5,\n...     n_infill_points=2\n... )\n&gt;&gt;&gt; # Need to initialize optimization state (X_, y_, surrogate)\n&gt;&gt;&gt; # Normally done inside optimize()\n&gt;&gt;&gt; np.random.seed(0)\n&gt;&gt;&gt; opt.X_ = np.random.rand(10, 2)\n&gt;&gt;&gt; opt.y_ = np.random.rand(10)\n&gt;&gt;&gt; opt._fit_surrogate(opt.X_, opt.y_)\n&gt;&gt;&gt; x_next = opt.suggest_next_infill_point()\n&gt;&gt;&gt; x_next.shape\n(2, 2)\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def suggest_next_infill_point(self) -&gt; np.ndarray:\n    \"\"\"Suggest next point to evaluate (dispatcher).\n\n    The returned point is in the **Transformed and Mapped Space** (Internal Optimization Space).\n    This means:\n    1. Transformations (e.g., log, sqrt) have been applied.\n    2. Dimension reduction has been applied (fixed variables removed).\n\n    Process:\n    1. Try candidates from acquisition function optimizer.\n    2. Handle acquisition failure (fallback).\n    3. Return last attempt if all fails.\n\n\n    Returns:\n        ndarray: Next point(s) to evaluate in **Transformed and Mapped Space**.\n        Shape is (n_infill_points, n_features).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     n_initial=5,\n        ...     n_infill_points=2\n        ... )\n        &gt;&gt;&gt; # Need to initialize optimization state (X_, y_, surrogate)\n        &gt;&gt;&gt; # Normally done inside optimize()\n        &gt;&gt;&gt; np.random.seed(0)\n        &gt;&gt;&gt; opt.X_ = np.random.rand(10, 2)\n        &gt;&gt;&gt; opt.y_ = np.random.rand(10)\n        &gt;&gt;&gt; opt._fit_surrogate(opt.X_, opt.y_)\n        &gt;&gt;&gt; x_next = opt.suggest_next_infill_point()\n        &gt;&gt;&gt; x_next.shape\n        (2, 2)\n    \"\"\"\n    # 1. Optimizer candidates\n    candidates = []\n    opt_candidates = self._try_optimizer_candidates(\n        n_needed=self.n_infill_points, current_batch=candidates\n    )\n    candidates.extend(opt_candidates)\n\n    if len(candidates) &gt;= self.n_infill_points:\n        return np.vstack(candidates)\n\n    # 2. Try fallback strategy to fill remaining slots\n    while len(candidates) &lt; self.n_infill_points:\n        # Just try one attempt at a time but loop\n        # We pass current batch to avoid dups\n        cand, x_last = self._try_fallback_strategy(\n            max_attempts=10, current_batch=candidates\n        )\n        if cand is not None:\n            candidates.append(cand)\n        else:\n            # Fallback failed to find unique point even after retries\n            # Break and fill with last attempts or just return what we have?\n            # If we return partial batch, we might fail downstream if code expects n points?\n            # Actually code should handle any number of points returned by this method?\n            # Or duplicate valid points?\n            # Warn and use duplicate if absolutely necessary?\n            if self.verbose:\n                print(\n                    \"Warning: Could not fill all infill points with unique candidates.\"\n                )\n            break\n\n    if len(candidates) &gt; 0:\n        return np.vstack(candidates)\n\n    # 3. Return last attempt (duplicate) if absolutely nothing found\n    # This returns a single point (1, d).\n    # Should we return n copies?\n    # If n_infill_points &gt; 1, we should probably output (n, d)\n\n    if self.verbose:\n        print(\n            \"Warning: Could not find unique point after optimization candidates and fallback attempts. \"\n            \"Returning last candidate (duplicate).\"\n        )\n\n    # Verify x_last is not None\n    if x_last is None:\n        # Should practically not happen\n        x_next = self._handle_acquisition_failure()\n        return x_next.reshape(1, -1)\n\n    # Return duplicated x_last to fill n_infill_points? OR just 1?\n    # Let's return 1 and let loop repeat it?\n    # But loop repeats based on x_next logic.\n    # If we return 1 point, it is treated as 1 point.\n    # If user asked for n_infill_points, maybe we should just return what we have (1 duplicated).\n\n    return x_last.reshape(1, -1)\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.to_all_dim","title":"<code>to_all_dim(X_red)</code>","text":"<p>Expand reduced-dimensional points to full-dimensional representation.</p> <p>This method restores points from the reduced optimization space to the full-dimensional space by inserting fixed values for constant dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>X_red</code> <code>ndarray</code> <p>Points in reduced space, shape (n_samples, n_reduced_dims).</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>Points in full space, shape (n_samples, n_original_dims).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; # Create problem with one fixed dimension\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (2, 2), (-5, 5)],  # x1 is fixed at 2\n...     max_iter=1,\n...     n_initial=3\n... )\n&gt;&gt;&gt; X_red = np.array([[1.0, 3.0], [2.0, 4.0]])  # Only x0 and x2\n&gt;&gt;&gt; X_full = opt.to_all_dim(X_red)\n&gt;&gt;&gt; X_full.shape\n(2, 3)\n&gt;&gt;&gt; X_full[:, 1]  # Middle dimension should be 2.0\narray([2., 2.])\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def to_all_dim(self, X_red: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Expand reduced-dimensional points to full-dimensional representation.\n\n    This method restores points from the reduced optimization space to the\n    full-dimensional space by inserting fixed values for constant dimensions.\n\n    Args:\n        X_red (ndarray): Points in reduced space, shape (n_samples, n_reduced_dims).\n\n    Returns:\n        ndarray: Points in full space, shape (n_samples, n_original_dims).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; # Create problem with one fixed dimension\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (2, 2), (-5, 5)],  # x1 is fixed at 2\n        ...     max_iter=1,\n        ...     n_initial=3\n        ... )\n        &gt;&gt;&gt; X_red = np.array([[1.0, 3.0], [2.0, 4.0]])  # Only x0 and x2\n        &gt;&gt;&gt; X_full = opt.to_all_dim(X_red)\n        &gt;&gt;&gt; X_full.shape\n        (2, 3)\n        &gt;&gt;&gt; X_full[:, 1]  # Middle dimension should be 2.0\n        array([2., 2.])\n    \"\"\"\n    if not self.red_dim:\n        # No reduction occurred, return as-is\n        return X_red\n\n    # Number of samples and full dimensions\n    n_samples = X_red.shape[0]\n    n_full_dims = len(self.ident)\n\n    # Initialize full-dimensional array\n    X_full = np.zeros((n_samples, n_full_dims))\n\n    # Track index in reduced array\n    red_idx = 0\n\n    # Fill in values dimension by dimension\n    for i in range(n_full_dims):\n        if self.ident[i]:\n            # Fixed dimension: use stored value\n            X_full[:, i] = self.all_lower[i]\n        else:\n            # Varying dimension: use value from reduced array\n            X_full[:, i] = X_red[:, red_idx]\n            red_idx += 1\n\n    return X_full\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.to_red_dim","title":"<code>to_red_dim(X_full)</code>","text":"<p>Reduce full-dimensional points to optimization space.</p> <p>This method removes fixed dimensions from full-dimensional points, extracting only the varying dimensions used in optimization.</p> <p>Parameters:</p> Name Type Description Default <code>X_full</code> <code>ndarray</code> <p>Points in full space, shape (n_samples, n_original_dims).</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>Points in reduced space, shape (n_samples, n_reduced_dims).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; # Create problem with one fixed dimension\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=lambda X: np.sum(X**2, axis=1),\n...     bounds=[(-5, 5), (2, 2), (-5, 5)],  # x1 is fixed at 2\n...     max_iter=1,\n...     n_initial=3\n... )\n&gt;&gt;&gt; X_full = np.array([[1.0, 2.0, 3.0], [4.0, 2.0, 5.0]])\n&gt;&gt;&gt; X_red = opt.to_red_dim(X_full)\n&gt;&gt;&gt; X_red.shape\n(2, 2)\n&gt;&gt;&gt; np.array_equal(X_red, np.array([[1.0, 3.0], [4.0, 5.0]]))\nTrue\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def to_red_dim(self, X_full: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Reduce full-dimensional points to optimization space.\n\n    This method removes fixed dimensions from full-dimensional points,\n    extracting only the varying dimensions used in optimization.\n\n    Args:\n        X_full (ndarray): Points in full space, shape (n_samples, n_original_dims).\n\n    Returns:\n        ndarray: Points in reduced space, shape (n_samples, n_reduced_dims).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; # Create problem with one fixed dimension\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=lambda X: np.sum(X**2, axis=1),\n        ...     bounds=[(-5, 5), (2, 2), (-5, 5)],  # x1 is fixed at 2\n        ...     max_iter=1,\n        ...     n_initial=3\n        ... )\n        &gt;&gt;&gt; X_full = np.array([[1.0, 2.0, 3.0], [4.0, 2.0, 5.0]])\n        &gt;&gt;&gt; X_red = opt.to_red_dim(X_full)\n        &gt;&gt;&gt; X_red.shape\n        (2, 2)\n        &gt;&gt;&gt; np.array_equal(X_red, np.array([[1.0, 3.0], [4.0, 5.0]]))\n        True\n    \"\"\"\n    if not self.red_dim:\n        # No reduction occurred, return as-is\n        return X_full\n\n    # Handle 1D array\n    if X_full.ndim == 1:\n        return X_full[~self.ident]\n\n    # Select only non-fixed dimensions (2D)\n    return X_full[:, ~self.ident]\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.transform_bounds","title":"<code>transform_bounds()</code>","text":"<p>Transform bounds from original to internal scale.</p> <p>Updates <code>self.bounds</code> (and <code>self.lower</code>, <code>self.upper</code>) from Natural Space to Transformed Space.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10), (0.1, 100)])\n&gt;&gt;&gt; spot.var_trans = ['log10', 'sqrt']\n&gt;&gt;&gt; spot.transform_bounds()\n&gt;&gt;&gt; print(spot.bounds)\n[(0.0, 1.0), (0.31622776601683794, 10.0)]\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def transform_bounds(self) -&gt; None:\n    \"\"\"Transform bounds from original to internal scale.\n\n    Updates `self.bounds` (and `self.lower`, `self.upper`) from **Natural Space**\n    to **Transformed Space**.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10), (0.1, 100)])\n        &gt;&gt;&gt; spot.var_trans = ['log10', 'sqrt']\n        &gt;&gt;&gt; spot.transform_bounds()\n        &gt;&gt;&gt; print(spot.bounds)\n        [(0.0, 1.0), (0.31622776601683794, 10.0)]\n    \"\"\"\n    for i, trans in enumerate(self.var_trans):\n        if trans is not None:\n            lower_t = self.transform_value(self.lower[i], trans)\n            upper_t = self.transform_value(self.upper[i], trans)\n\n            # Handle reversed bounds (e.g., reciprocal transformation)\n            if lower_t &gt; upper_t:\n                self.lower[i], self.upper[i] = upper_t, lower_t\n            else:\n                self.lower[i], self.upper[i] = lower_t, upper_t\n\n    # Update self.bounds to reflect transformed bounds\n    # Convert numpy types to Python native types (int or float based on var_type)\n    self.bounds = []\n    for i in range(len(self.lower)):\n        # Check if var_type has this index (handle mismatched lengths)\n        if i &lt; len(self.var_type) and (\n            self.var_type[i] == \"int\" or self.var_type[i] == \"factor\"\n        ):\n            self.bounds.append((int(self.lower[i]), int(self.upper[i])))\n        else:\n            self.bounds.append((float(self.lower[i]), float(self.upper[i])))\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.transform_value","title":"<code>transform_value(x, trans)</code>","text":"<p>Apply transformation to a single float value.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>Value to transform</p> required <code>trans</code> <code>Optional[str]</code> <p>Transformation name. Can be one of \u2018id\u2019, \u2018log10\u2019, \u2018log\u2019, \u2018ln\u2019, \u2018sqrt\u2019,    \u2018exp\u2019, \u2018square\u2019, \u2018cube\u2019, \u2018inv\u2019, \u2018reciprocal\u2019, or None.    Also supports dynamic strings like \u2018log(x)\u2019, \u2018sqrt(x)\u2019, \u2018pow(x, p)\u2019.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Transformed value</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If x is not a float.</p> <code>ValueError</code> <p>If an unknown transformation is specified.</p> Notes <p>See also inverse_transform_value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10)])\n&gt;&gt;&gt; spot.transform_value(10, 'log10')\n1.0\n&gt;&gt;&gt; spot.transform_value(100, 'log(x)')\n4.605170185988092\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def transform_value(self, x: float, trans: Optional[str]) -&gt; float:\n    \"\"\"Apply transformation to a single float value.\n\n    Args:\n        x: Value to transform\n        trans: Transformation name. Can be one of 'id', 'log10', 'log', 'ln', 'sqrt',\n               'exp', 'square', 'cube', 'inv', 'reciprocal', or None.\n               Also supports dynamic strings like 'log(x)', 'sqrt(x)', 'pow(x, p)'.\n\n    Returns:\n        Transformed value\n\n    Raises:\n        TypeError: If x is not a float.\n        ValueError: If an unknown transformation is specified.\n\n    Notes:\n        See also inverse_transform_value.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; spot = SpotOptim(fun=lambda x: x, bounds=[(1, 10)])\n        &gt;&gt;&gt; spot.transform_value(10, 'log10')\n        1.0\n        &gt;&gt;&gt; spot.transform_value(100, 'log(x)')\n        4.605170185988092\n    \"\"\"\n    # Ensure x is a float\n    if not isinstance(x, float):\n        try:\n            x = float(x)\n        except (ValueError, TypeError):\n            raise TypeError(\n                f\"transform_value expects a float, got {type(x).__name__} (value: {x})\"\n            )\n    if trans is None or trans == \"id\":\n        return x\n    elif trans == \"log10\":\n        return np.log10(x)\n    elif trans == \"log\" or trans == \"ln\":\n        return np.log(x)\n    elif trans == \"sqrt\":\n        return np.sqrt(x)\n    elif trans == \"exp\":\n        return np.exp(x)\n    elif trans == \"square\":\n        return x**2\n    elif trans == \"cube\":\n        return x**3\n    elif trans == \"inv\" or trans == \"reciprocal\":\n        return 1.0 / x\n\n    # Dynamic Transformations\n    import re\n\n    if trans == \"log(x)\":\n        return np.log(x)\n    if trans == \"sqrt(x)\":\n        return np.sqrt(x)\n\n    m = re.match(r\"pow\\(x,\\s*([0-9.]+)\\)\", trans)\n    if m:\n        p = float(m.group(1))\n        return x**p\n\n    m = re.match(r\"pow\\(([0-9.]+),\\s*x\\)\", trans)\n    if m:\n        base = float(m.group(1))\n        return base**x\n\n    m = re.match(r\"log\\(x,\\s*([0-9.]+)\\)\", trans)\n    if m:\n        base = float(m.group(1))\n        return np.log(x) / np.log(base)\n\n    raise ValueError(f\"Unknown transformation: {trans}\")\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptim.update_stats","title":"<code>update_stats()</code>","text":"<p>Update optimization statistics.</p> <p>Updates various statistics related to the optimization progress: - <code>min_y</code>: Minimum y value found so far - <code>min_X</code>: X value corresponding to minimum y - <code>counter</code>: Total number of function evaluations</p> <p>Note: <code>success_rate</code> is updated separately via <code>_update_success_rate()</code> method, which is called after each batch of function evaluations.</p> <p>If <code>noise</code> is True (repeats &gt; 1), additionally computes: 1. <code>mean_X</code>: Unique design points (aggregated from repeated evaluations) 2. <code>mean_y</code>: Mean y values per design point 3. <code>var_y</code>: Variance of y values per design point 4. <code>min_mean_X</code>: X value of the best mean y value 5. <code>min_mean_y</code>: Best mean y value 6. <code>min_var_y</code>: Variance of the best mean y value</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; # Without noise\n&gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n...                 bounds=[(-5, 5), (-5, 5)],\n...                 max_iter=10, n_initial=5)\n&gt;&gt;&gt; opt.X_ = np.array([[1, 2], [3, 4], [0, 1]])\n&gt;&gt;&gt; opt.y_ = np.array([5.0, 25.0, 1.0])\n&gt;&gt;&gt; opt.update_stats()\n&gt;&gt;&gt; opt.min_y\n1.0\n&gt;&gt;&gt; opt.min_X\narray([0, 1])\n&gt;&gt;&gt; opt.counter\n3\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With noise\n&gt;&gt;&gt; opt_noise = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n...                       bounds=[(-5, 5), (-5, 5)],\n...                       n_initial=5,\n...                       repeats_initial=2)\n&gt;&gt;&gt; opt_noise.noise = True\n&gt;&gt;&gt; opt_noise.X_ = np.array([[1, 2], [1, 2], [3, 4]])\n&gt;&gt;&gt; opt_noise.y_ = np.array([5.0, 5.0, 25.0])\n&gt;&gt;&gt; opt_noise.update_stats()\n&gt;&gt;&gt; opt_noise.mean_y.shape\n(2,)\n</code></pre> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>def update_stats(self) -&gt; None:\n    \"\"\"Update optimization statistics.\n\n    Updates various statistics related to the optimization progress:\n    - `min_y`: Minimum y value found so far\n    - `min_X`: X value corresponding to minimum y\n    - `counter`: Total number of function evaluations\n\n    Note: `success_rate` is updated separately via `_update_success_rate()` method,\n    which is called after each batch of function evaluations.\n\n    If `noise` is True (repeats &gt; 1), additionally computes:\n    1. `mean_X`: Unique design points (aggregated from repeated evaluations)\n    2. `mean_y`: Mean y values per design point\n    3. `var_y`: Variance of y values per design point\n    4. `min_mean_X`: X value of the best mean y value\n    5. `min_mean_y`: Best mean y value\n    6. `min_var_y`: Variance of the best mean y value\n\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; # Without noise\n        &gt;&gt;&gt; opt = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n        ...                 bounds=[(-5, 5), (-5, 5)],\n        ...                 max_iter=10, n_initial=5)\n        &gt;&gt;&gt; opt.X_ = np.array([[1, 2], [3, 4], [0, 1]])\n        &gt;&gt;&gt; opt.y_ = np.array([5.0, 25.0, 1.0])\n        &gt;&gt;&gt; opt.update_stats()\n        &gt;&gt;&gt; opt.min_y\n        1.0\n        &gt;&gt;&gt; opt.min_X\n        array([0, 1])\n        &gt;&gt;&gt; opt.counter\n        3\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With noise\n        &gt;&gt;&gt; opt_noise = SpotOptim(fun=lambda X: np.sum(X**2, axis=1),\n        ...                       bounds=[(-5, 5), (-5, 5)],\n        ...                       n_initial=5,\n        ...                       repeats_initial=2)\n        &gt;&gt;&gt; opt_noise.noise = True\n        &gt;&gt;&gt; opt_noise.X_ = np.array([[1, 2], [1, 2], [3, 4]])\n        &gt;&gt;&gt; opt_noise.y_ = np.array([5.0, 5.0, 25.0])\n        &gt;&gt;&gt; opt_noise.update_stats()\n        &gt;&gt;&gt; opt_noise.mean_y.shape\n        (2,)\n    \"\"\"\n    if self.y_ is None or len(self.y_) == 0:\n        return\n\n    # Basic stats\n    self.min_y = np.min(self.y_)\n    self.min_X = self.X_[np.argmin(self.y_)]\n    self.counter = len(self.y_)\n\n    # Aggregated stats for noisy functions\n    if (self.repeats_initial &gt; 1) or (self.repeats_surrogate &gt; 1):\n        self.mean_X, self.mean_y, self.var_y = self._aggregate_mean_var(\n            self.X_, self.y_\n        )\n        # X value of the best mean y value so far\n        best_mean_idx = np.argmin(self.mean_y)\n        self.min_mean_X = self.mean_X[best_mean_idx]\n        # Best mean y value so far\n        self.min_mean_y = self.mean_y[best_mean_idx]\n        # Variance of the best mean y value so far\n        self.min_var_y = self.var_y[best_mean_idx]\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptimConfig","title":"<code>SpotOptimConfig</code>  <code>dataclass</code>","text":"<p>Configuration parameters for SpotOptim.</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>@dataclass\nclass SpotOptimConfig:\n    \"\"\"Configuration parameters for SpotOptim.\"\"\"\n\n    bounds: Optional[list] = None\n    max_iter: int = 20\n    n_initial: int = 10\n    surrogate: Optional[object] = None\n    acquisition: str = \"y\"\n    var_type: Optional[list] = None\n    var_name: Optional[list] = None\n    var_trans: Optional[list] = None\n    tolerance_x: Optional[float] = None\n    max_time: float = np.inf\n    repeats_initial: int = 1\n    repeats_surrogate: int = 1\n    ocba_delta: int = 0\n    tensorboard_log: bool = False\n    tensorboard_path: Optional[str] = None\n    tensorboard_clean: bool = False\n    fun_mo2so: Optional[Callable] = None\n    seed: Optional[int] = None\n    verbose: bool = False\n    warnings_filter: str = \"ignore\"\n    n_infill_points: int = 1\n    max_surrogate_points: Optional[Union[int, List[int]]] = None\n    selection_method: str = \"distant\"\n    acquisition_failure_strategy: str = \"random\"\n    penalty: bool = False\n    penalty_val: Optional[float] = None\n    acquisition_fun_return_size: int = 3\n    acquisition_optimizer: Union[str, Callable] = \"differential_evolution\"\n    restart_after_n: int = 100\n    restart_inject_best: bool = True\n    x0: Optional[np.ndarray] = None\n    de_x0_prob: float = 0.1\n    tricands_fringe: bool = False\n    prob_de_tricands: float = 0.8\n    window_size: Optional[int] = None\n    min_tol_metric: str = \"chebyshev\"\n    prob_surrogate: Optional[List[float]] = None\n    n_jobs: int = 1\n    acquisition_optimizer_kwargs: Optional[Dict[str, Any]] = None\n    args: Tuple = ()\n    kwargs: Optional[Dict[str, Any]] = None\n\n    def __post_init__(self):\n        if self.kwargs is None:\n            self.kwargs = {}\n</code></pre>"},{"location":"reference/spotoptim/SpotOptim/#spotoptim.SpotOptim.SpotOptimState","title":"<code>SpotOptimState</code>  <code>dataclass</code>","text":"<p>Mutable state of the optimization process.</p> Source code in <code>src/spotoptim/SpotOptim.py</code> <pre><code>@dataclass\nclass SpotOptimState:\n    \"\"\"Mutable state of the optimization process.\"\"\"\n\n    X_: Optional[np.ndarray] = None\n    y_: Optional[np.ndarray] = None\n    y_mo: Optional[np.ndarray] = None\n    best_x_: Optional[np.ndarray] = None\n    best_y_: Optional[float] = None\n    n_iter_: int = 0\n    counter: int = 0\n\n    # Success tracking\n    success_rate: float = 0.0\n    success_counter: int = 0\n    _success_history: List = field(default_factory=list)\n    _zero_success_count: int = 0\n\n    # Noise statistics\n    mean_X: Optional[np.ndarray] = None\n    mean_y: Optional[np.ndarray] = None\n    var_y: Optional[np.ndarray] = None\n    min_mean_X: Optional[np.ndarray] = None\n    min_mean_y: Optional[float] = None\n    min_var_y: Optional[float] = None\n\n    # Best found\n    min_X: Optional[np.ndarray] = None\n    min_y: Optional[float] = None\n\n    # Restart history\n    restarts_results_: List = field(default_factory=list)\n</code></pre>"},{"location":"reference/spotoptim/core/data/","title":"data","text":""},{"location":"reference/spotoptim/core/data/#spotoptim.core.data.SpotDataFromArray","title":"<code>SpotDataFromArray</code>","text":"<p>               Bases: <code>SpotDataSet</code></p> <p>Data handler for numpy arrays or torch tensors.</p> Source code in <code>src/spotoptim/core/data.py</code> <pre><code>class SpotDataFromArray(SpotDataSet):\n    \"\"\"\n    Data handler for numpy arrays or torch tensors.\n    \"\"\"\n\n    def __init__(\n        self,\n        x_train: Union[np.ndarray, torch.Tensor],\n        y_train: Union[np.ndarray, torch.Tensor],\n        x_val: Optional[Union[np.ndarray, torch.Tensor]] = None,\n        y_val: Optional[Union[np.ndarray, torch.Tensor]] = None,\n        x_test: Optional[Union[np.ndarray, torch.Tensor]] = None,\n        y_test: Optional[Union[np.ndarray, torch.Tensor]] = None,\n        target_column: Optional[str] = None,\n    ):\n        # Determine dimensions\n        input_dim = x_train.shape[1] if hasattr(x_train, \"shape\") else 0\n        output_dim = (\n            y_train.shape[1]\n            if hasattr(y_train, \"shape\") and len(y_train.shape) &gt; 1\n            else 1\n        )\n\n        super().__init__(input_dim, output_dim, target_column)\n\n        self.x_train = x_train\n        self.y_train = y_train\n        self.x_val = x_val\n        self.y_val = y_val\n        self.x_test = x_test\n        self.y_test = y_test\n\n    def get_train_data(\n        self,\n    ) -&gt; Tuple[Union[np.ndarray, torch.Tensor], Union[np.ndarray, torch.Tensor]]:\n        return self.x_train, self.y_train\n\n    def get_validation_data(\n        self,\n    ) -&gt; Optional[\n        Tuple[Union[np.ndarray, torch.Tensor], Union[np.ndarray, torch.Tensor]]\n    ]:\n        if self.x_val is not None:\n            return self.x_val, self.y_val\n        return None\n\n    def get_test_data(\n        self,\n    ) -&gt; Optional[\n        Tuple[Union[np.ndarray, torch.Tensor], Union[np.ndarray, torch.Tensor]]\n    ]:\n        if self.x_test is not None:\n            return self.x_test, self.y_test\n        return None\n</code></pre>"},{"location":"reference/spotoptim/core/data/#spotoptim.core.data.SpotDataFromTorchDataset","title":"<code>SpotDataFromTorchDataset</code>","text":"<p>               Bases: <code>SpotDataSet</code></p> <p>Data handler for PyTorch Datasets.</p> Source code in <code>src/spotoptim/core/data.py</code> <pre><code>class SpotDataFromTorchDataset(SpotDataSet):\n    \"\"\"\n    Data handler for PyTorch Datasets.\n    \"\"\"\n\n    def __init__(\n        self,\n        train_dataset: Dataset,\n        input_dim: int,\n        output_dim: int,\n        val_dataset: Optional[Dataset] = None,\n        test_dataset: Optional[Dataset] = None,\n        target_column: Optional[str] = None,\n    ):\n        super().__init__(input_dim, output_dim, target_column)\n        self.train_dataset = train_dataset\n        self.val_dataset = val_dataset\n        self.test_dataset = test_dataset\n\n    def get_train_data(self) -&gt; Dataset:\n        return self.train_dataset\n\n    def get_validation_data(self) -&gt; Optional[Dataset]:\n        return self.val_dataset\n\n    def get_test_data(self) -&gt; Optional[Dataset]:\n        return self.test_dataset\n</code></pre>"},{"location":"reference/spotoptim/core/data/#spotoptim.core.data.SpotDataSet","title":"<code>SpotDataSet</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data handling in SpotOptim.</p> <p>Attributes:</p> Name Type Description <code>input_dim</code> <code>int</code> <p>Number of input features.</p> <code>output_dim</code> <code>int</code> <p>Number of output features.</p> <code>target_column</code> <code>str</code> <p>Name of the target column if applicable.</p> Source code in <code>src/spotoptim/core/data.py</code> <pre><code>class SpotDataSet(ABC):\n    \"\"\"\n    Abstract base class for data handling in SpotOptim.\n\n    Attributes:\n        input_dim (int): Number of input features.\n        output_dim (int): Number of output features.\n        target_column (str, optional): Name of the target column if applicable.\n    \"\"\"\n\n    def __init__(\n        self, input_dim: int, output_dim: int, target_column: Optional[str] = None\n    ):\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.target_column = target_column\n\n    @abstractmethod\n    def get_train_data(self) -&gt; Any:\n        \"\"\"Returns the training data.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_test_data(self) -&gt; Any:\n        \"\"\"Returns the test data.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_validation_data(self) -&gt; Any:\n        \"\"\"Returns the validation data.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/spotoptim/core/data/#spotoptim.core.data.SpotDataSet.get_test_data","title":"<code>get_test_data()</code>  <code>abstractmethod</code>","text":"<p>Returns the test data.</p> Source code in <code>src/spotoptim/core/data.py</code> <pre><code>@abstractmethod\ndef get_test_data(self) -&gt; Any:\n    \"\"\"Returns the test data.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/spotoptim/core/data/#spotoptim.core.data.SpotDataSet.get_train_data","title":"<code>get_train_data()</code>  <code>abstractmethod</code>","text":"<p>Returns the training data.</p> Source code in <code>src/spotoptim/core/data.py</code> <pre><code>@abstractmethod\ndef get_train_data(self) -&gt; Any:\n    \"\"\"Returns the training data.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/spotoptim/core/data/#spotoptim.core.data.SpotDataSet.get_validation_data","title":"<code>get_validation_data()</code>  <code>abstractmethod</code>","text":"<p>Returns the validation data.</p> Source code in <code>src/spotoptim/core/data.py</code> <pre><code>@abstractmethod\ndef get_validation_data(self) -&gt; Any:\n    \"\"\"Returns the validation data.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/spotoptim/core/experiment/","title":"experiment","text":""},{"location":"reference/spotoptim/core/experiment/#spotoptim.core.experiment.ExperimentControl","title":"<code>ExperimentControl</code>  <code>dataclass</code>","text":"<p>Controls the experiment configuration, replacing the legacy fun_control dictionary.</p> <p>This class serves as the central configuration object for optimization experiments, holding the dataset, model configuration, hyperparameters, and other settings.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>SpotDataSet</code> <p>The dataset object containing the data.</p> <code>model_class</code> <code>Any</code> <p>The class of the model to be instantiated.</p> <code>hyperparameters</code> <code>Any</code> <p>The hyperparameters for the model.</p> <code>seed</code> <code>int</code> <p>The random seed for reproducibility.</p> <code>device</code> <code>str</code> <p>The device to run the model on (e.g. \u201ccpu\u201d or \u201ccuda\u201d).</p> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>epochs</code> <code>Optional[int]</code> <p>The number of epochs to train the model.</p> <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>optimizer_class</code> <code>Optional[Any]</code> <p>The optimizer class to use for training.</p> <code>loss_function</code> <code>Optional[Any]</code> <p>The loss function to use for training.</p> <code>metrics</code> <code>List[str]</code> <p>The metrics to track during training.</p> <code>n_initial</code> <code>int</code> <p>The number of initial design points.</p> <code>max_evals</code> <code>int</code> <p>The maximum number of evaluations.</p> <code>experiment_name</code> <code>str</code> <p>The name of the experiment.</p> <code>verbosity</code> <code>int</code> <p>The verbosity level.</p> <p>Methods:</p> Name Description <code>to_dict</code> <p>Convert the object to a dictionary.</p> <code>torch_device</code> <p>Return the torch device object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n&gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n&gt;&gt;&gt; from spotoptim.nn.mlp import MLP\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 1. Prepare Data\n&gt;&gt;&gt; X = np.array([[0.1, 0.2], [0.3, 0.4]])\n&gt;&gt;&gt; y = np.array([[1.0], [2.0]])\n&gt;&gt;&gt; dataset = SpotDataFromArray(x_train=X, y_train=y)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 2. Define Hyperparameters\n&gt;&gt;&gt; params = {\"l1\": 16, \"num_hidden_layers\": 1, \"lr\": 1e-3}\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 3. Initialize Control with Real Model\n&gt;&gt;&gt; exp = ExperimentControl(\n...     dataset=dataset,\n...     model_class=MLP,\n...     hyperparameters=params,\n...     experiment_name=\"real_model_run\",\n...     seed=42\n... )\n&gt;&gt;&gt; print(exp.experiment_name)\nreal_model_run\n</code></pre> Source code in <code>src/spotoptim/core/experiment.py</code> <pre><code>@dataclass\nclass ExperimentControl:\n    \"\"\"\n    Controls the experiment configuration, replacing the legacy fun_control dictionary.\n\n    This class serves as the central configuration object for optimization experiments,\n    holding the dataset, model configuration, hyperparameters, and other settings.\n\n    Attributes:\n        dataset (SpotDataSet): The dataset object containing the data.\n        model_class (Any): The class of the model to be instantiated.\n        hyperparameters (Any): The hyperparameters for the model.\n        seed (int): The random seed for reproducibility.\n        device (str): The device to run the model on (e.g. \"cpu\" or \"cuda\").\n        num_workers (int): The number of workers to use for data loading.\n        epochs (Optional[int]): The number of epochs to train the model.\n        batch_size (int): The batch size for training.\n        optimizer_class (Optional[Any]): The optimizer class to use for training.\n        loss_function (Optional[Any]): The loss function to use for training.\n        metrics (List[str]): The metrics to track during training.\n        n_initial (int): The number of initial design points.\n        max_evals (int): The maximum number of evaluations.\n        experiment_name (str): The name of the experiment.\n        verbosity (int): The verbosity level.\n\n    Methods:\n        to_dict(): Convert the object to a dictionary.\n        torch_device(): Return the torch device object.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n        &gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n        &gt;&gt;&gt; from spotoptim.nn.mlp import MLP\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 1. Prepare Data\n        &gt;&gt;&gt; X = np.array([[0.1, 0.2], [0.3, 0.4]])\n        &gt;&gt;&gt; y = np.array([[1.0], [2.0]])\n        &gt;&gt;&gt; dataset = SpotDataFromArray(x_train=X, y_train=y)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 2. Define Hyperparameters\n        &gt;&gt;&gt; params = {\"l1\": 16, \"num_hidden_layers\": 1, \"lr\": 1e-3}\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 3. Initialize Control with Real Model\n        &gt;&gt;&gt; exp = ExperimentControl(\n        ...     dataset=dataset,\n        ...     model_class=MLP,\n        ...     hyperparameters=params,\n        ...     experiment_name=\"real_model_run\",\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt; print(exp.experiment_name)\n        real_model_run\n    \"\"\"\n\n    # Core Components\n    dataset: SpotDataSet\n    model_class: Any  # The class of the model to be instantiated\n    hyperparameters: Any  # Should be ParameterSet type, using Any to avoid circular import issues for now\n\n    # Execution Settings\n    seed: int = 123\n    device: str = \"cpu\"\n    num_workers: int = 0\n\n    # Model Training Settings\n    epochs: Optional[int] = None\n    batch_size: int = 32\n    optimizer_class: Optional[Any] = None  # Torch optimizer class\n    loss_function: Optional[Any] = None  # Torch loss function\n\n    # Metrics\n    metrics: List[str] = field(default_factory=list)\n\n    # Optimization Settings (for SpotOptim)\n    n_initial: int = 10\n    max_evals: int = 50\n\n    # Legacy/Misc\n    experiment_name: str = \"default_experiment\"\n    verbosity: int = 0\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert to dictionary for partial backward compatibility or logging.\n\n        Returns:\n            Dict[str, Any]: Dictionary representation of the object.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n            &gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Setup\n            &gt;&gt;&gt; dataset = SpotDataFromArray(np.zeros((5,2)), np.zeros((5,1)))\n            &gt;&gt;&gt; exp = ExperimentControl(dataset, model_class=None, hyperparameters={})\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Convert to dict\n            &gt;&gt;&gt; config = exp.to_dict()\n            &gt;&gt;&gt; config['seed']\n            123\n        \"\"\"\n        return self.__dict__.copy()\n\n    @property\n    def torch_device(self) -&gt; torch.device:\n        \"\"\"Returns the torch.device object.\n\n        Returns:\n            torch.device: The torch device object.\n\n        Examples:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n            &gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; dataset = SpotDataFromArray(np.zeros((5,2)), np.zeros((5,1)))\n            &gt;&gt;&gt; exp = ExperimentControl(\n            ...     dataset,\n            ...     model_class=None,\n            ...     hyperparameters={},\n            ...     device=\"cpu\"\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; exp.torch_device\n            device(type='cpu')\n        \"\"\"\n        return torch.device(self.device)\n</code></pre>"},{"location":"reference/spotoptim/core/experiment/#spotoptim.core.experiment.ExperimentControl.torch_device","title":"<code>torch_device</code>  <code>property</code>","text":"<p>Returns the torch.device object.</p> <p>Returns:</p> Type Description <code>device</code> <p>torch.device: The torch device object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n&gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n&gt;&gt;&gt;\n&gt;&gt;&gt; dataset = SpotDataFromArray(np.zeros((5,2)), np.zeros((5,1)))\n&gt;&gt;&gt; exp = ExperimentControl(\n...     dataset,\n...     model_class=None,\n...     hyperparameters={},\n...     device=\"cpu\"\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; exp.torch_device\ndevice(type='cpu')\n</code></pre>"},{"location":"reference/spotoptim/core/experiment/#spotoptim.core.experiment.ExperimentControl.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary for partial backward compatibility or logging.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary representation of the object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n&gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Setup\n&gt;&gt;&gt; dataset = SpotDataFromArray(np.zeros((5,2)), np.zeros((5,1)))\n&gt;&gt;&gt; exp = ExperimentControl(dataset, model_class=None, hyperparameters={})\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Convert to dict\n&gt;&gt;&gt; config = exp.to_dict()\n&gt;&gt;&gt; config['seed']\n123\n</code></pre> Source code in <code>src/spotoptim/core/experiment.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert to dictionary for partial backward compatibility or logging.\n\n    Returns:\n        Dict[str, Any]: Dictionary representation of the object.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n        &gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Setup\n        &gt;&gt;&gt; dataset = SpotDataFromArray(np.zeros((5,2)), np.zeros((5,1)))\n        &gt;&gt;&gt; exp = ExperimentControl(dataset, model_class=None, hyperparameters={})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Convert to dict\n        &gt;&gt;&gt; config = exp.to_dict()\n        &gt;&gt;&gt; config['seed']\n        123\n    \"\"\"\n    return self.__dict__.copy()\n</code></pre>"},{"location":"reference/spotoptim/data/base/","title":"base","text":""},{"location":"reference/spotoptim/data/base/#spotoptim.data.base.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all configurations.</p> <p>All configurations inherit from this class, be they stored in a file or generated on the fly.</p> <p>Attributes:</p> Name Type Description <code>desc</code> <code>str</code> <p>The description from the docstring.</p> <code>_repr_content</code> <code>dict</code> <p>The items that are displayed in the repr method.</p> Source code in <code>src/spotoptim/data/base.py</code> <pre><code>class Config(abc.ABC):\n    \"\"\"Base class for all configurations.\n\n    All configurations inherit from this class, be they stored in a file or generated on the fly.\n\n    Attributes:\n        desc (str): The description from the docstring.\n        _repr_content (dict): The items that are displayed in the __repr__ method.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize a Config object.\"\"\"\n        pass\n\n    @property\n    def desc(self) -&gt; str:\n        \"\"\"Return the description from the docstring.\n\n        Returns:\n            str: The description from the docstring.\n        \"\"\"\n        desc = re.split(pattern=r\"\\w+\\n\\s{4}\\-{3,}\", string=self.__doc__, maxsplit=0)[0]\n        return inspect.cleandoc(desc)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The items that are displayed in the __repr__ method.\n\n        This property can be overridden in order to modify the output of the __repr__ method.\n\n        Returns:\n            dict: A dictionary containing the items to be displayed in the __repr__ method.\n        \"\"\"\n        content = {}\n        content[\"Name\"] = self.__class__.__name__\n        return content\n</code></pre>"},{"location":"reference/spotoptim/data/base/#spotoptim.data.base.Config.desc","title":"<code>desc</code>  <code>property</code>","text":"<p>Return the description from the docstring.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The description from the docstring.</p>"},{"location":"reference/spotoptim/data/base/#spotoptim.data.base.Config.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a Config object.</p> Source code in <code>src/spotoptim/data/base.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize a Config object.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/spotoptim/data/base/#spotoptim.data.base.FileConfig","title":"<code>FileConfig</code>","text":"<p>               Bases: <code>Config</code></p> <p>Base class for configurations that are stored in a local file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The file\u2019s name.</p> required <code>directory</code> <code>Optional[str]</code> <p>The directory where the file is contained. Defaults to the location of the <code>datasets</code> module.</p> <code>None</code> <code>desc</code> <code>dict</code> <p>Extra config parameters to pass as keyword arguments.</p> <code>{}</code> Source code in <code>src/spotoptim/data/base.py</code> <pre><code>class FileConfig(Config):\n    \"\"\"Base class for configurations that are stored in a local file.\n\n    Args:\n        filename (str): The file's name.\n        directory (Optional[str]):\n            The directory where the file is contained.\n            Defaults to the location of the `datasets` module.\n        desc (dict): Extra config parameters to pass as keyword arguments.\n    \"\"\"\n\n    def __init__(self, filename: str, directory: Optional[str] = None, **desc):\n        super().__init__(**desc)\n        self.filename = filename\n        self.directory = directory\n\n    @property\n    def path(self) -&gt; pathlib.Path:\n        \"\"\"The path to the configuration file.\n\n        Returns:\n            pathlib.Path: The path to the configuration file.\n        \"\"\"\n        if self.directory:\n            return pathlib.Path(self.directory).joinpath(self.filename)\n        return pathlib.Path(__file__).parent.joinpath(self.filename)\n\n    @property\n    def _repr_content(self) -&gt; dict:\n        \"\"\"The content of the string representation of the FileConfig object.\n\n        Returns:\n            dict: A dictionary containing the content of the string representation of the FileConfig object.\n        \"\"\"\n        content = super()._repr_content\n        content[\"Path\"] = str(self.path)\n        return content\n</code></pre>"},{"location":"reference/spotoptim/data/base/#spotoptim.data.base.FileConfig.path","title":"<code>path</code>  <code>property</code>","text":"<p>The path to the configuration file.</p> <p>Returns:</p> Type Description <code>Path</code> <p>pathlib.Path: The path to the configuration file.</p>"},{"location":"reference/spotoptim/data/diabetes/","title":"diabetes","text":""},{"location":"reference/spotoptim/data/diabetes/#spotoptim.data.diabetes.DiabetesDataset","title":"<code>DiabetesDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Diabetes dataset wrapping sklearn\u2019s diabetes dataset or custom data.</p> Source code in <code>src/spotoptim/data/diabetes.py</code> <pre><code>class DiabetesDataset(Dataset):\n    \"\"\"\n    Diabetes dataset wrapping sklearn's diabetes dataset or custom data.\n    \"\"\"\n\n    def __init__(\n        self,\n        X: Optional[Union[np.ndarray, torch.Tensor]] = None,\n        y: Optional[Union[np.ndarray, torch.Tensor]] = None,\n        transform: Optional[Callable] = None,\n        target_transform: Optional[Callable] = None,\n    ):\n        \"\"\"\n        Args:\n            X: Features. If None, loads sklearn diabetes dataset.\n            y: Targets. If None, loads sklearn diabetes dataset.\n            transform: Optional transform to be applied on a sample.\n            target_transform: Optional transform to be applied on the target.\n        \"\"\"\n        if X is None or y is None:\n            diabetes = load_diabetes()\n            X = diabetes.data\n            y = diabetes.target\n\n        # Convert to tensors if numpy arrays\n        if isinstance(X, np.ndarray):\n            X = torch.from_numpy(X).float()\n        if isinstance(y, np.ndarray):\n            y = torch.from_numpy(y).float()\n\n        # Ensure y is 2D (N, 1)\n        if y.ndim == 1:\n            y = y.unsqueeze(1)\n\n        self.X = X\n        self.y = y\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.X)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        features = self.X[idx]\n        target = self.y[idx]\n\n        if self.transform:\n            features = self.transform(features)\n\n        if self.target_transform:\n            target = self.target_transform(target)\n\n        return features, target\n\n    @property\n    def n_features(self) -&gt; int:\n        return self.X.shape[1]\n\n    @property\n    def n_samples(self) -&gt; int:\n        return len(self.X)\n</code></pre>"},{"location":"reference/spotoptim/data/diabetes/#spotoptim.data.diabetes.DiabetesDataset.__init__","title":"<code>__init__(X=None, y=None, transform=None, target_transform=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>X</code> <code>Optional[Union[ndarray, Tensor]]</code> <p>Features. If None, loads sklearn diabetes dataset.</p> <code>None</code> <code>y</code> <code>Optional[Union[ndarray, Tensor]]</code> <p>Targets. If None, loads sklearn diabetes dataset.</p> <code>None</code> <code>transform</code> <code>Optional[Callable]</code> <p>Optional transform to be applied on a sample.</p> <code>None</code> <code>target_transform</code> <code>Optional[Callable]</code> <p>Optional transform to be applied on the target.</p> <code>None</code> Source code in <code>src/spotoptim/data/diabetes.py</code> <pre><code>def __init__(\n    self,\n    X: Optional[Union[np.ndarray, torch.Tensor]] = None,\n    y: Optional[Union[np.ndarray, torch.Tensor]] = None,\n    transform: Optional[Callable] = None,\n    target_transform: Optional[Callable] = None,\n):\n    \"\"\"\n    Args:\n        X: Features. If None, loads sklearn diabetes dataset.\n        y: Targets. If None, loads sklearn diabetes dataset.\n        transform: Optional transform to be applied on a sample.\n        target_transform: Optional transform to be applied on the target.\n    \"\"\"\n    if X is None or y is None:\n        diabetes = load_diabetes()\n        X = diabetes.data\n        y = diabetes.target\n\n    # Convert to tensors if numpy arrays\n    if isinstance(X, np.ndarray):\n        X = torch.from_numpy(X).float()\n    if isinstance(y, np.ndarray):\n        y = torch.from_numpy(y).float()\n\n    # Ensure y is 2D (N, 1)\n    if y.ndim == 1:\n        y = y.unsqueeze(1)\n\n    self.X = X\n    self.y = y\n    self.transform = transform\n    self.target_transform = target_transform\n</code></pre>"},{"location":"reference/spotoptim/data/diabetes/#spotoptim.data.diabetes.get_diabetes_dataloaders","title":"<code>get_diabetes_dataloaders(test_size=0.2, batch_size=32, scale_features=True, shuffle_train=True, shuffle_test=False, random_state=42, num_workers=0, pin_memory=False)</code>","text":"<p>Returns train and test dataloaders for the Diabetes dataset.</p> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>float</code> <p>Fraction of data to use for testing.</p> <code>0.2</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>32</code> <code>scale_features</code> <code>bool</code> <p>Whether to standardize features using StandardScaler.</p> <code>True</code> <code>shuffle_train</code> <code>bool</code> <p>Whether to shuffle the training data.</p> <code>True</code> <code>shuffle_test</code> <code>bool</code> <p>Whether to shuffle the test data.</p> <code>False</code> <code>random_state</code> <code>int</code> <p>Random seed for splitting.</p> <code>42</code> <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>If True, the data loader will copy Tensors into CUDA pinned memory before returning them.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[DataLoader, DataLoader, Optional[StandardScaler]]</code> <p>(train_loader, test_loader, scaler) scaler is the StandardScaler implementation if scale_features=True, else None.</p> Source code in <code>src/spotoptim/data/diabetes.py</code> <pre><code>def get_diabetes_dataloaders(\n    test_size: float = 0.2,\n    batch_size: int = 32,\n    scale_features: bool = True,\n    shuffle_train: bool = True,\n    shuffle_test: bool = False,\n    random_state: int = 42,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n) -&gt; Tuple[DataLoader, DataLoader, Optional[StandardScaler]]:\n    \"\"\"\n    Returns train and test dataloaders for the Diabetes dataset.\n\n    Args:\n        test_size (float): Fraction of data to use for testing.\n        batch_size (int): Batch size.\n        scale_features (bool): Whether to standardize features using StandardScaler.\n        shuffle_train (bool): Whether to shuffle the training data.\n        shuffle_test (bool): Whether to shuffle the test data.\n        random_state (int): Random seed for splitting.\n        num_workers (int): Number of subprocesses to use for data loading.\n        pin_memory (bool): If True, the data loader will copy Tensors into CUDA pinned memory before returning them.\n\n    Returns:\n        tuple: (train_loader, test_loader, scaler)\n            scaler is the StandardScaler implementation if scale_features=True, else None.\n    \"\"\"\n    # Load data\n    diabetes = load_diabetes()\n    X, y = diabetes.data, diabetes.target\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n\n    scaler = None\n    if scale_features:\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_test = scaler.transform(X_test)\n\n    # Create datasets\n    train_dataset = DiabetesDataset(X_train, y_train)\n    test_dataset = DiabetesDataset(X_test, y_test)\n\n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=shuffle_train,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=shuffle_test,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n\n    return train_loader, test_loader, scaler\n</code></pre>"},{"location":"reference/spotoptim/eda/plots/","title":"plots","text":""},{"location":"reference/spotoptim/eda/plots/#spotoptim.eda.plots.plot_ip_boxplots","title":"<code>plot_ip_boxplots(df, category_column_name=None, num_cols=2, figwidth=10, box_width=0.2, both_names=True, height_per_subplot=2.0, add_points=None, add_points_col=['red'])</code>","text":"<p>Generate infill-point boxplots (ip-boxplots). A separate ip-boxplot is generated for each numerical column in a DataFrame, arranged in a grid. Each subplot has its own scale, similar to how histograms are shown in plot_histograms(). Additional points can be added and highlighted in red.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the data to plot.</p> required <code>category_column_name</code> <code>str</code> <p>Column name for categorical grouping. Defaults to None.</p> <code>None</code> <code>num_cols</code> <code>int</code> <p>Number of columns in the subplot grid. Defaults to 2.</p> <code>2</code> <code>figwidth</code> <code>int</code> <p>Width of the entire figure. Defaults to 10.</p> <code>10</code> <code>box_width</code> <code>float</code> <p>Width of the boxplots. Defaults to 0.2.</p> <code>0.2</code> <code>both_names</code> <code>bool</code> <p>Whether to show both variable names and categories in titles. Defaults to True.</p> <code>True</code> <code>height_per_subplot</code> <code>float</code> <p>Height per subplot row. Defaults to 2.0.</p> <code>2.0</code> <code>add_points</code> <code>DataFrame</code> <p>DataFrame containing additional points to highlight. Defaults to None.</p> <code>None</code> <code>add_points_col</code> <code>list</code> <p>List of colors for the additional points. Defaults to [\u201cred\u201d].</p> <code>['red']</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotoptim.eda.plots import plot_ip_boxplots\n&gt;&gt;&gt; data = {'A': [1, 2, 2, 3, 4, 5, 100], 'B': [10, 10, 10, 10, 10, 10, 10]}\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; plot_ip_boxplots(df, num_cols=1)\n&gt;&gt;&gt; # Example with multiple added points and colors\n&gt;&gt;&gt; add_points = pd.DataFrame({'A': [1.5, 3.5], 'B': [10, 10]})\n&gt;&gt;&gt; plot_ip_boxplots(df, add_points=add_points, add_points_col=[\"red\", \"blue\"])\n</code></pre> Source code in <code>src/spotoptim/eda/plots.py</code> <pre><code>def plot_ip_boxplots(\n    df: pd.DataFrame,\n    category_column_name: str = None,\n    num_cols: int = 2,\n    figwidth: int = 10,\n    box_width: float = 0.2,\n    both_names=True,\n    height_per_subplot: float = 2.0,\n    add_points: pd.DataFrame = None,\n    add_points_col: list = [\"red\"],\n) -&gt; None:\n    \"\"\"\n    Generate infill-point boxplots (ip-boxplots). A separate ip-boxplot is generated for each numerical column in a DataFrame, arranged in a grid.\n    Each subplot has its own scale, similar to how histograms are shown in plot_histograms().\n    Additional points can be added and highlighted in red.\n\n    Args:\n        df (pd.DataFrame):\n            DataFrame containing the data to plot.\n        category_column_name (str, optional):\n            Column name for categorical grouping. Defaults to None.\n        num_cols (int, optional):\n            Number of columns in the subplot grid. Defaults to 2.\n        figwidth (int, optional):\n            Width of the entire figure. Defaults to 10.\n        box_width (float, optional):\n            Width of the boxplots. Defaults to 0.2.\n        both_names (bool, optional):\n            Whether to show both variable names and categories in titles. Defaults to True.\n        height_per_subplot (float, optional):\n            Height per subplot row. Defaults to 2.0.\n        add_points (pd.DataFrame, optional):\n            DataFrame containing additional points to highlight. Defaults to None.\n        add_points_col (list, optional):\n            List of colors for the additional points. Defaults to [\"red\"].\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotoptim.eda.plots import plot_ip_boxplots\n        &gt;&gt;&gt; data = {'A': [1, 2, 2, 3, 4, 5, 100], 'B': [10, 10, 10, 10, 10, 10, 10]}\n        &gt;&gt;&gt; df = pd.DataFrame(data)\n        &gt;&gt;&gt; plot_ip_boxplots(df, num_cols=1)\n        &gt;&gt;&gt; # Example with multiple added points and colors\n        &gt;&gt;&gt; add_points = pd.DataFrame({'A': [1.5, 3.5], 'B': [10, 10]})\n        &gt;&gt;&gt; plot_ip_boxplots(df, add_points=add_points, add_points_col=[\"red\", \"blue\"])\n    \"\"\"\n    if df.ndim == 1:\n        df = df.to_frame()\n    numerical_columns = df.select_dtypes(include=\"number\").columns.tolist()\n    num_plots = len(numerical_columns)\n    num_rows = (num_plots + num_cols - 1) // num_cols\n    fig, axes = plt.subplots(\n        nrows=num_rows,\n        ncols=num_cols,\n        figsize=(figwidth, num_rows * height_per_subplot),\n    )\n    # Ensure axes is always an array\n    if num_rows == 1 and num_cols == 1:\n        axes = np.array([axes])\n    else:\n        axes = axes.flatten()\n    for i, col in enumerate(numerical_columns):\n        ax = axes[i]\n        if category_column_name and category_column_name in df.columns:\n            unique_categories = sorted(df[category_column_name].dropna().unique())\n            plot_data = [\n                df.loc[df[category_column_name] == cat_value, col].dropna()\n                for cat_value in unique_categories\n            ]\n        else:\n            plot_data = [df[col].dropna()]\n        ax.boxplot(\n            plot_data,\n            orientation=\"horizontal\",\n            patch_artist=True,\n            boxprops=dict(facecolor=\"lightblue\", color=\"black\"),\n            medianprops=dict(color=\"red\"),\n            whiskerprops=dict(color=\"black\"),\n            capprops=dict(color=\"black\"),\n            flierprops=dict(marker=\"o\", color=\"black\", alpha=0.5),\n            widths=box_width,\n        )\n        if add_points is not None and col in add_points.columns:\n            if len(add_points) != len(add_points_col):\n                raise ValueError(\n                    f\"Length of add_points ({len(add_points)}) and add_points_col ({len(add_points_col)}) must be the same.\"\n                )\n\n            points_data = add_points[[col]].copy()\n            points_data[\"color\"] = add_points_col\n            points_data = points_data.dropna(subset=[col])\n\n            points = points_data[col]\n            colors = points_data[\"color\"]\n\n            ax.scatter(\n                points,\n                [1] * len(points),\n                c=colors,\n                marker=\"D\",\n                edgecolor=\"k\",\n                label=\"Additional Points\",\n                zorder=3,\n            )\n        if both_names:\n            ax.set_title(col)\n        else:\n            ax.set_title(col)\n        ax.set_xlabel(\"Value\")\n        if category_column_name and category_column_name in df.columns:\n            ax.set_yticklabels(unique_categories)\n            ax.set_ylabel(category_column_name)\n        else:\n            ax.set_yticklabels([\"\"])\n        ax.xaxis.grid(True, linestyle=\"--\", linewidth=0.5)\n        ax.yaxis.grid(True, linestyle=\"--\", linewidth=0.5)\n    for j in range(num_plots, len(axes)):\n        fig.delaxes(axes[j])\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/spotoptim/eda/plots/#spotoptim.eda.plots.plot_ip_histograms","title":"<code>plot_ip_histograms(df, bins=10, num_cols=2, figwidth=10, thrs_unique=5, add_points=None, add_points_col=['red'])</code>","text":"<p>Generate infill-point histograms (ip-histograms) for each numerical column in the DataFrame within a single figure. The title of each histogram shows the total, unique values count, outliers, and standard deviation. If there are fewer unique values than the threshold thrs_unique, the ip-histogram is colored differently. Additional points can be added and highlighted in red.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the data to plot.</p> required <code>bins</code> <code>int</code> <p>Number of bins for the histograms. Defaults to 10.</p> <code>10</code> <code>num_cols</code> <code>int</code> <p>Number of columns in the subplot grid. Defaults to 2.</p> <code>2</code> <code>figwidth</code> <code>int</code> <p>Width of the entire figure. Defaults to 10.</p> <code>10</code> <code>thrs_unique</code> <code>int</code> <p>Threshold for unique values to change histogram color. Defaults to 5.</p> <code>5</code> <code>add_points</code> <code>DataFrame</code> <p>DataFrame containing additional points to highlight. Defaults to None.</p> <code>None</code> <code>add_points_col</code> <code>list</code> <p>List of colors for the additional points. Defaults to [\u201cred\u201d].</p> <code>['red']</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotoptim.eda.plots import plot_ip_histograms\n&gt;&gt;&gt; data = {'A': [1, 2, 2, 3, 4, 5, 100], 'B': [10, 10, 10, 10, 10, 10, 10]}\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; plot_ip_histograms(df, bins=5, num_cols=1, thrs_unique=3)\n&gt;&gt;&gt; # Example with multiple added points and colors\n&gt;&gt;&gt; add_points = pd.DataFrame({'A': [1.5, 3.5], 'B': [10, 10]})\n&gt;&gt;&gt; plot_ip_histograms(df, add_points=add_points, add_points_col=[\"red\", \"blue\"])\n</code></pre> Source code in <code>src/spotoptim/eda/plots.py</code> <pre><code>def plot_ip_histograms(\n    df: pd.DataFrame,\n    bins=10,\n    num_cols=2,\n    figwidth=10,\n    thrs_unique=5,\n    add_points: pd.DataFrame = None,\n    add_points_col: list = [\"red\"],\n) -&gt; None:\n    \"\"\"\n    Generate infill-point histograms (ip-histograms) for each numerical column in the DataFrame within a single figure.\n    The title of each histogram shows the total, unique values count, outliers, and standard deviation.\n    If there are fewer unique values than the threshold thrs_unique, the ip-histogram is colored differently.\n    Additional points can be added and highlighted in red.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the data to plot.\n        bins (int, optional): Number of bins for the histograms. Defaults to 10.\n        num_cols (int, optional): Number of columns in the subplot grid. Defaults to 2.\n        figwidth (int, optional): Width of the entire figure. Defaults to 10.\n        thrs_unique (int, optional): Threshold for unique values to change histogram color. Defaults to 5.\n        add_points (pd.DataFrame, optional): DataFrame containing additional points to highlight. Defaults to None.\n        add_points_col (list, optional): List of colors for the additional points. Defaults to [\"red\"].\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotoptim.eda.plots import plot_ip_histograms\n        &gt;&gt;&gt; data = {'A': [1, 2, 2, 3, 4, 5, 100], 'B': [10, 10, 10, 10, 10, 10, 10]}\n        &gt;&gt;&gt; df = pd.DataFrame(data)\n        &gt;&gt;&gt; plot_ip_histograms(df, bins=5, num_cols=1, thrs_unique=3)\n        &gt;&gt;&gt; # Example with multiple added points and colors\n        &gt;&gt;&gt; add_points = pd.DataFrame({'A': [1.5, 3.5], 'B': [10, 10]})\n        &gt;&gt;&gt; plot_ip_histograms(df, add_points=add_points, add_points_col=[\"red\", \"blue\"])\n    \"\"\"\n    numerical_columns = df.select_dtypes(include=\"number\").columns.tolist()\n    num_plots = len(numerical_columns)\n    num_rows = (num_plots + num_cols - 1) // num_cols\n    fig, axes = plt.subplots(\n        nrows=num_rows, ncols=num_cols, figsize=(figwidth, num_rows * 5)\n    )\n    # Ensure axes is always an array\n    if num_rows == 1 and num_cols == 1:\n        axes = np.array([axes])\n    else:\n        axes = axes.flatten()\n    for i, col in enumerate(numerical_columns):\n        ax = axes[i]\n        data = df[col].dropna()\n        total_points = data.size\n        unique_values = data.nunique()\n        num_outliers = calculate_outliers(data)\n        std_dev = data.std()\n        fill_color = \"lightcoral\" if unique_values &lt; thrs_unique else \"lightblue\"\n        ax.hist(data, bins=bins, alpha=0.7, color=fill_color, edgecolor=\"black\")\n        if add_points is not None and col in add_points.columns:\n            if len(add_points) != len(add_points_col):\n                raise ValueError(\n                    f\"Length of add_points ({len(add_points)}) and add_points_col ({len(add_points_col)}) must be the same.\"\n                )\n\n            points_data = add_points[[col]].copy()\n            points_data[\"color\"] = add_points_col\n            points_data = points_data.dropna(subset=[col])\n\n            points = points_data[col]\n            colors = points_data[\"color\"]\n\n            ax.scatter(\n                points,\n                [0] * len(points),\n                label=\"Additional Points\",\n                zorder=3,\n                c=colors,\n                marker=\"D\",\n                edgecolor=\"k\",\n            )\n        ax.set_title(\n            f\"Total={total_points}, Unique={unique_values}, Outliers={num_outliers}, StdDev={std_dev:.2f}\"\n        )\n        ax.set_xlabel(col)\n        ax.set_ylabel(\"Frequency\")\n        ax.grid(True, linestyle=\"--\", linewidth=0.5)\n    for i in range(num_plots, len(axes)):\n        fig.delaxes(axes[i])\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/","title":"confirmatory_factor_analyzer","text":"<p>Confirmatory factor analysis using machine learning methods. Re-implementation of the factor-analyzer package.</p> <p>See https://factor-analyzer.readthedocs.io/en/latest/introduction.html for more details.</p> <p>Authors of the original implementation: * Jeremy Biggs (jeremy.m.biggs@gmail.com) * Nitin Madnani (nmadnani@ets.org) Organization: Educational Testing Service Date: 2022-09-05</p> <p>This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.</p> <p>This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer","title":"<code>ConfirmatoryFactorAnalyzer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Fit a confirmatory factor analysis model using maximum likelihood.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer--parameters","title":"Parameters","text":"<p>specification : :class:<code>ModelSpecification</code> or None, optional     A model specification. This must be a :class:<code>ModelSpecification</code> object     or <code>None</code>. If <code>None</code>, a :class:<code>ModelSpecification</code> object will be     generated assuming that <code>n_factors</code> == <code>n_variables</code>, and that     all variables load on all factors. Note that this could mean the     factor model is not identified, and the optimization could fail.     Defaults to <code>None</code>. n_obs : int or None, optional     The number of observations in the original data set.     If this is not passed and <code>is_cov_matrix</code> is <code>True</code>,     then an error will be raised.     Defaults to <code>None</code>. is_cov_matrix : bool, optional     Whether the input <code>X</code> is a covariance matrix. If <code>False</code>,     assume it is the full data set.     Defaults to <code>False</code>. bounds : list of tuples or None, optional     A list of minimum and maximum boundaries for each element     of the input array. This must equal <code>x0</code>, which is the     input array from your parsed and combined model specification.</p> <pre><code>The length is:\n((n_factors * n_variables) + n_variables + n_factors +\n(((n_factors * n_factors) - n_factors) // 2)\n\nIf `None`, nothing will be bounded.\nDefaults to ``None``.\n</code></pre> <p>max_iter : int, optional     The maximum number of iterations for the optimization routine.     Defaults to 200. tol : float or None, optional     The tolerance for convergence.     Defaults to <code>None</code>. disp : bool, optional     Whether to print the scipy optimization <code>fmin</code> message to     standard output.     Defaults to <code>True</code>.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer--raises","title":"Raises","text":"<p>ValueError     If <code>is_cov_matrix</code> is <code>True</code>, and <code>n_obs</code> is not provided.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer--attributes","title":"Attributes","text":"<p>model : ModelSpecification     The model specification object. loadings_ : :obj:<code>numpy.ndarray</code>     The factor loadings matrix.     <code>None</code>, if <code>`fit()``` has not been called. error_vars_ : :obj:</code>numpy.ndarray<code>The error variance matrix factor_varcovs_ : :obj:</code>numpy.ndarray`     The factor covariance matrix. log_likelihood_ : float     The log likelihood from the optimization routine. aic_ : float     The Akaike information criterion. bic_ : float     The Bayesian information criterion.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer--examples","title":"Examples","text":"<p>import pandas as pd from factor_analyzer import (ConfirmatoryFactorAnalyzer, \u2026                              ModelSpecificationParser) X = pd.read_csv(\u2018tests/data/test11.csv\u2019) model_dict = {\u201cF1\u201d: [\u201cV1\u201d, \u201cV2\u201d, \u201cV3\u201d, \u201cV4\u201d], \u2026               \u201cF2\u201d: [\u201cV5\u201d, \u201cV6\u201d, \u201cV7\u201d, \u201cV8\u201d]} model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict) cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False) cfa.fit(X.values) cfa.loadings_ array([[0.99131285, 0.        ],        [0.46074919, 0.        ],        [0.3502267 , 0.        ],        [0.58331488, 0.        ],        [0.        , 0.98621042],        [0.        , 0.73389239],        [0.        , 0.37602988],        [0.        , 0.50049507]]) cfa.factor_varcovs_ array([[1.        , 0.17385704],        [0.17385704, 1.        ]]) cfa.get_standard_errors() (array([[0.06779949, 0.        ],        [0.04369956, 0.        ],        [0.04153113, 0.        ],        [0.04766645, 0.        ],        [0.        , 0.06025341],        [0.        , 0.04913149],        [0.        , 0.0406604 ],        [0.        , 0.04351208]]),  array([0.11929873, 0.05043616, 0.04645803, 0.05803088,         0.10176889, 0.06607524, 0.04742321, 0.05373646])) cfa.transform(X.values) array([[-0.46852166, -1.08708035],        [ 2.59025301,  1.20227783],        [-0.47215977,  2.65697245],        \u2026,        [-1.5930886 , -0.91804114],        [ 0.19430887,  0.88174818],        [-0.27863554, -0.7695101 ]])</p> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>class ConfirmatoryFactorAnalyzer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Fit a confirmatory factor analysis model using maximum likelihood.\n\n    Parameters\n    ----------\n    specification : :class:`ModelSpecification` or None, optional\n        A model specification. This must be a :class:`ModelSpecification` object\n        or ``None``. If ``None``, a :class:`ModelSpecification` object will be\n        generated assuming that ``n_factors`` == ``n_variables``, and that\n        all variables load on all factors. Note that this could mean the\n        factor model is not identified, and the optimization could fail.\n        Defaults to `None`.\n    n_obs : int or None, optional\n        The number of observations in the original data set.\n        If this is not passed and ``is_cov_matrix`` is ``True``,\n        then an error will be raised.\n        Defaults to ``None``.\n    is_cov_matrix : bool, optional\n        Whether the input ``X`` is a covariance matrix. If ``False``,\n        assume it is the full data set.\n        Defaults to ``False``.\n    bounds : list of tuples or None, optional\n        A list of minimum and maximum boundaries for each element\n        of the input array. This must equal ``x0``, which is the\n        input array from your parsed and combined model specification.\n\n        The length is:\n        ((n_factors * n_variables) + n_variables + n_factors +\n        (((n_factors * n_factors) - n_factors) // 2)\n\n        If `None`, nothing will be bounded.\n        Defaults to ``None``.\n    max_iter : int, optional\n        The maximum number of iterations for the optimization routine.\n        Defaults to 200.\n    tol : float or None, optional\n        The tolerance for convergence.\n        Defaults to ``None``.\n    disp : bool, optional\n        Whether to print the scipy optimization ``fmin`` message to\n        standard output.\n        Defaults to ``True``.\n\n    Raises\n    ------\n    ValueError\n        If `is_cov_matrix` is `True`, and `n_obs` is not provided.\n\n    Attributes\n    ----------\n    model : ModelSpecification\n        The model specification object.\n    loadings_ : :obj:`numpy.ndarray`\n        The factor loadings matrix.\n        ``None``, if ``fit()``` has not been called.\n    error_vars_ : :obj:`numpy.ndarray`\n        The error variance matrix\n    factor_varcovs_ : :obj:`numpy.ndarray`\n        The factor covariance matrix.\n    log_likelihood_ : float\n        The log likelihood from the optimization routine.\n    aic_ : float\n        The Akaike information criterion.\n    bic_ : float\n        The Bayesian information criterion.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n    ...                              ModelSpecificationParser)\n    &gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n    &gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n    ...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n    &gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n    &gt;&gt;&gt; cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n    &gt;&gt;&gt; cfa.fit(X.values)\n    &gt;&gt;&gt; cfa.loadings_\n    array([[0.99131285, 0.        ],\n           [0.46074919, 0.        ],\n           [0.3502267 , 0.        ],\n           [0.58331488, 0.        ],\n           [0.        , 0.98621042],\n           [0.        , 0.73389239],\n           [0.        , 0.37602988],\n           [0.        , 0.50049507]])\n    &gt;&gt;&gt; cfa.factor_varcovs_\n    array([[1.        , 0.17385704],\n           [0.17385704, 1.        ]])\n    &gt;&gt;&gt; cfa.get_standard_errors()\n    (array([[0.06779949, 0.        ],\n           [0.04369956, 0.        ],\n           [0.04153113, 0.        ],\n           [0.04766645, 0.        ],\n           [0.        , 0.06025341],\n           [0.        , 0.04913149],\n           [0.        , 0.0406604 ],\n           [0.        , 0.04351208]]),\n     array([0.11929873, 0.05043616, 0.04645803, 0.05803088,\n            0.10176889, 0.06607524, 0.04742321, 0.05373646]))\n    &gt;&gt;&gt; cfa.transform(X.values)\n    array([[-0.46852166, -1.08708035],\n           [ 2.59025301,  1.20227783],\n           [-0.47215977,  2.65697245],\n           ...,\n           [-1.5930886 , -0.91804114],\n           [ 0.19430887,  0.88174818],\n           [-0.27863554, -0.7695101 ]])\n    \"\"\"\n\n    def __init__(\n        self,\n        specification=None,\n        n_obs=None,\n        is_cov_matrix=False,\n        bounds=None,\n        max_iter=200,\n        tol=None,\n        impute=\"median\",\n        disp=True,\n    ):\n        \"\"\"Initialize the analyzer object.\"\"\"\n        # if the input is going to be a covariance matrix, rather than\n        # the full data set, then users must pass the number of observations\n        if is_cov_matrix and n_obs is None:\n            raise ValueError(\n                \"If `is_cov_matrix=True`, you must provide \"\n                \"the number of observations, `n_obs`.\"\n            )\n\n        self.specification = specification\n        self.n_obs = n_obs\n        self.is_cov_matrix = is_cov_matrix\n        self.bounds = bounds\n        self.max_iter = max_iter\n        self.tol = tol\n        self.impute = impute\n        self.disp = disp\n\n        self.cov_ = None\n        self.mean_ = None\n        self.loadings_ = None\n        self.error_vars_ = None\n        self.factor_varcovs_ = None\n\n        self.log_likelihood_ = None\n        self.aic_ = None\n        self.bic_ = None\n\n        self._n_factors = None\n        self._n_variables = None\n        self._n_lower_diag = None\n\n    @staticmethod\n    def _combine(\n        loadings,\n        error_vars,\n        factor_vars,\n        factor_covs,\n        n_factors,\n        n_variables,\n        n_lower_diag,\n    ):\n        \"\"\"\n        Combine given multi-dimensional matrics into a one-dimensional matrix.\n\n        Combine a set of multi-dimensional loading, error variance, factor\n        variance, and factor covariance matrices into a one-dimensional\n        (X, 1) matrix, where the length of X is the length of all elements\n        across all of the matrices.\n\n        Args:\n            loadings (array-like): The loadings matrix (``n_factors`` * ``n_variables``)\n            error_vars (array-like): The error variance array. (``n_variables`` * 1)\n            factor_vars (array-like): The factor variance array. (``n_factors`` * 1)\n            factor_covs (array-like): The factor covariance array. (``n_lower_diag`` * 1)\n            n_factors (int): The number of factors.\n            n_variables (int): The number of variables.\n            n_lower_diag (int): The number of elements in the ``factor_covs`` array, which\n                is equal to the lower diagonal of the factor covariance\n                matrix.\n\n        Returns:\n            array (numpy.ndarray): The combined (X, 1) array.\n        \"\"\"\n        loadings = loadings.reshape(n_factors * n_variables, 1, order=\"F\")\n        error_vars = error_vars.reshape(n_variables, 1, order=\"F\")\n        factor_vars = factor_vars.reshape(n_factors, 1, order=\"F\")\n        factor_covs = factor_covs.reshape(n_lower_diag, 1, order=\"F\")\n        return np.concatenate([loadings, error_vars, factor_vars, factor_covs])\n\n    @staticmethod\n    def _split(x, n_factors, n_variables, n_lower_diag):\n        \"\"\"\n        Split given one-dimensional array into multi-dimensional arrays.\n\n        Given a one-dimensional array, split it into a set of\n        multi-dimensional loading, error variance, factor variance,\n        and factor covariance matrices.\n\n        Args:\n            x (array-like): The combined (X, 1) array to split.\n            n_factors (int): The number of factors.\n            n_variables (int): The number of variables.\n            n_lower_diag (int): The number of elements in the ``factor_covs`` array, which is\n                equal to the lower diagonal of the factor covariance matrix.\n\n        Returns:\n            tuple:\n                - loadings (array-like): The loadings matrix (``n_factors`` * ``n_variables``)\n                - error_vars (array-like): The error variance array (``n_variables`` * 1)\n                - factor_vars (array-like): The factor variance array (``n_factors`` * 1)\n                - factor_covs (array-like): The factor covariance array (``n_lower_diag`` * 1)\n        \"\"\"\n        loadings_ix = int(n_factors * n_variables)\n        error_vars_ix = n_variables + loadings_ix\n        factor_vars_ix = n_factors + error_vars_ix\n        factor_covs_ix = n_lower_diag + factor_vars_ix\n        return (\n            x[:loadings_ix].reshape((n_variables, n_factors), order=\"F\"),\n            x[loadings_ix:error_vars_ix].reshape((n_variables, 1), order=\"F\"),\n            x[error_vars_ix:factor_vars_ix].reshape((n_factors, 1), order=\"F\"),\n            x[factor_vars_ix:factor_covs_ix].reshape((n_lower_diag, 1), order=\"F\"),\n        )\n\n    def _objective(self, x0, cov_mtx, loadings):\n        \"\"\"\n        Get the objective function for the analyzer.\n\n        Args:\n            x0 (array-like): The combined (X, 1) array. These are the initial values for\n                the ``minimize`()` function.\n            cov_mtx (array-like): The covariance matrix from the original data set.\n            loadings (array-like): The loadings matrix (``n_factors`` * ``n_variables``)\n                from the model parser. This tells the objective function what\n                elements should be fixed.\n\n        Returns:\n            error (float): The error from the objective function.\n        \"\"\"\n        (\n            loadings_init,\n            error_vars_init,\n            factor_vars_init,\n            factor_covs_init,\n        ) = self._split(\n            x0, self.model.n_factors, self.model.n_variables, self.model.n_lower_diag\n        )\n\n        # set the loadings to zero where applicable\n        loadings_init[np.where(loadings == 0)] = 0\n\n        # combine factor variances and covariances into a single matrix\n        factor_varcov_init = merge_variance_covariance(\n            factor_vars_init, factor_covs_init\n        )\n\n        # make the error variance into a variance-covariance matrix\n        error_varcov_init = merge_variance_covariance(error_vars_init)\n\n        # make the factor variance-covariance matrix into a correlation matrix\n        with np.errstate(all=\"ignore\"):\n            factor_varcov_init = covariance_to_correlation(factor_varcov_init)\n\n        # calculate sigma-theta, needed for the objective function\n        sigma_theta = (\n            loadings_init.dot(factor_varcov_init).dot(loadings_init.T)\n            + error_varcov_init\n        )\n\n        with np.errstate(all=\"ignore\"):\n            error = -(\n                ((-self.n_obs * self.model.n_variables / 2) * np.log(2 * np.pi))\n                - (self.n_obs / 2)\n                * (\n                    np.log(np.linalg.det(sigma_theta))\n                    + np.trace(cov_mtx.dot(np.linalg.inv(sigma_theta)))\n                )\n            )\n\n            # make sure the error is greater than or\n            # equal to zero before we return it; we\n            # do not do this for the Bollen approach\n            error = 0.0 if error &lt; 0.0 else error\n\n        return error\n\n    def fit(self, X, y=None) -&gt; \"ConfirmatoryFactorAnalyzer\":\n        \"\"\"\n        Perform confirmatory factor analysis.\n\n        Args:\n            X (array-like): The data to use for confirmatory factor analysis. If this is just a\n                covariance matrix, make sure ``is_cov_matrix`` was set to ``True``.\n            y (ignored): Ignored.\n\n        Returns:\n            self: The fitted confirmatory factor analyzer object.\n\n        Raises:\n            ValueError: If the specification is not None or a :class:`ModelSpecification` object.\n            AssertionError: If ``is_cov_matrix`` was ``True`` and the matrix is not square.\n            AssertionError: If ``len(bounds)`` != ``len(x0)``\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n            ...                              ModelSpecificationParser)\n            &gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n            &gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n            ...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n            &gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n            &gt;&gt;&gt; cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n            &gt;&gt;&gt; cfa.fit(X.values)\n            &gt;&gt;&gt; cfa.loadings_\n            array([[0.99131285, 0.        ],\n                   [0.46074919, 0.        ],\n                   [0.3502267 , 0.        ],\n                   [0.58331488, 0.        ],\n                   [0.        , 0.98621042],\n                   [0.        , 0.73389239],\n                   [0.        , 0.37602988],\n                   [0.        , 0.50049507]])\n        \"\"\"\n        if self.specification is None:\n            self.model = ModelSpecificationParser.parse_model_specification_from_array(\n                X\n            )\n        elif isinstance(self.specification, ModelSpecification):\n            self.model = self.specification.copy()\n        else:\n            raise ValueError(\n                \"The `specification` must be None or `ModelSpecification` \"\n                \"instance, not {}\".format(type(self.specification))\n            )\n\n        if isinstance(X, pd.DataFrame):\n            X = X.values\n\n        # now check the array, and make sure it\n        # meets all of our expected criteria\n        X = check_array(X, ensure_all_finite=\"allow-nan\", estimator=self, copy=True)\n\n        # check to see if there are any null values, and if\n        # so impute using the desired imputation approach\n        if np.isnan(X).any() and not self.is_cov_matrix:\n            X = impute_values(X, how=self.impute)\n\n        if not self.is_cov_matrix:\n            # make sure that the columns are in the proper order\n            # data = data[variable_names].copy()\n            # get the number of observations from the data, if `n_obs` not passed;\n            # then, calculate the covariance matrix from the data set\n            self.n_obs = X.shape[0] if self.n_obs is None else self.n_obs\n            self.mean_ = np.mean(X, axis=0)\n            cov_mtx = cov(X)\n        else:\n            error_msg = (\n                \"If `is_cov_matrix=True`, then the rows and column in the data \"\n                \"set must be equal, and must equal the number of variables \"\n                \"in your model.\"\n            )\n            assert X.shape[0] == X.shape[1] == self.model.n_variables, error_msg\n            cov_mtx = X.copy()\n\n        self.cov_ = cov_mtx.copy()\n\n        # we initialize all of the arrays, setting the covariances\n        # lower than the expected variances, and the loadings to 1 or 0\n        loading_init = self.model.loadings\n        error_vars_init = np.full((self.model.n_variables, 1), 0.5)\n        factor_vars_init = np.full((self.model.n_factors, 1), 1.0)\n        factor_covs_init = np.full((self.model.n_lower_diag, 1), 0.05)\n\n        # we merge all of the arrays into a single 1d vector\n        x0 = self._combine(\n            loading_init,\n            error_vars_init,\n            factor_vars_init,\n            factor_covs_init,\n            self.model.n_factors,\n            self.model.n_variables,\n            self.model.n_lower_diag,\n        )\n\n        # if the bounds argument is None, then we initialized the\n        # boundaries to (None, None) for everything except factor covariances;\n        # at some point in the future, we may update this to place limits\n        # on the loading matrix boundaries, too, but the case in R and SAS\n        if self.bounds is not None:\n            error_msg = (\n                \"The length of `bounds` must equal the length of your \"\n                \"input array `x0`: {} != {}.\".format(len(self.bounds), len(x0))\n            )\n            assert len(self.bounds) == len(x0), error_msg\n\n        # fit the actual model using L-BFGS algorithm;\n        # the constraints are set inside the objective function,\n        # so that we can avoid using linear programming methods (e.g. SLSQP)\n        res = minimize(\n            self._objective,\n            x0.flatten(),\n            method=\"L-BFGS-B\",\n            options={\"maxiter\": self.max_iter, \"disp\": self.disp},\n            bounds=self.bounds,\n            args=(cov_mtx, self.model.loadings),\n        )\n\n        # if the optimizer failed to converge, print the message\n        if not res.success:\n            warnings.warn(\n                f\"The optimization routine failed to converge: {str(res.message)}\"\n            )\n\n        # we split all the 1d array back into the set of original arrays\n        loadings_res, error_vars_res, factor_vars_res, factor_covs_res = self._split(\n            res.x, self.model.n_factors, self.model.n_variables, self.model.n_lower_diag\n        )\n\n        # we combine the factor covariances and variances into\n        # a single variance-covariance matrix to make things easier,\n        # but also check to make see if anything was fixed\n        factor_varcovs_res = merge_variance_covariance(factor_vars_res, factor_covs_res)\n        with np.errstate(all=\"ignore\"):\n            factor_varcovs_res = covariance_to_correlation(factor_varcovs_res)\n\n        self.loadings_ = loadings_res\n        self.error_vars_ = error_vars_res\n        self.factor_varcovs_ = factor_varcovs_res\n\n        # we also calculate the log-likelihood, AIC, and BIC\n        self.log_likelihood_ = -res.fun\n        self.aic_ = 2 * res.fun + 2 * (x0.shape[0] + self.model.n_variables)\n        if self.n_obs is not None:\n            self.bic_ = 2 * res.fun + np.log(self.n_obs) * (\n                x0.shape[0] + self.model.n_variables\n            )\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Get the factor scores for a new data set.\n\n        Args:\n            X (array-like): The data to score using the fitted factor model, shape (``n_samples``, ``n_features``).\n\n        Returns:\n            scores (numpy.ndarray): The latent variables of X, shape (``n_samples``, ``n_components``).\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n            ...                              ModelSpecificationParser)\n            &gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n            &gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n            ...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n            &gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n            &gt;&gt;&gt; cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n            &gt;&gt;&gt; cfa.fit(X.values)\n            &gt;&gt;&gt; cfa.transform(X.values)\n            array([[-0.46852166, -1.08708035],\n                   [ 2.59025301,  1.20227783],\n                   [-0.47215977,  2.65697245],\n                   ...,\n                   [-1.5930886 , -0.91804114],\n                   [ 0.19430887,  0.88174818],\n               [-0.27863554, -0.7695101 ]])\n\n        References:\n            https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157408/\n        \"\"\"\n        # check if the data is a data frame,\n        # so we can convert it to an array\n        if isinstance(X, pd.DataFrame):\n            X = X.values\n\n        # now check the array, and make sure it\n        # meets all of our expected criteria\n        X = check_array(X, ensure_all_finite=True, estimator=self, copy=True)\n\n        # meets all of our expected criteria\n        check_is_fitted(self, [\"loadings_\", \"error_vars_\"])\n\n        # see if we saved the original mean and std\n        if self.mean_ is None:\n            warnings.warn(\n                \"Could not find original mean; using the mean \"\n                \"from the current data set.\"\n            )\n            mean = np.mean(X, axis=0)\n        else:\n            mean = self.mean_\n\n        # get the scaled data\n        X_scale = X - mean\n\n        # get the loadings and error variances\n        loadings = self.loadings_.copy()\n        error_vars = self.error_vars_.copy()\n\n        # make the error variance an identity matrix,\n        # and take the inverse of this matrix\n        error_covs = np.eye(error_vars.shape[0])\n        np.fill_diagonal(error_covs, error_vars)\n        error_covs_inv = np.linalg.inv(error_covs)\n\n        # calculate the weights, using Bartlett's formula\n        # (lambda' error_covs\u02c6\u22121 lambda)\u02c6\u22121 lambda' error_covs\u02c6\u22121 (X - muX)\n        weights = (\n            np.linalg.pinv(loadings.T.dot(error_covs_inv).dot(loadings))\n            .dot(loadings.T)\n            .dot(error_covs_inv)\n        )\n\n        scores = weights.dot(X_scale.T).T\n        return scores\n\n    def get_model_implied_cov(self):\n        \"\"\"\n        Get the model-implied covariance matrix (sigma) for an estimated model.\n\n        Returns:\n            model_implied_cov (numpy.ndarray): The model-implied covariance matrix.\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n            ...                              ModelSpecificationParser)\n            &gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n            &gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n            ...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n            &gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n            &gt;&gt;&gt; cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n            &gt;&gt;&gt; cfa.fit(X.values)\n            &gt;&gt;&gt; cfa.get_model_implied_cov()\n            array([[2.07938612, 0.45674659, 0.34718423, 0.57824753, 0.16997013,\n                    0.12648394, 0.06480751, 0.08625868],\n                   [0.45674659, 1.16703337, 0.16136667, 0.26876186, 0.07899988,\n                    0.05878807, 0.03012168, 0.0400919 ],\n                   [0.34718423, 0.16136667, 1.07364855, 0.20429245, 0.06004974,\n                    0.04468625, 0.02289622, 0.03047483],\n                   [0.57824753, 0.26876186, 0.20429245, 1.28809317, 0.10001495,\n                    0.07442652, 0.03813447, 0.05075691],\n                   [0.16997013, 0.07899988, 0.06004974, 0.10001495, 2.0364391 ,\n                    0.72377232, 0.37084458, 0.49359346],\n                   [0.12648394, 0.05878807, 0.04468625, 0.07442652, 0.72377232,\n                    1.48080077, 0.27596546, 0.36730952],\n                   [0.06480751, 0.03012168, 0.02289622, 0.03813447, 0.37084458,\n                    0.27596546, 1.11761918, 0.1882011 ],\n                   [0.08625868, 0.0400919 , 0.03047483, 0.05075691, 0.49359346,\n                    0.36730952, 0.1882011 , 1.28888233]])\n        \"\"\"\n        # meets all of our expected criteria\n        check_is_fitted(self, [\"loadings_\", \"factor_varcovs_\"])\n        error = np.diag(self.error_vars_.flatten())\n        return self.loadings_.dot(self.factor_varcovs_).dot(self.loadings_.T) + error\n\n    def _get_derivatives_implied_cov(self):\n        \"\"\"\n        Compute the derivatives for the implied covariance matrix (sigma).\n\n        Returns:\n            tuple:\n                - loadings_dx (numpy.ndarray): The derivative of the loadings matrix.\n                - factor_covs_dx (numpy.ndarray): The derivative of the factor covariance matrix.\n                - error_covs_dx (numpy.ndarray): The derivative of the error covariance matrix.\n        \"\"\"\n        # meets all of our expected criteria\n        check_is_fitted(self, \"loadings_\")\n\n        loadings = self.loadings_.copy()\n        factor_covs = self.factor_varcovs_.copy()\n\n        sym_lower_var_idx = get_symmetric_lower_idxs(self.model.n_variables)\n        sym_upper_fac_idx = get_symmetric_upper_idxs(self.model.n_factors, diag=False)\n        sym_lower_fac_idx = get_symmetric_lower_idxs(self.model.n_factors, diag=False)\n\n        factors_diag = np.eye(self.model.n_factors)\n        factors_diag_mult = (\n            factors_diag.dot(factor_covs).dot(factors_diag.T).dot(loadings.T)\n        )\n\n        # calculate the derivative of the loadings matrix, using the commutation matrix\n        loadings_dx = np.eye(self.model.n_variables**2) + commutation_matrix(\n            self.model.n_variables, self.model.n_variables\n        )\n        loadings_dx = loadings_dx.dot(\n            np.kron(factors_diag_mult, np.eye(self.model.n_variables)).T\n        )\n\n        # calculate the derivative of the factor_covs matrix\n        factor_covs_dx = loadings.dot(factors_diag)\n        factor_covs_dx = np.kron(factor_covs_dx, factor_covs_dx)\n\n        off_diag = (\n            factor_covs_dx[:, sym_lower_fac_idx] + factor_covs_dx[:, sym_upper_fac_idx]\n        )\n\n        combine_indices = np.concatenate([sym_upper_fac_idx, sym_lower_fac_idx])\n        combine_diag = np.concatenate([off_diag, off_diag], axis=1)\n\n        factor_covs_dx[:, combine_indices] = combine_diag\n        factor_covs_dx = factor_covs_dx[:, : factor_covs.size]\n\n        # calculate the derivative of the error_cov matrix,\n        # which we assume will always be a diagonal matrix\n        error_covs_dx = np.eye(self.model.n_variables**2)\n\n        # make sure these matrices are symmetric\n        loadings_dx = loadings_dx[sym_lower_var_idx, :]\n        factor_covs_dx = factor_covs_dx[sym_lower_var_idx, :]\n        error_covs_dx = error_covs_dx[sym_lower_var_idx, :]\n\n        # we also want to calculate the derivative for the intercepts\n        intercept_dx = np.zeros(\n            (loadings_dx.shape[0], self.model.n_variables), dtype=float\n        )\n        return (\n            loadings_dx[:, self.model.loadings_free].copy(),\n            factor_covs_dx[:, self.model.factor_covs_free].copy(),\n            error_covs_dx[:, self.model.error_vars_free].copy(),\n            intercept_dx,\n        )\n\n    def _get_derivatives_implied_mu(self):\n        \"\"\"\n        Compute the \"derivatives\" for the implied means.\n\n        Note that the derivatives of the implied means will not correspond\n        to the actual mean values of the original data set, because that\n        data could be a covariance matrix, rather than the full data set.\n        Thus, we assume the mean values are zero and the data are normally\n        distributed.\n\n        Returns:\n            tuple:\n                - loadings_dx (numpy.ndarray): The derivative of the loadings means.\n                - factor_covs_dx (numpy.ndarray): The derivative of the factor covariance means.\n                - error_covs_dx (numpy.ndarray): The derivative of the error covariance means.\n        \"\"\"\n        # meets all of our expected criteria\n        check_is_fitted(self, \"loadings_\")\n\n        # initialize some matrices that we'll use below to\n        # correct the shape of the mean loadings derivatives\n        factors_zero = np.zeros((self.model.n_factors, 1))\n        factors_diag = np.eye(self.model.n_factors)\n\n        # the mean derivatives will just be zeros for both\n        # the error covariance and factor covariance matrices,\n        # since we don't have actual mean values\n        error_covs_dx = np.zeros(\n            (self.model.n_variables, len(self.model.error_vars_free))\n        )\n        factor_covs_dx = np.zeros(\n            (self.model.n_variables, len(self.model.factor_covs_free))\n        )\n\n        # again, the implied means are just going to be diagonal matrices\n        # corresponding to the number of variables and factors\n        loadings_dx = np.kron(\n            factors_diag.dot(factors_zero).T, np.eye(self.model.n_variables)\n        )\n        loadings_dx = loadings_dx[:, self.model.loadings_free].copy()\n\n        # we also calculate the derivative for the intercept, which will be zeros again\n        intercept_dx = np.zeros((loadings_dx.shape[0], self.model.n_variables))\n        intercept_dx[: self.model.n_variables, : self.model.n_variables] = np.eye(\n            self.model.n_variables\n        )\n\n        return (loadings_dx, factor_covs_dx, error_covs_dx, intercept_dx)\n\n    def get_standard_errors(self) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Get standard errors from the implied covariance matrix and implied means.\n\n        Returns:\n            tuple:\n                - loadings_se (numpy.ndarray): The standard errors for the factor loadings.\n                - error_vars_se (numpy.ndarray): The standard errors for the error variances.\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n            ...                              ModelSpecificationParser)\n            &gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n            &gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n            ...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n            &gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n            &gt;&gt;&gt; cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n            &gt;&gt;&gt; cfa.fit(X.values)\n            &gt;&gt;&gt; cfa.get_standard_errors()\n            (array([[0.06779949, 0.        ],\n                   [0.04369956, 0.        ],\n                   [0.04153113, 0.        ],\n                   [0.04766645, 0.        ],\n                   [0.        , 0.06025341],\n                   [0.        , 0.04913149],\n                   [0.        , 0.0406604 ],\n                   [0.        , 0.04351208]]),\n             array([0.11929873, 0.05043616, 0.04645803, 0.05803088,\n                    0.10176889, 0.06607524, 0.04742321, 0.05373646]))\n        \"\"\"\n        # meets all of our expected criteria\n        check_is_fitted(self, [\"loadings_\", \"n_obs\"])\n\n        (\n            loadings_dx,\n            factor_covs_dx,\n            error_covs_dx,\n            intercept_dx,\n        ) = self._get_derivatives_implied_cov()\n\n        (\n            loadings_dx_mu,\n            factor_covs_dx_mu,\n            error_covs_dx_mu,\n            intercept_dx_mu,\n        ) = self._get_derivatives_implied_mu()\n\n        # combine all of our derivatives; below we will  merge all of these\n        # together in a single matrix, delta, to use the delta rule\n        # (basically, using the gradients to calculate the information)\n        loadings_dx = np.append(loadings_dx_mu, loadings_dx, axis=0)\n        factor_covs_dx = np.append(factor_covs_dx_mu, factor_covs_dx, axis=0)\n        error_cov_dx = np.append(error_covs_dx_mu, error_covs_dx, axis=0)\n        intercept_dx = np.append(intercept_dx_mu, intercept_dx, axis=0)\n\n        # get get the implied covariance, invert it, and take the Kronecker product\n        sigma = self.get_model_implied_cov()\n        sigma_inv = np.linalg.inv(sigma)\n        sigma_inv_kron = np.kron(sigma_inv, sigma_inv)\n\n        # we get the fisher information matrix for H1, which is the unrestricted\n        # model information; we'll use this with the deltas to calculate the full\n        # (inverted) information matrix below, and then invert the whole thing\n        h1_information = 0.5 * duplication_matrix_pre_post(sigma_inv_kron)\n        h1_information = block_diag(sigma_inv, h1_information)\n\n        # we concatenate all derivatives into a delta matrix\n        delta = np.concatenate(\n            (loadings_dx, error_cov_dx, factor_covs_dx, intercept_dx), axis=1\n        )\n\n        # calculate the fisher information matrix\n        information = delta.T.dot(h1_information).dot(delta)\n        information = (1 / self.n_obs) * np.linalg.inv(information)\n\n        # calculate the standard errors from the diagonal of the\n        # information / cov matrix; also take the absolute value,\n        # just in case anything is negative\n        se = np.sqrt(np.abs(np.diag(information)))\n\n        # get the indexes for the standard errors\n        # for the loadings and the errors variances;\n        # in the future, we may add the factor and intercept\n        # covariances, but these sometimes require transformations\n        # that are more complicated, so for now we won't return them\n        loadings_idx = len(self.model.loadings_free)\n        error_vars_idx = self.model.n_variables + loadings_idx\n\n        # get the loading standard errors and reshape them into the\n        # format of the original loadings matrix\n        loadings_se = np.zeros((self.model.n_factors * self.model.n_variables,))\n        loadings_se[self.model.loadings_free] = se[:loadings_idx]\n        loadings_se = loadings_se.reshape(\n            (self.model.n_variables, self.model.n_factors), order=\"F\"\n        )\n\n        # get the error variance standard errors\n        error_vars_se = se[loadings_idx:error_vars_idx]\n        return loadings_se, error_vars_se\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.__init__","title":"<code>__init__(specification=None, n_obs=None, is_cov_matrix=False, bounds=None, max_iter=200, tol=None, impute='median', disp=True)</code>","text":"<p>Initialize the analyzer object.</p> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>def __init__(\n    self,\n    specification=None,\n    n_obs=None,\n    is_cov_matrix=False,\n    bounds=None,\n    max_iter=200,\n    tol=None,\n    impute=\"median\",\n    disp=True,\n):\n    \"\"\"Initialize the analyzer object.\"\"\"\n    # if the input is going to be a covariance matrix, rather than\n    # the full data set, then users must pass the number of observations\n    if is_cov_matrix and n_obs is None:\n        raise ValueError(\n            \"If `is_cov_matrix=True`, you must provide \"\n            \"the number of observations, `n_obs`.\"\n        )\n\n    self.specification = specification\n    self.n_obs = n_obs\n    self.is_cov_matrix = is_cov_matrix\n    self.bounds = bounds\n    self.max_iter = max_iter\n    self.tol = tol\n    self.impute = impute\n    self.disp = disp\n\n    self.cov_ = None\n    self.mean_ = None\n    self.loadings_ = None\n    self.error_vars_ = None\n    self.factor_varcovs_ = None\n\n    self.log_likelihood_ = None\n    self.aic_ = None\n    self.bic_ = None\n\n    self._n_factors = None\n    self._n_variables = None\n    self._n_lower_diag = None\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Perform confirmatory factor analysis.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The data to use for confirmatory factor analysis. If this is just a covariance matrix, make sure <code>is_cov_matrix</code> was set to <code>True</code>.</p> required <code>y</code> <code>ignored</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>ConfirmatoryFactorAnalyzer</code> <p>The fitted confirmatory factor analyzer object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specification is not None or a :class:<code>ModelSpecification</code> object.</p> <code>AssertionError</code> <p>If <code>is_cov_matrix</code> was <code>True</code> and the matrix is not square.</p> <code>AssertionError</code> <p>If <code>len(bounds)</code> != <code>len(x0)</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n...                              ModelSpecificationParser)\n&gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n&gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n&gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n&gt;&gt;&gt; cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n&gt;&gt;&gt; cfa.fit(X.values)\n&gt;&gt;&gt; cfa.loadings_\narray([[0.99131285, 0.        ],\n       [0.46074919, 0.        ],\n       [0.3502267 , 0.        ],\n       [0.58331488, 0.        ],\n       [0.        , 0.98621042],\n       [0.        , 0.73389239],\n       [0.        , 0.37602988],\n       [0.        , 0.50049507]])\n</code></pre> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>def fit(self, X, y=None) -&gt; \"ConfirmatoryFactorAnalyzer\":\n    \"\"\"\n    Perform confirmatory factor analysis.\n\n    Args:\n        X (array-like): The data to use for confirmatory factor analysis. If this is just a\n            covariance matrix, make sure ``is_cov_matrix`` was set to ``True``.\n        y (ignored): Ignored.\n\n    Returns:\n        self: The fitted confirmatory factor analyzer object.\n\n    Raises:\n        ValueError: If the specification is not None or a :class:`ModelSpecification` object.\n        AssertionError: If ``is_cov_matrix`` was ``True`` and the matrix is not square.\n        AssertionError: If ``len(bounds)`` != ``len(x0)``\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n        ...                              ModelSpecificationParser)\n        &gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n        &gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n        ...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n        &gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n        &gt;&gt;&gt; cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n        &gt;&gt;&gt; cfa.fit(X.values)\n        &gt;&gt;&gt; cfa.loadings_\n        array([[0.99131285, 0.        ],\n               [0.46074919, 0.        ],\n               [0.3502267 , 0.        ],\n               [0.58331488, 0.        ],\n               [0.        , 0.98621042],\n               [0.        , 0.73389239],\n               [0.        , 0.37602988],\n               [0.        , 0.50049507]])\n    \"\"\"\n    if self.specification is None:\n        self.model = ModelSpecificationParser.parse_model_specification_from_array(\n            X\n        )\n    elif isinstance(self.specification, ModelSpecification):\n        self.model = self.specification.copy()\n    else:\n        raise ValueError(\n            \"The `specification` must be None or `ModelSpecification` \"\n            \"instance, not {}\".format(type(self.specification))\n        )\n\n    if isinstance(X, pd.DataFrame):\n        X = X.values\n\n    # now check the array, and make sure it\n    # meets all of our expected criteria\n    X = check_array(X, ensure_all_finite=\"allow-nan\", estimator=self, copy=True)\n\n    # check to see if there are any null values, and if\n    # so impute using the desired imputation approach\n    if np.isnan(X).any() and not self.is_cov_matrix:\n        X = impute_values(X, how=self.impute)\n\n    if not self.is_cov_matrix:\n        # make sure that the columns are in the proper order\n        # data = data[variable_names].copy()\n        # get the number of observations from the data, if `n_obs` not passed;\n        # then, calculate the covariance matrix from the data set\n        self.n_obs = X.shape[0] if self.n_obs is None else self.n_obs\n        self.mean_ = np.mean(X, axis=0)\n        cov_mtx = cov(X)\n    else:\n        error_msg = (\n            \"If `is_cov_matrix=True`, then the rows and column in the data \"\n            \"set must be equal, and must equal the number of variables \"\n            \"in your model.\"\n        )\n        assert X.shape[0] == X.shape[1] == self.model.n_variables, error_msg\n        cov_mtx = X.copy()\n\n    self.cov_ = cov_mtx.copy()\n\n    # we initialize all of the arrays, setting the covariances\n    # lower than the expected variances, and the loadings to 1 or 0\n    loading_init = self.model.loadings\n    error_vars_init = np.full((self.model.n_variables, 1), 0.5)\n    factor_vars_init = np.full((self.model.n_factors, 1), 1.0)\n    factor_covs_init = np.full((self.model.n_lower_diag, 1), 0.05)\n\n    # we merge all of the arrays into a single 1d vector\n    x0 = self._combine(\n        loading_init,\n        error_vars_init,\n        factor_vars_init,\n        factor_covs_init,\n        self.model.n_factors,\n        self.model.n_variables,\n        self.model.n_lower_diag,\n    )\n\n    # if the bounds argument is None, then we initialized the\n    # boundaries to (None, None) for everything except factor covariances;\n    # at some point in the future, we may update this to place limits\n    # on the loading matrix boundaries, too, but the case in R and SAS\n    if self.bounds is not None:\n        error_msg = (\n            \"The length of `bounds` must equal the length of your \"\n            \"input array `x0`: {} != {}.\".format(len(self.bounds), len(x0))\n        )\n        assert len(self.bounds) == len(x0), error_msg\n\n    # fit the actual model using L-BFGS algorithm;\n    # the constraints are set inside the objective function,\n    # so that we can avoid using linear programming methods (e.g. SLSQP)\n    res = minimize(\n        self._objective,\n        x0.flatten(),\n        method=\"L-BFGS-B\",\n        options={\"maxiter\": self.max_iter, \"disp\": self.disp},\n        bounds=self.bounds,\n        args=(cov_mtx, self.model.loadings),\n    )\n\n    # if the optimizer failed to converge, print the message\n    if not res.success:\n        warnings.warn(\n            f\"The optimization routine failed to converge: {str(res.message)}\"\n        )\n\n    # we split all the 1d array back into the set of original arrays\n    loadings_res, error_vars_res, factor_vars_res, factor_covs_res = self._split(\n        res.x, self.model.n_factors, self.model.n_variables, self.model.n_lower_diag\n    )\n\n    # we combine the factor covariances and variances into\n    # a single variance-covariance matrix to make things easier,\n    # but also check to make see if anything was fixed\n    factor_varcovs_res = merge_variance_covariance(factor_vars_res, factor_covs_res)\n    with np.errstate(all=\"ignore\"):\n        factor_varcovs_res = covariance_to_correlation(factor_varcovs_res)\n\n    self.loadings_ = loadings_res\n    self.error_vars_ = error_vars_res\n    self.factor_varcovs_ = factor_varcovs_res\n\n    # we also calculate the log-likelihood, AIC, and BIC\n    self.log_likelihood_ = -res.fun\n    self.aic_ = 2 * res.fun + 2 * (x0.shape[0] + self.model.n_variables)\n    if self.n_obs is not None:\n        self.bic_ = 2 * res.fun + np.log(self.n_obs) * (\n            x0.shape[0] + self.model.n_variables\n        )\n    return self\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.get_model_implied_cov","title":"<code>get_model_implied_cov()</code>","text":"<p>Get the model-implied covariance matrix (sigma) for an estimated model.</p> <p>Returns:</p> Name Type Description <code>model_implied_cov</code> <code>ndarray</code> <p>The model-implied covariance matrix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n...                              ModelSpecificationParser)\n&gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n&gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n&gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n&gt;&gt;&gt; cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n&gt;&gt;&gt; cfa.fit(X.values)\n&gt;&gt;&gt; cfa.get_model_implied_cov()\narray([[2.07938612, 0.45674659, 0.34718423, 0.57824753, 0.16997013,\n        0.12648394, 0.06480751, 0.08625868],\n       [0.45674659, 1.16703337, 0.16136667, 0.26876186, 0.07899988,\n        0.05878807, 0.03012168, 0.0400919 ],\n       [0.34718423, 0.16136667, 1.07364855, 0.20429245, 0.06004974,\n        0.04468625, 0.02289622, 0.03047483],\n       [0.57824753, 0.26876186, 0.20429245, 1.28809317, 0.10001495,\n        0.07442652, 0.03813447, 0.05075691],\n       [0.16997013, 0.07899988, 0.06004974, 0.10001495, 2.0364391 ,\n        0.72377232, 0.37084458, 0.49359346],\n       [0.12648394, 0.05878807, 0.04468625, 0.07442652, 0.72377232,\n        1.48080077, 0.27596546, 0.36730952],\n       [0.06480751, 0.03012168, 0.02289622, 0.03813447, 0.37084458,\n        0.27596546, 1.11761918, 0.1882011 ],\n       [0.08625868, 0.0400919 , 0.03047483, 0.05075691, 0.49359346,\n        0.36730952, 0.1882011 , 1.28888233]])\n</code></pre> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>def get_model_implied_cov(self):\n    \"\"\"\n    Get the model-implied covariance matrix (sigma) for an estimated model.\n\n    Returns:\n        model_implied_cov (numpy.ndarray): The model-implied covariance matrix.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n        ...                              ModelSpecificationParser)\n        &gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n        &gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n        ...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n        &gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n        &gt;&gt;&gt; cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n        &gt;&gt;&gt; cfa.fit(X.values)\n        &gt;&gt;&gt; cfa.get_model_implied_cov()\n        array([[2.07938612, 0.45674659, 0.34718423, 0.57824753, 0.16997013,\n                0.12648394, 0.06480751, 0.08625868],\n               [0.45674659, 1.16703337, 0.16136667, 0.26876186, 0.07899988,\n                0.05878807, 0.03012168, 0.0400919 ],\n               [0.34718423, 0.16136667, 1.07364855, 0.20429245, 0.06004974,\n                0.04468625, 0.02289622, 0.03047483],\n               [0.57824753, 0.26876186, 0.20429245, 1.28809317, 0.10001495,\n                0.07442652, 0.03813447, 0.05075691],\n               [0.16997013, 0.07899988, 0.06004974, 0.10001495, 2.0364391 ,\n                0.72377232, 0.37084458, 0.49359346],\n               [0.12648394, 0.05878807, 0.04468625, 0.07442652, 0.72377232,\n                1.48080077, 0.27596546, 0.36730952],\n               [0.06480751, 0.03012168, 0.02289622, 0.03813447, 0.37084458,\n                0.27596546, 1.11761918, 0.1882011 ],\n               [0.08625868, 0.0400919 , 0.03047483, 0.05075691, 0.49359346,\n                0.36730952, 0.1882011 , 1.28888233]])\n    \"\"\"\n    # meets all of our expected criteria\n    check_is_fitted(self, [\"loadings_\", \"factor_varcovs_\"])\n    error = np.diag(self.error_vars_.flatten())\n    return self.loadings_.dot(self.factor_varcovs_).dot(self.loadings_.T) + error\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.get_standard_errors","title":"<code>get_standard_errors()</code>","text":"<p>Get standard errors from the implied covariance matrix and implied means.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[ndarray, ndarray]</code> <ul> <li>loadings_se (numpy.ndarray): The standard errors for the factor loadings.</li> <li>error_vars_se (numpy.ndarray): The standard errors for the error variances.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n...                              ModelSpecificationParser)\n&gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n&gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n&gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n&gt;&gt;&gt; cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n&gt;&gt;&gt; cfa.fit(X.values)\n&gt;&gt;&gt; cfa.get_standard_errors()\n(array([[0.06779949, 0.        ],\n       [0.04369956, 0.        ],\n       [0.04153113, 0.        ],\n       [0.04766645, 0.        ],\n       [0.        , 0.06025341],\n       [0.        , 0.04913149],\n       [0.        , 0.0406604 ],\n       [0.        , 0.04351208]]),\n array([0.11929873, 0.05043616, 0.04645803, 0.05803088,\n        0.10176889, 0.06607524, 0.04742321, 0.05373646]))\n</code></pre> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>def get_standard_errors(self) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Get standard errors from the implied covariance matrix and implied means.\n\n    Returns:\n        tuple:\n            - loadings_se (numpy.ndarray): The standard errors for the factor loadings.\n            - error_vars_se (numpy.ndarray): The standard errors for the error variances.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n        ...                              ModelSpecificationParser)\n        &gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n        &gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n        ...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n        &gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n        &gt;&gt;&gt; cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n        &gt;&gt;&gt; cfa.fit(X.values)\n        &gt;&gt;&gt; cfa.get_standard_errors()\n        (array([[0.06779949, 0.        ],\n               [0.04369956, 0.        ],\n               [0.04153113, 0.        ],\n               [0.04766645, 0.        ],\n               [0.        , 0.06025341],\n               [0.        , 0.04913149],\n               [0.        , 0.0406604 ],\n               [0.        , 0.04351208]]),\n         array([0.11929873, 0.05043616, 0.04645803, 0.05803088,\n                0.10176889, 0.06607524, 0.04742321, 0.05373646]))\n    \"\"\"\n    # meets all of our expected criteria\n    check_is_fitted(self, [\"loadings_\", \"n_obs\"])\n\n    (\n        loadings_dx,\n        factor_covs_dx,\n        error_covs_dx,\n        intercept_dx,\n    ) = self._get_derivatives_implied_cov()\n\n    (\n        loadings_dx_mu,\n        factor_covs_dx_mu,\n        error_covs_dx_mu,\n        intercept_dx_mu,\n    ) = self._get_derivatives_implied_mu()\n\n    # combine all of our derivatives; below we will  merge all of these\n    # together in a single matrix, delta, to use the delta rule\n    # (basically, using the gradients to calculate the information)\n    loadings_dx = np.append(loadings_dx_mu, loadings_dx, axis=0)\n    factor_covs_dx = np.append(factor_covs_dx_mu, factor_covs_dx, axis=0)\n    error_cov_dx = np.append(error_covs_dx_mu, error_covs_dx, axis=0)\n    intercept_dx = np.append(intercept_dx_mu, intercept_dx, axis=0)\n\n    # get get the implied covariance, invert it, and take the Kronecker product\n    sigma = self.get_model_implied_cov()\n    sigma_inv = np.linalg.inv(sigma)\n    sigma_inv_kron = np.kron(sigma_inv, sigma_inv)\n\n    # we get the fisher information matrix for H1, which is the unrestricted\n    # model information; we'll use this with the deltas to calculate the full\n    # (inverted) information matrix below, and then invert the whole thing\n    h1_information = 0.5 * duplication_matrix_pre_post(sigma_inv_kron)\n    h1_information = block_diag(sigma_inv, h1_information)\n\n    # we concatenate all derivatives into a delta matrix\n    delta = np.concatenate(\n        (loadings_dx, error_cov_dx, factor_covs_dx, intercept_dx), axis=1\n    )\n\n    # calculate the fisher information matrix\n    information = delta.T.dot(h1_information).dot(delta)\n    information = (1 / self.n_obs) * np.linalg.inv(information)\n\n    # calculate the standard errors from the diagonal of the\n    # information / cov matrix; also take the absolute value,\n    # just in case anything is negative\n    se = np.sqrt(np.abs(np.diag(information)))\n\n    # get the indexes for the standard errors\n    # for the loadings and the errors variances;\n    # in the future, we may add the factor and intercept\n    # covariances, but these sometimes require transformations\n    # that are more complicated, so for now we won't return them\n    loadings_idx = len(self.model.loadings_free)\n    error_vars_idx = self.model.n_variables + loadings_idx\n\n    # get the loading standard errors and reshape them into the\n    # format of the original loadings matrix\n    loadings_se = np.zeros((self.model.n_factors * self.model.n_variables,))\n    loadings_se[self.model.loadings_free] = se[:loadings_idx]\n    loadings_se = loadings_se.reshape(\n        (self.model.n_variables, self.model.n_factors), order=\"F\"\n    )\n\n    # get the error variance standard errors\n    error_vars_se = se[loadings_idx:error_vars_idx]\n    return loadings_se, error_vars_se\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ConfirmatoryFactorAnalyzer.transform","title":"<code>transform(X)</code>","text":"<p>Get the factor scores for a new data set.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The data to score using the fitted factor model, shape (<code>n_samples</code>, <code>n_features</code>).</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray</code> <p>The latent variables of X, shape (<code>n_samples</code>, <code>n_components</code>).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n...                              ModelSpecificationParser)\n&gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n&gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n&gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n&gt;&gt;&gt; cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n&gt;&gt;&gt; cfa.fit(X.values)\n&gt;&gt;&gt; cfa.transform(X.values)\narray([[-0.46852166, -1.08708035],\n       [ 2.59025301,  1.20227783],\n       [-0.47215977,  2.65697245],\n       ...,\n       [-1.5930886 , -0.91804114],\n       [ 0.19430887,  0.88174818],\n   [-0.27863554, -0.7695101 ]])\n</code></pre> References <p>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157408/</p> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    Get the factor scores for a new data set.\n\n    Args:\n        X (array-like): The data to score using the fitted factor model, shape (``n_samples``, ``n_features``).\n\n    Returns:\n        scores (numpy.ndarray): The latent variables of X, shape (``n_samples``, ``n_components``).\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n        ...                              ModelSpecificationParser)\n        &gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n        &gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n        ...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n        &gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n        &gt;&gt;&gt; cfa = ConfirmatoryFactorAnalyzer(model_spec, disp=False)\n        &gt;&gt;&gt; cfa.fit(X.values)\n        &gt;&gt;&gt; cfa.transform(X.values)\n        array([[-0.46852166, -1.08708035],\n               [ 2.59025301,  1.20227783],\n               [-0.47215977,  2.65697245],\n               ...,\n               [-1.5930886 , -0.91804114],\n               [ 0.19430887,  0.88174818],\n           [-0.27863554, -0.7695101 ]])\n\n    References:\n        https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6157408/\n    \"\"\"\n    # check if the data is a data frame,\n    # so we can convert it to an array\n    if isinstance(X, pd.DataFrame):\n        X = X.values\n\n    # now check the array, and make sure it\n    # meets all of our expected criteria\n    X = check_array(X, ensure_all_finite=True, estimator=self, copy=True)\n\n    # meets all of our expected criteria\n    check_is_fitted(self, [\"loadings_\", \"error_vars_\"])\n\n    # see if we saved the original mean and std\n    if self.mean_ is None:\n        warnings.warn(\n            \"Could not find original mean; using the mean \"\n            \"from the current data set.\"\n        )\n        mean = np.mean(X, axis=0)\n    else:\n        mean = self.mean_\n\n    # get the scaled data\n    X_scale = X - mean\n\n    # get the loadings and error variances\n    loadings = self.loadings_.copy()\n    error_vars = self.error_vars_.copy()\n\n    # make the error variance an identity matrix,\n    # and take the inverse of this matrix\n    error_covs = np.eye(error_vars.shape[0])\n    np.fill_diagonal(error_covs, error_vars)\n    error_covs_inv = np.linalg.inv(error_covs)\n\n    # calculate the weights, using Bartlett's formula\n    # (lambda' error_covs\u02c6\u22121 lambda)\u02c6\u22121 lambda' error_covs\u02c6\u22121 (X - muX)\n    weights = (\n        np.linalg.pinv(loadings.T.dot(error_covs_inv).dot(loadings))\n        .dot(loadings.T)\n        .dot(error_covs_inv)\n    )\n\n    scores = weights.dot(X_scale.T).T\n    return scores\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification","title":"<code>ModelSpecification</code>","text":"<p>Encapsulate the model specification for CFA.</p> <p>This class contains a number of specification properties that are used in the CFA procedure.</p> <p>Parameters:</p> Name Type Description Default <code>loadings</code> <code>array - like</code> <p>The factor loadings specification.</p> required <code>n_factors</code> <code>int</code> <p>The number of factors.</p> required <code>n_variables</code> <code>int</code> <p>The number of variables.</p> required <code>factor_names</code> <code>list of str</code> <p>A list of factor names, if available. Defaults to <code>None</code>.</p> <code>None</code> <code>variable_names</code> <code>list of str</code> <p>A list of variable names, if available. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>class ModelSpecification:\n    \"\"\"\n    Encapsulate the model specification for CFA.\n\n    This class contains a number of specification properties\n    that are used in the CFA procedure.\n\n    Args:\n        loadings (array-like): The factor loadings specification.\n        n_factors (int): The number of factors.\n        n_variables (int): The number of variables.\n        factor_names (list of str, optional): A list of factor names, if available.\n            Defaults to ``None``.\n        variable_names (list of str, optional): A list of variable names, if available.\n            Defaults to ``None``.\n    \"\"\"\n\n    def __init__(\n        self, loadings, n_factors, n_variables, factor_names=None, variable_names=None\n    ):\n        \"\"\"Initialize the specification.\"\"\"\n        assert isinstance(loadings, np.ndarray)\n        assert loadings.shape[0] == n_variables\n        assert loadings.shape[1] == n_factors\n\n        self._loadings = loadings\n        self._n_factors = n_factors\n        self._n_variables = n_variables\n        self._factor_names = factor_names\n        self._variable_names = variable_names\n\n        self._n_lower_diag = get_symmetric_lower_idxs(n_factors, False).shape[0]\n\n        self._error_vars = np.full((n_variables, 1), np.nan)\n        self._factor_covs = np.full((n_factors, n_factors), np.nan)\n\n        self._loadings_free = get_free_parameter_idxs(loadings, eq=1)\n        self._error_vars_free = merge_variance_covariance(self._error_vars)\n        self._error_vars_free = get_free_parameter_idxs(self._error_vars_free, eq=-1)\n        self._factor_covs_free = get_symmetric_lower_idxs(n_factors, False)\n\n    def __str__(self):\n        \"\"\"Represent model specification as a string.\"\"\"\n        return f\"&lt;ModelSpecification object at {hex(id(self))}&gt;\"\n\n    def copy(self):\n        \"\"\"Return a copy of the model specification.\"\"\"\n        return deepcopy(self)\n\n    @property\n    def loadings(self):\n        \"\"\"Get the factor loadings specification.\"\"\"\n        return self._loadings.copy()\n\n    @property\n    def error_vars(self):\n        \"\"\"Get the error variance specification.\"\"\"\n        return self._error_vars.copy()\n\n    @property\n    def factor_covs(self):\n        \"\"\"Get the factor covariance specification.\"\"\"\n        return self._factor_covs.copy()\n\n    @property\n    def loadings_free(self):\n        \"\"\"Get the indices of \"free\" factor loading parameters.\"\"\"\n        return self._loadings_free.copy()\n\n    @property\n    def error_vars_free(self):\n        \"\"\"Get the indices of \"free\" error variance parameters.\"\"\"\n        return self._error_vars_free.copy()\n\n    @property\n    def factor_covs_free(self):\n        \"\"\"Get the indices of \"free\" factor covariance parameters.\"\"\"\n        return self._factor_covs_free.copy()\n\n    @property\n    def n_variables(self):\n        \"\"\"Get the number of variables.\"\"\"\n        return self._n_variables\n\n    @property\n    def n_factors(self):\n        \"\"\"Get the number of factors.\"\"\"\n        return self._n_factors\n\n    @property\n    def n_lower_diag(self):\n        \"\"\"Get the lower diagonal of the factor covariance matrix.\"\"\"\n        return self._n_lower_diag\n\n    @property\n    def factor_names(self):\n        \"\"\"Get list of factor names, if available.\"\"\"\n        return self._factor_names\n\n    @property\n    def variable_names(self):\n        \"\"\"Get list of variable names, if available.\"\"\"\n        return self._variable_names\n\n    def get_model_specification_as_dict(self):\n        \"\"\"\n        Get the model specification as a dictionary.\n\n        Returns:\n            model_specification (dict): The model specification keys and values,\n                as a dictionary.\n        \"\"\"\n        return {\n            \"loadings\": self._loadings.copy(),\n            \"error_vars\": self._error_vars.copy(),\n            \"factor_covs\": self._factor_covs.copy(),\n            \"loadings_free\": self._loadings_free.copy(),\n            \"error_vars_free\": self._error_vars_free.copy(),\n            \"factor_covs_free\": self._factor_covs_free.copy(),\n            \"n_variables\": self._n_variables,\n            \"n_factors\": self._n_factors,\n            \"n_lower_diag\": self._n_lower_diag,\n            \"variable_names\": self._variable_names,\n            \"factor_names\": self._factor_names,\n        }\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.error_vars","title":"<code>error_vars</code>  <code>property</code>","text":"<p>Get the error variance specification.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.error_vars_free","title":"<code>error_vars_free</code>  <code>property</code>","text":"<p>Get the indices of \u201cfree\u201d error variance parameters.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.factor_covs","title":"<code>factor_covs</code>  <code>property</code>","text":"<p>Get the factor covariance specification.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.factor_covs_free","title":"<code>factor_covs_free</code>  <code>property</code>","text":"<p>Get the indices of \u201cfree\u201d factor covariance parameters.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.factor_names","title":"<code>factor_names</code>  <code>property</code>","text":"<p>Get list of factor names, if available.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.loadings","title":"<code>loadings</code>  <code>property</code>","text":"<p>Get the factor loadings specification.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.loadings_free","title":"<code>loadings_free</code>  <code>property</code>","text":"<p>Get the indices of \u201cfree\u201d factor loading parameters.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.n_factors","title":"<code>n_factors</code>  <code>property</code>","text":"<p>Get the number of factors.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.n_lower_diag","title":"<code>n_lower_diag</code>  <code>property</code>","text":"<p>Get the lower diagonal of the factor covariance matrix.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.n_variables","title":"<code>n_variables</code>  <code>property</code>","text":"<p>Get the number of variables.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.variable_names","title":"<code>variable_names</code>  <code>property</code>","text":"<p>Get list of variable names, if available.</p>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.__init__","title":"<code>__init__(loadings, n_factors, n_variables, factor_names=None, variable_names=None)</code>","text":"<p>Initialize the specification.</p> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>def __init__(\n    self, loadings, n_factors, n_variables, factor_names=None, variable_names=None\n):\n    \"\"\"Initialize the specification.\"\"\"\n    assert isinstance(loadings, np.ndarray)\n    assert loadings.shape[0] == n_variables\n    assert loadings.shape[1] == n_factors\n\n    self._loadings = loadings\n    self._n_factors = n_factors\n    self._n_variables = n_variables\n    self._factor_names = factor_names\n    self._variable_names = variable_names\n\n    self._n_lower_diag = get_symmetric_lower_idxs(n_factors, False).shape[0]\n\n    self._error_vars = np.full((n_variables, 1), np.nan)\n    self._factor_covs = np.full((n_factors, n_factors), np.nan)\n\n    self._loadings_free = get_free_parameter_idxs(loadings, eq=1)\n    self._error_vars_free = merge_variance_covariance(self._error_vars)\n    self._error_vars_free = get_free_parameter_idxs(self._error_vars_free, eq=-1)\n    self._factor_covs_free = get_symmetric_lower_idxs(n_factors, False)\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.__str__","title":"<code>__str__()</code>","text":"<p>Represent model specification as a string.</p> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>def __str__(self):\n    \"\"\"Represent model specification as a string.\"\"\"\n    return f\"&lt;ModelSpecification object at {hex(id(self))}&gt;\"\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.copy","title":"<code>copy()</code>","text":"<p>Return a copy of the model specification.</p> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>def copy(self):\n    \"\"\"Return a copy of the model specification.\"\"\"\n    return deepcopy(self)\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecification.get_model_specification_as_dict","title":"<code>get_model_specification_as_dict()</code>","text":"<p>Get the model specification as a dictionary.</p> <p>Returns:</p> Name Type Description <code>model_specification</code> <code>dict</code> <p>The model specification keys and values, as a dictionary.</p> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>def get_model_specification_as_dict(self):\n    \"\"\"\n    Get the model specification as a dictionary.\n\n    Returns:\n        model_specification (dict): The model specification keys and values,\n            as a dictionary.\n    \"\"\"\n    return {\n        \"loadings\": self._loadings.copy(),\n        \"error_vars\": self._error_vars.copy(),\n        \"factor_covs\": self._factor_covs.copy(),\n        \"loadings_free\": self._loadings_free.copy(),\n        \"error_vars_free\": self._error_vars_free.copy(),\n        \"factor_covs_free\": self._factor_covs_free.copy(),\n        \"n_variables\": self._n_variables,\n        \"n_factors\": self._n_factors,\n        \"n_lower_diag\": self._n_lower_diag,\n        \"variable_names\": self._variable_names,\n        \"factor_names\": self._factor_names,\n    }\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecificationParser","title":"<code>ModelSpecificationParser</code>","text":"<p>Generate the model specification for CFA.</p> <p>This class includes two static methods to generate a :class:<code>ModelSpecification</code> object from either a dictionary or a numpy array.</p> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>class ModelSpecificationParser:\n    \"\"\"\n    Generate the model specification for CFA.\n\n    This class includes two static methods to generate a\n    :class:`ModelSpecification` object from either a dictionary\n    or a numpy array.\n    \"\"\"\n\n    @staticmethod\n    def parse_model_specification_from_dict(\n        X, specification=None\n    ) -&gt; \"ModelSpecification\":\n        \"\"\"\n        Generate the model specification from a dictionary.\n\n        The keys in the dictionary should be the factor names, and the\n        values should be the feature names. If this method is used to\n        create the :class:`ModelSpecification` object, then factor names\n        and variable names will be added as properties to that object.\n\n        Args:\n            X (array-like): The data set that will be used for CFA.\n            specification (dict, optional): A dictionary with the loading details. If ``None``, the matrix will\n                be created assuming all variables load on all factors.\n                Defaults to ``None``.\n\n        Returns:\n            ModelSpecification: A model specification object.\n\n        Raises:\n            ValueError: If ``specification`` is not in the expected format.\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n            ...                              ModelSpecificationParser)\n            &gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n            &gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n            ...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n            &gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n        \"\"\"\n        if specification is None:\n            factor_names, variable_names = None, None\n            n_variables, n_factors = X.shape[1], X.shape[1]\n            loadings = np.ones((n_factors, n_factors), dtype=int)\n        elif isinstance(specification, dict):\n            factor_names = list(specification)\n            variable_names = unique_elements(\n                [v for f in specification.values() for v in f]\n            )\n            loadings_new = {}\n            for factor in factor_names:\n                loadings_for_factor = pd.Series(variable_names).isin(\n                    specification[factor]\n                )\n                loadings_for_factor = loadings_for_factor.astype(int)\n                loadings_new[factor] = loadings_for_factor\n            loadings = pd.DataFrame(loadings_new).values\n            n_variables, n_factors = loadings.shape\n        else:\n            raise ValueError(\n                \"The model `specification` must be either a dict \"\n                \"or None, not {}\".format(type(specification))\n            )\n\n        return ModelSpecification(\n            **{\n                \"loadings\": loadings,\n                \"n_variables\": n_variables,\n                \"n_factors\": n_factors,\n                \"factor_names\": factor_names,\n                \"variable_names\": variable_names,\n            }\n        )\n\n    @staticmethod\n    def parse_model_specification_from_array(\n        X, specification=None\n    ) -&gt; \"ModelSpecification\":\n        \"\"\"\n        Generate the model specification from a numpy array.\n\n        The columns should correspond to the factors, and the rows\n        should correspond to the variables. If this method is used\n        to create the :class:`ModelSpecification` object, then *no* factor\n        names and variable names will be added as properties to that\n        object.\n\n        Args:\n            X (array-like): The data set that will be used for CFA.\n            specification (array-like, optional): An array with the loading details. If ``None``, the matrix will\n                be created assuming all variables load on all factors.\n                Defaults to ``None``.\n\n        Returns:\n            ModelSpecification: A model specification object.\n\n        Raises:\n            ValueError: If ``specification`` is not in the expected format.\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n            ...                              ModelSpecificationParser)\n            &gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n            &gt;&gt;&gt; model_array = np.array([[1, 1, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1]])\n            &gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_array(X,\n            ...                                                                            model_array)\n        \"\"\"\n        if specification is None:\n            n_variables, n_factors = X.shape[1], X.shape[1]\n            loadings = np.ones((n_factors, n_factors), dtype=int)\n        elif isinstance(specification, (np.ndarray, pd.DataFrame)):\n            n_variables, n_factors = specification.shape\n            if isinstance(specification, pd.DataFrame):\n                loadings = specification.values.copy()\n            else:\n                loadings = specification.copy()\n        else:\n            raise ValueError(\n                \"The model `specification` must be either a numpy array \"\n                \"or None, not {}\".format(type(specification))\n            )\n\n        return ModelSpecification(\n            **{\"loadings\": loadings, \"n_variables\": n_variables, \"n_factors\": n_factors}\n        )\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecificationParser.parse_model_specification_from_array","title":"<code>parse_model_specification_from_array(X, specification=None)</code>  <code>staticmethod</code>","text":"<p>Generate the model specification from a numpy array.</p> <p>The columns should correspond to the factors, and the rows should correspond to the variables. If this method is used to create the :class:<code>ModelSpecification</code> object, then no factor names and variable names will be added as properties to that object.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The data set that will be used for CFA.</p> required <code>specification</code> <code>array - like</code> <p>An array with the loading details. If <code>None</code>, the matrix will be created assuming all variables load on all factors. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ModelSpecification</code> <code>ModelSpecification</code> <p>A model specification object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>specification</code> is not in the expected format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n...                              ModelSpecificationParser)\n&gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n&gt;&gt;&gt; model_array = np.array([[1, 1, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1]])\n&gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_array(X,\n...                                                                            model_array)\n</code></pre> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>@staticmethod\ndef parse_model_specification_from_array(\n    X, specification=None\n) -&gt; \"ModelSpecification\":\n    \"\"\"\n    Generate the model specification from a numpy array.\n\n    The columns should correspond to the factors, and the rows\n    should correspond to the variables. If this method is used\n    to create the :class:`ModelSpecification` object, then *no* factor\n    names and variable names will be added as properties to that\n    object.\n\n    Args:\n        X (array-like): The data set that will be used for CFA.\n        specification (array-like, optional): An array with the loading details. If ``None``, the matrix will\n            be created assuming all variables load on all factors.\n            Defaults to ``None``.\n\n    Returns:\n        ModelSpecification: A model specification object.\n\n    Raises:\n        ValueError: If ``specification`` is not in the expected format.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n        ...                              ModelSpecificationParser)\n        &gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n        &gt;&gt;&gt; model_array = np.array([[1, 1, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1]])\n        &gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_array(X,\n        ...                                                                            model_array)\n    \"\"\"\n    if specification is None:\n        n_variables, n_factors = X.shape[1], X.shape[1]\n        loadings = np.ones((n_factors, n_factors), dtype=int)\n    elif isinstance(specification, (np.ndarray, pd.DataFrame)):\n        n_variables, n_factors = specification.shape\n        if isinstance(specification, pd.DataFrame):\n            loadings = specification.values.copy()\n        else:\n            loadings = specification.copy()\n    else:\n        raise ValueError(\n            \"The model `specification` must be either a numpy array \"\n            \"or None, not {}\".format(type(specification))\n        )\n\n    return ModelSpecification(\n        **{\"loadings\": loadings, \"n_variables\": n_variables, \"n_factors\": n_factors}\n    )\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/confirmatory_factor_analyzer/#spotoptim.factor_analyzer.confirmatory_factor_analyzer.ModelSpecificationParser.parse_model_specification_from_dict","title":"<code>parse_model_specification_from_dict(X, specification=None)</code>  <code>staticmethod</code>","text":"<p>Generate the model specification from a dictionary.</p> <p>The keys in the dictionary should be the factor names, and the values should be the feature names. If this method is used to create the :class:<code>ModelSpecification</code> object, then factor names and variable names will be added as properties to that object.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The data set that will be used for CFA.</p> required <code>specification</code> <code>dict</code> <p>A dictionary with the loading details. If <code>None</code>, the matrix will be created assuming all variables load on all factors. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ModelSpecification</code> <code>ModelSpecification</code> <p>A model specification object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>specification</code> is not in the expected format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n...                              ModelSpecificationParser)\n&gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n&gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n&gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n</code></pre> Source code in <code>src/spotoptim/factor_analyzer/confirmatory_factor_analyzer.py</code> <pre><code>@staticmethod\ndef parse_model_specification_from_dict(\n    X, specification=None\n) -&gt; \"ModelSpecification\":\n    \"\"\"\n    Generate the model specification from a dictionary.\n\n    The keys in the dictionary should be the factor names, and the\n    values should be the feature names. If this method is used to\n    create the :class:`ModelSpecification` object, then factor names\n    and variable names will be added as properties to that object.\n\n    Args:\n        X (array-like): The data set that will be used for CFA.\n        specification (dict, optional): A dictionary with the loading details. If ``None``, the matrix will\n            be created assuming all variables load on all factors.\n            Defaults to ``None``.\n\n    Returns:\n        ModelSpecification: A model specification object.\n\n    Raises:\n        ValueError: If ``specification`` is not in the expected format.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import (ConfirmatoryFactorAnalyzer,\n        ...                              ModelSpecificationParser)\n        &gt;&gt;&gt; X = pd.read_csv('tests/data/test11.csv')\n        &gt;&gt;&gt; model_dict = {\"F1\": [\"V1\", \"V2\", \"V3\", \"V4\"],\n        ...               \"F2\": [\"V5\", \"V6\", \"V7\", \"V8\"]}\n        &gt;&gt;&gt; model_spec = ModelSpecificationParser.parse_model_specification_from_dict(X, model_dict)\n    \"\"\"\n    if specification is None:\n        factor_names, variable_names = None, None\n        n_variables, n_factors = X.shape[1], X.shape[1]\n        loadings = np.ones((n_factors, n_factors), dtype=int)\n    elif isinstance(specification, dict):\n        factor_names = list(specification)\n        variable_names = unique_elements(\n            [v for f in specification.values() for v in f]\n        )\n        loadings_new = {}\n        for factor in factor_names:\n            loadings_for_factor = pd.Series(variable_names).isin(\n                specification[factor]\n            )\n            loadings_for_factor = loadings_for_factor.astype(int)\n            loadings_new[factor] = loadings_for_factor\n        loadings = pd.DataFrame(loadings_new).values\n        n_variables, n_factors = loadings.shape\n    else:\n        raise ValueError(\n            \"The model `specification` must be either a dict \"\n            \"or None, not {}\".format(type(specification))\n        )\n\n    return ModelSpecification(\n        **{\n            \"loadings\": loadings,\n            \"n_variables\": n_variables,\n            \"n_factors\": n_factors,\n            \"factor_names\": factor_names,\n            \"variable_names\": variable_names,\n        }\n    )\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/","title":"factor_analyzer","text":"<p>Factor analysis using MINRES or ML, with optional rotation using Varimax or Promax.</p> <p>Confirmatory factor analysis using machine learning methods. Re-implementation of the factor-analyzer package.</p> <p>See https://factor-analyzer.readthedocs.io/en/latest/introduction.html for more details.</p> <p>Authors of the original implementation: * Jeremy Biggs (jeremy.m.biggs@gmail.com) * Nitin Madnani (nmadnani@ets.org) Organization: Educational Testing Service Date: 2022-09-05</p> <p>This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.</p> <p>This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer","title":"<code>FactorAnalyzer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>The main exploratory factor analysis class.</p> This class <p>(1) Fits a factor analysis model using minres, maximum likelihood,     or principal factor extraction and returns the loading matrix (2) Optionally performs a rotation, with method including:</p> <pre><code>(a) varimax (orthogonal rotation)\n(b) promax (oblique rotation)\n(c) oblimin (oblique rotation)\n(d) oblimax (orthogonal rotation)\n(e) quartimin (oblique rotation)\n(f) quartimax (orthogonal rotation)\n(g) equamax (orthogonal rotation)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_factors</code> <code>int</code> <p>The number of factors to select. Defaults to 3.</p> <code>3</code> <code>rotation</code> <code>str</code> <p>The type of rotation to perform after fitting the factor analysis model. If set to None, no rotation will be performed, nor will any associated Kaiser normalization. Possible values include:     (a) varimax (orthogonal rotation)     (b) promax (oblique rotation)     (c) oblimin (oblique rotation)     (d) oblimax (orthogonal rotation)     (e) quartimin (oblique rotation)     (f) quartimax (orthogonal rotation)     (g) equamax (orthogonal rotation) Defaults to \u2018promax\u2019.</p> <code>'promax'</code> <code>method</code> <code>str</code> <p>The fitting method to use, either \u2018minres\u2019, \u2018ml\u2019, or \u2018principal\u2019. Defaults to \u2018minres\u2019.</p> <code>'minres'</code> <code>use_smc</code> <code>bool</code> <p>Whether to use squared multiple correlation as starting guesses for factor analysis. Defaults to True.</p> <code>True</code> <code>bounds</code> <code>tuple</code> <p>The lower and upper bounds on the variables for \u201cL-BFGS-B\u201d optimization. Defaults to (0.005, 1).</p> <code>(0.005, 1)</code> <code>impute</code> <code>str</code> <p>How to handle missing values, if any, in the data: (a) use list-wise deletion (\u2018drop\u2019), or (b) impute the column median (\u2018median\u2019), or impute the column mean (\u2018mean\u2019). Defaults to \u2018median\u2019.</p> <code>'median'</code> <code>is_corr_matrix</code> <code>bool</code> <p>Set to True if the data is the correlation matrix. Defaults to False.</p> <code>False</code> <code>svd_method</code> <code>str</code> <p>The SVD method to use when method is \u2018principal\u2019. If \u2018lapack\u2019, use standard SVD from scipy.linalg. If \u2018randomized\u2019, use faster randomized_svd function from scikit-learn. Defaults to \u2018randomized\u2019.</p> <code>'randomized'</code> <code>rotation_kwargs</code> <code>dict</code> <p>Dictionary containing keyword arguments for the rotation method. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>loadings_</code> <code>ndarray</code> <p>The factor loadings matrix. None, if fit() has not been called.</p> <code>corr_</code> <code>ndarray</code> <p>The original correlation matrix. None, if fit() has not been called.</p> <code>rotation_matrix_</code> <code>ndarray</code> <p>The rotation matrix, if a rotation has been performed. None otherwise.</p> <code>structure_</code> <code>ndarray or None</code> <p>The structure loading matrix. This only exists if rotation is \u2018promax\u2019.</p> <code>phi_</code> <code>ndarray or None</code> <p>The factor correlations matrix. This only exists if rotation is \u2018oblique\u2019.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer--notes","title":"Notes","text":"<p>This code was partly derived from the excellent R package <code>psych</code>.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer--references","title":"References","text":"<p>[1] https://github.com/cran/psych/blob/master/R/fa.R</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer--examples","title":"Examples","text":"<p>import pandas as pd from factor_analyzer import FactorAnalyzer df_features = pd.read_csv(\u2018tests/data/test02.csv\u2019) fa = FactorAnalyzer(rotation=None) fa.fit(df_features) FactorAnalyzer(bounds=(0.005, 1), impute=\u2019median\u2019, is_corr_matrix=False,         method=\u2019minres\u2019, n_factors=3, rotation=None, rotation_kwargs={},         use_smc=True) fa.loadings_ array([[-0.12991218,  0.16398154,  0.73823498],        [ 0.03899558,  0.04658425,  0.01150343],        [ 0.34874135,  0.61452341, -0.07255667],        [ 0.45318006,  0.71926681, -0.07546472],        [ 0.36688794,  0.44377343, -0.01737067],        [ 0.74141382, -0.15008235,  0.29977512],        [ 0.741675  , -0.16123009, -0.20744495],        [ 0.82910167, -0.20519428,  0.04930817],        [ 0.76041819, -0.23768727, -0.1206858 ],        [ 0.81533404, -0.12494695,  0.17639683]]) fa.get_communalities() array([0.588758  , 0.00382308, 0.50452402, 0.72841183, 0.33184336,        0.66208428, 0.61911036, 0.73194557, 0.64929612, 0.71149718])</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer.py</code> <pre><code>class FactorAnalyzer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    The main exploratory factor analysis class.\n\n    This class:\n        (1) Fits a factor analysis model using minres, maximum likelihood,\n            or principal factor extraction and returns the loading matrix\n        (2) Optionally performs a rotation, with method including:\n\n            (a) varimax (orthogonal rotation)\n            (b) promax (oblique rotation)\n            (c) oblimin (oblique rotation)\n            (d) oblimax (orthogonal rotation)\n            (e) quartimin (oblique rotation)\n            (f) quartimax (orthogonal rotation)\n            (g) equamax (orthogonal rotation)\n\n    Parameters:\n        n_factors (int): The number of factors to select. Defaults to 3.\n        rotation (str, optional): The type of rotation to perform after fitting the factor analysis model.\n            If set to None, no rotation will be performed, nor will any associated Kaiser normalization.\n            Possible values include:\n                (a) varimax (orthogonal rotation)\n                (b) promax (oblique rotation)\n                (c) oblimin (oblique rotation)\n                (d) oblimax (orthogonal rotation)\n                (e) quartimin (oblique rotation)\n                (f) quartimax (orthogonal rotation)\n                (g) equamax (orthogonal rotation)\n            Defaults to 'promax'.\n        method (str, optional): The fitting method to use, either 'minres', 'ml', or 'principal'.\n            Defaults to 'minres'.\n        use_smc (bool, optional): Whether to use squared multiple correlation as starting guesses for factor analysis.\n            Defaults to True.\n        bounds (tuple, optional): The lower and upper bounds on the variables for \"L-BFGS-B\" optimization.\n            Defaults to (0.005, 1).\n        impute (str, optional): How to handle missing values, if any, in the data: (a) use list-wise\n            deletion ('drop'), or (b) impute the column median ('median'), or impute the column mean ('mean').\n            Defaults to 'median'.\n        is_corr_matrix (bool, optional): Set to True if the data is the correlation matrix.\n            Defaults to False.\n        svd_method (str, optional): The SVD method to use when method is 'principal'.\n            If 'lapack', use standard SVD from scipy.linalg.\n            If 'randomized', use faster randomized_svd function from scikit-learn.\n            Defaults to 'randomized'.\n        rotation_kwargs (dict, optional): Dictionary containing keyword arguments for the rotation method.\n            Defaults to None.\n\n    Attributes:\n        loadings_ (numpy.ndarray): The factor loadings matrix. None, if fit() has not been called.\n        corr_ (numpy.ndarray): The original correlation matrix. None, if fit() has not been called.\n        rotation_matrix_ (numpy.ndarray): The rotation matrix, if a rotation has been performed. None otherwise.\n        structure_ (numpy.ndarray or None): The structure loading matrix. This only exists if rotation is 'promax'.\n        phi_ (numpy.ndarray or None): The factor correlations matrix. This only exists if rotation is 'oblique'.\n\n    Notes\n    -----\n    This code was partly derived from the excellent R package `psych`.\n\n    References\n    ----------\n    [1] https://github.com/cran/psych/blob/master/R/fa.R\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n    &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n    &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n    &gt;&gt;&gt; fa.fit(df_features)\n    FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n            method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n            use_smc=True)\n    &gt;&gt;&gt; fa.loadings_\n    array([[-0.12991218,  0.16398154,  0.73823498],\n           [ 0.03899558,  0.04658425,  0.01150343],\n           [ 0.34874135,  0.61452341, -0.07255667],\n           [ 0.45318006,  0.71926681, -0.07546472],\n           [ 0.36688794,  0.44377343, -0.01737067],\n           [ 0.74141382, -0.15008235,  0.29977512],\n           [ 0.741675  , -0.16123009, -0.20744495],\n           [ 0.82910167, -0.20519428,  0.04930817],\n           [ 0.76041819, -0.23768727, -0.1206858 ],\n           [ 0.81533404, -0.12494695,  0.17639683]])\n    &gt;&gt;&gt; fa.get_communalities()\n    array([0.588758  , 0.00382308, 0.50452402, 0.72841183, 0.33184336,\n           0.66208428, 0.61911036, 0.73194557, 0.64929612, 0.71149718])\n    \"\"\"\n\n    def __init__(\n        self,\n        n_factors=3,\n        rotation=\"promax\",\n        method=\"minres\",\n        use_smc=True,\n        is_corr_matrix=False,\n        bounds=(0.005, 1),\n        impute=\"median\",\n        svd_method=\"randomized\",\n        rotation_kwargs=None,\n    ):\n        \"\"\"Initialize the factor analyzer.\"\"\"\n        self.n_factors = n_factors\n        self.rotation = rotation\n        self.method = method\n        self.use_smc = use_smc\n        self.bounds = bounds\n        self.impute = impute\n        self.is_corr_matrix = is_corr_matrix\n        self.svd_method = svd_method\n        self.rotation_kwargs = rotation_kwargs\n\n        # default matrices to None\n        self.mean_ = None\n        self.std_ = None\n\n        self.phi_ = None\n        self.structure_ = None\n\n        self.corr_ = None\n        self.loadings_ = None\n        self.rotation_matrix_ = None\n        self.weights_ = None\n\n    def _arg_checker(self):\n        \"\"\"\n        Check the input parameters to make sure they're properly formattted.\n\n        We need to do this to ensure that the FactorAnalyzer class can be\n        properly cloned when used with grid search CV, for example.\n        \"\"\"\n        self.rotation = (\n            self.rotation.lower() if isinstance(self.rotation, str) else self.rotation\n        )\n        if self.rotation not in POSSIBLE_ROTATIONS + [None]:\n            raise ValueError(\n                f\"The rotation must be one of the following: {POSSIBLE_ROTATIONS + [None]}\"\n            )\n\n        self.method = (\n            self.method.lower() if isinstance(self.method, str) else self.method\n        )\n        if self.method not in POSSIBLE_METHODS:\n            raise ValueError(\n                f\"The method must be one of the following: {POSSIBLE_METHODS}\"\n            )\n\n        self.impute = (\n            self.impute.lower() if isinstance(self.impute, str) else self.impute\n        )\n        if self.impute not in POSSIBLE_IMPUTATIONS:\n            raise ValueError(\n                f\"The imputation must be one of the following: {POSSIBLE_IMPUTATIONS}\"\n            )\n\n        self.svd_method = (\n            self.svd_method.lower()\n            if isinstance(self.svd_method, str)\n            else self.svd_method\n        )\n        if self.svd_method not in POSSIBLE_SVDS:\n            raise ValueError(\n                f\"The SVD method must be one of the following: {POSSIBLE_SVDS}\"\n            )\n\n        if self.method == \"principal\" and self.is_corr_matrix:\n            raise ValueError(\n                \"The principal method is only implemented using \"\n                \"the full data set, not the correlation matrix.\"\n            )\n\n        self.rotation_kwargs = (\n            {} if self.rotation_kwargs is None else self.rotation_kwargs\n        )\n\n    @staticmethod\n    def _fit_uls_objective(psi, corr_mtx, n_factors):  # noqa: D401\n        \"\"\"\n        The objective function passed for unweighted least-squares (ULS).\n\n        Parameters\n        ----------\n        psi : array-like\n            Value passed to minimize the objective function.\n        corr_mtx : array-like\n            The correlation matrix.\n        n_factors : int\n            The number of factors to select.\n\n        Returns\n        -------\n        error : float\n            The scalar error calculated from the residuals of the loading\n            matrix.\n        \"\"\"\n        np.fill_diagonal(corr_mtx, 1 - psi)\n\n        # get the eigen values and vectors for n_factors\n        values, vectors = sp.linalg.eigh(corr_mtx)\n        values = values[::-1]\n\n        # this is a bit of a hack, borrowed from R's `fac()` function;\n        # if values are smaller than the smallest representable positive\n        # number * 100, set them to that number instead.\n        values = np.maximum(values, np.finfo(float).eps * 100)\n\n        # sort the values and vectors in ascending order\n        values = values[:n_factors]\n        vectors = vectors[:, ::-1][:, :n_factors]\n\n        # calculate the loadings\n        if n_factors &gt; 1:\n            loadings = np.dot(vectors, np.diag(np.sqrt(values)))\n        else:\n            loadings = vectors * np.sqrt(values[0])\n\n        # calculate the error from the loadings model\n        model = np.dot(loadings, loadings.T)\n\n        # note that in a more recent version of the `fa()` source\n        # code on GitHub, the minres objective function only sums the\n        # lower triangle of the residual matrix; this could be\n        # implemented here using `np.tril()` when this change is\n        # merged into the stable version of `psych`.\n        residual = (corr_mtx - model) ** 2\n        error = np.sum(residual)\n        return error\n\n    @staticmethod\n    def _normalize_uls(solution, corr_mtx, n_factors):\n        \"\"\"\n        Weighted least squares normalization for loadings using MINRES.\n\n        Parameters\n        ----------\n        solution : array-like\n            The solution from the L-BFGS-B optimization.\n        corr_mtx : array-like\n            The correlation matrix.\n        n_factors : int\n            The number of factors to select.\n\n        Returns\n        -------\n        loadings : :obj:`numpy.ndarray`\n            The factor loading matrix\n        \"\"\"\n        np.fill_diagonal(corr_mtx, 1 - solution)\n\n        # get the eigenvalues and vectors for n_factors\n        values, vectors = np.linalg.eigh(corr_mtx)\n\n        # sort the values and vectors in ascending order\n        values = values[::-1][:n_factors]\n        vectors = vectors[:, ::-1][:, :n_factors]\n\n        # calculate loadings\n        # if values are smaller than 0, set them to zero\n        loadings = np.dot(vectors, np.diag(np.sqrt(np.maximum(values, 0))))\n        return loadings\n\n    @staticmethod\n    def _fit_ml_objective(psi, corr_mtx, n_factors):  # noqa: D401\n        \"\"\"\n        The objective function for maximum likelihood.\n\n        Parameters\n        ----------\n        psi : array-like\n            Value passed to minimize the objective function.\n        corr_mtx : array-like\n            The correlation matrix.\n        n_factors : int\n            The number of factors to select.\n\n        Returns\n        -------\n        error : float\n            The scalar error calculated from the residuals\n            of the loading matrix.\n\n        Note\n        ----\n        The ML objective is based on the `factanal()` function from ``stats``\n        package in R. It may generate different results from the ``fa()``\n        function in ``psych``.\n\n        References\n        ----------\n        [1] https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/factanal.R\n        \"\"\"\n        sc = np.diag(1 / np.sqrt(psi))\n        sstar = np.dot(np.dot(sc, corr_mtx), sc)\n\n        # get the eigenvalues and eigenvectors for n_factors\n        values, _ = np.linalg.eigh(sstar)\n        values = values[::-1][n_factors:]\n\n        # calculate the error\n        error = -(np.sum(np.log(values) - values) - n_factors + corr_mtx.shape[0])\n        return error\n\n    @staticmethod\n    def _normalize_ml(solution, corr_mtx, n_factors):\n        \"\"\"\n        Normalize loadings estimated using maximum likelihood.\n\n        Parameters\n        ----------\n        solution : array-like\n            The solution from the L-BFGS-B optimization.\n        corr_mtx : array-like\n            The correlation matrix.\n        n_factors : int\n            The number of factors to select.\n\n        Returns\n        -------\n        loadings : :obj:`numpy.ndarray`\n            The factor loading matrix\n        \"\"\"\n        sc = np.diag(1 / np.sqrt(solution))\n        sstar = np.dot(np.dot(sc, corr_mtx), sc)\n\n        # get the eigenvalues for n_factors\n        values, vectors = np.linalg.eigh(sstar)\n\n        # sort the values and vectors in ascending order\n        values = values[::-1][:n_factors]\n        vectors = vectors[:, ::-1][:, :n_factors]\n\n        values = np.maximum(values - 1, 0)\n\n        # get the loadings\n        loadings = np.dot(vectors, np.diag(np.sqrt(values)))\n\n        return np.dot(np.diag(np.sqrt(solution)), loadings)\n\n    def _fit_principal(self, X):\n        \"\"\"\n        Fit factor analysis model using principal factor analysis.\n\n        Parameters\n        ----------\n        X : array-like\n            The full data set.\n\n        Returns\n        -------\n        loadings : :obj:`numpy.ndarray`\n            The factor loadings matrix.\n        \"\"\"\n        # standardize the data\n        X = X.copy()\n        X = (X - X.mean(0)) / X.std(0)\n\n        # if the number of rows is less than the number of columns,\n        # warn the user that the number of factors will be constrained\n        nrows, ncols = X.shape\n        if nrows &lt; ncols and self.n_factors &gt;= nrows:\n            warnings.warn(\n                \"The number of factors will be \"\n                \"constrained to min(n_samples, n_features)\"\n                \"={}.\".format(min(nrows, ncols))\n            )\n\n        # perform the randomized singular value decomposition\n        if self.svd_method == \"randomized\":\n            _, _, V = randomized_svd(X, self.n_factors, random_state=1234567890)\n        # otherwise, perform the full SVD\n        else:\n            _, _, V = np.linalg.svd(X, full_matrices=False)\n\n        corr_mtx = np.dot(X, V.T)\n        loadings = np.array([[pearsonr(x, c)[0] for c in corr_mtx.T] for x in X.T])\n        return loadings\n\n    def _fit_factor_analysis(self, corr_mtx):\n        \"\"\"\n        Fit factor analysis model using either MINRES or maximum likelihood.\n\n        Parameters\n        ----------\n        corr_mtx : array-like\n            The correlation matrix.\n\n        Returns\n        -------\n        loadings : :obj:`numpy.ndarray`\n\n        Raises\n        ------\n        ValueError\n            If any of the correlations are null, most likely due to having\n            zero standard deviation.\n        \"\"\"\n        # if `use_smc` is True, get get squared multiple correlations\n        # and use these as initial guesses for optimizer\n        if self.use_smc:\n            smc_mtx = smc(corr_mtx)\n            start = (np.diag(corr_mtx) - smc_mtx.T).squeeze()\n        # otherwise, just start with a guess of 0.5 for everything\n        else:\n            start = [0.5 for _ in range(corr_mtx.shape[0])]\n\n        # if `bounds`, set initial boundaries for all variables;\n        # this must be a list passed to `minimize()`\n        if self.bounds is not None:\n            bounds = [self.bounds for _ in range(corr_mtx.shape[0])]\n        else:\n            bounds = self.bounds\n\n        # minimize the appropriate objective function\n        # and the L-BFGS-B algorithm\n        if self.method == \"ml\" or self.method == \"mle\":\n            objective = self._fit_ml_objective\n        else:\n            objective = self._fit_uls_objective\n\n        # use scipy to perform the actual minimization\n        res = minimize(\n            objective,\n            start,\n            method=\"L-BFGS-B\",\n            bounds=bounds,\n            options={\"maxiter\": 1000},\n            args=(corr_mtx, self.n_factors),\n        )\n\n        if not res.success:\n            warnings.warn(f\"Failed to converge: {res.message}\")\n\n        # transform the final loading matrix (using wls for MINRES,\n        # and ml normalization for ML), and convert to DataFrame\n        if self.method == \"ml\" or self.method == \"mle\":\n            loadings = self._normalize_ml(res.x, corr_mtx, self.n_factors)\n        else:\n            loadings = self._normalize_uls(res.x, corr_mtx, self.n_factors)\n        return loadings\n\n    def fit(self, X, y=None) -&gt; \"FactorAnalyzer\":\n        \"\"\"\n        Fit factor analysis model using either MINRES, ML, or principal factor analysis.\n\n        By default, use SMC as starting guesses.\n\n        Parameters:\n            X (array-like): The data to analyze.\n            y (ignored): Ignored.\n\n        Returns:\n            self: The fitted factor analyzer object.\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n            &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n            &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n            &gt;&gt;&gt; fa.fit(df_features)\n            FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n                    method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n                    use_smc=True)\n            &gt;&gt;&gt; fa.loadings_\n            array([[-0.12991218,  0.16398154,  0.73823498],\n                   [ 0.03899558,  0.04658425,  0.01150343],\n                   [ 0.34874135,  0.61452341, -0.07255667],\n                   [ 0.45318006,  0.71926681, -0.07546472],\n                   [ 0.36688794,  0.44377343, -0.01737067],\n                   [ 0.74141382, -0.15008235,  0.29977512],\n                   [ 0.741675  , -0.16123009, -0.20744495],\n                   [ 0.82910167, -0.20519428,  0.04930817],\n                   [ 0.76041819, -0.23768727, -0.1206858 ],\n                   [ 0.81533404, -0.12494695,  0.17639683]])\n        \"\"\"\n        # check the input arguments\n        self._arg_checker()\n\n        # check if the data is a data frame,\n        # so we can convert it to an array\n        if isinstance(X, pd.DataFrame):\n            X = X.copy().values\n        else:\n            X = X.copy()\n\n        # now check the array, and make sure it\n        # meets all of our expected criteria\n        X = check_array(X, ensure_all_finite=\"allow-nan\", estimator=self, copy=True)\n\n        # check to see if there are any null values, and if\n        # so impute using the desired imputation approach\n        if np.isnan(X).any() and not self.is_corr_matrix:\n            X = impute_values(X, how=self.impute)\n\n        # get the correlation matrix\n        if self.is_corr_matrix:\n            corr_mtx = X\n        else:\n            corr_mtx = corr(X)\n            self.std_ = np.std(X, axis=0)\n            self.mean_ = np.mean(X, axis=0)\n\n        # save the original correlation matrix\n        self.corr_ = corr_mtx.copy()\n\n        # fit factor analysis model\n        if self.method == \"principal\":\n            loadings = self._fit_principal(X)\n        else:\n            loadings = self._fit_factor_analysis(corr_mtx)\n\n        # only used if we do an oblique rotations;\n        # default rotation matrix to None\n        phi = None\n        structure = None\n        rotation_mtx = None\n\n        # whether to rotate the loadings matrix\n        if self.rotation is not None:\n            if loadings.shape[1] &lt;= 1:\n                warnings.warn(\n                    \"No rotation will be performed when \"\n                    \"the number of factors equals 1.\"\n                )\n            else:\n                if \"method\" in self.rotation_kwargs:\n                    warnings.warn(\n                        \"You cannot pass a rotation method to \"\n                        \"`rotation_kwargs`. This will be ignored.\"\n                    )\n                    self.rotation_kwargs.pop(\"method\")\n                rotator = Rotator(method=self.rotation, **self.rotation_kwargs)\n                loadings = rotator.fit_transform(loadings)\n                rotation_mtx = rotator.rotation_\n                phi = rotator.phi_\n                # update the rotation matrix for everything, except promax\n                if self.rotation != \"promax\":\n                    rotation_mtx = np.linalg.inv(rotation_mtx).T\n\n        if self.n_factors &gt; 1:\n            # update loading signs to match column sums\n            # this is to ensure that signs align with R\n            signs = np.sign(loadings.sum(0))\n            signs[(signs == 0)] = 1\n            loadings = np.dot(loadings, np.diag(signs))\n\n            if phi is not None:\n                # update phi, if it exists -- that is, if the rotation is oblique\n                # create the structure matrix for any oblique rotation\n                phi = np.dot(np.dot(np.diag(signs), phi), np.diag(signs))\n                structure = (\n                    np.dot(loadings, phi)\n                    if self.rotation in OBLIQUE_ROTATIONS\n                    else None\n                )\n\n        # resort the factors according to their variance,\n        # unless the method is principal\n        if self.method != \"principal\":\n            variance = self._get_factor_variance(loadings)[0]\n            new_order = list(reversed(np.argsort(variance)))\n            loadings = loadings[:, new_order].copy()\n\n            # if the structure matrix exists, reorder\n            if structure is not None:\n                structure = structure[:, new_order].copy()\n\n        self.phi_ = phi\n        self.structure_ = structure\n\n        self.loadings_ = loadings\n        self.rotation_matrix_ = rotation_mtx\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Get factor scores for a new data set.\n\n        Parameters:\n            X (array-like): The data to score using the fitted factor model.\n                            Shape should be (n_samples, n_features).\n\n        Returns:\n            X_new (numpy.ndarray): The latent variables of X.\n                                   Shape is (n_samples, n_components).\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n            &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n            &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n            &gt;&gt;&gt; fa.fit(df_features)\n            FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n                    method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n                    use_smc=True)\n            &gt;&gt;&gt; fa.transform(df_features)\n            array([[-1.05141425,  0.57687826,  0.1658788 ],\n                   [-1.59940101,  0.89632125,  0.03824552],\n                   [-1.21768164, -1.16319406,  0.57135189],\n                   ...,\n                   [ 0.13601554,  0.03601086,  0.28813877],\n                   [ 1.86904519, -0.3532394 , -0.68170573],\n                   [ 0.86133386,  0.18280695, -0.79170903]])\n        \"\"\"\n        # check if the data is a data frame,\n        # so we can convert it to an array\n        if isinstance(X, pd.DataFrame):\n            X = X.copy().values\n        else:\n            X = X.copy()\n\n        # now check the array, and make sure it\n        # meets all of our expected criteria\n        X = check_array(X, ensure_all_finite=True, estimator=self, copy=True)\n\n        # meets all of our expected criteria\n        check_is_fitted(self, \"loadings_\")\n\n        # see if we saved the original mean and std\n        if self.mean_ is None or self.std_ is None:\n            warnings.warn(\n                \"Could not find original mean and standard deviation; using\"\n                \"the mean and standard deviation from the current data set.\"\n            )\n            mean = np.mean(X, axis=0)\n            std = np.std(X, axis=0)\n        else:\n            mean = self.mean_\n            std = self.std_\n\n        # get the scaled data\n        X_scale = (X - mean) / std\n\n        # use the structure matrix, if it exists;\n        # otherwise, just use the loadings matrix\n        if self.structure_ is not None:\n            structure = self.structure_\n        else:\n            structure = self.loadings_\n\n        try:\n            self.weights_ = np.linalg.solve(self.corr_, structure)\n        except Exception as error:\n            warnings.warn(\n                \"Unable to calculate the factor score weights; \"\n                \"factor loadings used instead: {}\".format(error)\n            )\n            self.weights_ = self.loadings_\n\n        scores = np.dot(X_scale, self.weights_)\n        return scores\n\n    def get_eigenvalues(self):\n        \"\"\"\n        Calculate the eigenvalues, given the factor correlation matrix.\n\n        Returns\n        -------\n        original_eigen_values : :obj:`numpy.ndarray`\n            The original eigenvalues\n        common_factor_eigen_values : :obj:`numpy.ndarray`\n            The common factor eigenvalues\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n        &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n        &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n        &gt;&gt;&gt; fa.fit(df_features)\n        FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n                method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n                use_smc=True)\n        &gt;&gt;&gt; fa.get_eigenvalues()\n        (array([ 3.51018854,  1.28371018,  0.73739507,  0.1334704 ,  0.03445558,\n                0.0102918 , -0.00740013, -0.03694786, -0.05959139, -0.07428112]),\n         array([ 3.51018905,  1.2837105 ,  0.73739508,  0.13347082,  0.03445601,\n                0.01029184, -0.0074    , -0.03694834, -0.05959057, -0.07428059]))\n        \"\"\"\n        # meets all of our expected criteria\n        check_is_fitted(self, [\"loadings_\", \"corr_\"])\n        corr_mtx = self.corr_.copy()\n\n        e_values, _ = np.linalg.eigh(corr_mtx)\n        e_values = e_values[::-1]\n\n        communalities = self.get_communalities()\n        communalities = communalities.copy()\n        np.fill_diagonal(corr_mtx, communalities)\n\n        values, _ = np.linalg.eigh(corr_mtx)\n        values = values[::-1]\n        return e_values, values\n\n    def get_communalities(self):\n        \"\"\"\n        Calculate the communalities, given the factor loading matrix.\n\n        Returns\n        -------\n        communalities : :obj:`numpy.ndarray`\n            The communalities from the factor loading matrix.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n        &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n        &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n        &gt;&gt;&gt; fa.fit(df_features)\n        FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n                method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n                use_smc=True)\n        &gt;&gt;&gt; fa.get_communalities()\n        array([0.588758  , 0.00382308, 0.50452402, 0.72841183, 0.33184336,\n               0.66208428, 0.61911036, 0.73194557, 0.64929612, 0.71149718])\n        \"\"\"\n        # meets all of our expected criteria\n        check_is_fitted(self, \"loadings_\")\n        loadings = self.loadings_.copy()\n        communalities = (loadings**2).sum(axis=1)\n        return communalities\n\n    def get_uniquenesses(self):\n        \"\"\"\n        Calculate the uniquenesses, given the factor loading matrix.\n\n        Returns\n        -------\n        uniquenesses : :obj:`numpy.ndarray`\n            The uniquenesses from the factor loading matrix.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n        &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n        &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n        &gt;&gt;&gt; fa.fit(df_features)\n        FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n                method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n                use_smc=True)\n        &gt;&gt;&gt; fa.get_uniquenesses()\n        array([0.411242  , 0.99617692, 0.49547598, 0.27158817, 0.66815664,\n               0.33791572, 0.38088964, 0.26805443, 0.35070388, 0.28850282])\n        \"\"\"\n        # meets all of our expected criteria\n        check_is_fitted(self, \"loadings_\")\n        communalities = self.get_communalities()\n        communalities = communalities.copy()\n        uniqueness = 1 - communalities\n        return uniqueness\n\n    @staticmethod\n    def _get_factor_variance(loadings):\n        \"\"\"\n        Get the factor variances.\n\n        This is a private helper method to get the factor variances,\n        because sometimes we need them even before the model is fitted.\n\n        Parameters\n        ----------\n        loadings : array-like\n            The factor loading matrix, in whatever state.\n\n        Returns\n        -------\n        variance : :obj:`numpy.ndarray`\n            The factor variances.\n        proportional_variance : :obj:`numpy.ndarray`\n            The proportional factor variances.\n        cumulative_variances : :obj:`numpy.ndarray`\n            The cumulative factor variances.\n        \"\"\"\n        n_rows = loadings.shape[0]\n\n        # calculate variance\n        loadings = loadings**2\n        variance = np.sum(loadings, axis=0)\n\n        # calculate proportional variance\n        proportional_variance = variance / n_rows\n\n        # calculate cumulative variance\n        cumulative_variance = np.cumsum(proportional_variance, axis=0)\n\n        return (variance, proportional_variance, cumulative_variance)\n\n    def get_factor_variance(self):\n        \"\"\"\n        Calculate factor variance information.\n\n        The factor variance information including the variance,\n        proportional variance, and cumulative variance for each factor.\n\n        Returns\n        -------\n        variance : :obj:`numpy.ndarray`\n            The factor variances.\n        proportional_variance : :obj:`numpy.ndarray`\n            The proportional factor variances.\n        cumulative_variances : :obj:`numpy.ndarray`\n            The cumulative factor variances.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n        &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n        &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n        &gt;&gt;&gt; fa.fit(df_features)\n        FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n                method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n                use_smc=True)\n        &gt;&gt;&gt; # 1. Sum of squared loadings (variance)\n        ... # 2. Proportional variance\n        ... # 3. Cumulative variance\n        &gt;&gt;&gt; fa.get_factor_variance()\n        (array([3.51018854, 1.28371018, 0.73739507]),\n         array([0.35101885, 0.12837102, 0.07373951]),\n         array([0.35101885, 0.47938987, 0.55312938]))\n        \"\"\"\n        # meets all of our expected criteria\n        check_is_fitted(self, \"loadings_\")\n        loadings = self.loadings_.copy()\n        return self._get_factor_variance(loadings)\n\n    def sufficiency(self, num_observations: int) -&gt; Tuple[float, int, float]:\n        \"\"\"\n        Perform the sufficiency test.\n\n        The test calculates statistics under the null hypothesis that\n        the selected number of factors is sufficient.\n\n        Parameters\n        ----------\n        num_observations: int\n            The number of observations in the input data that this factor\n            analyzer was fit using.\n\n        Returns\n        -------\n        statistic: float\n            The test statistic\n        degrees: int\n            The degrees of freedom\n        pvalue: float\n            The p-value of the test\n\n        References\n        ----------\n        [1] Lawley, D. N. and Maxwell, A. E. (1971). Factor Analysis as a\n             Statistical Method. Second edition. Butterworths. P. 36.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n        &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test01.csv')\n        &gt;&gt;&gt; fa = FactorAnalyzer(n_factors=3, rotation=None, method=\"ml\")\n        &gt;&gt;&gt; fa.fit(df_features)\n        &gt;&gt;&gt; fa.sufficiency(df_features.shape[0])\n        (1475.8755629859675, 663, 8.804286459822274e-64)\n        \"\"\"\n        nvar = self.corr_.shape[0]\n        degrees = ((nvar - self.n_factors) ** 2 - nvar - self.n_factors) // 2\n        obj = self._fit_ml_objective(\n            self.get_uniquenesses(), self.corr_, self.n_factors\n        )\n        statistic = (\n            num_observations - 1 - (2 * nvar + 5) / 6 - (2 * self.n_factors) / 3\n        ) * obj\n        pvalue = chi2.sf(statistic, df=degrees)\n        return statistic, degrees, pvalue\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.__init__","title":"<code>__init__(n_factors=3, rotation='promax', method='minres', use_smc=True, is_corr_matrix=False, bounds=(0.005, 1), impute='median', svd_method='randomized', rotation_kwargs=None)</code>","text":"<p>Initialize the factor analyzer.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer.py</code> <pre><code>def __init__(\n    self,\n    n_factors=3,\n    rotation=\"promax\",\n    method=\"minres\",\n    use_smc=True,\n    is_corr_matrix=False,\n    bounds=(0.005, 1),\n    impute=\"median\",\n    svd_method=\"randomized\",\n    rotation_kwargs=None,\n):\n    \"\"\"Initialize the factor analyzer.\"\"\"\n    self.n_factors = n_factors\n    self.rotation = rotation\n    self.method = method\n    self.use_smc = use_smc\n    self.bounds = bounds\n    self.impute = impute\n    self.is_corr_matrix = is_corr_matrix\n    self.svd_method = svd_method\n    self.rotation_kwargs = rotation_kwargs\n\n    # default matrices to None\n    self.mean_ = None\n    self.std_ = None\n\n    self.phi_ = None\n    self.structure_ = None\n\n    self.corr_ = None\n    self.loadings_ = None\n    self.rotation_matrix_ = None\n    self.weights_ = None\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit factor analysis model using either MINRES, ML, or principal factor analysis.</p> <p>By default, use SMC as starting guesses.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The data to analyze.</p> required <code>y</code> <code>ignored</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>FactorAnalyzer</code> <p>The fitted factor analyzer object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n&gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n&gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n&gt;&gt;&gt; fa.fit(df_features)\nFactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n        method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n        use_smc=True)\n&gt;&gt;&gt; fa.loadings_\narray([[-0.12991218,  0.16398154,  0.73823498],\n       [ 0.03899558,  0.04658425,  0.01150343],\n       [ 0.34874135,  0.61452341, -0.07255667],\n       [ 0.45318006,  0.71926681, -0.07546472],\n       [ 0.36688794,  0.44377343, -0.01737067],\n       [ 0.74141382, -0.15008235,  0.29977512],\n       [ 0.741675  , -0.16123009, -0.20744495],\n       [ 0.82910167, -0.20519428,  0.04930817],\n       [ 0.76041819, -0.23768727, -0.1206858 ],\n       [ 0.81533404, -0.12494695,  0.17639683]])\n</code></pre> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer.py</code> <pre><code>def fit(self, X, y=None) -&gt; \"FactorAnalyzer\":\n    \"\"\"\n    Fit factor analysis model using either MINRES, ML, or principal factor analysis.\n\n    By default, use SMC as starting guesses.\n\n    Parameters:\n        X (array-like): The data to analyze.\n        y (ignored): Ignored.\n\n    Returns:\n        self: The fitted factor analyzer object.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n        &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n        &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n        &gt;&gt;&gt; fa.fit(df_features)\n        FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n                method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n                use_smc=True)\n        &gt;&gt;&gt; fa.loadings_\n        array([[-0.12991218,  0.16398154,  0.73823498],\n               [ 0.03899558,  0.04658425,  0.01150343],\n               [ 0.34874135,  0.61452341, -0.07255667],\n               [ 0.45318006,  0.71926681, -0.07546472],\n               [ 0.36688794,  0.44377343, -0.01737067],\n               [ 0.74141382, -0.15008235,  0.29977512],\n               [ 0.741675  , -0.16123009, -0.20744495],\n               [ 0.82910167, -0.20519428,  0.04930817],\n               [ 0.76041819, -0.23768727, -0.1206858 ],\n               [ 0.81533404, -0.12494695,  0.17639683]])\n    \"\"\"\n    # check the input arguments\n    self._arg_checker()\n\n    # check if the data is a data frame,\n    # so we can convert it to an array\n    if isinstance(X, pd.DataFrame):\n        X = X.copy().values\n    else:\n        X = X.copy()\n\n    # now check the array, and make sure it\n    # meets all of our expected criteria\n    X = check_array(X, ensure_all_finite=\"allow-nan\", estimator=self, copy=True)\n\n    # check to see if there are any null values, and if\n    # so impute using the desired imputation approach\n    if np.isnan(X).any() and not self.is_corr_matrix:\n        X = impute_values(X, how=self.impute)\n\n    # get the correlation matrix\n    if self.is_corr_matrix:\n        corr_mtx = X\n    else:\n        corr_mtx = corr(X)\n        self.std_ = np.std(X, axis=0)\n        self.mean_ = np.mean(X, axis=0)\n\n    # save the original correlation matrix\n    self.corr_ = corr_mtx.copy()\n\n    # fit factor analysis model\n    if self.method == \"principal\":\n        loadings = self._fit_principal(X)\n    else:\n        loadings = self._fit_factor_analysis(corr_mtx)\n\n    # only used if we do an oblique rotations;\n    # default rotation matrix to None\n    phi = None\n    structure = None\n    rotation_mtx = None\n\n    # whether to rotate the loadings matrix\n    if self.rotation is not None:\n        if loadings.shape[1] &lt;= 1:\n            warnings.warn(\n                \"No rotation will be performed when \"\n                \"the number of factors equals 1.\"\n            )\n        else:\n            if \"method\" in self.rotation_kwargs:\n                warnings.warn(\n                    \"You cannot pass a rotation method to \"\n                    \"`rotation_kwargs`. This will be ignored.\"\n                )\n                self.rotation_kwargs.pop(\"method\")\n            rotator = Rotator(method=self.rotation, **self.rotation_kwargs)\n            loadings = rotator.fit_transform(loadings)\n            rotation_mtx = rotator.rotation_\n            phi = rotator.phi_\n            # update the rotation matrix for everything, except promax\n            if self.rotation != \"promax\":\n                rotation_mtx = np.linalg.inv(rotation_mtx).T\n\n    if self.n_factors &gt; 1:\n        # update loading signs to match column sums\n        # this is to ensure that signs align with R\n        signs = np.sign(loadings.sum(0))\n        signs[(signs == 0)] = 1\n        loadings = np.dot(loadings, np.diag(signs))\n\n        if phi is not None:\n            # update phi, if it exists -- that is, if the rotation is oblique\n            # create the structure matrix for any oblique rotation\n            phi = np.dot(np.dot(np.diag(signs), phi), np.diag(signs))\n            structure = (\n                np.dot(loadings, phi)\n                if self.rotation in OBLIQUE_ROTATIONS\n                else None\n            )\n\n    # resort the factors according to their variance,\n    # unless the method is principal\n    if self.method != \"principal\":\n        variance = self._get_factor_variance(loadings)[0]\n        new_order = list(reversed(np.argsort(variance)))\n        loadings = loadings[:, new_order].copy()\n\n        # if the structure matrix exists, reorder\n        if structure is not None:\n            structure = structure[:, new_order].copy()\n\n    self.phi_ = phi\n    self.structure_ = structure\n\n    self.loadings_ = loadings\n    self.rotation_matrix_ = rotation_mtx\n    return self\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.get_communalities","title":"<code>get_communalities()</code>","text":"<p>Calculate the communalities, given the factor loading matrix.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.get_communalities--returns","title":"Returns","text":"<p>communalities : :obj:<code>numpy.ndarray</code>     The communalities from the factor loading matrix.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.get_communalities--examples","title":"Examples","text":"<p>import pandas as pd from factor_analyzer import FactorAnalyzer df_features = pd.read_csv(\u2018tests/data/test02.csv\u2019) fa = FactorAnalyzer(rotation=None) fa.fit(df_features) FactorAnalyzer(bounds=(0.005, 1), impute=\u2019median\u2019, is_corr_matrix=False,         method=\u2019minres\u2019, n_factors=3, rotation=None, rotation_kwargs={},         use_smc=True) fa.get_communalities() array([0.588758  , 0.00382308, 0.50452402, 0.72841183, 0.33184336,        0.66208428, 0.61911036, 0.73194557, 0.64929612, 0.71149718])</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer.py</code> <pre><code>def get_communalities(self):\n    \"\"\"\n    Calculate the communalities, given the factor loading matrix.\n\n    Returns\n    -------\n    communalities : :obj:`numpy.ndarray`\n        The communalities from the factor loading matrix.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n    &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n    &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n    &gt;&gt;&gt; fa.fit(df_features)\n    FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n            method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n            use_smc=True)\n    &gt;&gt;&gt; fa.get_communalities()\n    array([0.588758  , 0.00382308, 0.50452402, 0.72841183, 0.33184336,\n           0.66208428, 0.61911036, 0.73194557, 0.64929612, 0.71149718])\n    \"\"\"\n    # meets all of our expected criteria\n    check_is_fitted(self, \"loadings_\")\n    loadings = self.loadings_.copy()\n    communalities = (loadings**2).sum(axis=1)\n    return communalities\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.get_eigenvalues","title":"<code>get_eigenvalues()</code>","text":"<p>Calculate the eigenvalues, given the factor correlation matrix.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.get_eigenvalues--returns","title":"Returns","text":"<p>original_eigen_values : :obj:<code>numpy.ndarray</code>     The original eigenvalues common_factor_eigen_values : :obj:<code>numpy.ndarray</code>     The common factor eigenvalues</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.get_eigenvalues--examples","title":"Examples","text":"<p>import pandas as pd from factor_analyzer import FactorAnalyzer df_features = pd.read_csv(\u2018tests/data/test02.csv\u2019) fa = FactorAnalyzer(rotation=None) fa.fit(df_features) FactorAnalyzer(bounds=(0.005, 1), impute=\u2019median\u2019, is_corr_matrix=False,         method=\u2019minres\u2019, n_factors=3, rotation=None, rotation_kwargs={},         use_smc=True) fa.get_eigenvalues() (array([ 3.51018854,  1.28371018,  0.73739507,  0.1334704 ,  0.03445558,         0.0102918 , -0.00740013, -0.03694786, -0.05959139, -0.07428112]),  array([ 3.51018905,  1.2837105 ,  0.73739508,  0.13347082,  0.03445601,         0.01029184, -0.0074    , -0.03694834, -0.05959057, -0.07428059]))</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer.py</code> <pre><code>def get_eigenvalues(self):\n    \"\"\"\n    Calculate the eigenvalues, given the factor correlation matrix.\n\n    Returns\n    -------\n    original_eigen_values : :obj:`numpy.ndarray`\n        The original eigenvalues\n    common_factor_eigen_values : :obj:`numpy.ndarray`\n        The common factor eigenvalues\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n    &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n    &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n    &gt;&gt;&gt; fa.fit(df_features)\n    FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n            method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n            use_smc=True)\n    &gt;&gt;&gt; fa.get_eigenvalues()\n    (array([ 3.51018854,  1.28371018,  0.73739507,  0.1334704 ,  0.03445558,\n            0.0102918 , -0.00740013, -0.03694786, -0.05959139, -0.07428112]),\n     array([ 3.51018905,  1.2837105 ,  0.73739508,  0.13347082,  0.03445601,\n            0.01029184, -0.0074    , -0.03694834, -0.05959057, -0.07428059]))\n    \"\"\"\n    # meets all of our expected criteria\n    check_is_fitted(self, [\"loadings_\", \"corr_\"])\n    corr_mtx = self.corr_.copy()\n\n    e_values, _ = np.linalg.eigh(corr_mtx)\n    e_values = e_values[::-1]\n\n    communalities = self.get_communalities()\n    communalities = communalities.copy()\n    np.fill_diagonal(corr_mtx, communalities)\n\n    values, _ = np.linalg.eigh(corr_mtx)\n    values = values[::-1]\n    return e_values, values\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.get_factor_variance","title":"<code>get_factor_variance()</code>","text":"<p>Calculate factor variance information.</p> <p>The factor variance information including the variance, proportional variance, and cumulative variance for each factor.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.get_factor_variance--returns","title":"Returns","text":"<p>variance : :obj:<code>numpy.ndarray</code>     The factor variances. proportional_variance : :obj:<code>numpy.ndarray</code>     The proportional factor variances. cumulative_variances : :obj:<code>numpy.ndarray</code>     The cumulative factor variances.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.get_factor_variance--examples","title":"Examples","text":"<p>import pandas as pd from factor_analyzer import FactorAnalyzer df_features = pd.read_csv(\u2018tests/data/test02.csv\u2019) fa = FactorAnalyzer(rotation=None) fa.fit(df_features) FactorAnalyzer(bounds=(0.005, 1), impute=\u2019median\u2019, is_corr_matrix=False,         method=\u2019minres\u2019, n_factors=3, rotation=None, rotation_kwargs={},         use_smc=True)</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer.py</code> <pre><code>def get_factor_variance(self):\n    \"\"\"\n    Calculate factor variance information.\n\n    The factor variance information including the variance,\n    proportional variance, and cumulative variance for each factor.\n\n    Returns\n    -------\n    variance : :obj:`numpy.ndarray`\n        The factor variances.\n    proportional_variance : :obj:`numpy.ndarray`\n        The proportional factor variances.\n    cumulative_variances : :obj:`numpy.ndarray`\n        The cumulative factor variances.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n    &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n    &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n    &gt;&gt;&gt; fa.fit(df_features)\n    FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n            method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n            use_smc=True)\n    &gt;&gt;&gt; # 1. Sum of squared loadings (variance)\n    ... # 2. Proportional variance\n    ... # 3. Cumulative variance\n    &gt;&gt;&gt; fa.get_factor_variance()\n    (array([3.51018854, 1.28371018, 0.73739507]),\n     array([0.35101885, 0.12837102, 0.07373951]),\n     array([0.35101885, 0.47938987, 0.55312938]))\n    \"\"\"\n    # meets all of our expected criteria\n    check_is_fitted(self, \"loadings_\")\n    loadings = self.loadings_.copy()\n    return self._get_factor_variance(loadings)\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.get_factor_variance--1-sum-of-squared-loadings-variance","title":"1. Sum of squared loadings (variance)","text":"<p>\u2026 # 2. Proportional variance \u2026 # 3. Cumulative variance fa.get_factor_variance() (array([3.51018854, 1.28371018, 0.73739507]),  array([0.35101885, 0.12837102, 0.07373951]),  array([0.35101885, 0.47938987, 0.55312938]))</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.get_uniquenesses","title":"<code>get_uniquenesses()</code>","text":"<p>Calculate the uniquenesses, given the factor loading matrix.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.get_uniquenesses--returns","title":"Returns","text":"<p>uniquenesses : :obj:<code>numpy.ndarray</code>     The uniquenesses from the factor loading matrix.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.get_uniquenesses--examples","title":"Examples","text":"<p>import pandas as pd from factor_analyzer import FactorAnalyzer df_features = pd.read_csv(\u2018tests/data/test02.csv\u2019) fa = FactorAnalyzer(rotation=None) fa.fit(df_features) FactorAnalyzer(bounds=(0.005, 1), impute=\u2019median\u2019, is_corr_matrix=False,         method=\u2019minres\u2019, n_factors=3, rotation=None, rotation_kwargs={},         use_smc=True) fa.get_uniquenesses() array([0.411242  , 0.99617692, 0.49547598, 0.27158817, 0.66815664,        0.33791572, 0.38088964, 0.26805443, 0.35070388, 0.28850282])</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer.py</code> <pre><code>def get_uniquenesses(self):\n    \"\"\"\n    Calculate the uniquenesses, given the factor loading matrix.\n\n    Returns\n    -------\n    uniquenesses : :obj:`numpy.ndarray`\n        The uniquenesses from the factor loading matrix.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n    &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n    &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n    &gt;&gt;&gt; fa.fit(df_features)\n    FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n            method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n            use_smc=True)\n    &gt;&gt;&gt; fa.get_uniquenesses()\n    array([0.411242  , 0.99617692, 0.49547598, 0.27158817, 0.66815664,\n           0.33791572, 0.38088964, 0.26805443, 0.35070388, 0.28850282])\n    \"\"\"\n    # meets all of our expected criteria\n    check_is_fitted(self, \"loadings_\")\n    communalities = self.get_communalities()\n    communalities = communalities.copy()\n    uniqueness = 1 - communalities\n    return uniqueness\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.sufficiency","title":"<code>sufficiency(num_observations)</code>","text":"<p>Perform the sufficiency test.</p> <p>The test calculates statistics under the null hypothesis that the selected number of factors is sufficient.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.sufficiency--parameters","title":"Parameters","text":"<p>num_observations: int     The number of observations in the input data that this factor     analyzer was fit using.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.sufficiency--returns","title":"Returns","text":"<p>statistic: float     The test statistic degrees: int     The degrees of freedom pvalue: float     The p-value of the test</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.sufficiency--references","title":"References","text":"<p>[1] Lawley, D. N. and Maxwell, A. E. (1971). Factor Analysis as a      Statistical Method. Second edition. Butterworths. P. 36.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.sufficiency--examples","title":"Examples","text":"<p>import pandas as pd from factor_analyzer import FactorAnalyzer df_features = pd.read_csv(\u2018tests/data/test01.csv\u2019) fa = FactorAnalyzer(n_factors=3, rotation=None, method=\u201dml\u201d) fa.fit(df_features) fa.sufficiency(df_features.shape[0]) (1475.8755629859675, 663, 8.804286459822274e-64)</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer.py</code> <pre><code>def sufficiency(self, num_observations: int) -&gt; Tuple[float, int, float]:\n    \"\"\"\n    Perform the sufficiency test.\n\n    The test calculates statistics under the null hypothesis that\n    the selected number of factors is sufficient.\n\n    Parameters\n    ----------\n    num_observations: int\n        The number of observations in the input data that this factor\n        analyzer was fit using.\n\n    Returns\n    -------\n    statistic: float\n        The test statistic\n    degrees: int\n        The degrees of freedom\n    pvalue: float\n        The p-value of the test\n\n    References\n    ----------\n    [1] Lawley, D. N. and Maxwell, A. E. (1971). Factor Analysis as a\n         Statistical Method. Second edition. Butterworths. P. 36.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n    &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test01.csv')\n    &gt;&gt;&gt; fa = FactorAnalyzer(n_factors=3, rotation=None, method=\"ml\")\n    &gt;&gt;&gt; fa.fit(df_features)\n    &gt;&gt;&gt; fa.sufficiency(df_features.shape[0])\n    (1475.8755629859675, 663, 8.804286459822274e-64)\n    \"\"\"\n    nvar = self.corr_.shape[0]\n    degrees = ((nvar - self.n_factors) ** 2 - nvar - self.n_factors) // 2\n    obj = self._fit_ml_objective(\n        self.get_uniquenesses(), self.corr_, self.n_factors\n    )\n    statistic = (\n        num_observations - 1 - (2 * nvar + 5) / 6 - (2 * self.n_factors) / 3\n    ) * obj\n    pvalue = chi2.sf(statistic, df=degrees)\n    return statistic, degrees, pvalue\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.FactorAnalyzer.transform","title":"<code>transform(X)</code>","text":"<p>Get factor scores for a new data set.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The data to score using the fitted factor model.             Shape should be (n_samples, n_features).</p> required <p>Returns:</p> Name Type Description <code>X_new</code> <code>ndarray</code> <p>The latent variables of X.                    Shape is (n_samples, n_components).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n&gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n&gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n&gt;&gt;&gt; fa.fit(df_features)\nFactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n        method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n        use_smc=True)\n&gt;&gt;&gt; fa.transform(df_features)\narray([[-1.05141425,  0.57687826,  0.1658788 ],\n       [-1.59940101,  0.89632125,  0.03824552],\n       [-1.21768164, -1.16319406,  0.57135189],\n       ...,\n       [ 0.13601554,  0.03601086,  0.28813877],\n       [ 1.86904519, -0.3532394 , -0.68170573],\n       [ 0.86133386,  0.18280695, -0.79170903]])\n</code></pre> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    Get factor scores for a new data set.\n\n    Parameters:\n        X (array-like): The data to score using the fitted factor model.\n                        Shape should be (n_samples, n_features).\n\n    Returns:\n        X_new (numpy.ndarray): The latent variables of X.\n                               Shape is (n_samples, n_components).\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer\n        &gt;&gt;&gt; df_features = pd.read_csv('tests/data/test02.csv')\n        &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n        &gt;&gt;&gt; fa.fit(df_features)\n        FactorAnalyzer(bounds=(0.005, 1), impute='median', is_corr_matrix=False,\n                method='minres', n_factors=3, rotation=None, rotation_kwargs={},\n                use_smc=True)\n        &gt;&gt;&gt; fa.transform(df_features)\n        array([[-1.05141425,  0.57687826,  0.1658788 ],\n               [-1.59940101,  0.89632125,  0.03824552],\n               [-1.21768164, -1.16319406,  0.57135189],\n               ...,\n               [ 0.13601554,  0.03601086,  0.28813877],\n               [ 1.86904519, -0.3532394 , -0.68170573],\n               [ 0.86133386,  0.18280695, -0.79170903]])\n    \"\"\"\n    # check if the data is a data frame,\n    # so we can convert it to an array\n    if isinstance(X, pd.DataFrame):\n        X = X.copy().values\n    else:\n        X = X.copy()\n\n    # now check the array, and make sure it\n    # meets all of our expected criteria\n    X = check_array(X, ensure_all_finite=True, estimator=self, copy=True)\n\n    # meets all of our expected criteria\n    check_is_fitted(self, \"loadings_\")\n\n    # see if we saved the original mean and std\n    if self.mean_ is None or self.std_ is None:\n        warnings.warn(\n            \"Could not find original mean and standard deviation; using\"\n            \"the mean and standard deviation from the current data set.\"\n        )\n        mean = np.mean(X, axis=0)\n        std = np.std(X, axis=0)\n    else:\n        mean = self.mean_\n        std = self.std_\n\n    # get the scaled data\n    X_scale = (X - mean) / std\n\n    # use the structure matrix, if it exists;\n    # otherwise, just use the loadings matrix\n    if self.structure_ is not None:\n        structure = self.structure_\n    else:\n        structure = self.loadings_\n\n    try:\n        self.weights_ = np.linalg.solve(self.corr_, structure)\n    except Exception as error:\n        warnings.warn(\n            \"Unable to calculate the factor score weights; \"\n            \"factor loadings used instead: {}\".format(error)\n        )\n        self.weights_ = self.loadings_\n\n    scores = np.dot(X_scale, self.weights_)\n    return scores\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.calculate_bartlett_sphericity","title":"<code>calculate_bartlett_sphericity(x)</code>","text":"<p>Compute the Bartlett sphericity test.</p> <p>H0: The matrix of population correlations is equal to I. H1: The matrix of population correlations is not equal to I.</p> <p>The formula for Bartlett\u2019s Sphericity test is:</p> <p>.. math:: -1 * (n - 1 - ((2p + 5) / 6)) * ln(det(R))</p> <p>Where R det(R) is the determinant of the correlation matrix, and p is the number of variables.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.calculate_bartlett_sphericity--parameters","title":"Parameters","text":"<p>x : array-like     The array for which to calculate sphericity.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.calculate_bartlett_sphericity--returns","title":"Returns","text":"<p>statistic : float     The chi-square value. p_value : float     The associated p-value for the test.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer.py</code> <pre><code>def calculate_bartlett_sphericity(x):\n    \"\"\"\n    Compute the Bartlett sphericity test.\n\n    H0: The matrix of population correlations is equal to I.\n    H1: The matrix of population correlations is not equal to I.\n\n    The formula for Bartlett's Sphericity test is:\n\n    .. math:: -1 * (n - 1 - ((2p + 5) / 6)) * ln(det(R))\n\n    Where R det(R) is the determinant of the correlation matrix,\n    and p is the number of variables.\n\n    Parameters\n    ----------\n    x : array-like\n        The array for which to calculate sphericity.\n\n    Returns\n    -------\n    statistic : float\n        The chi-square value.\n    p_value : float\n        The associated p-value for the test.\n    \"\"\"\n    n, p = x.shape\n    x_corr = corr(x)\n\n    corr_det = np.linalg.det(x_corr)\n    statistic = -np.log(corr_det) * (n - 1 - (2 * p + 5) / 6)\n    degrees_of_freedom = p * (p - 1) / 2\n    p_value = chi2.sf(statistic, degrees_of_freedom)\n    return statistic, p_value\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.calculate_kmo","title":"<code>calculate_kmo(x)</code>","text":"<p>Calculate the Kaiser-Meyer-Olkin criterion for items and overall.</p> <p>This statistic represents the degree to which each observed variable is predicted, without error, by the other variables in the dataset. In general, a KMO &lt; 0.6 is considered inadequate.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.calculate_kmo--parameters","title":"Parameters","text":"<p>x : array-like     The array from which to calculate KMOs.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer/#spotoptim.factor_analyzer.factor_analyzer.calculate_kmo--returns","title":"Returns","text":"<p>kmo_per_variable : :obj:<code>numpy.ndarray</code>     The KMO score per item. kmo_total : float     The overall KMO score.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer.py</code> <pre><code>def calculate_kmo(x):\n    \"\"\"\n    Calculate the Kaiser-Meyer-Olkin criterion for items and overall.\n\n    This statistic represents the degree to which each observed variable is\n    predicted, without error, by the other variables in the dataset.\n    In general, a KMO &lt; 0.6 is considered inadequate.\n\n    Parameters\n    ----------\n    x : array-like\n        The array from which to calculate KMOs.\n\n    Returns\n    -------\n    kmo_per_variable : :obj:`numpy.ndarray`\n        The KMO score per item.\n    kmo_total : float\n        The overall KMO score.\n    \"\"\"\n    # calculate the partial correlations\n    partial_corr = partial_correlations(x)\n\n    # calcualte the pair-wise correlations\n    x_corr = corr(x)\n\n    # fill matrix diagonals with zeros\n    # and square all elements\n    np.fill_diagonal(x_corr, 0)\n    np.fill_diagonal(partial_corr, 0)\n\n    partial_corr = partial_corr**2\n    x_corr = x_corr**2\n\n    # calculate KMO per item\n    partial_corr_sum = np.sum(partial_corr, axis=0)\n    corr_sum = np.sum(x_corr, axis=0)\n    kmo_per_item = corr_sum / (corr_sum + partial_corr_sum)\n\n    # calculate KMO overall\n    corr_sum_total = np.sum(x_corr)\n    partial_corr_sum_total = np.sum(partial_corr)\n    kmo_total = corr_sum_total / (corr_sum_total + partial_corr_sum_total)\n    return kmo_per_item, kmo_total\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_rotator/","title":"factor_analyzer_rotator","text":"<p>Class to perform various rotations of factor loading matrices.</p> <p>Confirmatory factor analysis using machine learning methods. Re-implementation of the factor-analyzer package.</p> <p>See https://factor-analyzer.readthedocs.io/en/latest/introduction.html for more details.</p> <p>Authors of the original implementation: * Jeremy Biggs (jeremy.m.biggs@gmail.com) * Nitin Madnani (nmadnani@ets.org) Organization: Educational Testing Service Date: 2022-09-05</p> <p>This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.</p> <p>This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_rotator/#spotoptim.factor_analyzer.factor_analyzer_rotator.Rotator","title":"<code>Rotator</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Perform rotations on an unrotated factor loading matrix.</p> <p>The Rotator class takes an (unrotated) factor loading matrix and performs one of several rotations.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The factor rotation method. Options include: (a) varimax (orthogonal rotation) (b) promax (oblique rotation) (c) oblimin (oblique rotation) (d) oblimax (orthogonal rotation) (e) quartimin (oblique rotation) (f) quartimax (orthogonal rotation) (g) equamax (orthogonal rotation) (h) geomin_obl (oblique rotation) (i) geomin_ort (orthogonal rotation) Defaults to \u2018varimax\u2019.</p> <code>'varimax'</code> <code>normalize</code> <code>bool</code> <p>Whether to perform Kaiser normalization and de-normalization prior to and following rotation. Used for \u2018varimax\u2019 and \u2018promax\u2019 rotations. If None, default for \u2018promax\u2019 is False, and default for \u2018varimax\u2019 is True. Defaults to None.</p> <code>True</code> <code>power</code> <code>int</code> <p>The exponent to which to raise the promax loadings (minus 1). Numbers should generally range from 2 to 4. Defaults to 4.</p> <code>4</code> <code>kappa</code> <code>float</code> <p>The kappa value for the \u2018equamax\u2019 objective. Ignored if the method is not \u2018equamax\u2019. Defaults to 0.</p> <code>0</code> <code>gamma</code> <code>int</code> <p>The gamma level for the \u2018oblimin\u2019 objective. Ignored if the method is not \u2018oblimin\u2019. Defaults to 0.</p> <code>0</code> <code>delta</code> <code>float</code> <p>The delta level for \u2018geomin\u2019 objectives. Ignored if the method is not \u2018geomin_*\u2019. Defaults to 0.01.</p> <code>0.01</code> <code>max_iter</code> <code>int</code> <p>The maximum number of iterations. Used for \u2018varimax\u2019 and \u2018oblique\u2019 rotations. Defaults to 1000.</p> <code>500</code> <code>tol</code> <code>float</code> <p>The convergence threshold. Used for \u2018varimax\u2019 and \u2018oblique\u2019 rotations. Defaults to 1e-5.</p> <code>1e-05</code> <p>Attributes:</p> Name Type Description <code>loadings_</code> <code>ndarray</code> <p>The loadings matrix. Shape (n_features, n_factors).</p> <code>rotation_</code> <code>ndarray</code> <p>The rotation matrix. Shape (n_factors, n_factors).</p> <code>phi_</code> <code>ndarray or None</code> <p>The factor correlations matrix. This only exists if method is \u2018oblique\u2019.</p> Notes <p>Most of the rotations in this class are ported from R\u2019s <code>GPARotation</code> package.</p> References <p>[1] https://cran.r-project.org/web/packages/GPArotation/index.html</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from factor_analyzer import FactorAnalyzer, Rotator\n&gt;&gt;&gt; df_features = pd.read_csv('test02.csv')\n&gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n&gt;&gt;&gt; fa.fit(df_features)\n&gt;&gt;&gt; rotator = Rotator()\n&gt;&gt;&gt; rotator.fit_transform(fa.loadings_)\narray([[-0.07693215,  0.04499572,  0.76211208],\n       [ 0.01842035,  0.05757874,  0.01297908],\n       [ 0.06067925,  0.70692662, -0.03311798],\n       [ 0.11314343,  0.84525117, -0.03407129],\n       [ 0.15307233,  0.5553474 , -0.00121802],\n       [ 0.77450832,  0.1474666 ,  0.20118338],\n       [ 0.7063001 ,  0.17229555, -0.30093981],\n       [ 0.83990851,  0.15058874, -0.06182469],\n       [ 0.76620579,  0.1045194 , -0.22649615],\n       [ 0.81372945,  0.20915845,  0.07479506]])\n</code></pre> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_rotator.py</code> <pre><code>class Rotator(BaseEstimator):\n    \"\"\"\n    Perform rotations on an unrotated factor loading matrix.\n\n    The Rotator class takes an (unrotated) factor loading matrix and\n    performs one of several rotations.\n\n    Args:\n        method (str, optional): The factor rotation method. Options include:\n            (a) varimax (orthogonal rotation)\n            (b) promax (oblique rotation)\n            (c) oblimin (oblique rotation)\n            (d) oblimax (orthogonal rotation)\n            (e) quartimin (oblique rotation)\n            (f) quartimax (orthogonal rotation)\n            (g) equamax (orthogonal rotation)\n            (h) geomin_obl (oblique rotation)\n            (i) geomin_ort (orthogonal rotation)\n            Defaults to 'varimax'.\n        normalize (bool, optional): Whether to perform Kaiser normalization and de-normalization prior\n            to and following rotation. Used for 'varimax' and 'promax' rotations.\n            If None, default for 'promax' is False, and default for 'varimax' is True.\n            Defaults to None.\n        power (int, optional): The exponent to which to raise the promax loadings (minus 1).\n            Numbers should generally range from 2 to 4.\n            Defaults to 4.\n        kappa (float, optional): The kappa value for the 'equamax' objective. Ignored if the method\n            is not 'equamax'.\n            Defaults to 0.\n        gamma (int, optional): The gamma level for the 'oblimin' objective. Ignored if the method\n            is not 'oblimin'.\n            Defaults to 0.\n        delta (float, optional): The delta level for 'geomin' objectives. Ignored if the method is\n            not 'geomin_*'.\n            Defaults to 0.01.\n        max_iter (int, optional): The maximum number of iterations. Used for 'varimax' and 'oblique'\n            rotations.\n            Defaults to 1000.\n        tol (float, optional): The convergence threshold. Used for 'varimax' and 'oblique' rotations.\n            Defaults to 1e-5.\n\n    Attributes:\n        loadings_ (numpy.ndarray): The loadings matrix. Shape (n_features, n_factors).\n        rotation_ (numpy.ndarray): The rotation matrix. Shape (n_factors, n_factors).\n        phi_ (numpy.ndarray or None): The factor correlations matrix. This only exists if method is\n            'oblique'.\n\n    Notes:\n        Most of the rotations in this class are ported from R's ``GPARotation`` package.\n\n    References:\n        [1] https://cran.r-project.org/web/packages/GPArotation/index.html\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer, Rotator\n        &gt;&gt;&gt; df_features = pd.read_csv('test02.csv')\n        &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n        &gt;&gt;&gt; fa.fit(df_features)\n        &gt;&gt;&gt; rotator = Rotator()\n        &gt;&gt;&gt; rotator.fit_transform(fa.loadings_)\n        array([[-0.07693215,  0.04499572,  0.76211208],\n               [ 0.01842035,  0.05757874,  0.01297908],\n               [ 0.06067925,  0.70692662, -0.03311798],\n               [ 0.11314343,  0.84525117, -0.03407129],\n               [ 0.15307233,  0.5553474 , -0.00121802],\n               [ 0.77450832,  0.1474666 ,  0.20118338],\n               [ 0.7063001 ,  0.17229555, -0.30093981],\n               [ 0.83990851,  0.15058874, -0.06182469],\n               [ 0.76620579,  0.1045194 , -0.22649615],\n               [ 0.81372945,  0.20915845,  0.07479506]])\n    \"\"\"\n\n    def __init__(\n        self,\n        method=\"varimax\",\n        normalize=True,\n        power=4,\n        kappa=0,\n        gamma=0,\n        delta=0.01,\n        max_iter=500,\n        tol=1e-5,\n    ):\n        \"\"\"Initialize the rotator class.\"\"\"\n        self.method = method\n        self.normalize = normalize\n        self.power = power\n        self.kappa = kappa\n        self.gamma = gamma\n        self.delta = delta\n        self.max_iter = max_iter\n        self.tol = tol\n\n        self.loadings_ = None\n        self.rotation_ = None\n        self.phi_ = None\n\n    def _oblimax_obj(self, loadings):  # noqa: D401\n        \"\"\"\n        The Oblimax function objective.\n\n        Args:\n            loadings (array-like): The loading matrix\n\n        Returns:\n            gradient_dict (dict): A dictionary containing the following keys:\n                (1) ``grad`` : :obj:`numpy.ndarray`, containing the gradients.\n                (2) ``criterion`` : float, containing the criterion for the objective.\n        \"\"\"\n        gradient = -(\n            4 * loadings**3 / (np.sum(loadings**4))\n            - 4 * loadings / (np.sum(loadings**2))\n        )\n        criterion = np.log(np.sum(loadings**4)) - 2 * np.log(np.sum(loadings**2))\n        return {\"grad\": gradient, \"criterion\": criterion}\n\n    def _quartimax_obj(self, loadings):\n        \"\"\"\n        Quartimax function objective.\n\n        Args:\n            loadings (array-like): The loading matrix.\n\n        Returns:\n            gradient_dict (dict): A dictionary containing the following keys:\n                (1) ``grad`` : :obj:`numpy.ndarray`, containing the gradients.\n                (2) ``criterion`` : float, containing the criterion for the objective.\n        \"\"\"\n        gradient = -(loadings**3)\n        criterion = -np.sum(np.diag(np.dot((loadings**2).T, loadings**2))) / 4\n        return {\"grad\": gradient, \"criterion\": criterion}\n\n    def _oblimin_obj(self, loadings):  # noqa: D401\n        \"\"\"\n        The Oblimin function objective.\n\n        Args:\n            loadings (array-like): The loading matrix\n\n        Returns:\n            gradient_dict (dict): A dictionary containing the following keys:\n                (1) ``grad`` : :obj:`numpy.ndarray`, containing the gradients.\n                (2) ``criterion`` : float, containing the criterion for the objective.\n        \"\"\"\n        X = np.dot(loadings**2, np.eye(loadings.shape[1]) != 1)\n        if self.gamma != 0:\n            p = loadings.shape[0]\n            X = np.diag(np.full(1, p)) - np.dot(np.zeros((p, p)), X)\n        gradient = loadings * X\n        criterion = np.sum(loadings**2 * X) / 4\n        return {\"grad\": gradient, \"criterion\": criterion}\n\n    def _quartimin_obj(self, loadings):  # noqa: D401\n        \"\"\"\n        The Quartimin function objective.\n\n        Args:\n            loadings (array-like): The loading matrix.\n\n        Returns:\n            gradient_dict (dict): A dictionary containing the following keys:\n                (1) ``grad`` : :obj:`numpy.ndarray`, containing the gradients.\n                (2) ``criterion`` : float, containing the criterion for the objective.\n        \"\"\"\n        X = np.dot(loadings**2, np.eye(loadings.shape[1]) != 1)\n        gradient = loadings * X\n        criterion = np.sum(loadings**2 * X) / 4\n        return {\"grad\": gradient, \"criterion\": criterion}\n\n    def _equamax_obj(self, loadings):  # noqa: D401\n        \"\"\"\n        The Equamax function objective.\n\n        Args:\n            loadings (array-like): The loading matrix.\n\n        Returns:\n            gradient_dict (dict): A dictionary containing the following keys:\n                (1) ``grad`` : :obj:`numpy.ndarray`, containing the gradients.\n                (2) ``criterion`` : float, containing the criterion for the objective.\n        \"\"\"\n        p, k = loadings.shape\n\n        N = np.ones(k) - np.eye(k)\n        M = np.ones(p) - np.eye(p)\n\n        loadings_squared = loadings**2\n        f1 = (\n            (1 - self.kappa)\n            * np.sum(np.diag(np.dot(loadings_squared.T, np.dot(loadings_squared, N))))\n            / 4\n        )\n        f2 = (\n            self.kappa\n            * np.sum(np.diag(np.dot(loadings_squared.T, np.dot(M, loadings_squared))))\n            / 4\n        )\n\n        gradient = (1 - self.kappa) * loadings * np.dot(\n            loadings_squared, N\n        ) + self.kappa * loadings * np.dot(M, loadings_squared)\n\n        criterion = f1 + f2\n        return {\"grad\": gradient, \"criterion\": criterion}\n\n    def _geomin_obj(self, loadings):  # noqa: D401\n        \"\"\"\n        The Geomin function objective.\n\n        Args:\n            loadings (array-like): The loading matrix.\n\n        Returns:\n            gradient_dict (dict): A dictionary containing the following keys:\n                (1) ``grad`` : :obj:`numpy.ndarray`, containing the gradients.\n                (2) ``criterion`` : float, containing the criterion for the objective.\n        \"\"\"\n        p, k = loadings.shape\n\n        loadings2 = loadings**2 + self.delta\n\n        pro = np.exp(np.log(loadings2).sum(1) / k)\n        rep = np.repeat(pro, k, axis=0).reshape(p, k)\n\n        gradient = (2 / k) * (loadings / loadings2) * rep\n        criterion = np.sum(pro)\n        return {\"grad\": gradient, \"criterion\": criterion}\n\n    def _oblique(self, loadings, method):\n        \"\"\"\n        Perform oblique rotations, except 'promax'.\n\n        A generic function for performing all oblique rotations, except for\n        promax, which is implemented separately.\n\n        Args:\n            loadings (array-like): The loading matrix\n            method (str): The obligue rotation method to use.\n\n        Returns:\n            loadings (:obj:`numpy.ndarray`): The loadings matrix. Shape (``n_features``, ``n_factors``).\n            rotation_mtx (:obj:`numpy.ndarray`): The rotation matrix. Shape (``n_factors``, ``n_factors``).\n            phi (:obj:`numpy.ndarray`): The factor correlations matrix. This only exists if the\n                rotation is oblique. Shape (``n_factors``, ``n_factors``).\n        \"\"\"\n        if method == \"oblimin\":\n            objective = self._oblimin_obj\n        elif method == \"quartimin\":\n            objective = self._quartimin_obj\n        elif method == \"geomin_obl\":\n            objective = self._geomin_obj\n\n        # initialize the rotation matrix\n        _, n_cols = loadings.shape\n        rotation_matrix = np.eye(n_cols)\n\n        # default alpha level\n        alpha = 1\n        rotation_matrix_inv = np.linalg.inv(rotation_matrix)\n        new_loadings = np.dot(loadings, rotation_matrix_inv.T)\n\n        obj = objective(new_loadings)\n        gradient = -np.dot(new_loadings.T, np.dot(obj[\"grad\"], rotation_matrix_inv)).T\n        criterion = obj[\"criterion\"]\n\n        obj_t = objective(new_loadings)\n\n        # main iteration loop, up to `max_iter`, calculate the gradient\n        for _ in range(0, self.max_iter + 1):\n            gradient_new = gradient - np.dot(\n                rotation_matrix,\n                np.diag(np.dot(np.ones(gradient.shape[0]), rotation_matrix * gradient)),\n            )\n            s = np.sqrt(np.sum(np.diag(np.dot(gradient_new.T, gradient_new))))\n\n            if s &lt; self.tol:\n                break\n\n            alpha = 2 * alpha\n\n            # calculate the Hessian of the objective function\n            for _ in range(0, 11):\n                X = rotation_matrix - alpha * gradient_new\n\n                v = 1 / np.sqrt(np.dot(np.ones(X.shape[0]), X**2))\n                new_rotation_matrix = np.dot(X, np.diag(v))\n                new_loadings = np.dot(loadings, np.linalg.inv(new_rotation_matrix).T)\n\n                obj_t = objective(new_loadings)\n                improvement = criterion - obj_t[\"criterion\"]\n\n                if improvement &gt; 0.5 * s**2 * alpha:\n                    break\n\n                alpha = alpha / 2\n\n            rotation_matrix = new_rotation_matrix\n            criterion = obj_t[\"criterion\"]\n            gradient = -np.dot(\n                np.dot(new_loadings.T, obj_t[\"grad\"]),\n                np.linalg.inv(new_rotation_matrix),\n            ).T\n\n        # calculate phi\n        phi = np.dot(rotation_matrix.T, rotation_matrix)\n\n        # convert loadings matrix to data frame\n        loadings = new_loadings.copy()\n        return loadings, rotation_matrix, phi\n\n    def _orthogonal(self, loadings, method):\n        \"\"\"\n        Perform orthogonal rotations, except 'varimax'.\n\n        A generic function for performing all orthogonal rotations, except for\n        varimax, which is implemented separately.\n\n        Args:\n            loadings (:obj:`numpy.ndarray`): The loading matrix\n            method (str): The orthogonal rotation method to use.\n\n        Returns:\n            loadings (:obj:`numpy.ndarray`): The loadings matrix\n            rotation_mtx (:obj:`numpy.ndarray`): The rotation matrix. Shape (``n_factors``, ``n_factors``).\n        \"\"\"\n        if method == \"oblimax\":\n            objective = self._oblimax_obj\n        elif method == \"quartimax\":\n            objective = self._quartimax_obj\n        elif method == \"equamax\":\n            objective = self._equamax_obj\n        elif method == \"geomin_ort\":\n            objective = self._geomin_obj\n\n        arr = loadings.copy()\n\n        # initialize the rotation matrix\n        _, n_cols = arr.shape\n        rotation_matrix = np.eye(n_cols)\n\n        # default alpha level\n        alpha = 1\n        new_loadings = np.dot(arr, rotation_matrix)\n\n        obj = objective(new_loadings)\n        gradient = np.dot(arr.T, obj[\"grad\"])\n        criterion = obj[\"criterion\"]\n\n        obj_t = objective(new_loadings)\n\n        # main iteration loop, up to `max_iter`, calculate the gradient\n        for _ in range(0, self.max_iter + 1):\n            M = np.dot(rotation_matrix.T, gradient)\n            S = (M + M.T) / 2\n            gradient_new = gradient - np.dot(rotation_matrix, S)\n            s = np.sqrt(np.sum(np.diag(np.dot(gradient_new.T, gradient_new))))\n\n            if s &lt; self.tol:\n                break\n\n            alpha = 2 * alpha\n\n            # calculate the Hessian of the objective function\n            for _ in range(0, 11):\n                X = rotation_matrix - alpha * gradient_new\n                U, _, V = np.linalg.svd(X)\n                new_rotation_matrix = np.dot(U, V)\n                new_loadings = np.dot(arr, new_rotation_matrix)\n\n                obj_t = objective(new_loadings)\n\n                if obj_t[\"criterion\"] &lt; (criterion - 0.5 * s**2 * alpha):\n                    break\n\n                alpha = alpha / 2\n\n            rotation_matrix = new_rotation_matrix\n            criterion = obj_t[\"criterion\"]\n            gradient = np.dot(arr.T, obj_t[\"grad\"])\n\n        # convert loadings matrix to data frame\n        loadings = new_loadings.copy()\n        return loadings, rotation_matrix\n\n    def _varimax(self, loadings):\n        \"\"\"\n        Perform varimax (orthogonal) rotation, with optional Kaiser normalization.\n\n        Args:\n            loadings (array-like): The loading matrix.\n\n        Returns:\n            loadings (:obj:`numpy.ndarray`): The loadings matrix. Shape (``n_features``, ``n_factors``).\n            rotation_mtx (:obj:`numpy.ndarray`): The rotation matrix. Shape (``n_factors``, ``n_factors``).\n        \"\"\"\n        X = loadings.copy()\n        n_rows, n_cols = X.shape\n        if n_cols &lt; 2:\n            return X\n\n        # normalize the loadings matrix\n        # using sqrt of the sum of squares (Kaiser)\n        if self.normalize:\n            normalized_mtx = np.apply_along_axis(\n                lambda x: np.sqrt(np.sum(x**2)), 1, X.copy()\n            )\n            X = (X.T / normalized_mtx).T\n\n        # initialize the rotation matrix\n        # to N x N identity matrix\n        rotation_mtx = np.eye(n_cols)\n\n        d = 0\n        for _ in range(self.max_iter):\n            old_d = d\n\n            # take inner product of loading matrix\n            # and rotation matrix\n            basis = np.dot(X, rotation_mtx)\n\n            # transform data for singular value decomposition using updated formula :\n            # B &lt;- t(x) %*% (z^3 - z %*% diag(drop(rep(1, p) %*% z^2))/p)\n            diagonal = np.diag(np.squeeze(np.repeat(1, n_rows).dot(basis**2)))\n            transformed = X.T.dot(basis**3 - basis.dot(diagonal) / n_rows)\n\n            # perform SVD on\n            # the transformed matrix\n            U, S, V = np.linalg.svd(transformed)\n\n            # take inner product of U and V, and sum of S\n            rotation_mtx = np.dot(U, V)\n            d = np.sum(S)\n\n            # check convergence\n            if d &lt; old_d * (1 + self.tol):\n                break\n\n        # take inner product of loading matrix\n        # and rotation matrix\n        X = np.dot(X, rotation_mtx)\n\n        # de-normalize the data\n        if self.normalize:\n            X = X.T * normalized_mtx\n        else:\n            X = X.T\n\n        # convert loadings matrix to data frame\n        loadings = X.T.copy()\n        return loadings, rotation_mtx\n\n    def _promax(self, loadings):\n        \"\"\"\n        Perform promax (oblique) rotation, with optional Kaiser normalization.\n\n        Args:\n            loadings (array-like): The loading matrix\n\n        Returns:\n            loadings (:obj:`numpy.ndarray`): The loadings matrix. Shape (``n_features``, ``n_factors``).\n            rotation_mtx (:obj:`numpy.ndarray`): The rotation matrix. Shape (``n_factors``, ``n_factors``).\n            phi (:obj:`numpy.ndarray` or None): The factor correlations matrix. This only exists if the rotation\n                is oblique. Shape (``n_factors``, ``n_factors``).\n        \"\"\"\n        X = loadings.copy()\n        _, n_cols = X.shape\n        if n_cols &lt; 2:\n            return X\n\n        if self.normalize:\n            # pre-normalization is done in R's\n            # `kaiser()` function when rotate='Promax'.\n            array = X.copy()\n            h2 = np.diag(np.dot(array, array.T))\n            h2 = np.reshape(h2, (h2.shape[0], 1))\n            weights = array / np.sqrt(h2)\n\n        else:\n            weights = X.copy()\n\n        # first get varimax rotation\n        X, rotation_mtx = self._varimax(weights)\n        Y = X * np.abs(X) ** (self.power - 1)\n\n        # fit linear regression model\n        coef = np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, Y))\n\n        # calculate diagonal of inverse square\n        try:\n            diag_inv = np.diag(sp.linalg.inv(np.dot(coef.T, coef)))\n        except np.linalg.LinAlgError:\n            diag_inv = np.diag(sp.linalg.pinv(np.dot(coef.T, coef)))\n\n        # transform and calculate inner products\n        coef = np.dot(coef, np.diag(np.sqrt(diag_inv)))\n        z = np.dot(X, coef)\n\n        if self.normalize:\n            # post-normalization is done in R's\n            # `kaiser()` function when rotate='Promax'\n            z = z * np.sqrt(h2)\n\n        rotation_mtx = np.dot(rotation_mtx, coef)\n\n        coef_inv = np.linalg.inv(coef)\n        phi = np.dot(coef_inv, coef_inv.T)\n\n        # convert loadings matrix to data frame\n        loadings = z.copy()\n        return loadings, rotation_mtx, phi\n\n    def fit(self, X, y=None) -&gt; \"Rotator\":\n        \"\"\"\n        Compute the factor rotation.\n\n        Args:\n            X (array-like): The factor loading matrix. Shape (n_features, n_factors).\n            y (ignored): Ignored.\n\n        Returns:\n            self: The rotator object.\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer, Rotator\n            &gt;&gt;&gt; df_features = pd.read_csv('test02.csv')\n            &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n            &gt;&gt;&gt; fa.fit(df_features)\n            &gt;&gt;&gt; rotator = Rotator()\n            &gt;&gt;&gt; rotator.fit(fa.loadings_)\n        \"\"\"\n        self.fit_transform(X)\n        return self\n\n    def fit_transform(self, X, y=None):\n        \"\"\"\n        Compute the factor rotation, and return the new loading matrix.\n\n        Args:\n            X (array-like): The factor loading matrix. Shape (n_features, n_factors).\n            y (ignored): Ignored.\n\n        Returns:\n            loadings_ (numpy.ndarray): The loadings matrix. Shape (n_features, n_factors).\n\n        Raises:\n            ValueError: If `method` is not in the list of acceptable methods.\n\n        Examples:\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer, Rotator\n            &gt;&gt;&gt; df_features = pd.read_csv('test02.csv')\n            &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n            &gt;&gt;&gt; fa.fit(df_features)\n            &gt;&gt;&gt; rotator = Rotator()\n            &gt;&gt;&gt; rotator.fit_transform(fa.loadings_)\n            array([[-0.07693215,  0.04499572,  0.76211208],\n                   [ 0.01842035,  0.05757874,  0.01297908],\n                   [ 0.06067925,  0.70692662, -0.03311798],\n                   [ 0.11314343,  0.84525117, -0.03407129],\n                   [ 0.15307233,  0.5553474 , -0.00121802],\n                   [ 0.77450832,  0.1474666 ,  0.20118338],\n                   [ 0.7063001 ,  0.17229555, -0.30093981],\n                   [ 0.83990851,  0.15058874, -0.06182469],\n                   [ 0.76620579,  0.1045194 , -0.22649615],\n                   [ 0.81372945,  0.20915845,  0.07479506]])\n        \"\"\"\n        # default phi to None\n        # it will only be calculated\n        # for oblique rotations\n        phi = None\n        method = self.method.lower()\n        if method == \"varimax\":\n            new_loadings, new_rotation_mtx = self._varimax(X)\n\n        elif method == \"promax\":\n            new_loadings, new_rotation_mtx, phi = self._promax(X)\n\n        elif method in OBLIQUE_ROTATIONS:\n            new_loadings, new_rotation_mtx, phi = self._oblique(X, method)\n\n        elif method in ORTHOGONAL_ROTATIONS:\n            new_loadings, new_rotation_mtx = self._orthogonal(X, method)\n\n        else:\n            raise ValueError(\n                \"The value for `method` must be one of the \"\n                \"following: {}.\".format(\", \".join(POSSIBLE_ROTATIONS))\n            )\n\n        self.loadings_, self.rotation_, self.phi_ = (\n            new_loadings,\n            new_rotation_mtx,\n            phi,\n        )\n        return self.loadings_\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_rotator/#spotoptim.factor_analyzer.factor_analyzer_rotator.Rotator.__init__","title":"<code>__init__(method='varimax', normalize=True, power=4, kappa=0, gamma=0, delta=0.01, max_iter=500, tol=1e-05)</code>","text":"<p>Initialize the rotator class.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_rotator.py</code> <pre><code>def __init__(\n    self,\n    method=\"varimax\",\n    normalize=True,\n    power=4,\n    kappa=0,\n    gamma=0,\n    delta=0.01,\n    max_iter=500,\n    tol=1e-5,\n):\n    \"\"\"Initialize the rotator class.\"\"\"\n    self.method = method\n    self.normalize = normalize\n    self.power = power\n    self.kappa = kappa\n    self.gamma = gamma\n    self.delta = delta\n    self.max_iter = max_iter\n    self.tol = tol\n\n    self.loadings_ = None\n    self.rotation_ = None\n    self.phi_ = None\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_rotator/#spotoptim.factor_analyzer.factor_analyzer_rotator.Rotator.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Compute the factor rotation.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The factor loading matrix. Shape (n_features, n_factors).</p> required <code>y</code> <code>ignored</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>Rotator</code> <p>The rotator object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from factor_analyzer import FactorAnalyzer, Rotator\n&gt;&gt;&gt; df_features = pd.read_csv('test02.csv')\n&gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n&gt;&gt;&gt; fa.fit(df_features)\n&gt;&gt;&gt; rotator = Rotator()\n&gt;&gt;&gt; rotator.fit(fa.loadings_)\n</code></pre> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_rotator.py</code> <pre><code>def fit(self, X, y=None) -&gt; \"Rotator\":\n    \"\"\"\n    Compute the factor rotation.\n\n    Args:\n        X (array-like): The factor loading matrix. Shape (n_features, n_factors).\n        y (ignored): Ignored.\n\n    Returns:\n        self: The rotator object.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer, Rotator\n        &gt;&gt;&gt; df_features = pd.read_csv('test02.csv')\n        &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n        &gt;&gt;&gt; fa.fit(df_features)\n        &gt;&gt;&gt; rotator = Rotator()\n        &gt;&gt;&gt; rotator.fit(fa.loadings_)\n    \"\"\"\n    self.fit_transform(X)\n    return self\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_rotator/#spotoptim.factor_analyzer.factor_analyzer_rotator.Rotator.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Compute the factor rotation, and return the new loading matrix.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The factor loading matrix. Shape (n_features, n_factors).</p> required <code>y</code> <code>ignored</code> <p>Ignored.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>loadings_</code> <code>ndarray</code> <p>The loadings matrix. Shape (n_features, n_factors).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>method</code> is not in the list of acceptable methods.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from factor_analyzer import FactorAnalyzer, Rotator\n&gt;&gt;&gt; df_features = pd.read_csv('test02.csv')\n&gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n&gt;&gt;&gt; fa.fit(df_features)\n&gt;&gt;&gt; rotator = Rotator()\n&gt;&gt;&gt; rotator.fit_transform(fa.loadings_)\narray([[-0.07693215,  0.04499572,  0.76211208],\n       [ 0.01842035,  0.05757874,  0.01297908],\n       [ 0.06067925,  0.70692662, -0.03311798],\n       [ 0.11314343,  0.84525117, -0.03407129],\n       [ 0.15307233,  0.5553474 , -0.00121802],\n       [ 0.77450832,  0.1474666 ,  0.20118338],\n       [ 0.7063001 ,  0.17229555, -0.30093981],\n       [ 0.83990851,  0.15058874, -0.06182469],\n       [ 0.76620579,  0.1045194 , -0.22649615],\n       [ 0.81372945,  0.20915845,  0.07479506]])\n</code></pre> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_rotator.py</code> <pre><code>def fit_transform(self, X, y=None):\n    \"\"\"\n    Compute the factor rotation, and return the new loading matrix.\n\n    Args:\n        X (array-like): The factor loading matrix. Shape (n_features, n_factors).\n        y (ignored): Ignored.\n\n    Returns:\n        loadings_ (numpy.ndarray): The loadings matrix. Shape (n_features, n_factors).\n\n    Raises:\n        ValueError: If `method` is not in the list of acceptable methods.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from factor_analyzer import FactorAnalyzer, Rotator\n        &gt;&gt;&gt; df_features = pd.read_csv('test02.csv')\n        &gt;&gt;&gt; fa = FactorAnalyzer(rotation=None)\n        &gt;&gt;&gt; fa.fit(df_features)\n        &gt;&gt;&gt; rotator = Rotator()\n        &gt;&gt;&gt; rotator.fit_transform(fa.loadings_)\n        array([[-0.07693215,  0.04499572,  0.76211208],\n               [ 0.01842035,  0.05757874,  0.01297908],\n               [ 0.06067925,  0.70692662, -0.03311798],\n               [ 0.11314343,  0.84525117, -0.03407129],\n               [ 0.15307233,  0.5553474 , -0.00121802],\n               [ 0.77450832,  0.1474666 ,  0.20118338],\n               [ 0.7063001 ,  0.17229555, -0.30093981],\n               [ 0.83990851,  0.15058874, -0.06182469],\n               [ 0.76620579,  0.1045194 , -0.22649615],\n               [ 0.81372945,  0.20915845,  0.07479506]])\n    \"\"\"\n    # default phi to None\n    # it will only be calculated\n    # for oblique rotations\n    phi = None\n    method = self.method.lower()\n    if method == \"varimax\":\n        new_loadings, new_rotation_mtx = self._varimax(X)\n\n    elif method == \"promax\":\n        new_loadings, new_rotation_mtx, phi = self._promax(X)\n\n    elif method in OBLIQUE_ROTATIONS:\n        new_loadings, new_rotation_mtx, phi = self._oblique(X, method)\n\n    elif method in ORTHOGONAL_ROTATIONS:\n        new_loadings, new_rotation_mtx = self._orthogonal(X, method)\n\n    else:\n        raise ValueError(\n            \"The value for `method` must be one of the \"\n            \"following: {}.\".format(\", \".join(POSSIBLE_ROTATIONS))\n        )\n\n    self.loadings_, self.rotation_, self.phi_ = (\n        new_loadings,\n        new_rotation_mtx,\n        phi,\n    )\n    return self.loadings_\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/","title":"factor_analyzer_utils","text":"<p>Utility functions, used primarily by the confirmatory factor analysis module.</p> <p>Confirmatory factor analysis using machine learning methods. Re-implementation of the factor-analyzer package.</p> <p>See https://factor-analyzer.readthedocs.io/en/latest/introduction.html for more details.</p> <p>Authors of the original implementation: * Jeremy Biggs (jeremy.m.biggs@gmail.com) * Nitin Madnani (nmadnani@ets.org) Organization: Educational Testing Service Date: 2022-09-05</p> <p>This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.</p> <p>This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.</p> <p>You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.</p>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.apply_impute_nan","title":"<code>apply_impute_nan(x, how='mean')</code>","text":"<p>Apply a function to impute np.nan values with the mean or the median.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>The 1-D array to impute.</p> required <code>how</code> <code>str</code> <p>Whether to impute the \u2018mean\u2019 or \u2018median\u2019. Defaults to \u2018mean\u2019.</p> <code>'mean'</code> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>The array, with the missing values imputed.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def apply_impute_nan(x, how=\"mean\"):\n    \"\"\"\n    Apply a function to impute np.nan values with the mean or the median.\n\n    Args:\n        x (array-like): The 1-D array to impute.\n        how (str, optional): Whether to impute the 'mean' or 'median'.\n            Defaults to 'mean'.\n\n    Returns:\n        x (numpy.ndarray): The array, with the missing values imputed.\n    \"\"\"\n    if how == \"mean\":\n        x[np.isnan(x)] = np.nanmean(x)\n    elif how == \"median\":\n        x[np.isnan(x)] = np.nanmedian(x)\n    return x\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.commutation_matrix","title":"<code>commutation_matrix(p, q)</code>","text":"<p>Calculate the commutation matrix.</p> <p>This matrix transforms the vectorized form of the matrix into the vectorized form of its transpose.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>int</code> <p>The number of rows.</p> required <code>q</code> <code>int</code> <p>The number of columns.</p> required <p>Returns:</p> Name Type Description <code>commutation_matrix</code> <code>:obj:`numpy.ndarray`</code> <p>The commutation matrix</p> References <p>https://en.wikipedia.org/wiki/Commutation_matrix</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def commutation_matrix(p, q):\n    \"\"\"\n    Calculate the commutation matrix.\n\n    This matrix transforms the vectorized form of the matrix into the\n    vectorized form of its transpose.\n\n    Args:\n        p (int): The number of rows.\n        q (int): The number of columns.\n\n    Returns:\n        commutation_matrix (:obj:`numpy.ndarray`): The commutation matrix\n\n    References:\n        https://en.wikipedia.org/wiki/Commutation_matrix\n    \"\"\"\n    identity = np.eye(p * q)\n    indices = np.arange(p * q).reshape((p, q), order=\"F\")\n    return identity.take(indices.ravel(), axis=0)\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.corr","title":"<code>corr(x)</code>","text":"<p>Calculate the correlation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>A 1-D or 2-D array containing multiple variables and observations. Each column of x represents a variable, and each row a single observation of all those variables.</p> required <p>Returns:</p> Name Type Description <code>r</code> <code>ndarray</code> <p>The correlation matrix of the variables.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def corr(x):\n    \"\"\"\n    Calculate the correlation matrix.\n\n    Args:\n        x (array-like): A 1-D or 2-D array containing multiple variables\n            and observations. Each column of x represents a variable,\n            and each row a single observation of all those variables.\n\n    Returns:\n        r (numpy.ndarray): The correlation matrix of the variables.\n    \"\"\"\n    x = (x - np.mean(x, axis=0)) / np.std(x, axis=0, ddof=0)\n    r = cov(x)\n    return r\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.cov","title":"<code>cov(x, ddof=0)</code>","text":"<p>Calculate the covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>A 1-D or 2-D array containing multiple variables and observations. Each column of x represents a variable, and each row a single observation of all those variables.</p> required <code>ddof</code> <code>int</code> <p>Means Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>r</code> <code>ndarray</code> <p>The covariance matrix of the variables.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def cov(x, ddof=0):\n    \"\"\"\n    Calculate the covariance matrix.\n\n    Args:\n        x (array-like): A 1-D or 2-D array containing multiple variables\n            and observations. Each column of x represents a variable,\n            and each row a single observation of all those variables.\n        ddof (int, optional): Means Delta Degrees of Freedom. The divisor used in calculations\n            is N - ddof, where N represents the number of elements.\n            Defaults to 0.\n\n    Returns:\n        r (numpy.ndarray): The covariance matrix of the variables.\n    \"\"\"\n    r = np.cov(x, rowvar=False, ddof=ddof)\n    return r\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.covariance_to_correlation","title":"<code>covariance_to_correlation(m)</code>","text":"<p>Compute cross-correlations from the given covariance matrix.</p> <p>This is a port of R cov2cor() function.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>array - like</code> <p>The covariance matrix.</p> required <p>Returns:</p> Name Type Description <code>retval</code> <code>ndarray</code> <p>The cross-correlation matrix.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input matrix is not square.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def covariance_to_correlation(m):\n    \"\"\"\n    Compute cross-correlations from the given covariance matrix.\n\n    This is a port of R cov2cor() function.\n\n    Args:\n        m (array-like): The covariance matrix.\n\n    Returns:\n        retval (numpy.ndarray): The cross-correlation matrix.\n\n    Raises:\n        ValueError: If the input matrix is not square.\n    \"\"\"\n    # make sure the matrix is square\n    numrows, numcols = m.shape\n    if not numrows == numcols:\n        raise ValueError(\"Input matrix must be square\")\n\n    Is = np.sqrt(1 / np.diag(m))\n    retval = Is * m * np.repeat(Is, numrows).reshape(numrows, numrows)\n    np.fill_diagonal(retval, 1.0)\n    return retval\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.duplication_matrix","title":"<code>duplication_matrix(n=1)</code>","text":"<p>Calculate the duplication matrix.</p> <p>A function to create the duplication matrix (Dn), which is the unique n2 \u00d7 n(n+1)/2 matrix which, for any n \u00d7 n symmetric matrix A, transforms vech(A) into vec(A), as in Dn vech(A) = vec(A).</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The dimension of the n x n symmetric matrix. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>duplication_matrix</code> <code>:obj:`numpy.ndarray`</code> <p>The duplication matrix.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>n</code> is not a positive integer greater than 1.</p> References <p>https://en.wikipedia.org/wiki/Duplication_and_elimination_matrices</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def duplication_matrix(n=1):\n    \"\"\"\n    Calculate the duplication matrix.\n\n    A function to create the duplication matrix (Dn), which is\n    the unique n2 \u00d7 n(n+1)/2 matrix which, for any n \u00d7 n symmetric\n    matrix A, transforms vech(A) into vec(A), as in Dn vech(A) = vec(A).\n\n    Args:\n        n (int, optional): The dimension of the n x n symmetric matrix.\n            Defaults to 1.\n\n    Returns:\n        duplication_matrix (:obj:`numpy.ndarray`): The duplication matrix.\n\n    Raises:\n        ValueError: If ``n`` is not a positive integer greater than 1.\n\n    References:\n        https://en.wikipedia.org/wiki/Duplication_and_elimination_matrices\n    \"\"\"\n    if n &lt; 1:\n        raise ValueError(\n            \"The argument `n` must be a \" \"positive integer greater than 1.\"\n        )\n\n    dup = np.zeros((int(n * n), int(n * (n + 1) / 2)))\n    count = 0\n    for j in range(n):\n        dup[j * n + j, count + j] = 1\n        if j &lt; n - 1:\n            for i in range(j + 1, n):\n                dup[j * n + i, count + i] = 1\n                dup[i * n + j, count + i] = 1\n        count += n - j - 1\n    return dup\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.duplication_matrix_pre_post","title":"<code>duplication_matrix_pre_post(x)</code>","text":"<p>Transform given input symmetric matrix using pre-post duplication.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>The input matrix.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>:obj:`numpy.ndarray`</code> <p>The transformed matrix.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>x</code> is not symmetric.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def duplication_matrix_pre_post(x):\n    \"\"\"\n    Transform given input symmetric matrix using pre-post duplication.\n\n    Args:\n        x (array-like): The input matrix.\n\n    Returns:\n        out (:obj:`numpy.ndarray`): The transformed matrix.\n\n    Raises:\n        AssertionError: If ``x`` is not symmetric.\n    \"\"\"\n    assert x.shape[0] == x.shape[1]\n\n    n2 = x.shape[1]\n    n = int(np.sqrt(n2))\n\n    idx1 = get_symmetric_lower_idxs(n)\n    idx2 = get_symmetric_upper_idxs(n)\n\n    out = x[idx1, :] + x[idx2, :]\n    u = np.where([i in idx2 for i in idx1])[0]\n    out[u, :] = out[u, :] / 2.0\n    out = out[:, idx1] + out[:, idx2]\n    out[:, u] = out[:, u] / 2.0\n    return out\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.fill_lower_diag","title":"<code>fill_lower_diag(x)</code>","text":"<p>Fill the lower diagonal of a square matrix, given a 1-D input array.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>The flattened input matrix that will be used to fill the lower diagonal of the square matrix.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>:obj:`numpy.ndarray`</code> <p>The output square matrix, with the lower diagonal filled by x.</p> References <p>[1] https://stackoverflow.com/questions/51439271/     convert-1d-array-to-lower-triangular-matrix</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def fill_lower_diag(x):\n    \"\"\"\n    Fill the lower diagonal of a square matrix, given a 1-D input array.\n\n    Args:\n        x (array-like): The flattened input matrix that will be used to fill the lower\n            diagonal of the square matrix.\n\n    Returns:\n        out (:obj:`numpy.ndarray`): The output square matrix, with the lower diagonal filled by x.\n\n    References:\n        [1] https://stackoverflow.com/questions/51439271/\n            convert-1d-array-to-lower-triangular-matrix\n    \"\"\"\n    x = np.array(x)\n    x = x if len(x.shape) == 1 else np.squeeze(x, axis=1)\n    n = int(np.sqrt(len(x) * 2)) + 1\n    out = np.zeros((n, n), dtype=float)\n    out[np.tri(n, dtype=bool, k=-1)] = x\n    return out\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.get_first_idxs_from_values","title":"<code>get_first_idxs_from_values(x, eq=1, use_columns=True)</code>","text":"<p>Get the indexes  for a given value.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>The input matrix.</p> required <code>eq</code> <code>str or int</code> <p>The given value to find. Defaults to 1.</p> <code>1</code> <code>use_columns</code> <code>bool</code> <p>Whether to get the first indexes using the columns. If <code>False</code>, then use the rows instead. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[List[int], List[int]]</code> <ul> <li>row_idx (list): A list of row indexes.</li> <li>col_idx (list): A list of column indexes.</li> </ul> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def get_first_idxs_from_values(\n    x, eq=1, use_columns=True\n) -&gt; Tuple[List[int], List[int]]:\n    \"\"\"\n    Get the indexes  for a given value.\n\n    Args:\n        x (array-like): The input matrix.\n        eq (str or int, optional): The given value to find.\n            Defaults to 1.\n        use_columns (bool, optional): Whether to get the first indexes using the columns.\n            If ``False``, then use the rows instead.\n            Defaults to ``True``.\n\n    Returns:\n        tuple:\n            - row_idx (list): A list of row indexes.\n            - col_idx (list): A list of column indexes.\n    \"\"\"\n    x = np.array(x)\n    if use_columns:\n        n = x.shape[1]\n        row_idx = [np.where(x[:, i] == eq)[0][0] for i in range(n)]\n        col_idx = list(range(n))\n    else:\n        n = x.shape[0]\n        col_idx = [np.where(x[i, :] == eq)[0][0] for i in range(n)]\n        row_idx = list(range(n))\n    return row_idx, col_idx\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.get_free_parameter_idxs","title":"<code>get_free_parameter_idxs(x, eq=1)</code>","text":"<p>Get the free parameter indices from the flattened matrix.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>The input matrix.</p> required <code>eq</code> <code>str or int</code> <p>The value that free parameters should be equal to. <code>np.nan</code> fields will be populated with this value. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>idx</code> <code>:obj:`numpy.ndarray`</code> <p>The free parameter indexes.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def get_free_parameter_idxs(x, eq=1):\n    \"\"\"\n    Get the free parameter indices from the flattened matrix.\n\n    Args:\n        x (array-like): The input matrix.\n        eq (str or int, optional): The value that free parameters should be equal to. ``np.nan`` fields\n            will be populated with this value.\n            Defaults to 1.\n\n    Returns:\n        idx (:obj:`numpy.ndarray`): The free parameter indexes.\n    \"\"\"\n    x[np.isnan(x)] = eq\n    x = x.flatten(order=\"F\")\n    return np.where(x == eq)[0]\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.get_symmetric_lower_idxs","title":"<code>get_symmetric_lower_idxs(n=1, diag=True)</code>","text":"<p>Get the indices for the lower triangle of a symmetric matrix.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The dimension of the n x n symmetric matrix. Defaults to 1.</p> <code>1</code> <code>diag</code> <code>bool</code> <p>Whether to include the diagonal.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>indices</code> <code>:obj:`numpy.ndarray`</code> <p>The indices for the lower triangle.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def get_symmetric_lower_idxs(n=1, diag=True):\n    \"\"\"\n    Get the indices for the lower triangle of a symmetric matrix.\n\n    Args:\n        n (int, optional): The dimension of the n x n symmetric matrix.\n            Defaults to 1.\n        diag (bool, optional): Whether to include the diagonal.\n\n    Returns:\n        indices (:obj:`numpy.ndarray`): The indices for the lower triangle.\n    \"\"\"\n    rows = np.repeat(np.arange(n), n).reshape(n, n)\n    cols = rows.T\n    if diag:\n        return np.where((rows &gt;= cols).T.flatten())[0]\n    return np.where((cols &gt; rows).T.flatten())[0]\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.get_symmetric_upper_idxs","title":"<code>get_symmetric_upper_idxs(n=1, diag=True)</code>","text":"<p>Get the indices for the upper triangle of a symmetric matrix.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The dimension of the n x n symmetric matrix. Defaults to 1.</p> <code>1</code> <code>diag</code> <code>bool</code> <p>Whether to include the diagonal.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>indices</code> <code>:obj:`numpy.ndarray`</code> <p>The indices for the upper triangle.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def get_symmetric_upper_idxs(n=1, diag=True):\n    \"\"\"\n    Get the indices for the upper triangle of a symmetric matrix.\n\n    Args:\n        n (int, optional): The dimension of the n x n symmetric matrix.\n            Defaults to 1.\n        diag (bool, optional): Whether to include the diagonal.\n\n    Returns:\n        indices (:obj:`numpy.ndarray`): The indices for the upper triangle.\n    \"\"\"\n    rows = np.repeat(np.arange(n), n).reshape(n, n)\n    cols = rows.T\n    temp = np.arange(n * n).reshape(n, n)\n    if diag:\n        return temp.T[(rows &gt;= cols).T]\n    return temp.T[(cols &gt; rows).T]\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.impute_values","title":"<code>impute_values(x, how='mean')</code>","text":"<p>Impute np.nan values with the mean or median, or drop the containing rows.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>An array to impute.</p> required <code>how</code> <code>str</code> <p>Whether to impute the \u2018mean\u2019 or \u2018median\u2019. Defaults to \u2018mean\u2019.</p> <code>'mean'</code> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>The array, with the missing values imputed or with rows dropped.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def impute_values(x, how=\"mean\"):\n    \"\"\"\n    Impute np.nan values with the mean or median, or drop the containing rows.\n\n    Args:\n        x (array-like): An array to impute.\n        how (str, optional): Whether to impute the 'mean' or 'median'.\n            Defaults to 'mean'.\n\n    Returns:\n        x (numpy.ndarray): The array, with the missing values imputed or with rows dropped.\n    \"\"\"\n    # impute mean or median, if `how` is set to 'mean' or 'median'\n    if how in [\"mean\", \"median\"]:\n        x = np.apply_along_axis(apply_impute_nan, 0, x, how=how)\n    # drop missing if `how` is set to 'drop'\n    elif how == \"drop\":\n        x = x[~np.isnan(x).any(1), :].copy()\n    return x\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.inv_chol","title":"<code>inv_chol(x, logdet=False)</code>","text":"<p>Calculate matrix inverse using Cholesky decomposition.</p> <p>Optionally, calculate the log determinant of the Cholesky.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>The matrix to invert.</p> required <code>logdet</code> <code>bool</code> <p>Whether to calculate the log determinant, instead of the inverse. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[ndarray, Optional[float]]</code> <ul> <li>chol_inv (array-like): The inverted matrix.</li> <li>chol_logdet (array-like or None): The log determinant, if logdet was True, otherwise None.</li> </ul> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def inv_chol(x, logdet=False) -&gt; Tuple[np.ndarray, Optional[float]]:\n    \"\"\"\n    Calculate matrix inverse using Cholesky decomposition.\n\n    Optionally, calculate the log determinant of the Cholesky.\n\n    Args:\n        x (array-like): The matrix to invert.\n        logdet (bool, optional): Whether to calculate the log determinant, instead of the inverse.\n            Defaults to False.\n\n    Returns:\n        tuple:\n            - chol_inv (array-like): The inverted matrix.\n            - chol_logdet (array-like or None): The log determinant, if logdet was True, otherwise None.\n    \"\"\"\n    chol = cholesky(x, lower=True)\n\n    chol_inv = np.linalg.inv(chol)\n    chol_inv = np.dot(chol_inv.T, chol_inv)\n    chol_logdet = None\n\n    if logdet:\n        chol_diag = np.diag(chol)\n        chol_logdet = np.sum(np.log(chol_diag * chol_diag))\n\n    return chol_inv, chol_logdet\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.merge_variance_covariance","title":"<code>merge_variance_covariance(variances, covariances=None)</code>","text":"<p>Merge variances and covariances into a single variance-covariance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>variances</code> <code>array - like</code> <p>The variances that will be used to fill the diagonal of the square matrix.</p> required <code>covariances</code> <code>array - like or None</code> <p>The flattened input matrix that will be used to fill the lower and upper diagonal of the square matrix. If None, then only the variances will be used. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>variance_covariance</code> <code>:obj:`numpy.ndarray`</code> <p>The variance-covariance matrix.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def merge_variance_covariance(variances, covariances=None):\n    \"\"\"\n    Merge variances and covariances into a single variance-covariance matrix.\n\n    Args:\n        variances (array-like): The variances that will be used to fill the diagonal of the\n            square matrix.\n        covariances (array-like or None, optional): The flattened input matrix that will be used to fill the lower and\n            upper diagonal of the square matrix. If None, then only the variances\n            will be used.\n            Defaults to ``None``.\n\n    Returns:\n        variance_covariance (:obj:`numpy.ndarray`): The variance-covariance matrix.\n    \"\"\"\n    variances = (\n        variances if len(variances.shape) == 1 else np.squeeze(variances, axis=1)\n    )\n    if covariances is None:\n        variance_covariance = np.zeros((variances.shape[0], variances.shape[0]))\n    else:\n        variance_covariance = fill_lower_diag(covariances)\n        variance_covariance += variance_covariance.T\n    np.fill_diagonal(variance_covariance, variances)\n    return variance_covariance\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.partial_correlations","title":"<code>partial_correlations(x)</code>","text":"<p>Compute partial correlations between variable pairs.</p> <p>This is a python port of the <code>pcor()</code> function implemented in the <code>ppcor</code> R package, which computes partial correlations for each pair of variables in the given array, excluding all other variables.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>An array containing the feature values.</p> required <p>Returns:</p> Name Type Description <code>pcor</code> <code>:obj:`numpy.ndarray`</code> <p>An array containing the partial correlations of of each pair of variables in the given array, excluding all other variables.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def partial_correlations(x):\n    \"\"\"\n    Compute partial correlations between variable pairs.\n\n    This is a python port of the ``pcor()`` function implemented in\n    the ``ppcor`` R package, which computes partial correlations\n    for each pair of variables in the given array, excluding all\n    other variables.\n\n    Args:\n        x (array-like): An array containing the feature values.\n\n    Returns:\n        pcor (:obj:`numpy.ndarray`): An array containing the partial correlations of of each\n            pair of variables in the given array, excluding all other\n            variables.\n    \"\"\"\n    numrows, numcols = x.shape\n    x_cov = cov(x, ddof=1)\n    # create empty array for when we cannot compute the\n    # matrix inversion\n    empty_array = np.empty((numcols, numcols))\n    empty_array[:] = np.nan\n    if numcols &gt; numrows:\n        icvx = empty_array\n    else:\n        # if the determinant is less than the lowest representable\n        # 32 bit integer, then we use the pseudo-inverse;\n        # otherwise, use the inverse; if a linear algebra error\n        # occurs, then we just set the matrix to empty\n        try:\n            assert np.linalg.det(x_cov) &gt; np.finfo(np.float32).eps\n            icvx = np.linalg.inv(x_cov)\n        except AssertionError:\n            icvx = np.linalg.pinv(x_cov)\n            warnings.warn(\n                \"The inverse of the variance-covariance matrix \"\n                \"was calculated using the Moore-Penrose generalized \"\n                \"matrix inversion, due to its determinant being at \"\n                \"or very close to zero.\"\n            )\n        except np.linalg.LinAlgError:\n            icvx = empty_array\n\n    pcor = -1 * covariance_to_correlation(icvx)\n    np.fill_diagonal(pcor, 1.0)\n    return pcor\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.smc","title":"<code>smc(corr_mtx, sort=False)</code>","text":"<p>Calculate the squared multiple correlations.</p> <p>This is equivalent to regressing each variable on all others and calculating the r-squared values.</p> <p>Parameters:</p> Name Type Description Default <code>corr_mtx</code> <code>array - like</code> <p>The correlation matrix used to calculate SMC.</p> required <code>sort</code> <code>bool</code> <p>Whether to sort the values for SMC before returning. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>smc</code> <code>ndarray</code> <p>The squared multiple correlations matrix.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def smc(corr_mtx, sort=False):\n    \"\"\"\n    Calculate the squared multiple correlations.\n\n    This is equivalent to regressing each variable on all others and\n    calculating the r-squared values.\n\n    Args:\n        corr_mtx (array-like): The correlation matrix used to calculate SMC.\n        sort (bool, optional): Whether to sort the values for SMC before returning.\n            Defaults to False.\n\n    Returns:\n        smc (numpy.ndarray): The squared multiple correlations matrix.\n    \"\"\"\n    corr_inv = np.linalg.inv(corr_mtx)\n    smc = 1 - 1 / np.diag(corr_inv)\n\n    if sort:\n        smc = np.sort(smc)\n    return smc\n</code></pre>"},{"location":"reference/spotoptim/factor_analyzer/factor_analyzer_utils/#spotoptim.factor_analyzer.factor_analyzer_utils.unique_elements","title":"<code>unique_elements(seq)</code>","text":"<p>Get first unique instance of every list element, while maintaining order.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>list - like</code> <p>The list of elements.</p> required <p>Returns:</p> Name Type Description <code>seq</code> <code>list</code> <p>The updated list of elements.</p> Source code in <code>src/spotoptim/factor_analyzer/factor_analyzer_utils.py</code> <pre><code>def unique_elements(seq):\n    \"\"\"\n    Get first unique instance of every list element, while maintaining order.\n\n    Args:\n        seq (list-like): The list of elements.\n\n    Returns:\n        seq (list): The updated list of elements.\n    \"\"\"\n    seen = set()\n    return [x for x in seq if not (x in seen or seen.add(x))]\n</code></pre>"},{"location":"reference/spotoptim/function/forr08a/","title":"forr08a","text":""},{"location":"reference/spotoptim/function/forr08a/#spotoptim.function.forr08a.aerofoilcd","title":"<code>aerofoilcd(X)</code>","text":"<p>Computes the drag coefficient (cd) of an aerofoil based on the shape parameter X.</p> <p>This function reads the drag coefficient data from the \u201ccd_data.csv\u201d file and uses the input X (rounded to the nearest 0.01) to return the corresponding drag coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 1D NumPy array of values in the range [0, 1] representing the shape parameters.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D NumPy array of drag coefficients (cd) corresponding to the input X.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any value in X is outside the range [0, 1].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import aerofoilcd\n&gt;&gt;&gt; X = np.array([0.5, 0.75])\n&gt;&gt;&gt; aerofoilcd(X)\narray([0.029975, 0.033375])\n</code></pre> Source code in <code>src/spotoptim/function/forr08a.py</code> <pre><code>def aerofoilcd(X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Computes the drag coefficient (cd) of an aerofoil based on the shape parameter X.\n\n    This function reads the drag coefficient data from the \"cd_data.csv\" file and uses\n    the input X (rounded to the nearest 0.01) to return the corresponding drag coefficients.\n\n    Args:\n        X (np.ndarray): A 1D NumPy array of values in the range [0, 1] representing the shape parameters.\n\n    Returns:\n        np.ndarray: A 1D NumPy array of drag coefficients (cd) corresponding to the input X.\n\n    Raises:\n        ValueError: If any value in X is outside the range [0, 1].\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import aerofoilcd\n        &gt;&gt;&gt; X = np.array([0.5, 0.75])\n        &gt;&gt;&gt; aerofoilcd(X)\n        array([0.029975, 0.033375])\n    \"\"\"\n    # Ensure X is a NumPy array\n    X = np.asarray(X)\n\n    # Validate the input\n    if np.any((X &lt; 0) | (X &gt; 1)):\n        raise ValueError(\"All values in X must be in the range [0, 1].\")\n\n    # The given data as a string\n    data = (\n        \"0.031745,0.031568,0.031355,0.031607,0.03132,0.031242,0.030959,0.030593,0.030347,\"\n        \"0.030153,0.030089,0.029881,0.029967,0.029686,0.029612,0.029727,0.029445,0.030188,\"\n        \"0.029907,0.029634,0.02978,0.029585,0.029301,0.029543,0.029663,0.029137,0.029611,\"\n        \"0.029395,0.02918,0.029369,0.029272,0.029384,0.029249,0.029545,0.029641,0.029975,\"\n        \"0.029801,0.029857,0.030131,0.029678,0.029451,0.029899,0.029922,0.030228,0.02979,\"\n        \"0.03004,0.030188,0.030366,0.030399,0.030193,0.030012,0.030109,0.030629,0.030551,\"\n        \"0.030721,0.031211,0.031132,0.031236,0.031379,0.031531,0.03117,0.031808,0.0318,\"\n        \"0.032141,0.032216,0.032451,0.032545,0.032836,0.032843,0.032888,0.033098,0.033271,\"\n        \"0.033478,0.03328,0.033375,0.033979,0.034197,0.034406,0.034315,0.034662,0.035125,\"\n        \"0.035306,0.035021,0.03526,0.035988,0.03579,0.036927,0.036705,0.037232,0.037563,\"\n        \"0.037501,0.037802,0.038302,0.038676,0.038898,0.03891,0.03916,0.039584,0.038509,\"\n        \"0.040168,0.039062\"\n    )\n\n    # Convert the string to a NumPy array\n    cd_data = np.fromstring(data, sep=\",\")\n\n    # Compute the indices based on X (rounded to the nearest 0.01)\n    indices = np.round(X * 100).astype(int)\n\n    # Return the corresponding drag coefficients\n    return cd_data[indices]\n</code></pre>"},{"location":"reference/spotoptim/function/forr08a/#spotoptim.function.forr08a.branin","title":"<code>branin(x)</code>","text":"<p>Branin\u2019s test function that takes a 2D input vector <code>x</code> in the range [0, 1] for each dimension and returns the corresponding scalar function value. The function is vectorized to handle multiple inputs.</p> The function is defined as <p>f(x) = a * (X2 - b * X1^2 + c * X1 - d)^2 + e * (1 - ff) * cos(X1) + e + 5 * x1</p> <p>where:     X1 = 15 * x1 - 5     X2 = 15 * x2</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>A 2D NumPy array of shape (n_samples, 2) where each row is a 2D input vector.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The calculated function values for the input <code>x</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>x</code> does not have exactly 2 columns or if any value in <code>x</code> is outside the range [0, 1].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import branin\n&gt;&gt;&gt; # Single input\n&gt;&gt;&gt; print(branin(np.array([[0.5, 0.5]])))\n[26.63]\n&gt;&gt;&gt; # Multiple inputs\n&gt;&gt;&gt; x = np.array([[0.0, 0.0], [0.25, 0.25], [0.5, 0.5], [0.75, 0.75], [1.0, 1.0]])\n&gt;&gt;&gt; print(branin(x))\n[308.1291, 34.0028, 26.63, 126.3879, 150.8722]\n</code></pre> Source code in <code>src/spotoptim/function/forr08a.py</code> <pre><code>def branin(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Branin's test function that takes a 2D input vector `x` in the range [0, 1] for each dimension\n    and returns the corresponding scalar function value. The function is vectorized to handle\n    multiple inputs.\n\n    The function is defined as:\n        f(x) = a * (X2 - b * X1^2 + c * X1 - d)^2 + e * (1 - ff) * cos(X1) + e + 5 * x1\n    where:\n        X1 = 15 * x1 - 5\n        X2 = 15 * x2\n\n    Args:\n        x (np.ndarray): A 2D NumPy array of shape (n_samples, 2) where each row is a 2D input vector.\n\n    Returns:\n        np.ndarray: The calculated function values for the input `x`.\n\n    Raises:\n        ValueError: If `x` does not have exactly 2 columns or if any value in `x` is outside the range [0, 1].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import branin\n        &gt;&gt;&gt; # Single input\n        &gt;&gt;&gt; print(branin(np.array([[0.5, 0.5]])))\n        [26.63]\n        &gt;&gt;&gt; # Multiple inputs\n        &gt;&gt;&gt; x = np.array([[0.0, 0.0], [0.25, 0.25], [0.5, 0.5], [0.75, 0.75], [1.0, 1.0]])\n        &gt;&gt;&gt; print(branin(x))\n        [308.1291, 34.0028, 26.63, 126.3879, 150.8722]\n    \"\"\"\n    # Ensure x is a NumPy array\n    x = np.asarray(x)\n\n    # Check if x has exactly 2 columns\n    if x.shape[1] != 2:\n        raise IndexError(\"Input to branin must have exactly 2 columns.\")\n\n    # Check if all values are within the range [0, 1]\n    if np.any((x &lt; 0) | (x &gt; 1)):\n        raise ValueError(\n            \"Variable outside of range - use x in [0, 1] for both dimensions.\"\n        )\n\n    # Extract x1 and x2\n    x1, x2 = x[:, 0], x[:, 1]\n\n    # Transform x1 and x2 to X1 and X2\n    X1 = 15 * x1 - 5\n    X2 = 15 * x2\n\n    # Define constants\n    a = 1\n    b = 5.1 / (4 * np.pi**2)\n    c = 5 / np.pi\n    d = 6\n    e = 10\n    ff = 1 / (8 * np.pi)\n\n    # Compute the function values\n    f = a * (X2 - b * X1**2 + c * X1 - d) ** 2 + e * (1 - ff) * np.cos(X1) + e + 5 * x1\n\n    return f\n</code></pre>"},{"location":"reference/spotoptim/function/forr08a/#spotoptim.function.forr08a.onevar","title":"<code>onevar(x)</code>","text":"<p>One-variable test function that takes a scalar or 1D array input <code>x</code> in the range [0, 1] and returns the corresponding function values. The function is vectorized to handle multiple inputs.</p> The function is defined as <p>f(x) = ((6x - 2)^2) * np.sin((6x - 2) * 2)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>A scalar or 1D NumPy array of values in the range [0, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The calculated function values for the input <code>x</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any value in <code>x</code> is outside the range [0, 1].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import onevar\n&gt;&gt;&gt; # Single input\n&gt;&gt;&gt; print(onevar(np.array([0.5])))\n[0.9093]\n&gt;&gt;&gt; # Multiple inputs\n&gt;&gt;&gt; x = np.array([0.0, 0.25, 0.5, 0.75, 1.0])\n&gt;&gt;&gt; print(onevar(x))\n[3.0272, -0.2104, 0.9093,  -5.9933, 15.8297]\n</code></pre> Source code in <code>src/spotoptim/function/forr08a.py</code> <pre><code>def onevar(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    One-variable test function that takes a scalar or 1D array input `x` in the range [0, 1]\n    and returns the corresponding function values. The function is vectorized to handle\n    multiple inputs.\n\n    The function is defined as:\n        f(x) = ((6x - 2)^2) * np.sin((6x - 2) * 2)\n\n    Args:\n        x (np.ndarray): A scalar or 1D NumPy array of values in the range [0, 1].\n\n    Returns:\n        np.ndarray: The calculated function values for the input `x`.\n\n    Raises:\n        ValueError: If any value in `x` is outside the range [0, 1].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotpython.surrogate.functions.forr08a import onevar\n        &gt;&gt;&gt; # Single input\n        &gt;&gt;&gt; print(onevar(np.array([0.5])))\n        [0.9093]\n        &gt;&gt;&gt; # Multiple inputs\n        &gt;&gt;&gt; x = np.array([0.0, 0.25, 0.5, 0.75, 1.0])\n        &gt;&gt;&gt; print(onevar(x))\n        [3.0272, -0.2104, 0.9093,  -5.9933, 15.8297]\n    \"\"\"\n    # Ensure x is a NumPy array\n    x = np.asarray(x)\n\n    # Check if all values are within the range [0, 1]\n    if np.any((x &lt; 0) | (x &gt; 1)):\n        raise ValueError(\"Variable outside of range - use x in [0, 1]\")\n\n    # Compute the function values\n    f = ((6 * x - 2) ** 2) * np.sin((6 * x - 2) * 2)\n\n    return f\n</code></pre>"},{"location":"reference/spotoptim/function/mo/","title":"mo","text":"<p>Analytical multi-objective test functions for optimization benchmarking.</p> <p>This module provides well-known multi-objective analytical test functions commonly used for evaluating and benchmarking multiobjective optimization algorithms.</p>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.activity_pred","title":"<code>activity_pred(X)</code>","text":"<p>Compute activity predictions for each row in the input array.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2D array where each row is a configuration.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 1D array of activity predictions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import activity_pred\n&gt;&gt;&gt; # Example input data\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; activity_pred(X)\narray([  1.5,  10.5])\n</code></pre> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def activity_pred(X) -&gt; np.ndarray:\n    \"\"\"\n    Compute activity predictions for each row in the input array.\n\n    Args:\n        X (np.ndarray): 2D array where each row is a configuration.\n\n    Returns:\n        np.ndarray: 1D array of activity predictions.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.function.mo import activity_pred\n        &gt;&gt;&gt; # Example input data\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; activity_pred(X)\n        array([  1.5,  10.5])\n    \"\"\"\n    return (\n        59.85\n        + 3.583 * X[:, 0]\n        + 0.2546 * X[:, 1]\n        + 2.2298 * X[:, 2]\n        + 0.83479 * X[:, 0] ** 2\n        + 0.07484 * X[:, 1] ** 2\n        + 0.05716 * X[:, 2] ** 2\n        - 0.3875 * X[:, 0] * X[:, 1]\n        - 0.375 * X[:, 0] * X[:, 2]\n        + 0.3125 * X[:, 1] * X[:, 2]\n    )\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.conversion_pred","title":"<code>conversion_pred(X)</code>","text":"<p>Compute conversion predictions for each row in the input array.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2D array where each row is a configuration.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 1D array of conversion predictions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import conversion_pred\n&gt;&gt;&gt; # Example input data\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; conversion_pred(X)\narray([  3.5,  19.5])\n</code></pre> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def conversion_pred(X) -&gt; np.ndarray:\n    \"\"\"\n    Compute conversion predictions for each row in the input array.\n\n    Args:\n        X (np.ndarray): 2D array where each row is a configuration.\n\n    Returns:\n        np.ndarray: 1D array of conversion predictions.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.function.mo import conversion_pred\n        &gt;&gt;&gt; # Example input data\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; conversion_pred(X)\n        array([  3.5,  19.5])\n\n    \"\"\"\n    return (\n        81.09\n        + 1.0284 * X[:, 0]\n        + 4.043 * X[:, 1]\n        + 6.2037 * X[:, 2]\n        - 1.8366 * X[:, 0] ** 2\n        + 2.9382 * X[:, 1] ** 2\n        - 5.1915 * X[:, 2] ** 2\n        + 2.2150 * X[:, 0] * X[:, 1]\n        + 11.375 * X[:, 0] * X[:, 2]\n        - 3.875 * X[:, 1] * X[:, 2]\n    )\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.dtlz1","title":"<code>dtlz1(X, n_obj=3)</code>","text":"<p>DTLZ1 multi-objective test function (scalable objectives).</p> <p>DTLZ1 is a scalable test problem with a linear Pareto front. It has (11^k - 1) local Pareto fronts where k = n - n_obj + 1.</p> Mathematical formulation <p>f_i(X) = 0.5 * x1 * \u2026 * x_{M-i} * [1 + g(X)] for i = 1, \u2026, M-1 f_M(X) = 0.5 * [1 - x_{M-1}] * [1 + g(X)] g(X) = 100 * [k + sum((x_i - 0.5)^2 - cos(20\u03c0(x_i - 0.5)) for i in X_M)]</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.</p> required <code>n_obj</code> <code>int</code> <p>Number of objectives. Defaults to 3. Must be at least 2 and at most n_features.</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Objective values with shape (n_samples, n_obj).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If n_obj is invalid or X has insufficient dimensions.</p> Note <ul> <li>Number of objectives: Scalable (typically 3)</li> <li>Typical number of variables: n_obj + k - 1 (often k = 5, so n = 7 for 3 objectives)</li> <li>Search domain: [0, 1]^n</li> <li>Pareto front: Linear hyperplane</li> <li>Characteristics: Multimodal, many local fronts</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.mo import dtlz1\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n&gt;&gt;&gt; result = dtlz1(X, n_obj=3)\n&gt;&gt;&gt; result.shape\n(1, 3)\n</code></pre> References <p>Deb, K., Thiele, L., Laumanns, M., &amp; Zitzler, E. (2005). \u201cScalable test problems for evolutionary multiobjective optimization.\u201d In Evolutionary multiobjective optimization (pp. 105-145). Springer.</p> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def dtlz1(X, n_obj: int = 3) -&gt; np.ndarray:\n    \"\"\"DTLZ1 multi-objective test function (scalable objectives).\n\n    DTLZ1 is a scalable test problem with a linear Pareto front. It has\n    (11^k - 1) local Pareto fronts where k = n - n_obj + 1.\n\n    Mathematical formulation:\n        f_i(X) = 0.5 * x1 * ... * x_{M-i} * [1 + g(X)] for i = 1, ..., M-1\n        f_M(X) = 0.5 * [1 - x_{M-1}] * [1 + g(X)]\n        g(X) = 100 * [k + sum((x_i - 0.5)^2 - cos(20\u03c0(x_i - 0.5)) for i in X_M)]\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n        n_obj (int, optional): Number of objectives. Defaults to 3.\n            Must be at least 2 and at most n_features.\n\n    Returns:\n        np.ndarray: Objective values with shape (n_samples, n_obj).\n\n    Raises:\n        ValueError: If n_obj is invalid or X has insufficient dimensions.\n\n    Note:\n        - Number of objectives: Scalable (typically 3)\n        - Typical number of variables: n_obj + k - 1 (often k = 5, so n = 7 for 3 objectives)\n        - Search domain: [0, 1]^n\n        - Pareto front: Linear hyperplane\n        - Characteristics: Multimodal, many local fronts\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.function.mo import dtlz1\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n        &gt;&gt;&gt; result = dtlz1(X, n_obj=3)\n        &gt;&gt;&gt; result.shape\n        (1, 3)\n\n    References:\n        Deb, K., Thiele, L., Laumanns, M., &amp; Zitzler, E. (2005). \"Scalable test problems\n        for evolutionary multiobjective optimization.\" In Evolutionary multiobjective\n        optimization (pp. 105-145). Springer.\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n    n = X.shape[1]\n\n    if n_obj &lt; 2:\n        raise ValueError(f\"Number of objectives must be at least 2, got {n_obj}\")\n    if n_obj &gt; n:\n        raise ValueError(\n            f\"Number of objectives ({n_obj}) cannot exceed number of variables ({n})\"\n        )\n\n    k = n - n_obj + 1\n    X_m = X[:, n_obj - 1 :]\n\n    g = 100 * (k + np.sum((X_m - 0.5) ** 2 - np.cos(20 * np.pi * (X_m - 0.5)), axis=1))\n\n    f = np.zeros((X.shape[0], n_obj))\n\n    for i in range(n_obj):\n        f[:, i] = 0.5 * (1 + g)\n        for j in range(n_obj - i - 1):\n            f[:, i] *= X[:, j]\n        if i &gt; 0:\n            f[:, i] *= 1 - X[:, n_obj - i - 1]\n\n    return f\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.dtlz2","title":"<code>dtlz2(X, n_obj=3)</code>","text":"<p>DTLZ2 multi-objective test function (scalable objectives).</p> <p>DTLZ2 is a scalable test problem with a concave Pareto front (a unit sphere).</p> Mathematical formulation <p>f_i(X) = [1 + g(X)] * cos(x1 * \u03c0/2) * \u2026 * cos(x_{M-i} * \u03c0/2) * sin(x_{M-i+1} * \u03c0/2) f_M(X) = [1 + g(X)] * sin(x1 * \u03c0/2) g(X) = sum((x_i - 0.5)^2 for i in X_M)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.</p> required <code>n_obj</code> <code>int</code> <p>Number of objectives. Defaults to 3. Must be at least 2 and at most n_features.</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Objective values with shape (n_samples, n_obj).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If n_obj is invalid or X has insufficient dimensions.</p> Note <ul> <li>Number of objectives: Scalable (typically 3)</li> <li>Typical number of variables: n_obj + k - 1 (often k = 10, so n = 12 for 3 objectives)</li> <li>Search domain: [0, 1]^n</li> <li>Pareto front: Concave (sphere: sum(f_i^2) = 1)</li> <li>Characteristics: Concave, unimodal</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.mo import dtlz2\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n&gt;&gt;&gt; result = dtlz2(X, n_obj=3)\n&gt;&gt;&gt; result.shape\n(1, 3)\n</code></pre> References <p>Deb, K., Thiele, L., Laumanns, M., &amp; Zitzler, E. (2005). \u201cScalable test problems for evolutionary multiobjective optimization.\u201d In Evolutionary multiobjective optimization (pp. 105-145). Springer.</p> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def dtlz2(X, n_obj: int = 3) -&gt; np.ndarray:\n    \"\"\"DTLZ2 multi-objective test function (scalable objectives).\n\n    DTLZ2 is a scalable test problem with a concave Pareto front (a unit sphere).\n\n    Mathematical formulation:\n        f_i(X) = [1 + g(X)] * cos(x1 * \u03c0/2) * ... * cos(x_{M-i} * \u03c0/2) * sin(x_{M-i+1} * \u03c0/2)\n        f_M(X) = [1 + g(X)] * sin(x1 * \u03c0/2)\n        g(X) = sum((x_i - 0.5)^2 for i in X_M)\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n        n_obj (int, optional): Number of objectives. Defaults to 3.\n            Must be at least 2 and at most n_features.\n\n    Returns:\n        np.ndarray: Objective values with shape (n_samples, n_obj).\n\n    Raises:\n        ValueError: If n_obj is invalid or X has insufficient dimensions.\n\n    Note:\n        - Number of objectives: Scalable (typically 3)\n        - Typical number of variables: n_obj + k - 1 (often k = 10, so n = 12 for 3 objectives)\n        - Search domain: [0, 1]^n\n        - Pareto front: Concave (sphere: sum(f_i^2) = 1)\n        - Characteristics: Concave, unimodal\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.function.mo import dtlz2\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n        &gt;&gt;&gt; result = dtlz2(X, n_obj=3)\n        &gt;&gt;&gt; result.shape\n        (1, 3)\n\n    References:\n        Deb, K., Thiele, L., Laumanns, M., &amp; Zitzler, E. (2005). \"Scalable test problems\n        for evolutionary multiobjective optimization.\" In Evolutionary multiobjective\n        optimization (pp. 105-145). Springer.\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n    n = X.shape[1]\n\n    if n_obj &lt; 2:\n        raise ValueError(f\"Number of objectives must be at least 2, got {n_obj}\")\n    if n_obj &gt; n:\n        raise ValueError(\n            f\"Number of objectives ({n_obj}) cannot exceed number of variables ({n})\"\n        )\n\n    X_m = X[:, n_obj - 1 :]\n    g = np.sum((X_m - 0.5) ** 2, axis=1)\n\n    f = np.zeros((X.shape[0], n_obj))\n\n    for i in range(n_obj):\n        f[:, i] = 1 + g\n\n        for j in range(n_obj - i - 1):\n            f[:, i] *= np.cos(X[:, j] * np.pi / 2)\n\n        if i &gt; 0:\n            f[:, i] *= np.sin(X[:, n_obj - i - 1] * np.pi / 2)\n\n    return f\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.fonseca_fleming","title":"<code>fonseca_fleming(X)</code>","text":"<p>Fonseca-Fleming multi-objective test function (2 objectives).</p> <p>The Fonseca-Fleming function is a classical bi-objective problem with a concave Pareto front. The difficulty increases with the number of variables.</p> Mathematical formulation <p>f1(X) = 1 - exp(-sum((x_i - 1/sqrt(n))^2 for i=1 to n)) f2(X) = 1 - exp(-sum((x_i + 1/sqrt(n))^2 for i=1 to n))</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values</p> Note <ul> <li>Number of objectives: 2</li> <li>Typical number of variables: 2-10</li> <li>Search domain: [-4, 4]^n</li> <li>Pareto front: Concave</li> <li>Characteristics: Concave, symmetric</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.mo import fonseca_fleming\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0])\n&gt;&gt;&gt; result = fonseca_fleming(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n</code></pre> <pre><code>&gt;&gt;&gt; X = np.array([[0.0, 0.0], [1.0, 1.0]])\n&gt;&gt;&gt; result = fonseca_fleming(X)\n&gt;&gt;&gt; result.shape\n(2, 2)\n</code></pre> References <p>Fonseca, C. M., &amp; Fleming, P. J. (1995). \u201cAn overview of evolutionary algorithms in multiobjective optimization.\u201d Evolutionary computation, 3(1), 1-16.</p> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def fonseca_fleming(X) -&gt; np.ndarray:\n    \"\"\"Fonseca-Fleming multi-objective test function (2 objectives).\n\n    The Fonseca-Fleming function is a classical bi-objective problem with a\n    concave Pareto front. The difficulty increases with the number of variables.\n\n    Mathematical formulation:\n        f1(X) = 1 - exp(-sum((x_i - 1/sqrt(n))^2 for i=1 to n))\n        f2(X) = 1 - exp(-sum((x_i + 1/sqrt(n))^2 for i=1 to n))\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n\n    Returns:\n        np.ndarray: Objective values with shape (n_samples, 2) where:\n            - Column 0: f1 values\n            - Column 1: f2 values\n\n    Note:\n        - Number of objectives: 2\n        - Typical number of variables: 2-10\n        - Search domain: [-4, 4]^n\n        - Pareto front: Concave\n        - Characteristics: Concave, symmetric\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.function.mo import fonseca_fleming\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([0.0, 0.0])\n        &gt;&gt;&gt; result = fonseca_fleming(X)\n        &gt;&gt;&gt; result.shape\n        (1, 2)\n\n        &gt;&gt;&gt; X = np.array([[0.0, 0.0], [1.0, 1.0]])\n        &gt;&gt;&gt; result = fonseca_fleming(X)\n        &gt;&gt;&gt; result.shape\n        (2, 2)\n\n    References:\n        Fonseca, C. M., &amp; Fleming, P. J. (1995). \"An overview of evolutionary algorithms\n        in multiobjective optimization.\" Evolutionary computation, 3(1), 1-16.\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n    n = X.shape[1]\n\n    sqrt_n_inv = 1.0 / np.sqrt(n)\n\n    f1 = 1 - np.exp(-np.sum((X - sqrt_n_inv) ** 2, axis=1))\n    f2 = 1 - np.exp(-np.sum((X + sqrt_n_inv) ** 2, axis=1))\n\n    return np.column_stack([f1, f2])\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.fun_myer16a","title":"<code>fun_myer16a(X, fun_control=None)</code>","text":"<p>Compute both conversion and activity predictions for each row in the input array.</p> Notes <p>Implements a response surface experiment described by Myers, Montgomery, and Anderson-Cook (2016). The function computes two objectives: conversion and activity.</p> References <ul> <li>Myers, R. H., Montgomery, D. C., and Anderson-Cook, C. M. Response surface methodology:   process and product optimization using designed experiments. John Wiley &amp; Sons, 2016.</li> <li>Kuhn, M. desirability: Function optimization and ranking via desirability functions. Tech. rep., 9 2016.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2D array where each row is a configuration.</p> required <code>fun_control</code> <code>dict</code> <p>Additional control parameters (not used here).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 2D array where each row contains [conversion_pred, activity_pred].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import fun_myer16a\n&gt;&gt;&gt; # Example input data\n&gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; fun_myer16a(X)\narray([[  3.5,   1.5],\n       [ 19.5,  10.5]])\n</code></pre> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def fun_myer16a(X, fun_control=None) -&gt; np.ndarray:\n    \"\"\"\n    Compute both conversion and activity predictions for each row in the input array.\n\n    Notes:\n        Implements a response surface experiment described by Myers, Montgomery, and Anderson-Cook (2016).\n        The function computes two objectives: conversion and activity.\n\n    References:\n        - Myers, R. H., Montgomery, D. C., and Anderson-Cook, C. M. Response surface methodology:\n          process and product optimization using designed experiments. John Wiley &amp; Sons, 2016.\n        - Kuhn, M. desirability: Function optimization and ranking via desirability functions. Tech. rep., 9 2016.\n\n    Args:\n        X (np.ndarray): 2D array where each row is a configuration.\n        fun_control (dict, optional): Additional control parameters (not used here).\n\n    Returns:\n        np.ndarray: 2D array where each row contains [conversion_pred, activity_pred].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.function.mo import fun_myer16a\n        &gt;&gt;&gt; # Example input data\n        &gt;&gt;&gt; X = np.array([[1, 2, 3], [4, 5, 6]])\n        &gt;&gt;&gt; fun_myer16a(X)\n        array([[  3.5,   1.5],\n               [ 19.5,  10.5]])\n    \"\"\"\n    return np.column_stack((conversion_pred(X), activity_pred(X)))\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.kursawe","title":"<code>kursawe(X)</code>","text":"<p>Kursawe multi-objective test function (2 objectives, minimization).</p> <p>The Kursawe function is a classic bi-objective minimization benchmark with a non-convex, disconnected Pareto front, often used to test an optimizer\u2019s ability to maintain diversity and avoid getting trapped in local fronts.</p> Mathematical formulation <p>f1(X) = sum(-10 * exp(-0.2 * sqrt(x_i^2 + x_{i+1}^2)) for i=1 to n-1) f2(X) = sum(|x_i|^0.8 + 5 * sin(x_i^3) for i=1 to n)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If X has fewer than 2 dimensions.</p> Note <ul> <li>Number of objectives: 2</li> <li>Typical number of variables: 3</li> <li>Search domain: [-5, 5]^n</li> <li>Pareto front: Disconnected</li> <li>Characteristics: Non-convex, disconnected, multimodal</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.mo import kursawe\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; result = kursawe(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n</code></pre> <pre><code>&gt;&gt;&gt; X = np.array([[0.0, 0.0, 0.0], [1.0, 1.0, 1.0]])\n&gt;&gt;&gt; result = kursawe(X)\n&gt;&gt;&gt; result.shape\n(2, 2)\n</code></pre> References <p>Kursawe, F. (1991). \u201cA variant of evolution strategies for vector optimization.\u201d In International Conference on Parallel Problem Solving from Nature (pp. 193-197). Springer.</p> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def kursawe(X) -&gt; np.ndarray:\n    \"\"\"Kursawe multi-objective test function (2 objectives, minimization).\n\n    The Kursawe function is a classic bi-objective minimization benchmark with a\n    non-convex, disconnected Pareto front, often used to test an optimizer's ability\n    to maintain diversity and avoid getting trapped in local fronts.\n\n    Mathematical formulation:\n        f1(X) = sum(-10 * exp(-0.2 * sqrt(x_i^2 + x_{i+1}^2)) for i=1 to n-1)\n        f2(X) = sum(|x_i|^0.8 + 5 * sin(x_i^3) for i=1 to n)\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n            Must have at least 2 dimensions.\n\n    Returns:\n        np.ndarray: Objective values with shape (n_samples, 2) where:\n            - Column 0: f1 values\n            - Column 1: f2 values\n\n    Raises:\n        ValueError: If X has fewer than 2 dimensions.\n\n    Note:\n        - Number of objectives: 2\n        - Typical number of variables: 3\n        - Search domain: [-5, 5]^n\n        - Pareto front: Disconnected\n        - Characteristics: Non-convex, disconnected, multimodal\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.function.mo import kursawe\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n        &gt;&gt;&gt; result = kursawe(X)\n        &gt;&gt;&gt; result.shape\n        (1, 2)\n\n        &gt;&gt;&gt; X = np.array([[0.0, 0.0, 0.0], [1.0, 1.0, 1.0]])\n        &gt;&gt;&gt; result = kursawe(X)\n        &gt;&gt;&gt; result.shape\n        (2, 2)\n\n    References:\n        Kursawe, F. (1991). \"A variant of evolution strategies for vector optimization.\"\n        In International Conference on Parallel Problem Solving from Nature (pp. 193-197).\n        Springer.\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n\n    if X.shape[1] &lt; 2:\n        raise ValueError(f\"Kursawe requires at least 2 dimensions, got {X.shape[1]}\")\n\n    f1 = np.sum(-10 * np.exp(-0.2 * np.sqrt(X[:, :-1] ** 2 + X[:, 1:] ** 2)), axis=1)\n    f2 = np.sum(np.abs(X) ** 0.8 + 5 * np.sin(X**3), axis=1)\n\n    return np.column_stack([f1, f2])\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.mo_conv2_max","title":"<code>mo_conv2_max(X)</code>","text":"<p>Convex bi-objective maximization test function (2 objectives).</p> <p>A smooth, convex two-objective maximization problem on [0, 1]^2 using flipped versions of the minimization objectives:     f1(x, y) = 2 - (x^2 + y^2)     f2(x, y) = 2 - ((1 - x)^2 + (1 - y)^2)</p> Properties <ul> <li>Domain: [0, 1]^2</li> <li>Objectives: maximize both f1 and f2</li> <li>Ideal points: (0, 0) for f1 (gives f1=2); (1, 1) for f2 (gives f2=2)</li> <li>Pareto set: line x = y in [0, 1]</li> <li>Pareto front: convex quadratic trade-off f1 = 2 - 2t^2, f2 = 2 - 2(1 - t)^2, t \u2208 [0, 1]</li> </ul> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have exactly 2 dimensions.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values (to be maximized) - Column 1: f2 values (to be maximized)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If X does not have exactly 2 dimensions.</p> Note <ul> <li>Number of objectives: 2</li> <li>Number of variables: 2</li> <li>Search domain: [0, 1]^2</li> <li>Ideal points: (1, 1) for f1, (0, 0) for f2</li> <li>Pareto front: Convex, quadratic</li> <li>Problem type: Maximization</li> <li>Characteristics: Convex, smooth, bounded</li> </ul> <p>Examples:</p> <p>Single point evaluation:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_max\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0])\n&gt;&gt;&gt; result = mo_conv2_max(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; result[0]  # f1 maximum\narray([0., 2.])\n</code></pre> <pre><code>&gt;&gt;&gt; X = np.array([1.0, 1.0])\n&gt;&gt;&gt; result = mo_conv2_max(X)\n&gt;&gt;&gt; result[0]  # f2 maximum\narray([2., 0.])\n</code></pre> <p>Multiple points evaluation:</p> <pre><code>&gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; result = mo_conv2_min(X)\n&gt;&gt;&gt; result.shape\n(3, 2)\n&gt;&gt;&gt; result[1]  # Pareto front\narray([0.5, 0.5])\n</code></pre> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def mo_conv2_max(X) -&gt; np.ndarray:\n    \"\"\"Convex bi-objective maximization test function (2 objectives).\n\n    A smooth, convex two-objective maximization problem on [0, 1]^2 using flipped\n    versions of the minimization objectives:\n        f1(x, y) = 2 - (x^2 + y^2)\n        f2(x, y) = 2 - ((1 - x)^2 + (1 - y)^2)\n\n    Properties:\n        - Domain: [0, 1]^2\n        - Objectives: maximize both f1 and f2\n        - Ideal points: (0, 0) for f1 (gives f1=2); (1, 1) for f2 (gives f2=2)\n        - Pareto set: line x = y in [0, 1]\n        - Pareto front: convex quadratic trade-off f1 = 2 - 2t^2, f2 = 2 - 2(1 - t)^2, t \u2208 [0, 1]\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n            Must have exactly 2 dimensions.\n\n    Returns:\n        np.ndarray: Objective values with shape (n_samples, 2) where:\n            - Column 0: f1 values (to be maximized)\n            - Column 1: f2 values (to be maximized)\n\n    Raises:\n        ValueError: If X does not have exactly 2 dimensions.\n\n    Note:\n        - Number of objectives: 2\n        - Number of variables: 2\n        - Search domain: [0, 1]^2\n        - Ideal points: (1, 1) for f1, (0, 0) for f2\n        - Pareto front: Convex, quadratic\n        - Problem type: Maximization\n        - Characteristics: Convex, smooth, bounded\n\n    Examples:\n        Single point evaluation:\n\n        &gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_max\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([0.0, 0.0])\n        &gt;&gt;&gt; result = mo_conv2_max(X)\n        &gt;&gt;&gt; result.shape\n        (1, 2)\n        &gt;&gt;&gt; result[0]  # f1 maximum\n        array([0., 2.])\n\n        &gt;&gt;&gt; X = np.array([1.0, 1.0])\n        &gt;&gt;&gt; result = mo_conv2_max(X)\n        &gt;&gt;&gt; result[0]  # f2 maximum\n        array([2., 0.])\n\n        Multiple points evaluation:\n\n        &gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n        &gt;&gt;&gt; result = mo_conv2_min(X)\n        &gt;&gt;&gt; result.shape\n        (3, 2)\n        &gt;&gt;&gt; result[1]  # Pareto front\n        array([0.5, 0.5])\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n\n    if X.shape[1] != 2:\n        raise ValueError(\n            f\"mo_conv2_min requires exactly 2 dimensions, got {X.shape[1]}\"\n        )\n\n    x = X[:, 0]\n    y = X[:, 1]\n\n    # Objective 1: f1(x, y) = x^2 + y^2\n    f1 = 2.0 - (x**2 + y**2)\n\n    # Objective 2: f2(x, y) = (1-x)^2 + (1-y)^2\n    f2 = 2.0 - ((1 - x) ** 2 + (1 - y) ** 2)\n\n    return np.column_stack([f1, f2])\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.mo_conv2_min","title":"<code>mo_conv2_min(X)</code>","text":"<p>Convex bi-objective minimization test function (2 objectives).</p> <p>A smooth, convex two-objective problem on [0, 1]^2:     f1(x, y) = x^2 + y^2     f2(x, y) = (x - 1)^2 + (y - 1)^2</p> Properties <ul> <li>Domain: [0, 1]^2</li> <li>Objectives: minimize both f1 and f2</li> <li>Ideal points: (0, 0) for f1; (1, 1) for f2</li> <li>Pareto set: line x = y in [0, 1]</li> <li>Pareto front: convex quadratic trade-off f1 = 2t^2, f2 = 2(1 - t)^2, t \u2208 [0, 1]</li> </ul> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have exactly 2 dimensions.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values (to be minimized) - Column 1: f2 values (to be minimized)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If X does not have exactly 2 dimensions.</p> Note <ul> <li>Number of objectives: 2</li> <li>Number of variables: 2</li> <li>Search domain: [0, 1]^2</li> <li>Ideal points: (0, 0) for f1, (1, 1) for f2</li> <li>Pareto front: Convex, quadratic</li> <li>Problem type: Minimization</li> <li>Characteristics: Convex, smooth, bounded</li> </ul> <p>Examples:</p> <p>Single point evaluation:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_min\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0])\n&gt;&gt;&gt; result = mo_conv2_min(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; result[0]  # f1 minimum\narray([0., 2.])\n</code></pre> <pre><code>&gt;&gt;&gt; X = np.array([1.0, 1.0])\n&gt;&gt;&gt; result = mo_conv2_min(X)\n&gt;&gt;&gt; result[0]  # f2 minimum\narray([2., 0.])\n</code></pre> <p>Multiple points evaluation:</p> <pre><code>&gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; result = mo_conv2_min(X)\n&gt;&gt;&gt; result.shape\n(3, 2)\n&gt;&gt;&gt; result[1]  # Pareto front\narray([0.5, 0.5])\n</code></pre> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def mo_conv2_min(X) -&gt; np.ndarray:\n    \"\"\"Convex bi-objective minimization test function (2 objectives).\n\n    A smooth, convex two-objective problem on [0, 1]^2:\n        f1(x, y) = x^2 + y^2\n        f2(x, y) = (x - 1)^2 + (y - 1)^2\n\n    Properties:\n        - Domain: [0, 1]^2\n        - Objectives: minimize both f1 and f2\n        - Ideal points: (0, 0) for f1; (1, 1) for f2\n        - Pareto set: line x = y in [0, 1]\n        - Pareto front: convex quadratic trade-off f1 = 2t^2, f2 = 2(1 - t)^2, t \u2208 [0, 1]\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n            Must have exactly 2 dimensions.\n\n    Returns:\n        np.ndarray: Objective values with shape (n_samples, 2) where:\n            - Column 0: f1 values (to be minimized)\n            - Column 1: f2 values (to be minimized)\n\n    Raises:\n        ValueError: If X does not have exactly 2 dimensions.\n\n    Note:\n        - Number of objectives: 2\n        - Number of variables: 2\n        - Search domain: [0, 1]^2\n        - Ideal points: (0, 0) for f1, (1, 1) for f2\n        - Pareto front: Convex, quadratic\n        - Problem type: Minimization\n        - Characteristics: Convex, smooth, bounded\n\n    Examples:\n        Single point evaluation:\n\n        &gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_min\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([0.0, 0.0])\n        &gt;&gt;&gt; result = mo_conv2_min(X)\n        &gt;&gt;&gt; result.shape\n        (1, 2)\n        &gt;&gt;&gt; result[0]  # f1 minimum\n        array([0., 2.])\n\n        &gt;&gt;&gt; X = np.array([1.0, 1.0])\n        &gt;&gt;&gt; result = mo_conv2_min(X)\n        &gt;&gt;&gt; result[0]  # f2 minimum\n        array([2., 0.])\n\n        Multiple points evaluation:\n\n        &gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n        &gt;&gt;&gt; result = mo_conv2_min(X)\n        &gt;&gt;&gt; result.shape\n        (3, 2)\n        &gt;&gt;&gt; result[1]  # Pareto front\n        array([0.5, 0.5])\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n\n    if X.shape[1] != 2:\n        raise ValueError(\n            f\"mo_conv2_min requires exactly 2 dimensions, got {X.shape[1]}\"\n        )\n\n    x = X[:, 0]\n    y = X[:, 1]\n\n    # Objective 1: f1(x, y) = x^2 + y^2\n    f1 = x**2 + y**2\n\n    # Objective 2: f2(x, y) = (x-1)^2 + (y-1)^2\n    f2 = (x - 1) ** 2 + (y - 1) ** 2\n\n    return np.column_stack([f1, f2])\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.schaffer_n1","title":"<code>schaffer_n1(X)</code>","text":"<p>Schaffer N1 multi-objective test function (2 objectives).</p> <p>Schaffer N1 is a simple bi-objective problem with a convex Pareto front. It is one of the earliest multi-objective test functions.</p> Mathematical formulation <p>f1(X) = x^2 f2(X) = (x - 2)^2</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. This function uses only the first variable.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values</p> Note <ul> <li>Number of objectives: 2</li> <li>Number of variables: 1 (only first variable is used)</li> <li>Search domain: [-10, 10] or [-A, A]</li> <li>Pareto front: x \u2208 [0, 2]</li> <li>Characteristics: Convex, simple, unimodal</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.mo import schaffer_n1\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0])\n&gt;&gt;&gt; result = schaffer_n1(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; result[0]\narray([0., 4.])\n</code></pre> <pre><code>&gt;&gt;&gt; X = np.array([[0.0], [1.0], [2.0]])\n&gt;&gt;&gt; result = schaffer_n1(X)\n&gt;&gt;&gt; result.shape\n(3, 2)\n</code></pre> References <p>Schaffer, J. D. (1985). \u201cMultiple objective optimization with vector evaluated genetic algorithms.\u201d In Proceedings of the 1st international Conference on Genetic Algorithms (pp. 93-100).</p> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def schaffer_n1(X) -&gt; np.ndarray:\n    \"\"\"Schaffer N1 multi-objective test function (2 objectives).\n\n    Schaffer N1 is a simple bi-objective problem with a convex Pareto front.\n    It is one of the earliest multi-objective test functions.\n\n    Mathematical formulation:\n        f1(X) = x^2\n        f2(X) = (x - 2)^2\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n            This function uses only the first variable.\n\n    Returns:\n        np.ndarray: Objective values with shape (n_samples, 2) where:\n            - Column 0: f1 values\n            - Column 1: f2 values\n\n    Note:\n        - Number of objectives: 2\n        - Number of variables: 1 (only first variable is used)\n        - Search domain: [-10, 10] or [-A, A]\n        - Pareto front: x \u2208 [0, 2]\n        - Characteristics: Convex, simple, unimodal\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.function.mo import schaffer_n1\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([0.0])\n        &gt;&gt;&gt; result = schaffer_n1(X)\n        &gt;&gt;&gt; result.shape\n        (1, 2)\n        &gt;&gt;&gt; result[0]\n        array([0., 4.])\n\n        &gt;&gt;&gt; X = np.array([[0.0], [1.0], [2.0]])\n        &gt;&gt;&gt; result = schaffer_n1(X)\n        &gt;&gt;&gt; result.shape\n        (3, 2)\n\n    References:\n        Schaffer, J. D. (1985). \"Multiple objective optimization with vector evaluated\n        genetic algorithms.\" In Proceedings of the 1st international Conference on\n        Genetic Algorithms (pp. 93-100).\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n    x = X[:, 0]\n\n    f1 = x**2\n    f2 = (x - 2) ** 2\n\n    return np.column_stack([f1, f2])\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.zdt1","title":"<code>zdt1(X)</code>","text":"<p>ZDT1 multi-objective test function (2 objectives).</p> <p>ZDT1 is a classical bi-objective test problem with a convex Pareto front. It is one of the most widely used benchmark functions for multi-objective optimization.</p> Mathematical formulation <p>f1(X) = x1 f2(X) = g(X) * [1 - sqrt(x1 / g(X))] g(X) = 1 + 9 * sum(x_i for i=2 to n) / (n - 1)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If X has fewer than 2 dimensions.</p> Note <ul> <li>Number of objectives: 2</li> <li>Typical number of variables: 30</li> <li>Search domain: [0, 1]^n</li> <li>Pareto front: Convex, f1 \u2208 [0, 1], f2 = 1 - sqrt(f1)</li> <li>Characteristics: Convex, unimodal</li> </ul> <p>Examples:</p> <p>Single point evaluation:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.mo import zdt1\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; result = zdt1(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n&gt;&gt;&gt; result[0, 0]  # f1\n0.0\n&gt;&gt;&gt; result[0, 1]  # f2\n1.0\n</code></pre> <p>Multiple points evaluation:</p> <pre><code>&gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; result = zdt1(X)\n&gt;&gt;&gt; result.shape\n(3, 2)\n</code></pre> References <p>Zitzler, E., Deb, K., &amp; Thiele, L. (2000). \u201cComparison of multiobjective evolutionary algorithms: Empirical results.\u201d Evolutionary computation, 8(2), 173-195.</p> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def zdt1(X) -&gt; np.ndarray:\n    \"\"\"ZDT1 multi-objective test function (2 objectives).\n\n    ZDT1 is a classical bi-objective test problem with a convex Pareto front.\n    It is one of the most widely used benchmark functions for multi-objective optimization.\n\n    Mathematical formulation:\n        f1(X) = x1\n        f2(X) = g(X) * [1 - sqrt(x1 / g(X))]\n        g(X) = 1 + 9 * sum(x_i for i=2 to n) / (n - 1)\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n            Must have at least 2 dimensions.\n\n    Returns:\n        np.ndarray: Objective values with shape (n_samples, 2) where:\n            - Column 0: f1 values\n            - Column 1: f2 values\n\n    Raises:\n        ValueError: If X has fewer than 2 dimensions.\n\n    Note:\n        - Number of objectives: 2\n        - Typical number of variables: 30\n        - Search domain: [0, 1]^n\n        - Pareto front: Convex, f1 \u2208 [0, 1], f2 = 1 - sqrt(f1)\n        - Characteristics: Convex, unimodal\n\n    Examples:\n        Single point evaluation:\n\n        &gt;&gt;&gt; from spotoptim.function.mo import zdt1\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n        &gt;&gt;&gt; result = zdt1(X)\n        &gt;&gt;&gt; result.shape\n        (1, 2)\n        &gt;&gt;&gt; result[0, 0]  # f1\n        0.0\n        &gt;&gt;&gt; result[0, 1]  # f2\n        1.0\n\n        Multiple points evaluation:\n\n        &gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n        &gt;&gt;&gt; result = zdt1(X)\n        &gt;&gt;&gt; result.shape\n        (3, 2)\n\n    References:\n        Zitzler, E., Deb, K., &amp; Thiele, L. (2000). \"Comparison of multiobjective\n        evolutionary algorithms: Empirical results.\" Evolutionary computation, 8(2), 173-195.\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n\n    if X.shape[1] &lt; 2:\n        raise ValueError(f\"ZDT1 requires at least 2 dimensions, got {X.shape[1]}\")\n\n    n = X.shape[1]\n    f1 = X[:, 0]\n\n    g = 1 + 9 * np.sum(X[:, 1:], axis=1) / (n - 1)\n    f2 = g * (1 - np.sqrt(f1 / g))\n\n    return np.column_stack([f1, f2])\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.zdt2","title":"<code>zdt2(X)</code>","text":"<p>ZDT2 multi-objective test function (2 objectives).</p> <p>ZDT2 is similar to ZDT1 but has a non-convex Pareto front.</p> Mathematical formulation <p>f1(X) = x1 f2(X) = g(X) * [1 - (x1 / g(X))^2] g(X) = 1 + 9 * sum(x_i for i=2 to n) / (n - 1)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If X has fewer than 2 dimensions.</p> Note <ul> <li>Number of objectives: 2</li> <li>Typical number of variables: 30</li> <li>Search domain: [0, 1]^n</li> <li>Pareto front: Non-convex, f1 \u2208 [0, 1], f2 = 1 - f1^2</li> <li>Characteristics: Non-convex, unimodal</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.mo import zdt2\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; result = zdt2(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n</code></pre> References <p>Zitzler, E., Deb, K., &amp; Thiele, L. (2000). \u201cComparison of multiobjective evolutionary algorithms: Empirical results.\u201d Evolutionary computation, 8(2), 173-195.</p> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def zdt2(X) -&gt; np.ndarray:\n    \"\"\"ZDT2 multi-objective test function (2 objectives).\n\n    ZDT2 is similar to ZDT1 but has a non-convex Pareto front.\n\n    Mathematical formulation:\n        f1(X) = x1\n        f2(X) = g(X) * [1 - (x1 / g(X))^2]\n        g(X) = 1 + 9 * sum(x_i for i=2 to n) / (n - 1)\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n            Must have at least 2 dimensions.\n\n    Returns:\n        np.ndarray: Objective values with shape (n_samples, 2) where:\n            - Column 0: f1 values\n            - Column 1: f2 values\n\n    Raises:\n        ValueError: If X has fewer than 2 dimensions.\n\n    Note:\n        - Number of objectives: 2\n        - Typical number of variables: 30\n        - Search domain: [0, 1]^n\n        - Pareto front: Non-convex, f1 \u2208 [0, 1], f2 = 1 - f1^2\n        - Characteristics: Non-convex, unimodal\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.function.mo import zdt2\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n        &gt;&gt;&gt; result = zdt2(X)\n        &gt;&gt;&gt; result.shape\n        (1, 2)\n\n    References:\n        Zitzler, E., Deb, K., &amp; Thiele, L. (2000). \"Comparison of multiobjective\n        evolutionary algorithms: Empirical results.\" Evolutionary computation, 8(2), 173-195.\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n\n    if X.shape[1] &lt; 2:\n        raise ValueError(f\"ZDT2 requires at least 2 dimensions, got {X.shape[1]}\")\n\n    n = X.shape[1]\n    f1 = X[:, 0]\n\n    g = 1 + 9 * np.sum(X[:, 1:], axis=1) / (n - 1)\n    f2 = g * (1 - (f1 / g) ** 2)\n\n    return np.column_stack([f1, f2])\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.zdt3","title":"<code>zdt3(X)</code>","text":"<p>ZDT3 multi-objective test function (2 objectives).</p> <p>ZDT3 has a disconnected (discontinuous) Pareto front, making it more challenging.</p> Mathematical formulation <p>f1(X) = x1 f2(X) = g(X) * [1 - sqrt(x1 / g(X)) - (x1 / g(X)) * sin(10 * \u03c0 * x1)] g(X) = 1 + 9 * sum(x_i for i=2 to n) / (n - 1)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If X has fewer than 2 dimensions.</p> Note <ul> <li>Number of objectives: 2</li> <li>Typical number of variables: 30</li> <li>Search domain: [0, 1]^n</li> <li>Pareto front: Disconnected (5 separate regions)</li> <li>Characteristics: Discontinuous, multimodal</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.mo import zdt3\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; result = zdt3(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n</code></pre> References <p>Zitzler, E., Deb, K., &amp; Thiele, L. (2000). \u201cComparison of multiobjective evolutionary algorithms: Empirical results.\u201d Evolutionary computation, 8(2), 173-195.</p> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def zdt3(X) -&gt; np.ndarray:\n    \"\"\"ZDT3 multi-objective test function (2 objectives).\n\n    ZDT3 has a disconnected (discontinuous) Pareto front, making it more challenging.\n\n    Mathematical formulation:\n        f1(X) = x1\n        f2(X) = g(X) * [1 - sqrt(x1 / g(X)) - (x1 / g(X)) * sin(10 * \u03c0 * x1)]\n        g(X) = 1 + 9 * sum(x_i for i=2 to n) / (n - 1)\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n            Must have at least 2 dimensions.\n\n    Returns:\n        np.ndarray: Objective values with shape (n_samples, 2) where:\n            - Column 0: f1 values\n            - Column 1: f2 values\n\n    Raises:\n        ValueError: If X has fewer than 2 dimensions.\n\n    Note:\n        - Number of objectives: 2\n        - Typical number of variables: 30\n        - Search domain: [0, 1]^n\n        - Pareto front: Disconnected (5 separate regions)\n        - Characteristics: Discontinuous, multimodal\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.function.mo import zdt3\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n        &gt;&gt;&gt; result = zdt3(X)\n        &gt;&gt;&gt; result.shape\n        (1, 2)\n\n    References:\n        Zitzler, E., Deb, K., &amp; Thiele, L. (2000). \"Comparison of multiobjective\n        evolutionary algorithms: Empirical results.\" Evolutionary computation, 8(2), 173-195.\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n\n    if X.shape[1] &lt; 2:\n        raise ValueError(f\"ZDT3 requires at least 2 dimensions, got {X.shape[1]}\")\n\n    n = X.shape[1]\n    f1 = X[:, 0]\n\n    g = 1 + 9 * np.sum(X[:, 1:], axis=1) / (n - 1)\n    f2 = g * (1 - np.sqrt(f1 / g) - (f1 / g) * np.sin(10 * np.pi * f1))\n\n    return np.column_stack([f1, f2])\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.zdt4","title":"<code>zdt4(X)</code>","text":"<p>ZDT4 multi-objective test function (2 objectives).</p> <p>ZDT4 has 21^9 local Pareto fronts, testing the algorithm\u2019s ability to deal with multimodality.</p> Mathematical formulation <p>f1(X) = x1 f2(X) = g(X) * [1 - sqrt(x1 / g(X))] g(X) = 1 + 10 * (n - 1) + sum(x_i^2 - 10 * cos(4 * \u03c0 * x_i) for i=2 to n)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If X has fewer than 2 dimensions.</p> Note <ul> <li>Number of objectives: 2</li> <li>Typical number of variables: 10</li> <li>Search domain: x1 \u2208 [0, 1], x_i \u2208 [-5, 5] for i = 2, \u2026, n</li> <li>Pareto front: Convex, same as ZDT1</li> <li>Characteristics: Multimodal (many local fronts)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.mo import zdt4\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.5, 0.0, 0.0])\n&gt;&gt;&gt; result = zdt4(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n</code></pre> References <p>Zitzler, E., Deb, K., &amp; Thiele, L. (2000). \u201cComparison of multiobjective evolutionary algorithms: Empirical results.\u201d Evolutionary computation, 8(2), 173-195.</p> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def zdt4(X) -&gt; np.ndarray:\n    \"\"\"ZDT4 multi-objective test function (2 objectives).\n\n    ZDT4 has 21^9 local Pareto fronts, testing the algorithm's ability to deal with multimodality.\n\n    Mathematical formulation:\n        f1(X) = x1\n        f2(X) = g(X) * [1 - sqrt(x1 / g(X))]\n        g(X) = 1 + 10 * (n - 1) + sum(x_i^2 - 10 * cos(4 * \u03c0 * x_i) for i=2 to n)\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n            Must have at least 2 dimensions.\n\n    Returns:\n        np.ndarray: Objective values with shape (n_samples, 2) where:\n            - Column 0: f1 values\n            - Column 1: f2 values\n\n    Raises:\n        ValueError: If X has fewer than 2 dimensions.\n\n    Note:\n        - Number of objectives: 2\n        - Typical number of variables: 10\n        - Search domain: x1 \u2208 [0, 1], x_i \u2208 [-5, 5] for i = 2, ..., n\n        - Pareto front: Convex, same as ZDT1\n        - Characteristics: Multimodal (many local fronts)\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.function.mo import zdt4\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([0.5, 0.0, 0.0])\n        &gt;&gt;&gt; result = zdt4(X)\n        &gt;&gt;&gt; result.shape\n        (1, 2)\n\n    References:\n        Zitzler, E., Deb, K., &amp; Thiele, L. (2000). \"Comparison of multiobjective\n        evolutionary algorithms: Empirical results.\" Evolutionary computation, 8(2), 173-195.\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n\n    if X.shape[1] &lt; 2:\n        raise ValueError(f\"ZDT4 requires at least 2 dimensions, got {X.shape[1]}\")\n\n    n = X.shape[1]\n    f1 = X[:, 0]\n\n    g = (\n        1\n        + 10 * (n - 1)\n        + np.sum(X[:, 1:] ** 2 - 10 * np.cos(4 * np.pi * X[:, 1:]), axis=1)\n    )\n    f2 = g * (1 - np.sqrt(f1 / g))\n\n    return np.column_stack([f1, f2])\n</code></pre>"},{"location":"reference/spotoptim/function/mo/#spotoptim.function.mo.zdt6","title":"<code>zdt6(X)</code>","text":"<p>ZDT6 multi-objective test function (2 objectives).</p> <p>ZDT6 has a non-uniform search space with a non-convex Pareto front and low density of solutions near the Pareto front.</p> Mathematical formulation <p>f1(X) = 1 - exp(-4 * x1) * sin^6(6 * \u03c0 * x1) f2(X) = g(X) * [1 - (f1 / g(X))^2] g(X) = 1 + 9 * [sum(x_i for i=2 to n) / (n - 1)]^0.25</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points. Must have at least 2 dimensions.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Objective values with shape (n_samples, 2) where: - Column 0: f1 values - Column 1: f2 values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If X has fewer than 2 dimensions.</p> Note <ul> <li>Number of objectives: 2</li> <li>Typical number of variables: 10</li> <li>Search domain: [0, 1]^n</li> <li>Pareto front: Non-convex, non-uniform density</li> <li>Characteristics: Non-uniform, biased search space</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.mo import zdt6\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.5, 0.5, 0.5])\n&gt;&gt;&gt; result = zdt6(X)\n&gt;&gt;&gt; result.shape\n(1, 2)\n</code></pre> References <p>Zitzler, E., Deb, K., &amp; Thiele, L. (2000). \u201cComparison of multiobjective evolutionary algorithms: Empirical results.\u201d Evolutionary computation, 8(2), 173-195.</p> Source code in <code>src/spotoptim/function/mo.py</code> <pre><code>def zdt6(X) -&gt; np.ndarray:\n    \"\"\"ZDT6 multi-objective test function (2 objectives).\n\n    ZDT6 has a non-uniform search space with a non-convex Pareto front and\n    low density of solutions near the Pareto front.\n\n    Mathematical formulation:\n        f1(X) = 1 - exp(-4 * x1) * sin^6(6 * \u03c0 * x1)\n        f2(X) = g(X) * [1 - (f1 / g(X))^2]\n        g(X) = 1 + 9 * [sum(x_i for i=2 to n) / (n - 1)]^0.25\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n            Must have at least 2 dimensions.\n\n    Returns:\n        np.ndarray: Objective values with shape (n_samples, 2) where:\n            - Column 0: f1 values\n            - Column 1: f2 values\n\n    Raises:\n        ValueError: If X has fewer than 2 dimensions.\n\n    Note:\n        - Number of objectives: 2\n        - Typical number of variables: 10\n        - Search domain: [0, 1]^n\n        - Pareto front: Non-convex, non-uniform density\n        - Characteristics: Non-uniform, biased search space\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.function.mo import zdt6\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([0.5, 0.5, 0.5])\n        &gt;&gt;&gt; result = zdt6(X)\n        &gt;&gt;&gt; result.shape\n        (1, 2)\n\n    References:\n        Zitzler, E., Deb, K., &amp; Thiele, L. (2000). \"Comparison of multiobjective\n        evolutionary algorithms: Empirical results.\" Evolutionary computation, 8(2), 173-195.\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n\n    if X.shape[1] &lt; 2:\n        raise ValueError(f\"ZDT6 requires at least 2 dimensions, got {X.shape[1]}\")\n\n    n = X.shape[1]\n    f1 = 1 - np.exp(-4 * X[:, 0]) * (np.sin(6 * np.pi * X[:, 0]) ** 6)\n\n    g = 1 + 9 * ((np.sum(X[:, 1:], axis=1) / (n - 1)) ** 0.25)\n    f2 = g * (1 - (f1 / g) ** 2)\n\n    return np.column_stack([f1, f2])\n</code></pre>"},{"location":"reference/spotoptim/function/remote/","title":"remote","text":""},{"location":"reference/spotoptim/function/remote/#spotoptim.function.remote.objective_remote","title":"<code>objective_remote(X, url=DEFAULT_SERVER_URL, **kwargs)</code>","text":"<p>Evaluates an objective function remotely via an HTTP POST request.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data of shape (n_samples, n_features).</p> required <code>url</code> <code>str</code> <p>The URL of the remote computation server. Defaults to \u201chttp://139.6.66.164:8000/compute/\u201d.</p> <code>DEFAULT_SERVER_URL</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to include in the request payload (optional).</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The computed objective values of shape (n_samples,).</p> <p>Raises:</p> Type Description <code>RequestException</code> <p>If the remote request fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.remote import objective_remote\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; y = objective_remote(X)\n&gt;&gt;&gt; print(y)\n</code></pre> Source code in <code>src/spotoptim/function/remote.py</code> <pre><code>def objective_remote(\n    X: np.ndarray, url: str = DEFAULT_SERVER_URL, **kwargs\n) -&gt; np.ndarray:\n    \"\"\"\n    Evaluates an objective function remotely via an HTTP POST request.\n\n    Args:\n        X (np.ndarray): Input data of shape (n_samples, n_features).\n        url (str, optional): The URL of the remote computation server.\n            Defaults to \"http://139.6.66.164:8000/compute/\".\n        **kwargs (Any): Additional arguments to include in the request payload (optional).\n\n    Returns:\n        np.ndarray: The computed objective values of shape (n_samples,).\n\n    Raises:\n        requests.exceptions.RequestException: If the remote request fails.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.function.remote import objective_remote\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4]])\n        &gt;&gt;&gt; y = objective_remote(X)\n        &gt;&gt;&gt; print(y)\n    \"\"\"\n    # Prepare the payload\n    # X needs to be converted to a list for JSON serialization\n    payload = {\"X\": np.asarray(X).tolist()}\n\n    # Merge any additional kwargs into the payload\n    if kwargs:\n        payload.update(kwargs)\n\n    # Perform the request\n    response = requests.post(url, json=payload)\n    response.raise_for_status()\n\n    # Parse the response\n    result_data = response.json()\n\n    # Assuming the server returns a dictionary with 'fx' key containing the results\n    # Adjust this key if the server API differs, but based on client.py this is correct.\n    if \"fx\" not in result_data:\n        raise ValueError(f\"Server response missing 'fx' key. Response: {result_data}\")\n\n    return np.array(result_data[\"fx\"])\n</code></pre>"},{"location":"reference/spotoptim/function/so/","title":"so","text":"<p>Analytical single-objective test functions for optimization benchmarking.</p> <p>This module provides well-known analytical test functions commonly used for evaluating and benchmarking optimization algorithms.</p>"},{"location":"reference/spotoptim/function/so/#spotoptim.function.so.ackley","title":"<code>ackley(X)</code>","text":"<p>N-dimensional Ackley function.</p> <p>The Ackley function is a widely used test function for optimization algorithms. It is characterized by a nearly flat outer region and a large hole at the center. The function is multimodal with many local minima but only one global minimum.</p> Mathematical formula <p>f(X) = -a * exp(-b * sqrt(sum(x_i^2) / n)) - exp(sum(cos(c * x_i)) / n) + a + e</p> where <ul> <li>a = 20 (default)</li> <li>b = 0.2 (default)</li> <li>c = 2\u03c0 (default)</li> <li>e = exp(1) \u2248 2.71828</li> <li>n = number of dimensions</li> </ul> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Function values at the input points with shape (n_samples,).</p> Note <ul> <li>Global minimum: f(0, 0, \u2026, 0) = 0</li> <li>Typical search domain: [-32.768, 32.768]^N</li> <li>Characteristics: Non-convex, multimodal, separable</li> </ul> <p>Examples:</p> <p>Single point evaluation at global minimum:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function import ackley\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n&gt;&gt;&gt; ackley(X)\narray([0.])\n</code></pre> <p>Multiple points evaluation:</p> <pre><code>&gt;&gt;&gt; X = np.array([[0.0, 0.0], [1.0, 1.0], [-1.0, 1.0]])\n&gt;&gt;&gt; result = ackley(X)\n&gt;&gt;&gt; result[0]  # Should be close to 0\n0.0\n&gt;&gt;&gt; result[1] &gt; 0  # Should be positive\nTrue\n</code></pre> References <p>Ackley, D. H. (1987). \u201cA connectionist machine for genetic hillclimbing\u201d. Kluwer Academic Publishers.</p> Source code in <code>src/spotoptim/function/so.py</code> <pre><code>def ackley(X) -&gt; np.ndarray:\n    \"\"\"N-dimensional Ackley function.\n\n    The Ackley function is a widely used test function for optimization algorithms.\n    It is characterized by a nearly flat outer region and a large hole at the center.\n    The function is multimodal with many local minima but only one global minimum.\n\n    Mathematical formula:\n        f(X) = -a * exp(-b * sqrt(sum(x_i^2) / n)) - exp(sum(cos(c * x_i)) / n) + a + e\n\n    where:\n        - a = 20 (default)\n        - b = 0.2 (default)\n        - c = 2\u03c0 (default)\n        - e = exp(1) \u2248 2.71828\n        - n = number of dimensions\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n\n    Returns:\n        np.ndarray: Function values at the input points with shape (n_samples,).\n\n    Note:\n        - Global minimum: f(0, 0, ..., 0) = 0\n        - Typical search domain: [-32.768, 32.768]^N\n        - Characteristics: Non-convex, multimodal, separable\n\n    Examples:\n        Single point evaluation at global minimum:\n\n        &gt;&gt;&gt; from spotoptim.function import ackley\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([0.0, 0.0, 0.0])\n        &gt;&gt;&gt; ackley(X)\n        array([0.])\n\n        Multiple points evaluation:\n\n        &gt;&gt;&gt; X = np.array([[0.0, 0.0], [1.0, 1.0], [-1.0, 1.0]])\n        &gt;&gt;&gt; result = ackley(X)\n        &gt;&gt;&gt; result[0]  # Should be close to 0\n        0.0\n        &gt;&gt;&gt; result[1] &gt; 0  # Should be positive\n        True\n\n    References:\n        Ackley, D. H. (1987). \"A connectionist machine for genetic hillclimbing\".\n        Kluwer Academic Publishers.\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n\n    a = 20\n    b = 0.2\n    c = 2 * np.pi\n    n_dim = X.shape[1]\n\n    sum_sq = np.sum(X**2, axis=1)\n    sum_cos = np.sum(np.cos(c * X), axis=1)\n\n    term1 = -a * np.exp(-b * np.sqrt(sum_sq / n_dim))\n    term2 = -np.exp(sum_cos / n_dim)\n\n    return term1 + term2 + a + np.exp(1)\n</code></pre>"},{"location":"reference/spotoptim/function/so/#spotoptim.function.so.lennard_jones","title":"<code>lennard_jones(X, n_atoms=13)</code>","text":"<p>Lennard-Jones Atomic Cluster Potential Energy.</p> <p>Calculates the potential energy of a cluster of N atoms interacting via the Lennard-Jones potential. The optimization problem involves finding the atomic coordinates that minimize the total potential energy. This is a classic benchmark problem known for its high difficulty due to the exponential growth of local minima with N.</p> Input Domain Handling <p>The function accepts inputs in the range [0, 1] and internally maps them to the search domain [-2, 2] for each coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input points. - Shape (n_samples, 3 * n_atoms) for batch evaluation. - Shape (3 * n_atoms,) for single evaluation. The input represents the flattened [x1, y1, z1, x2, y2, z2, \u2026] coordinates.</p> required <code>n_atoms</code> <code>int</code> <p>Number of atoms in the cluster. Defaults to 13.</p> <code>13</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Potential energy values. Shape (n_samples,).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input dimensions do not match 3 * n_atoms.</p> Note <ul> <li>Global minimum for N=13: E \u2248 -44.3268</li> <li>Search domain: [-2, 2]^(3N) (mapped from [0, 1])</li> <li>Characteristics: Extremely rugged landscape, non-convex, many local minima.</li> </ul> <p>Examples:</p> <p>Single point evaluation (random configuration):</p> <pre><code>&gt;&gt;&gt; from spotoptim.function import lennard_jones\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; rng = np.random.default_rng(42)\n&gt;&gt;&gt; X = rng.random(39)  # 13 atoms * 3 coords, in [0, 1]\n&gt;&gt;&gt; lennard_jones(X)\narray([9.5...e+...])\n</code></pre> <p>Batch evaluation:</p> <pre><code>&gt;&gt;&gt; X = rng.random((5, 39))\n&gt;&gt;&gt; lennard_jones(X).shape\n(5,)\n</code></pre> References <p>Wales, D. J., &amp; Doye, J. P. (1997). Global optimization by basin-hopping and the lowest energy structures of Lennard-Jones clusters containing up to 110 atoms. The Journal of Physical Chemistry A, 101(28), 5111-5116.</p> Source code in <code>src/spotoptim/function/so.py</code> <pre><code>def lennard_jones(X: np.ndarray, n_atoms: int = 13) -&gt; np.ndarray:\n    \"\"\"Lennard-Jones Atomic Cluster Potential Energy.\n\n    Calculates the potential energy of a cluster of N atoms interacting via the\n    Lennard-Jones potential. The optimization problem involves finding the atomic\n    coordinates that minimize the total potential energy. This is a classic\n    benchmark problem known for its high difficulty due to the exponential growth\n    of local minima with N.\n\n    Input Domain Handling:\n        The function accepts inputs in the range [0, 1] and internally maps them\n        to the search domain [-2, 2] for each coordinate.\n\n    Args:\n        X (np.ndarray): Input points.\n            - Shape (n_samples, 3 * n_atoms) for batch evaluation.\n            - Shape (3 * n_atoms,) for single evaluation.\n            The input represents the flattened [x1, y1, z1, x2, y2, z2, ...] coordinates.\n        n_atoms (int, optional): Number of atoms in the cluster. Defaults to 13.\n\n    Returns:\n        np.ndarray: Potential energy values. Shape (n_samples,).\n\n    Raises:\n        ValueError: If input dimensions do not match 3 * n_atoms.\n\n    Note:\n        - Global minimum for N=13: E \u2248 -44.3268\n        - Search domain: [-2, 2]^(3N) (mapped from [0, 1])\n        - Characteristics: Extremely rugged landscape, non-convex, many local minima.\n\n    Examples:\n        Single point evaluation (random configuration):\n\n        &gt;&gt;&gt; from spotoptim.function import lennard_jones\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; rng = np.random.default_rng(42)\n        &gt;&gt;&gt; X = rng.random(39)  # 13 atoms * 3 coords, in [0, 1]\n        &gt;&gt;&gt; lennard_jones(X)\n        array([9.5...e+...])\n\n        Batch evaluation:\n\n        &gt;&gt;&gt; X = rng.random((5, 39))\n        &gt;&gt;&gt; lennard_jones(X).shape\n        (5,)\n\n    References:\n        Wales, D. J., &amp; Doye, J. P. (1997). Global optimization by basin-hopping and\n        the lowest energy structures of Lennard-Jones clusters containing up to 110 atoms.\n        The Journal of Physical Chemistry A, 101(28), 5111-5116.\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n    n_samples = X.shape[0]\n    expected_dim = 3 * n_atoms\n\n    if X.shape[1] != expected_dim:\n        raise ValueError(\n            f\"Input dimension must be 3 * n_atoms ({expected_dim}). Got {X.shape[1]}.\"\n        )\n\n    # Scale from [0, 1] to [-2, 2]\n    # [-2, 2] range has length 4.\n    # val_scaled = val_01 * (max - min) + min\n    coords_flattened = X * 4.0 - 2.0\n\n    # Reshape to (n_samples, n_atoms, 3)\n    coords = coords_flattened.reshape(n_samples, n_atoms, 3)\n\n    potential = np.zeros(n_samples)\n\n    # Calculate pairwise interactions\n    # Simple double loop over atoms is often sufficient for typical N (N &lt; ~150)\n    # vectorized along samples.\n    # Potential formula: 4*epsilon * sum_{i&lt;j} [ (sigma/r_ij)^12 - (sigma/r_ij)^6 ]\n    # We use reduced units: epsilon=1, sigma=1.\n\n    for i in range(n_atoms):\n        for j in range(i + 1, n_atoms):\n            # Difference vectors: (n_samples, 3)\n            d_vec = coords[:, i, :] - coords[:, j, :]\n\n            # Squared Euclidean distances: (n_samples,)\n            r2 = np.sum(d_vec**2, axis=1)\n\n            # Avoid division by zero / singularities by setting a lower bound\n            # In optimization, particles can overlap leading to infinity.\n            # We clip at a small value.\n            r2 = np.maximum(r2, 1e-12)\n\n            inv_r2 = 1.0 / r2\n            inv_r6 = inv_r2**3\n            inv_r12 = inv_r6**2\n\n            potential += inv_r12 - inv_r6\n\n    return 4.0 * potential\n</code></pre>"},{"location":"reference/spotoptim/function/so/#spotoptim.function.so.michalewicz","title":"<code>michalewicz(X, m=10)</code>","text":"<p>N-dimensional Michalewicz function.</p> <p>The Michalewicz function is a multimodal test function with steep ridges and valleys. The parameter m defines the steepness of the valleys and ridges. Larger values of m result in more difficult search problems. The number of local minima increases exponentially with the dimension.</p> Mathematical formula <p>f(X) = -sum_{i=1}^{n} sin(x_i) * [sin(i * x_i^2 / \u03c0)]^(2m)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.</p> required <code>m</code> <code>int</code> <p>Steepness parameter. Higher values make the function more difficult to optimize. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Function values at the input points with shape (n_samples,).</p> Note <ul> <li>Global minimum depends on dimension:<ul> <li>2D: f(2.20, 1.57) \u2248 -1.8013</li> <li>5D: f \u2248 -4.687658</li> <li>10D: f \u2248 -9.66015</li> </ul> </li> <li>Typical search domain: [0, \u03c0]^N</li> <li>Characteristics: Non-convex, multimodal, non-separable</li> </ul> <p>Examples:</p> <p>Single point evaluation:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function import michalewicz\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([2.20, 1.57])\n&gt;&gt;&gt; result = michalewicz(X)\n&gt;&gt;&gt; result[0]  # Should be close to -1.8013\n-1.801303...\n</code></pre> <p>Multiple points evaluation:</p> <pre><code>&gt;&gt;&gt; X = np.array([[2.20, 1.57], [1.0, 1.0]])\n&gt;&gt;&gt; michalewicz(X)\narray([-1.8013..., -1.4508...])\n</code></pre> <p>Using different steepness parameter:</p> <pre><code>&gt;&gt;&gt; X = np.array([2.20, 1.57])\n&gt;&gt;&gt; michalewicz(X, m=5)\narray([-1.6862...])\n</code></pre> References <p>Michalewicz, Z. (1996). \u201cGenetic Algorithms + Data Structures = Evolution Programs\u201d. Springer-Verlag.</p> Source code in <code>src/spotoptim/function/so.py</code> <pre><code>def michalewicz(X, m: int = 10) -&gt; np.ndarray:\n    \"\"\"N-dimensional Michalewicz function.\n\n    The Michalewicz function is a multimodal test function with steep ridges and valleys.\n    The parameter m defines the steepness of the valleys and ridges. Larger values of m\n    result in more difficult search problems. The number of local minima increases\n    exponentially with the dimension.\n\n    Mathematical formula:\n        f(X) = -sum_{i=1}^{n} sin(x_i) * [sin(i * x_i^2 / \u03c0)]^(2m)\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n        m (int, optional): Steepness parameter. Higher values make the function more\n            difficult to optimize. Defaults to 10.\n\n    Returns:\n        np.ndarray: Function values at the input points with shape (n_samples,).\n\n    Note:\n        - Global minimum depends on dimension:\n            - 2D: f(2.20, 1.57) \u2248 -1.8013\n            - 5D: f \u2248 -4.687658\n            - 10D: f \u2248 -9.66015\n        - Typical search domain: [0, \u03c0]^N\n        - Characteristics: Non-convex, multimodal, non-separable\n\n    Examples:\n        Single point evaluation:\n\n        &gt;&gt;&gt; from spotoptim.function import michalewicz\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([2.20, 1.57])\n        &gt;&gt;&gt; result = michalewicz(X)\n        &gt;&gt;&gt; result[0]  # Should be close to -1.8013\n        -1.801303...\n\n        Multiple points evaluation:\n\n        &gt;&gt;&gt; X = np.array([[2.20, 1.57], [1.0, 1.0]])\n        &gt;&gt;&gt; michalewicz(X)\n        array([-1.8013..., -1.4508...])\n\n        Using different steepness parameter:\n\n        &gt;&gt;&gt; X = np.array([2.20, 1.57])\n        &gt;&gt;&gt; michalewicz(X, m=5)\n        array([-1.6862...])\n\n    References:\n        Michalewicz, Z. (1996). \"Genetic Algorithms + Data Structures = Evolution Programs\".\n        Springer-Verlag.\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n\n    i = np.arange(1, X.shape[1] + 1)\n    # Broadcasting: (n_samples, n_features)\n    result = -np.sum(np.sin(X) * (np.sin(i * X**2 / np.pi)) ** (2 * m), axis=1)\n\n    return result\n</code></pre>"},{"location":"reference/spotoptim/function/so/#spotoptim.function.so.robot_arm_hard","title":"<code>robot_arm_hard(X)</code>","text":"<p>10-Link Robot Arm with Maze-Like Hard Constraints.</p> <p>A challenging constrained optimization problem where a 10-link planar robot arm must reach a target point (5.0, 5.0) while avoiding multiple obstacles forming a maze-like environment. This function tests an optimizer\u2019s ability to handle hard constraints and navigate complex feasible regions.</p> <p>The problem features three main difficulty factors: 1. \u2018The Great Wall\u2019: A vertical barrier at x=2.5 blocking direct paths 2. \u2018The Ceiling\u2019: A horizontal bar at y=8.5 preventing high loop strategies 3. \u2018The Target Trap\u2019: Obstacles surrounding the target, requiring precise approach</p> Mathematical formulation <p>f(X) = distance_cost + constraint_penalty + energy_regularization</p> where <ul> <li>distance_cost = (x_end - 5.0)^2 + (y_end - 5.0)^2</li> <li>constraint_penalty = 10,000 * sum(max(0, r - d + 0.05)^2) for all joints and obstacles</li> <li>energy_regularization = 0.01 * sum(angles^2)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input points with shape (n_samples, 10) or (10,). Each sample contains 10 joint angles normalized to [0, 1], which are internally mapped to [-1.2\u03c0, 1.2\u03c0] to allow looping strategies. Can be a 1D array for a single point or 2D array for multiple points.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Function values at the input points with shape (n_samples,). Lower values indicate better solutions (closer to target with fewer constraint violations).</p> Note <ul> <li>Dimension: 10 (one angle per link)</li> <li>Link length: L = 1.0 for all links</li> <li>Target position: (5.0, 5.0)</li> <li>Search domain: [0, 1]^10 (mapped internally to [-1.2\u03c0, 1.2\u03c0]^10)</li> <li>Characteristics: Highly constrained, non-convex, multimodal</li> <li>Constraint penalty: 10,000 per violation (effectively hard constraints)</li> <li>Number of obstacles: ~30 forming walls and traps</li> <li>Feasible region: Very small relative to search space</li> </ul> <p>Examples:</p> <p>Single point evaluation with random configuration:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function import robot_arm_hard\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.random.rand(10) * 0.5  # Conservative random angles\n&gt;&gt;&gt; result = robot_arm_hard(X)\n&gt;&gt;&gt; result.shape\n(1,)\n</code></pre> <p>Multiple points evaluation:</p> <pre><code>&gt;&gt;&gt; X = np.array([[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n...               [0.3, 0.4, 0.5, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]])\n&gt;&gt;&gt; robot_arm_hard(X)\narray([...])  # Returns costs for both configurations\n</code></pre> <p>Evaluating a straight configuration (all angles = 0.5, mapped to 0 radians):</p> <pre><code>&gt;&gt;&gt; X_straight = np.full(10, 0.5)\n&gt;&gt;&gt; cost_straight = robot_arm_hard(X_straight)\n&gt;&gt;&gt; cost_straight[0] &gt; 1000  # High cost due to obstacles\nTrue\n</code></pre> References <p>This function is inspired by robot motion planning problems with obstacles, commonly studied in:</p> <ul> <li>LaValle, S. M. (2006). \u201cPlanning Algorithms\u201d. Cambridge University Press.</li> <li>Choset, H., et al. (2005). \u201cPrinciples of Robot Motion: Theory, Algorithms, and Implementations\u201d.   MIT Press.</li> </ul> Source code in <code>src/spotoptim/function/so.py</code> <pre><code>def robot_arm_hard(X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"10-Link Robot Arm with Maze-Like Hard Constraints.\n\n    A challenging constrained optimization problem where a 10-link planar robot arm\n    must reach a target point (5.0, 5.0) while avoiding multiple obstacles forming\n    a maze-like environment. This function tests an optimizer's ability to handle\n    hard constraints and navigate complex feasible regions.\n\n    The problem features three main difficulty factors:\n    1. 'The Great Wall': A vertical barrier at x=2.5 blocking direct paths\n    2. 'The Ceiling': A horizontal bar at y=8.5 preventing high loop strategies\n    3. 'The Target Trap': Obstacles surrounding the target, requiring precise approach\n\n    Mathematical formulation:\n        f(X) = distance_cost + constraint_penalty + energy_regularization\n\n    where:\n        - distance_cost = (x_end - 5.0)^2 + (y_end - 5.0)^2\n        - constraint_penalty = 10,000 * sum(max(0, r - d + 0.05)^2) for all joints and obstacles\n        - energy_regularization = 0.01 * sum(angles^2)\n\n    Args:\n        X (np.ndarray): Input points with shape (n_samples, 10) or (10,).\n            Each sample contains 10 joint angles normalized to [0, 1], which are\n            internally mapped to [-1.2\u03c0, 1.2\u03c0] to allow looping strategies.\n            Can be a 1D array for a single point or 2D array for multiple points.\n\n    Returns:\n        np.ndarray: Function values at the input points with shape (n_samples,).\n            Lower values indicate better solutions (closer to target with fewer\n            constraint violations).\n\n    Note:\n        - Dimension: 10 (one angle per link)\n        - Link length: L = 1.0 for all links\n        - Target position: (5.0, 5.0)\n        - Search domain: [0, 1]^10 (mapped internally to [-1.2\u03c0, 1.2\u03c0]^10)\n        - Characteristics: Highly constrained, non-convex, multimodal\n        - Constraint penalty: 10,000 per violation (effectively hard constraints)\n        - Number of obstacles: ~30 forming walls and traps\n        - Feasible region: Very small relative to search space\n\n    Examples:\n        Single point evaluation with random configuration:\n\n        &gt;&gt;&gt; from spotoptim.function import robot_arm_hard\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.random.rand(10) * 0.5  # Conservative random angles\n        &gt;&gt;&gt; result = robot_arm_hard(X)\n        &gt;&gt;&gt; result.shape\n        (1,)\n\n        Multiple points evaluation:\n\n        &gt;&gt;&gt; X = np.array([[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n        ...               [0.3, 0.4, 0.5, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]])\n        &gt;&gt;&gt; robot_arm_hard(X)\n        array([...])  # Returns costs for both configurations\n\n        Evaluating a straight configuration (all angles = 0.5, mapped to 0 radians):\n\n        &gt;&gt;&gt; X_straight = np.full(10, 0.5)\n        &gt;&gt;&gt; cost_straight = robot_arm_hard(X_straight)\n        &gt;&gt;&gt; cost_straight[0] &gt; 1000  # High cost due to obstacles\n        True\n\n    References:\n        This function is inspired by robot motion planning problems with obstacles,\n        commonly studied in:\n\n        - LaValle, S. M. (2006). \"Planning Algorithms\". Cambridge University Press.\n        - Choset, H., et al. (2005). \"Principles of Robot Motion: Theory, Algorithms, and Implementations\".\n          MIT Press.\n    \"\"\"\n    X = np.atleast_2d(X)\n\n    # Map [0, 1] input to [-pi, pi]\n    # We use a slightly wider range [-1.2*pi, 1.2*pi] to allow 'looping'\n    # strategies which are sometimes necessary to solve hard mazes.\n    Angles = X * 2.4 * np.pi - 1.2 * np.pi\n\n    n_samples = Angles.shape[0]\n\n    # --- Configuration ---\n    L = 1.0\n    target = np.array([5.0, 5.0])\n\n    # Define Obstacles (The Maze)\n    obstacles = []\n\n    # 1. The Great Wall: Vertical barrier at x=2.5, from y=-2 to y=6\n    # This forces the arm to reach 'way over' (high energy) or 'snake around'.\n    # We create it using overlapping circles to form a wall.\n    for y_pos in np.linspace(-2, 6, 15):\n        obstacles.append({\"c\": np.array([2.5, y_pos]), \"r\": 0.6})\n\n    # 2. The Ceiling: A horizontal bar at y=8 to prevent trivial 'high loops'\n    for x_pos in np.linspace(0, 6, 10):\n        obstacles.append({\"c\": np.array([x_pos, 8.5]), \"r\": 0.6})\n\n    # 3. The Target Trap: Surround the target (5,5) with hazards\n    # Leaving only a small opening from the 'bottom-left'.\n    obstacles.append({\"c\": np.array([6.0, 5.0]), \"r\": 0.8})  # Right\n    obstacles.append({\"c\": np.array([5.0, 6.5]), \"r\": 0.8})  # Top\n    obstacles.append(\n        {\"c\": np.array([4.0, 4.0]), \"r\": 0.5}\n    )  # Bottom-Left (Partial Block)\n\n    # --- Forward Kinematics ---\n    # Cumulative angles\n    abs_angles = np.cumsum(Angles, axis=1)\n\n    # Link vectors\n    dx = L * np.cos(abs_angles)\n    dy = L * np.sin(abs_angles)\n\n    # Joint positions (n_samples, 10)\n    jx = np.cumsum(dx, axis=1)\n    jy = np.cumsum(dy, axis=1)\n\n    # --- Cost Calculation ---\n\n    # 1. Distance Cost (End effector to Target)\n    ex, ey = jx[:, -1], jy[:, -1]\n    dist_sq = (ex - target[0]) ** 2 + (ey - target[1]) ** 2\n\n    # 2. Hard Constraint Penalty\n    # We sum penalties for ALL joints against ALL obstacles\n    penalty = np.zeros(n_samples)\n\n    # Vectorized obstacle check could be faster, but loops are clearer for definition\n    for obs in obstacles:\n        ox, oy = obs[\"c\"]\n        r = obs[\"r\"]\n\n        # We check every joint\n        for j in range(10):\n            # Calculate distance from joint j to obstacle center\n            # (Avoid sqrt for speed where possible, but here we need true distance for linear penalty)\n            d_obs = np.sqrt((jx[:, j] - ox) ** 2 + (jy[:, j] - oy) ** 2)\n\n            # Constraint Violation\n            # If d_obs &lt; r, we are inside the obstacle.\n            # We use a \"Hard\" quadratic penalty that shoots up explicitly.\n            violation = np.maximum(0, r - d_obs + 0.05)  # 0.05 buffer 'skin'\n\n            # 10,000 multiplier makes this a \"Death Penalty\" - practically a hard constraint\n            penalty += 10000 * violation**2\n\n    # 3. Regularization (Optional but realistic)\n    # Penalize extreme contortions (minimize sum of squared relative angles)\n    # This smooths the landscape slightly but makes the \"optimal\" path harder to find\n    # because the arm 'wants' to be straight but 'needs' to bend.\n    energy = np.sum(Angles**2, axis=1) * 0.01\n\n    return dist_sq + penalty + energy\n</code></pre>"},{"location":"reference/spotoptim/function/so/#spotoptim.function.so.robot_arm_obstacle","title":"<code>robot_arm_obstacle(X)</code>","text":"<p>10-Link Planar Robot Arm Inverse Kinematics with Obstacle Avoidance.</p> <p>The goal is to minimize the distance of the end-effector to a target point while avoiding collision with a set of circular obstacles. This problem mimics a real-world inverse kinematics solver for a redundant manipulator.</p> Input Domain Handling <p>The function accepts inputs in the range [0, 1] and internally maps them to the search domain [-pi, pi] for each joint angle (radians).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input angles (normalized). - Shape (n_samples, 10). The input contains the normalized relative angles for the 10 links.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Cost values (Weighted sum of Distance + Penalty). Shape (n_samples,).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input dimensions do not match 10.</p> Note <ul> <li>Target: (5.0, 5.0)</li> <li>Obstacles:<ul> <li>Order 1: (2,2), r=1</li> <li>Order 2: (4,3), r=1.5</li> <li>Order 3: (3,6), r=1</li> </ul> </li> <li>Dimensions: 10 (fixed)</li> <li>Search domain: [-pi, pi]^10 (mapped from [0, 1])</li> <li>Characteristics: Multimodal, disjoint feasible regions, constrained.</li> </ul> <p>Examples:</p> <p>Single point evaluation:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.so import robot_arm_obstacle\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; rng = np.random.default_rng(42)\n&gt;&gt;&gt; X = rng.random(10)  # Random angles in [0, 1]\n&gt;&gt;&gt; robot_arm_obstacle(X)\narray([2547...])\n</code></pre> <p>Batch evaluation:</p> <pre><code>&gt;&gt;&gt; X = rng.random((5, 10))\n&gt;&gt;&gt; robot_arm_obstacle(X).shape\n(5,)\n</code></pre> Source code in <code>src/spotoptim/function/so.py</code> <pre><code>def robot_arm_obstacle(X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"10-Link Planar Robot Arm Inverse Kinematics with Obstacle Avoidance.\n\n    The goal is to minimize the distance of the end-effector to a target point\n    while avoiding collision with a set of circular obstacles. This problem mimics\n    a real-world inverse kinematics solver for a redundant manipulator.\n\n    Input Domain Handling:\n        The function accepts inputs in the range [0, 1] and internally maps them\n        to the search domain [-pi, pi] for each joint angle (radians).\n\n    Args:\n        X (np.ndarray): Input angles (normalized).\n            - Shape (n_samples, 10).\n            The input contains the normalized relative angles for the 10 links.\n\n    Returns:\n        np.ndarray: Cost values (Weighted sum of Distance + Penalty). Shape (n_samples,).\n\n    Raises:\n        ValueError: If input dimensions do not match 10.\n\n    Note:\n        - Target: (5.0, 5.0)\n        - Obstacles:\n            - Order 1: (2,2), r=1\n            - Order 2: (4,3), r=1.5\n            - Order 3: (3,6), r=1\n        - Dimensions: 10 (fixed)\n        - Search domain: [-pi, pi]^10 (mapped from [0, 1])\n        - Characteristics: Multimodal, disjoint feasible regions, constrained.\n\n    Examples:\n        Single point evaluation:\n\n        &gt;&gt;&gt; from spotoptim.function.so import robot_arm_obstacle\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; rng = np.random.default_rng(42)\n        &gt;&gt;&gt; X = rng.random(10)  # Random angles in [0, 1]\n        &gt;&gt;&gt; robot_arm_obstacle(X)\n        array([2547...])\n\n        Batch evaluation:\n\n        &gt;&gt;&gt; X = rng.random((5, 10))\n        &gt;&gt;&gt; robot_arm_obstacle(X).shape\n        (5,)\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n    n_samples = X.shape[0]\n    n_dims = X.shape[1]\n\n    if n_dims != 10:\n        raise ValueError(\n            f\"This function requires exactly 10 dimensions (joint angles), got {n_dims}.\"\n        )\n\n    # Scale from [0, 1] to [-pi, pi]\n    # [-pi, pi] range length is 2*pi\n    X_scaled = X * (2 * np.pi) - np.pi\n\n    # --- Problem Configuration ---\n    # Length of each arm link\n    L = np.ones(10) * 1.0\n\n    # Target coordinates (reachable within total length of 10)\n    target = np.array([5.0, 5.0])\n\n    # Obstacles: centers (x,y) and radii (r)\n    obstacles = [\n        {\"center\": np.array([2.0, 2.0]), \"radius\": 1.0},\n        {\"center\": np.array([4.0, 3.0]), \"radius\": 1.5},\n        {\"center\": np.array([3.0, 6.0]), \"radius\": 1.0},\n    ]\n\n    # --- Forward Kinematics ---\n    # X contains relative angles (theta).\n    # Absolute angles (cumulative sum of relative angles)\n    # shape: (n_samples, 10)\n    abs_angles = np.cumsum(X_scaled, axis=1)\n\n    # Calculate (dx, dy) for each link\n    dx = L * np.cos(abs_angles)\n    dy = L * np.sin(abs_angles)\n\n    # Calculate joint coordinates (cumulative sum of dx, dy)\n    # Positions shape: (n_samples, 10) for x and y separately\n    joint_positions_x = np.cumsum(dx, axis=1)\n    joint_positions_y = np.cumsum(dy, axis=1)\n\n    # --- Cost Calculation ---\n\n    # 1. Distance to Target (End-effector only)\n    end_effector_x = joint_positions_x[:, -1]\n    end_effector_y = joint_positions_y[:, -1]\n\n    dist_sq = (end_effector_x - target[0]) ** 2 + (end_effector_y - target[1]) ** 2\n\n    # 2. Obstacle Penalty\n    penalty = np.zeros(n_samples)\n\n    # Check every joint against obstacles\n    # (Simplified: checking joints only, not full link segments)\n    for obs in obstacles:\n        ox, oy = obs[\"center\"]\n        r = obs[\"radius\"]\n\n        # Check all 10 joints for this obstacle\n        for j in range(10):\n            jx = joint_positions_x[:, j]\n            jy = joint_positions_y[:, j]\n\n            # Distance from joint to obstacle center\n            d_obs = np.sqrt((jx - ox) ** 2 + (jy - oy) ** 2)\n\n            # If inside radius + buffer, add large penalty\n            # Using violation metric: max(0, r - d + buffer)\n            # Buffer = 0.1 to discourage grazing\n            violation = np.maximum(0, r - d_obs + 0.1)\n\n            # Penalty weight 1000\n            penalty += 1000 * violation**2\n\n    return dist_sq + penalty\n</code></pre>"},{"location":"reference/spotoptim/function/so/#spotoptim.function.so.rosenbrock","title":"<code>rosenbrock(X)</code>","text":"<p>N-dimensional Rosenbrock function.</p> <p>The Rosenbrock function is a classic test function for optimization algorithms. It is characterized by a long, narrow, parabolic-shaped valley. The global minimum is inside the valley and is hard to find for many algorithms.</p> For the 2D case <p>f(x, y) = (1 - x)^2 + 100 * (y - x^2)^2</p> The generalized form for N dimensions <p>f(X) = sum_{i=1}^{N-1} [100 * (x_{i+1} - x_i^2)^2 + (1 - x_i)^2]</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, n_features) or (n_features,). Can be a 1D array for a single point or 2D array for multiple points.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Function values at the input points with shape (n_samples,).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If X has fewer than 2 dimensions.</p> Note <ul> <li>Global minimum: f(1, 1, \u2026, 1) = 0</li> <li>Typical search domain: [-5, 10]^N or [-2, 2]^N</li> <li>Characteristics: Non-convex, unimodal</li> </ul> <p>Examples:</p> <p>Single point evaluation:</p> <pre><code>&gt;&gt;&gt; from spotoptim.function import rosenbrock\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([1.0, 1.0])\n&gt;&gt;&gt; rosenbrock(X)\narray([0.])\n</code></pre> <p>Multiple points evaluation:</p> <pre><code>&gt;&gt;&gt; X = np.array([[0.0, 0.0], [1.0, 1.0], [0.5, 0.5]])\n&gt;&gt;&gt; rosenbrock(X)\narray([1.00e+00, 0.00e+00, 3.06e+01])\n</code></pre> References <p>Rosenbrock, H.H. (1960). \u201cAn automatic method for finding the greatest or least value of a function\u201d. The Computer Journal. 3 (3): 175\u2013184.</p> Source code in <code>src/spotoptim/function/so.py</code> <pre><code>def rosenbrock(X) -&gt; np.ndarray:\n    \"\"\"N-dimensional Rosenbrock function.\n\n    The Rosenbrock function is a classic test function for optimization algorithms.\n    It is characterized by a long, narrow, parabolic-shaped valley. The global\n    minimum is inside the valley and is hard to find for many algorithms.\n\n    For the 2D case:\n        f(x, y) = (1 - x)^2 + 100 * (y - x^2)^2\n\n    The generalized form for N dimensions:\n        f(X) = sum_{i=1}^{N-1} [100 * (x_{i+1} - x_i^2)^2 + (1 - x_i)^2]\n\n    Args:\n        X (array-like): Input points with shape (n_samples, n_features) or (n_features,).\n            Can be a 1D array for a single point or 2D array for multiple points.\n\n    Returns:\n        np.ndarray: Function values at the input points with shape (n_samples,).\n\n    Raises:\n        ValueError: If X has fewer than 2 dimensions.\n\n    Note:\n        - Global minimum: f(1, 1, ..., 1) = 0\n        - Typical search domain: [-5, 10]^N or [-2, 2]^N\n        - Characteristics: Non-convex, unimodal\n\n    Examples:\n        Single point evaluation:\n\n        &gt;&gt;&gt; from spotoptim.function import rosenbrock\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.array([1.0, 1.0])\n        &gt;&gt;&gt; rosenbrock(X)\n        array([0.])\n\n        Multiple points evaluation:\n\n        &gt;&gt;&gt; X = np.array([[0.0, 0.0], [1.0, 1.0], [0.5, 0.5]])\n        &gt;&gt;&gt; rosenbrock(X)\n        array([1.00e+00, 0.00e+00, 3.06e+01])\n\n    References:\n        Rosenbrock, H.H. (1960). \"An automatic method for finding the\n        greatest or least value of a function\". The Computer Journal.\n        3 (3): 175\u2013184.\n    \"\"\"\n    X = np.atleast_2d(X).astype(float)\n\n    if X.shape[1] &lt; 2:\n        raise ValueError(\n            f\"Rosenbrock function requires at least 2 dimensions, got {X.shape[1]}\"\n        )\n\n    # For 2D case (optimized)\n    if X.shape[1] == 2:\n        x, y = X[:, 0], X[:, 1]\n        return (1 - x) ** 2 + 100 * (y - x**2) ** 2\n\n    # For N-dimensional case\n    result = np.zeros(X.shape[0], dtype=float)\n    for i in range(X.shape[1] - 1):\n        x_i = X[:, i]\n        x_i_plus_1 = X[:, i + 1]\n        result += 100 * (x_i_plus_1 - x_i**2) ** 2 + (1 - x_i) ** 2\n\n    return result\n</code></pre>"},{"location":"reference/spotoptim/function/so/#spotoptim.function.so.wingwt","title":"<code>wingwt(X)</code>","text":"<p>Aircraft Wing Weight function.</p> <p>The example models the weight of an unpainted light aircraft wing. The function accepts inputs in the unit cube [0,1]^9 and returns the wing weight.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input points with shape (n_samples, 9) or (9,) or (10,) or (n_samples, 10). Input variables order: [Sw, Wfw, A, L, q, l, Rtc, Nz, Wdg, Wp(optional)]</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Wing weight values at the input points with shape (n_samples,).</p> <p>Examples:</p> <p>Single point evaluation (Baseline Cessna C172 - Unpainted):</p> <pre><code>&gt;&gt;&gt; from spotoptim.function.so import wingwt\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Baseline configuration in unit cube\n&gt;&gt;&gt; x_base = np.array([0.48, 0.4, 0.38, 0.5, 0.62, 0.344, 0.4, 0.37, 0.38])\n&gt;&gt;&gt; wingwt(x_base)\narray([233.90...])\n</code></pre> <p>Batch evaluation:</p> <pre><code>&gt;&gt;&gt; X = np.vstack([x_base, x_base])\n&gt;&gt;&gt; wingwt(X)\narray([233.90..., 233.90...])\n</code></pre> References <p>Forrester, A., Sobester, A., &amp; Keane, A. (2008). Engineering design via surrogate modelling: a practical guide. John Wiley &amp; Sons.</p> Source code in <code>src/spotoptim/function/so.py</code> <pre><code>def wingwt(X) -&gt; np.ndarray:\n    \"\"\"Aircraft Wing Weight function.\n\n    The example models the weight of an unpainted light aircraft wing.\n    The function accepts inputs in the unit cube [0,1]^9 and returns the wing weight.\n\n    Args:\n        X (array-like): Input points with shape (n_samples, 9) or (9,) or (10,) or (n_samples, 10).\n            Input variables order: [Sw, Wfw, A, L, q, l, Rtc, Nz, Wdg, Wp(optional)]\n\n    Returns:\n        np.ndarray: Wing weight values at the input points with shape (n_samples,).\n\n    Examples:\n        Single point evaluation (Baseline Cessna C172 - Unpainted):\n\n        &gt;&gt;&gt; from spotoptim.function.so import wingwt\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; # Baseline configuration in unit cube\n        &gt;&gt;&gt; x_base = np.array([0.48, 0.4, 0.38, 0.5, 0.62, 0.344, 0.4, 0.37, 0.38])\n        &gt;&gt;&gt; wingwt(x_base)\n        array([233.90...])\n\n        Batch evaluation:\n\n        &gt;&gt;&gt; X = np.vstack([x_base, x_base])\n        &gt;&gt;&gt; wingwt(X)\n        array([233.90..., 233.90...])\n\n    References:\n        Forrester, A., Sobester, A., &amp; Keane, A. (2008). Engineering design via surrogate modelling:\n        a practical guide. John Wiley &amp; Sons.\n    \"\"\"\n    # Ensure x is a 2D array for batch evaluation\n    X = np.atleast_2d(X)\n\n    n_features = X.shape[1]\n    if n_features not in [9, 10]:\n        raise ValueError(f\"wingwt expects 9 or 10 features, got {n_features}\")\n\n    # Transform from unit cube to natural scales\n    Sw = X[:, 0] * (200 - 150) + 150\n    Wfw = X[:, 1] * (300 - 220) + 220\n    A = X[:, 2] * (10 - 6) + 6\n    L = (X[:, 3] * (10 - (-10)) - 10) * np.pi / 180\n    q = X[:, 4] * (45 - 16) + 16\n    taper = X[:, 5] * (1 - 0.5) + 0.5\n    Rtc = X[:, 6] * (0.18 - 0.08) + 0.08\n    Nz = X[:, 7] * (6 - 2.5) + 2.5\n    Wdg = X[:, 8] * (2500 - 1700) + 1700\n\n    # Paint weight (W_p)\n    # Range assumed [0.06, 0.08] based on typical value 0.064 if modeled.\n    # If not provided (9 inputs), assume unpainted (0.0).\n    if n_features == 10:\n        # User is explicitly providing W_p, scale it.\n        # Assuming domain [0.06, 0.08] for the 10th variable \"painted\" case.\n        # This is a guess to support the physics.\n        Wp = X[:, 9] * (0.08 - 0.06) + 0.06\n    else:\n        Wp = 0.0\n\n    # Calculate weight on natural scale\n    W = 0.036 * Sw**0.758 * Wfw**0.0035 * (A / np.cos(L) ** 2) ** 0.6 * q**0.006\n    W = W * taper**0.04 * (100 * Rtc / np.cos(L)) ** (-0.3) * (Nz * Wdg) ** (0.49)\n\n    # Add paint weight term\n    W = W + Sw * Wp\n\n    return W.ravel()\n</code></pre>"},{"location":"reference/spotoptim/function/torch_objective/","title":"torch_objective","text":""},{"location":"reference/spotoptim/function/torch_objective/#spotoptim.function.torch_objective.TorchObjective","title":"<code>TorchObjective</code>","text":"<p>A callable objective function for SpotOptim that trains and evaluates a PyTorch model.</p> Source code in <code>src/spotoptim/function/torch_objective.py</code> <pre><code>class TorchObjective:\n    \"\"\"\n    A callable objective function for SpotOptim that trains and evaluates a PyTorch model.\n    \"\"\"\n\n    def __init__(\n        self,\n        experiment: ExperimentControl,\n        seed: Optional[int] = None,\n        use_scaler: bool = False,\n    ):\n        \"\"\"\n        Initialize the TorchObjective.\n\n        Args:\n            experiment (ExperimentControl): The experiment control object containing configuration,\n                dataset, and hyperparameters.\n            seed (Optional[int]): Random seed for reproducibility. If None, attempst to use\n                experiment.seed. Defaults to None.\n            use_scaler (bool, optional): If True, uses TorchStandardScaler to scale the input data (features).\n                Currently supported/effective only for SpotDataFromArray. Defaults to False.\n\n        Examples:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; import torch.nn as nn\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n            &gt;&gt;&gt; from spotoptim.function.torch_objective import TorchObjective\n            &gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n            &gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 1. Define a simple model\n            &gt;&gt;&gt; class SimpleModel(nn.Module):\n            ...     def __init__(self, input_dim, output_dim, **kwargs):\n            ...         super().__init__()\n            ...         self.fc = nn.Linear(input_dim, output_dim)\n            ...     def forward(self, x):\n            ...         return self.fc(x)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 2. Prepare data\n            &gt;&gt;&gt; X = np.random.rand(10, 2)\n            &gt;&gt;&gt; y = np.random.rand(10, 1)\n            &gt;&gt;&gt; dataset = SpotDataFromArray(X, y)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 3. Define hyperparameters\n            &gt;&gt;&gt; params = ParameterSet()\n            &gt;&gt;&gt; params.add_float(\"lr\", 1e-4, 1e-2, default=1e-3)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 4. Setup Experiment\n            &gt;&gt;&gt; exp = ExperimentControl(\n            ...     name=\"test_exp\",\n            ...     model_class=SimpleModel,\n            ...     dataset=dataset,\n            ...     hyperparameters=params,\n            ...     metrics=[\"val_loss\"],\n            ...     epochs=2,\n            ...     batch_size=2\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 5. Initialize/Instantiate Objective\n            &gt;&gt;&gt; objective = TorchObjective(exp)\n            &gt;&gt;&gt; print(isinstance(objective, TorchObjective))\n            True\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 6. Initialize with Scaler\n            &gt;&gt;&gt; objective_scaled = TorchObjective(exp, use_scaler=True)\n            &gt;&gt;&gt; print(objective_scaled.use_scaler)\n            True\n        \"\"\"\n        self.experiment = experiment\n        self.device = experiment.torch_device\n\n        # Use provided seed, or fall back to experiment seed, or None\n        if seed is not None:\n            self.seed = seed\n        else:\n            exp_seed = getattr(experiment, \"seed\", None)\n            # Ensure it's a valid seed type (int) to avoid issues with Mocks in testing\n            if isinstance(exp_seed, int):\n                self.seed = exp_seed\n            else:\n                self.seed = None\n\n        self.use_scaler = use_scaler\n        self.scaler = TorchStandardScaler() if use_scaler else None\n\n    @property\n    def bounds(self) -&gt; List[Tuple[float, float]]:\n        \"\"\"\n        Returns the bounds of the hyperparameters.\n\n        Returns:\n            List[Tuple[float, float]]: A list of tuples defining the (min, max) bounds for each parameter.\n        \"\"\"\n        return self.experiment.hyperparameters.bounds\n\n    @property\n    def var_type(self) -&gt; List[str]:\n        \"\"\"\n        Returns the types of the hyperparameters.\n\n        Returns:\n            List[str]: A list of strings indicating the type of each parameter (e.g., 'float', 'int', 'factor').\n        \"\"\"\n        return self.experiment.hyperparameters.var_type\n\n    @property\n    def var_name(self) -&gt; List[str]:\n        \"\"\"\n        Returns the names of the hyperparameters.\n\n        Returns:\n            List[str]: A list of parameter names.\n        \"\"\"\n        return self.experiment.hyperparameters.var_name\n\n    @property\n    def var_trans(self) -&gt; List[str]:\n        \"\"\"\n        Returns the transformations of the hyperparameters.\n\n        Returns:\n            List[str]: A list of transformation strings (e.g., 'log', 'linear').\n        \"\"\"\n        return self.experiment.hyperparameters.var_trans\n\n    @property\n    def objective_names(self) -&gt; List[str]:\n        \"\"\"\n        Returns the names of the objectives.\n\n        Returns:\n            List[str]: A list of objective metric names.\n        \"\"\"\n        return self.experiment.metrics\n\n    def _get_hyperparameters(self, X: np.ndarray) -&gt; Dict[str, Any]:\n        \"\"\"\n        Converts the input vector X into a dictionary of hyperparameters.\n\n        This method handles mapping numeric values from the optimization process back to\n        parameter names and types defined in the experiment, including integer and\n        categorical (factor) handling.\n\n        Args:\n            X (np.ndarray): Input parameter vector from the optimizer. Can be 1D or 2D.\n\n        Returns:\n            Dict[str, Any]: A dictionary where keys are parameter names and values are\n                the corresponding values to be passed to the model.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n            &gt;&gt;&gt; from spotoptim.function.torch_objective import TorchObjective\n            &gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Setup parameters\n            &gt;&gt;&gt; params = ParameterSet()\n            &gt;&gt;&gt; params.add_float(\"lr\", 0.001, 0.1)\n            &gt;&gt;&gt; params.add_int(\"batch_size\", 16, 128)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Mock experiment for context (minimal setup)\n            &gt;&gt;&gt; class MockExp:\n            ...     hyperparameters = params\n            ...     torch_device = \"cpu\"\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; objective = TorchObjective(MockExp())\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Input vector matching parameter order (lr, batch_size)\n            &gt;&gt;&gt; X = np.array([0.01, 64.2])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Convert to dict\n            &gt;&gt;&gt; hyp = objective._get_hyperparameters(X)\n            &gt;&gt;&gt; print(hyp)\n            {'lr': 0.01, 'batch_size': 64}\n        \"\"\"\n        names = self.experiment.hyperparameters.names()\n        # Handle case where X is (1, n) or (n,)\n        if X.ndim == 2:\n            X = X[0]\n\n        params = {}\n        for i, name in enumerate(names):\n            val = X[i]\n            # Convert to int if type is int in ParameterSet, but we only have names here.\n            # We assume the user setup SpotOptim with correct types, so X comes in as\n            # floats/ints/indices. For categorical, SpotOptim usually handles mapping if configured,\n            # but if we passed names/bounds to SpotOptim, it handles it.\n            # However, SpotOptim passes values.\n            # We might need to cast to int if the parameter set says so.\n            # For now, let's rely on the model to cast or just pass as is.\n            # Actually, let's look at ParameterSet types if possible.\n            # self.experiment.hyperparameters is a ParameterSet object.\n\n            # Helper to cast if needed\n            p_type = self.experiment.hyperparameters._var_types[i]\n            if p_type == \"int\":\n                val = int(round(val))\n            elif p_type == \"factor\":\n                # If SpotOptim handles factors, it might pass an index or the value?\n                # SpotOptim usually handles factors by mapping to integers (indices).\n                # Implementation details of SpotOptim: \"Mapped to integers: 0 to n_levels - 1\"\n                # So we get an index. define choices.\n                choices = self.experiment.hyperparameters._parameters[i].get(\"choices\")\n                if choices:\n                    if isinstance(val, str):\n                        # SpotOptim passed the string value directly\n                        pass\n                    else:\n                        # SpotOptim passed a numeric index\n                        idx = int(round(val))\n                        # Clamp index just in case\n                        idx = max(0, min(idx, len(choices) - 1))\n                        val = choices[idx]\n\n            params[name] = val\n\n        return params\n\n    def _prepare_data(\n        self, batch_size: Optional[int] = None\n    ) -&gt; tuple[DataLoader, DataLoader]:\n        \"\"\"\n        Prepares DataLoaders from the experiment dataset.\n\n        Based on the data type (Array or TorchDataset), creates appropriate DataLoaders\n        using the provided batch size or the one specified in the experiment.\n\n        Args:\n            batch_size (Optional[int]): Batch size to use. If None, uses self.experiment.batch_size.\n\n        Returns:\n            tuple[DataLoader, DataLoader]: A tuple containing (train_loader, val_loader).\n                val_loader may be None if no validation data is available.\n        \"\"\"\n        data = self.experiment.dataset\n        if batch_size is None:\n            batch_size = self.experiment.batch_size\n        num_workers = self.experiment.num_workers\n\n        train_loader = None\n        val_loader = None\n\n        if isinstance(data, SpotDataFromArray):\n            x_train, y_train = data.get_train_data()\n            x_train = (\n                torch.tensor(x_train, dtype=torch.float32)\n                if not torch.is_tensor(x_train)\n                else x_train.float()\n            )\n            y_train = (\n                torch.tensor(y_train, dtype=torch.float32)\n                if not torch.is_tensor(y_train)\n                else y_train.float()\n            )\n            train_dataset = TensorDataset(x_train, y_train)\n            train_loader = DataLoader(\n                train_dataset,\n                batch_size=batch_size,\n                shuffle=True,\n                num_workers=num_workers,\n            )\n\n            # Fit scaler on training data if enabled\n            if self.scaler is not None:\n                # We need to extract tensor to fit\n                # x_train is already tensor or array\n                self.scaler.fit(\n                    x_train\n                    if torch.is_tensor(x_train)\n                    else torch.tensor(x_train, dtype=torch.float32)\n                )\n\n                # Re-create dataset with transformed data\n                # We can transform on the fly or transform checks\n                # Transform whole dataset\n                if torch.is_tensor(x_train):\n                    x_train = self.scaler.transform(x_train)\n                else:\n                    x_train = self.scaler.transform(\n                        torch.tensor(x_train, dtype=torch.float32)\n                    )\n\n                # Update trainloader\n                train_dataset = TensorDataset(x_train, y_train)\n                train_loader = DataLoader(\n                    train_dataset,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    num_workers=num_workers,\n                )\n\n            val_data = data.get_validation_data()\n            if val_data:\n                x_val, y_val = val_data\n                x_val = (\n                    torch.tensor(x_val, dtype=torch.float32)\n                    if not torch.is_tensor(x_val)\n                    else x_val.float()\n                )\n                y_val = (\n                    torch.tensor(y_val, dtype=torch.float32)\n                    if not torch.is_tensor(y_val)\n                    else y_val.float()\n                )\n\n                # Transform validation data\n                if self.scaler is not None:\n                    x_val = self.scaler.transform(x_val)\n\n                val_dataset = TensorDataset(x_val, y_val)\n                val_loader = DataLoader(\n                    val_dataset,\n                    batch_size=batch_size,\n                    shuffle=False,\n                    num_workers=num_workers,\n                )\n\n        elif isinstance(data, SpotDataFromTorchDataset):\n            train_dataset = data.get_train_data()\n            train_loader = DataLoader(\n                train_dataset,\n                batch_size=batch_size,\n                shuffle=True,\n                num_workers=num_workers,\n            )\n\n            val_dataset = data.get_validation_data()\n            if val_dataset:\n                val_loader = DataLoader(\n                    val_dataset,\n                    batch_size=batch_size,\n                    shuffle=False,\n                    num_workers=num_workers,\n                )\n\n        return train_loader, val_loader\n\n    def train_model(\n        self,\n        model: nn.Module,\n        train_loader: DataLoader,\n        val_loader: Optional[DataLoader],\n        params: Dict[str, Any],\n    ) -&gt; Dict[str, float]:\n        \"\"\"\n        Trains the model and returns a dictionary of metrics.\n\n        Executes the training loop for the specified number of epochs. Handles optimizer\n        creation, loss calculation, backward pass, and validation evaluation.\n\n        Args:\n            model (nn.Module): The PyTorch model to train.\n            train_loader (DataLoader): DataLoader for training data.\n            val_loader (Optional[DataLoader]): DataLoader for validation data (can be None).\n            params (Dict[str, Any]): Hyperparameters dictionary containing 'epochs', 'lr',\n                'optimizer' name, etc.\n\n        Returns:\n            Dict[str, float]: Dictionary containing computed metrics, e.g.,\n                {'val_loss': ..., 'train_loss': ..., 'mse': ..., 'epochs': ...}.\n\n        Examples:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; import torch.nn as nn\n            &gt;&gt;&gt; from torch.utils.data import DataLoader, TensorDataset\n            &gt;&gt;&gt; from spotoptim.function.torch_objective import TorchObjective\n            &gt;&gt;&gt; from unittest.mock import MagicMock\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 1. Create dataset and loader\n            &gt;&gt;&gt; X = torch.randn(10, 2)\n            &gt;&gt;&gt; y = torch.randn(10, 1)\n            &gt;&gt;&gt; loader = DataLoader(TensorDataset(X, y), batch_size=2)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 2. Create model\n            &gt;&gt;&gt; model = nn.Linear(2, 1)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 3. Mock Objective context\n            &gt;&gt;&gt; exp = MagicMock()\n            &gt;&gt;&gt; exp.loss_function = nn.MSELoss()\n            &gt;&gt;&gt; exp.epochs = 1\n            &gt;&gt;&gt; exp.torch_device = \"cpu\"\n            &gt;&gt;&gt; objective = TorchObjective(exp)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 4. Train\n            &gt;&gt;&gt; params = {'lr': 1e-2, 'optimizer': 'Adam'}\n            &gt;&gt;&gt; metrics = objective.train_model(model, loader, None, params)\n            &gt;&gt;&gt; print(f\"Train Loss: {metrics['train_loss']:.4f}\")\n            &gt;&gt;&gt; print(f\"Epochs: {metrics['epochs']}\")\n        \"\"\"\n        # Optimizer\n        lr = params.get(\"lr\", 1e-3)\n        optimizer_name = params.get(\"optimizer\", \"Adam\")\n\n        if hasattr(model, \"get_optimizer\"):\n            optimizer = model.get_optimizer(optimizer_name, lr=lr)\n        else:\n            opt_class = getattr(optim, optimizer_name, optim.Adam)\n            optimizer = opt_class(model.parameters(), lr=lr)\n\n        criterion = self.experiment.loss_function or nn.MSELoss()\n\n        base_epochs = self.experiment.epochs\n        if \"epochs\" in params:\n            epochs = int(params[\"epochs\"])\n        elif base_epochs is not None:\n            epochs = int(base_epochs)\n        else:\n            epochs = 100\n\n        model.to(self.device)\n\n        min_val_loss = float(\"inf\")\n        final_train_loss = 0.0\n\n        # Track actual epochs trained (if early stopping implementation added later)\n        # For now, we train for exact 'epochs'\n        trained_epochs = epochs\n\n        for epoch in range(epochs):\n            model.train()\n            train_loss = 0.0\n            steps = 0\n            for X_batch, y_batch in train_loader:\n                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n\n                optimizer.zero_grad()\n                outputs = model(X_batch)\n                loss = criterion(outputs, y_batch)\n                loss.backward()\n                optimizer.step()\n\n                train_loss += loss.item()\n                steps += 1\n\n            final_train_loss = train_loss / (steps if steps &gt; 0 else 1)\n\n            # Validation\n            if val_loader:\n                model.eval()\n                val_loss = 0.0\n                val_steps = 0\n                with torch.no_grad():\n                    for X_batch, y_batch in val_loader:\n                        X_batch, y_batch = X_batch.to(self.device), y_batch.to(\n                            self.device\n                        )\n                        outputs = model(X_batch)\n                        loss = criterion(outputs, y_batch)\n                        val_loss += loss.item()\n                        val_steps += 1\n\n                if val_steps &gt; 0:\n                    val_loss /= val_steps\n                    if val_loss &lt; min_val_loss:\n                        min_val_loss = val_loss\n            else:\n                # If no validation set, min_val_loss tracks train loss\n                if final_train_loss &lt; min_val_loss:\n                    min_val_loss = final_train_loss\n\n        # Collect metrics\n        metrics_out = {}\n\n        # Default MSE / val_loss\n        metrics_out[\"val_loss\"] = min_val_loss\n        metrics_out[\"train_loss\"] = final_train_loss\n        metrics_out[\"mse\"] = min_val_loss  # Alias\n        metrics_out[\"epochs\"] = float(trained_epochs)\n\n        return metrics_out\n\n    def __call__(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Evaluate the objective function for a given set of parameters.\n\n        This is the main entry point called by the optimizer. It iterates over the\n        samples in X, instantiates and trains a model for each sample, and collects\n        the requested metrics.\n\n        Args:\n            X (np.ndarray): Input array of shape (n_samples, n_params) or (n_params,).\n                Contains the hyperparameter configurations to evaluate.\n\n        Returns:\n            np.ndarray: Array of shape (n_samples, n_metrics) containing the evaluation results.\n\n        Raises:\n            TypeError: If the model class cannot be instantiated with the provided parameters.\n\n        Examples:\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; import torch.nn as nn\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n            &gt;&gt;&gt; from spotoptim.function.torch_objective import TorchObjective\n            &gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n            &gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 1. Define Model\n            &gt;&gt;&gt; class SimpleModel(nn.Module):\n            ...     def __init__(self, input_dim, output_dim, **kwargs):\n            ...         super().__init__()\n            ...         self.fc = nn.Linear(input_dim, output_dim)\n            ...     def forward(self, x):\n            ...         return self.fc(x)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 2. Setup Data &amp; Experiment\n            &gt;&gt;&gt; X_data = np.random.rand(10, 2)\n            &gt;&gt;&gt; y_data = np.random.rand(10, 1)\n            &gt;&gt;&gt; params = ParameterSet().add_float(\"lr\", 1e-4, 1e-2)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; exp = ExperimentControl(\n            ...     name=\"test_call\",\n            ...     model_class=SimpleModel,\n            ...     dataset=SpotDataFromArray(X_data, y_data),\n            ...     hyperparameters=params,\n            ...     metrics=[\"val_loss\"],\n            ...     epochs=1,\n            ...     batch_size=5\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 3. Initialize Objective\n            &gt;&gt;&gt; objective = TorchObjective(exp)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 4. Define input parameters to evaluate (e.g., lr=0.005)\n            &gt;&gt;&gt; X_eval = np.array([[0.005]])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # 5. Evaluate\n            &gt;&gt;&gt; results = objective(X_eval)\n            &gt;&gt;&gt; print(f\"Results shape: {results.shape}\")\n            Results shape: (1, 1)\n            &gt;&gt;&gt; print(f\"Val Loss: {results[0, 0]:.4f}\")\n        \"\"\"\n        X = np.atleast_2d(X)\n        n_samples = X.shape[0]\n        results = []\n\n        train_loader, val_loader = self._prepare_data()\n\n        # Check if batch_size is a tunable parameter\n        batch_size_tunable = \"batch_size\" in self.experiment.hyperparameters.names()\n\n        # If batch_size is NOT tunable, we can reuse the initial loaders for all samples\n        # to avoid overhead. If it IS tunable, we must recreate loaders per sample.\n        recreate_loaders = batch_size_tunable\n\n        # metrics to return\n        requested_metrics = (\n            self.experiment.metrics if self.experiment.metrics else [\"val_loss\"]\n        )\n\n        for i in range(n_samples):\n            # Set seed if available to ensure reproducibility for each evaluation\n            if self.seed is not None:\n                self._set_seed(self.seed)\n\n            # Decode hyperparameters\n            params = self._get_hyperparameters(X[i])\n\n            # Handle batch_size if it's being tuned\n            if recreate_loaders and \"batch_size\" in params:\n                current_batch_size = int(params[\"batch_size\"])\n                train_loader, val_loader = self._prepare_data(\n                    batch_size=current_batch_size\n                )\n\n            # Instantiate model\n            dataset = self.experiment.dataset\n            model_kwargs = {\n                \"input_dim\": dataset.input_dim,\n                \"in_channels\": dataset.input_dim,\n                \"output_dim\": dataset.output_dim,\n            }\n            model_kwargs.update(params)\n\n            # Filter kwargs based on model signature\n            sig = inspect.signature(self.experiment.model_class)\n            has_var_keyword = any(\n                p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values()\n            )\n\n            if has_var_keyword:\n                filtered_kwargs = model_kwargs\n            else:\n                filtered_kwargs = {\n                    k: v for k, v in model_kwargs.items() if k in sig.parameters\n                }\n\n            try:\n                model = self.experiment.model_class(**filtered_kwargs)\n            except TypeError as e:\n                raise TypeError(\n                    f\"Failed to instantiate model {self.experiment.model_class.__name__}: {e}\"\n                )\n\n            metrics_out = self.train_model(model, train_loader, val_loader, params)\n\n            # Extract requested metrics\n            row = []\n            for m in requested_metrics:\n                # Fuzzy match for common names\n                if m.lower() in [\"val_loss\", \"mse\", \"loss\"]:\n                    row.append(metrics_out.get(\"val_loss\", float(\"inf\")))\n                elif m.lower() in [\"epochs\", \"epoch\"]:\n                    row.append(metrics_out.get(\"epochs\", 0.0))\n                else:\n                    # Generic lookup\n                    row.append(metrics_out.get(m, float(\"nan\")))\n\n            results.append(row)\n\n        return np.array(results)\n\n    def _set_seed(self, seed: int):\n        \"\"\"\n        Sets the seed for random number generators.\n\n        Args:\n            seed (int): The seed value.\n        \"\"\"\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed(seed)\n            torch.cuda.manual_seed_all(seed)\n</code></pre>"},{"location":"reference/spotoptim/function/torch_objective/#spotoptim.function.torch_objective.TorchObjective.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Returns the bounds of the hyperparameters.</p> <p>Returns:</p> Type Description <code>List[Tuple[float, float]]</code> <p>List[Tuple[float, float]]: A list of tuples defining the (min, max) bounds for each parameter.</p>"},{"location":"reference/spotoptim/function/torch_objective/#spotoptim.function.torch_objective.TorchObjective.objective_names","title":"<code>objective_names</code>  <code>property</code>","text":"<p>Returns the names of the objectives.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of objective metric names.</p>"},{"location":"reference/spotoptim/function/torch_objective/#spotoptim.function.torch_objective.TorchObjective.var_name","title":"<code>var_name</code>  <code>property</code>","text":"<p>Returns the names of the hyperparameters.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of parameter names.</p>"},{"location":"reference/spotoptim/function/torch_objective/#spotoptim.function.torch_objective.TorchObjective.var_trans","title":"<code>var_trans</code>  <code>property</code>","text":"<p>Returns the transformations of the hyperparameters.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of transformation strings (e.g., \u2018log\u2019, \u2018linear\u2019).</p>"},{"location":"reference/spotoptim/function/torch_objective/#spotoptim.function.torch_objective.TorchObjective.var_type","title":"<code>var_type</code>  <code>property</code>","text":"<p>Returns the types of the hyperparameters.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings indicating the type of each parameter (e.g., \u2018float\u2019, \u2018int\u2019, \u2018factor\u2019).</p>"},{"location":"reference/spotoptim/function/torch_objective/#spotoptim.function.torch_objective.TorchObjective.__call__","title":"<code>__call__(X)</code>","text":"<p>Evaluate the objective function for a given set of parameters.</p> <p>This is the main entry point called by the optimizer. It iterates over the samples in X, instantiates and trains a model for each sample, and collects the requested metrics.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input array of shape (n_samples, n_params) or (n_params,). Contains the hyperparameter configurations to evaluate.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of shape (n_samples, n_metrics) containing the evaluation results.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the model class cannot be instantiated with the provided parameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n&gt;&gt;&gt; from spotoptim.function.torch_objective import TorchObjective\n&gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n&gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 1. Define Model\n&gt;&gt;&gt; class SimpleModel(nn.Module):\n...     def __init__(self, input_dim, output_dim, **kwargs):\n...         super().__init__()\n...         self.fc = nn.Linear(input_dim, output_dim)\n...     def forward(self, x):\n...         return self.fc(x)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 2. Setup Data &amp; Experiment\n&gt;&gt;&gt; X_data = np.random.rand(10, 2)\n&gt;&gt;&gt; y_data = np.random.rand(10, 1)\n&gt;&gt;&gt; params = ParameterSet().add_float(\"lr\", 1e-4, 1e-2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; exp = ExperimentControl(\n...     name=\"test_call\",\n...     model_class=SimpleModel,\n...     dataset=SpotDataFromArray(X_data, y_data),\n...     hyperparameters=params,\n...     metrics=[\"val_loss\"],\n...     epochs=1,\n...     batch_size=5\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 3. Initialize Objective\n&gt;&gt;&gt; objective = TorchObjective(exp)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 4. Define input parameters to evaluate (e.g., lr=0.005)\n&gt;&gt;&gt; X_eval = np.array([[0.005]])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 5. Evaluate\n&gt;&gt;&gt; results = objective(X_eval)\n&gt;&gt;&gt; print(f\"Results shape: {results.shape}\")\nResults shape: (1, 1)\n&gt;&gt;&gt; print(f\"Val Loss: {results[0, 0]:.4f}\")\n</code></pre> Source code in <code>src/spotoptim/function/torch_objective.py</code> <pre><code>def __call__(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Evaluate the objective function for a given set of parameters.\n\n    This is the main entry point called by the optimizer. It iterates over the\n    samples in X, instantiates and trains a model for each sample, and collects\n    the requested metrics.\n\n    Args:\n        X (np.ndarray): Input array of shape (n_samples, n_params) or (n_params,).\n            Contains the hyperparameter configurations to evaluate.\n\n    Returns:\n        np.ndarray: Array of shape (n_samples, n_metrics) containing the evaluation results.\n\n    Raises:\n        TypeError: If the model class cannot be instantiated with the provided parameters.\n\n    Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n        &gt;&gt;&gt; from spotoptim.function.torch_objective import TorchObjective\n        &gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n        &gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 1. Define Model\n        &gt;&gt;&gt; class SimpleModel(nn.Module):\n        ...     def __init__(self, input_dim, output_dim, **kwargs):\n        ...         super().__init__()\n        ...         self.fc = nn.Linear(input_dim, output_dim)\n        ...     def forward(self, x):\n        ...         return self.fc(x)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 2. Setup Data &amp; Experiment\n        &gt;&gt;&gt; X_data = np.random.rand(10, 2)\n        &gt;&gt;&gt; y_data = np.random.rand(10, 1)\n        &gt;&gt;&gt; params = ParameterSet().add_float(\"lr\", 1e-4, 1e-2)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; exp = ExperimentControl(\n        ...     name=\"test_call\",\n        ...     model_class=SimpleModel,\n        ...     dataset=SpotDataFromArray(X_data, y_data),\n        ...     hyperparameters=params,\n        ...     metrics=[\"val_loss\"],\n        ...     epochs=1,\n        ...     batch_size=5\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 3. Initialize Objective\n        &gt;&gt;&gt; objective = TorchObjective(exp)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 4. Define input parameters to evaluate (e.g., lr=0.005)\n        &gt;&gt;&gt; X_eval = np.array([[0.005]])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 5. Evaluate\n        &gt;&gt;&gt; results = objective(X_eval)\n        &gt;&gt;&gt; print(f\"Results shape: {results.shape}\")\n        Results shape: (1, 1)\n        &gt;&gt;&gt; print(f\"Val Loss: {results[0, 0]:.4f}\")\n    \"\"\"\n    X = np.atleast_2d(X)\n    n_samples = X.shape[0]\n    results = []\n\n    train_loader, val_loader = self._prepare_data()\n\n    # Check if batch_size is a tunable parameter\n    batch_size_tunable = \"batch_size\" in self.experiment.hyperparameters.names()\n\n    # If batch_size is NOT tunable, we can reuse the initial loaders for all samples\n    # to avoid overhead. If it IS tunable, we must recreate loaders per sample.\n    recreate_loaders = batch_size_tunable\n\n    # metrics to return\n    requested_metrics = (\n        self.experiment.metrics if self.experiment.metrics else [\"val_loss\"]\n    )\n\n    for i in range(n_samples):\n        # Set seed if available to ensure reproducibility for each evaluation\n        if self.seed is not None:\n            self._set_seed(self.seed)\n\n        # Decode hyperparameters\n        params = self._get_hyperparameters(X[i])\n\n        # Handle batch_size if it's being tuned\n        if recreate_loaders and \"batch_size\" in params:\n            current_batch_size = int(params[\"batch_size\"])\n            train_loader, val_loader = self._prepare_data(\n                batch_size=current_batch_size\n            )\n\n        # Instantiate model\n        dataset = self.experiment.dataset\n        model_kwargs = {\n            \"input_dim\": dataset.input_dim,\n            \"in_channels\": dataset.input_dim,\n            \"output_dim\": dataset.output_dim,\n        }\n        model_kwargs.update(params)\n\n        # Filter kwargs based on model signature\n        sig = inspect.signature(self.experiment.model_class)\n        has_var_keyword = any(\n            p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values()\n        )\n\n        if has_var_keyword:\n            filtered_kwargs = model_kwargs\n        else:\n            filtered_kwargs = {\n                k: v for k, v in model_kwargs.items() if k in sig.parameters\n            }\n\n        try:\n            model = self.experiment.model_class(**filtered_kwargs)\n        except TypeError as e:\n            raise TypeError(\n                f\"Failed to instantiate model {self.experiment.model_class.__name__}: {e}\"\n            )\n\n        metrics_out = self.train_model(model, train_loader, val_loader, params)\n\n        # Extract requested metrics\n        row = []\n        for m in requested_metrics:\n            # Fuzzy match for common names\n            if m.lower() in [\"val_loss\", \"mse\", \"loss\"]:\n                row.append(metrics_out.get(\"val_loss\", float(\"inf\")))\n            elif m.lower() in [\"epochs\", \"epoch\"]:\n                row.append(metrics_out.get(\"epochs\", 0.0))\n            else:\n                # Generic lookup\n                row.append(metrics_out.get(m, float(\"nan\")))\n\n        results.append(row)\n\n    return np.array(results)\n</code></pre>"},{"location":"reference/spotoptim/function/torch_objective/#spotoptim.function.torch_objective.TorchObjective.__init__","title":"<code>__init__(experiment, seed=None, use_scaler=False)</code>","text":"<p>Initialize the TorchObjective.</p> <p>Parameters:</p> Name Type Description Default <code>experiment</code> <code>ExperimentControl</code> <p>The experiment control object containing configuration, dataset, and hyperparameters.</p> required <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility. If None, attempst to use experiment.seed. Defaults to None.</p> <code>None</code> <code>use_scaler</code> <code>bool</code> <p>If True, uses TorchStandardScaler to scale the input data (features). Currently supported/effective only for SpotDataFromArray. Defaults to False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n&gt;&gt;&gt; from spotoptim.function.torch_objective import TorchObjective\n&gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n&gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 1. Define a simple model\n&gt;&gt;&gt; class SimpleModel(nn.Module):\n...     def __init__(self, input_dim, output_dim, **kwargs):\n...         super().__init__()\n...         self.fc = nn.Linear(input_dim, output_dim)\n...     def forward(self, x):\n...         return self.fc(x)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 2. Prepare data\n&gt;&gt;&gt; X = np.random.rand(10, 2)\n&gt;&gt;&gt; y = np.random.rand(10, 1)\n&gt;&gt;&gt; dataset = SpotDataFromArray(X, y)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 3. Define hyperparameters\n&gt;&gt;&gt; params = ParameterSet()\n&gt;&gt;&gt; params.add_float(\"lr\", 1e-4, 1e-2, default=1e-3)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 4. Setup Experiment\n&gt;&gt;&gt; exp = ExperimentControl(\n...     name=\"test_exp\",\n...     model_class=SimpleModel,\n...     dataset=dataset,\n...     hyperparameters=params,\n...     metrics=[\"val_loss\"],\n...     epochs=2,\n...     batch_size=2\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 5. Initialize/Instantiate Objective\n&gt;&gt;&gt; objective = TorchObjective(exp)\n&gt;&gt;&gt; print(isinstance(objective, TorchObjective))\nTrue\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 6. Initialize with Scaler\n&gt;&gt;&gt; objective_scaled = TorchObjective(exp, use_scaler=True)\n&gt;&gt;&gt; print(objective_scaled.use_scaler)\nTrue\n</code></pre> Source code in <code>src/spotoptim/function/torch_objective.py</code> <pre><code>def __init__(\n    self,\n    experiment: ExperimentControl,\n    seed: Optional[int] = None,\n    use_scaler: bool = False,\n):\n    \"\"\"\n    Initialize the TorchObjective.\n\n    Args:\n        experiment (ExperimentControl): The experiment control object containing configuration,\n            dataset, and hyperparameters.\n        seed (Optional[int]): Random seed for reproducibility. If None, attempst to use\n            experiment.seed. Defaults to None.\n        use_scaler (bool, optional): If True, uses TorchStandardScaler to scale the input data (features).\n            Currently supported/effective only for SpotDataFromArray. Defaults to False.\n\n    Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.core.experiment import ExperimentControl\n        &gt;&gt;&gt; from spotoptim.function.torch_objective import TorchObjective\n        &gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n        &gt;&gt;&gt; from spotoptim.core.data import SpotDataFromArray\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 1. Define a simple model\n        &gt;&gt;&gt; class SimpleModel(nn.Module):\n        ...     def __init__(self, input_dim, output_dim, **kwargs):\n        ...         super().__init__()\n        ...         self.fc = nn.Linear(input_dim, output_dim)\n        ...     def forward(self, x):\n        ...         return self.fc(x)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 2. Prepare data\n        &gt;&gt;&gt; X = np.random.rand(10, 2)\n        &gt;&gt;&gt; y = np.random.rand(10, 1)\n        &gt;&gt;&gt; dataset = SpotDataFromArray(X, y)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 3. Define hyperparameters\n        &gt;&gt;&gt; params = ParameterSet()\n        &gt;&gt;&gt; params.add_float(\"lr\", 1e-4, 1e-2, default=1e-3)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 4. Setup Experiment\n        &gt;&gt;&gt; exp = ExperimentControl(\n        ...     name=\"test_exp\",\n        ...     model_class=SimpleModel,\n        ...     dataset=dataset,\n        ...     hyperparameters=params,\n        ...     metrics=[\"val_loss\"],\n        ...     epochs=2,\n        ...     batch_size=2\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 5. Initialize/Instantiate Objective\n        &gt;&gt;&gt; objective = TorchObjective(exp)\n        &gt;&gt;&gt; print(isinstance(objective, TorchObjective))\n        True\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 6. Initialize with Scaler\n        &gt;&gt;&gt; objective_scaled = TorchObjective(exp, use_scaler=True)\n        &gt;&gt;&gt; print(objective_scaled.use_scaler)\n        True\n    \"\"\"\n    self.experiment = experiment\n    self.device = experiment.torch_device\n\n    # Use provided seed, or fall back to experiment seed, or None\n    if seed is not None:\n        self.seed = seed\n    else:\n        exp_seed = getattr(experiment, \"seed\", None)\n        # Ensure it's a valid seed type (int) to avoid issues with Mocks in testing\n        if isinstance(exp_seed, int):\n            self.seed = exp_seed\n        else:\n            self.seed = None\n\n    self.use_scaler = use_scaler\n    self.scaler = TorchStandardScaler() if use_scaler else None\n</code></pre>"},{"location":"reference/spotoptim/function/torch_objective/#spotoptim.function.torch_objective.TorchObjective.train_model","title":"<code>train_model(model, train_loader, val_loader, params)</code>","text":"<p>Trains the model and returns a dictionary of metrics.</p> <p>Executes the training loop for the specified number of epochs. Handles optimizer creation, loss calculation, backward pass, and validation evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The PyTorch model to train.</p> required <code>train_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>val_loader</code> <code>Optional[DataLoader]</code> <p>DataLoader for validation data (can be None).</p> required <code>params</code> <code>Dict[str, Any]</code> <p>Hyperparameters dictionary containing \u2018epochs\u2019, \u2018lr\u2019, \u2018optimizer\u2019 name, etc.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Dictionary containing computed metrics, e.g., {\u2018val_loss\u2019: \u2026, \u2018train_loss\u2019: \u2026, \u2018mse\u2019: \u2026, \u2018epochs\u2019: \u2026}.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; from torch.utils.data import DataLoader, TensorDataset\n&gt;&gt;&gt; from spotoptim.function.torch_objective import TorchObjective\n&gt;&gt;&gt; from unittest.mock import MagicMock\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 1. Create dataset and loader\n&gt;&gt;&gt; X = torch.randn(10, 2)\n&gt;&gt;&gt; y = torch.randn(10, 1)\n&gt;&gt;&gt; loader = DataLoader(TensorDataset(X, y), batch_size=2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 2. Create model\n&gt;&gt;&gt; model = nn.Linear(2, 1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 3. Mock Objective context\n&gt;&gt;&gt; exp = MagicMock()\n&gt;&gt;&gt; exp.loss_function = nn.MSELoss()\n&gt;&gt;&gt; exp.epochs = 1\n&gt;&gt;&gt; exp.torch_device = \"cpu\"\n&gt;&gt;&gt; objective = TorchObjective(exp)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 4. Train\n&gt;&gt;&gt; params = {'lr': 1e-2, 'optimizer': 'Adam'}\n&gt;&gt;&gt; metrics = objective.train_model(model, loader, None, params)\n&gt;&gt;&gt; print(f\"Train Loss: {metrics['train_loss']:.4f}\")\n&gt;&gt;&gt; print(f\"Epochs: {metrics['epochs']}\")\n</code></pre> Source code in <code>src/spotoptim/function/torch_objective.py</code> <pre><code>def train_model(\n    self,\n    model: nn.Module,\n    train_loader: DataLoader,\n    val_loader: Optional[DataLoader],\n    params: Dict[str, Any],\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Trains the model and returns a dictionary of metrics.\n\n    Executes the training loop for the specified number of epochs. Handles optimizer\n    creation, loss calculation, backward pass, and validation evaluation.\n\n    Args:\n        model (nn.Module): The PyTorch model to train.\n        train_loader (DataLoader): DataLoader for training data.\n        val_loader (Optional[DataLoader]): DataLoader for validation data (can be None).\n        params (Dict[str, Any]): Hyperparameters dictionary containing 'epochs', 'lr',\n            'optimizer' name, etc.\n\n    Returns:\n        Dict[str, float]: Dictionary containing computed metrics, e.g.,\n            {'val_loss': ..., 'train_loss': ..., 'mse': ..., 'epochs': ...}.\n\n    Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt; from torch.utils.data import DataLoader, TensorDataset\n        &gt;&gt;&gt; from spotoptim.function.torch_objective import TorchObjective\n        &gt;&gt;&gt; from unittest.mock import MagicMock\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 1. Create dataset and loader\n        &gt;&gt;&gt; X = torch.randn(10, 2)\n        &gt;&gt;&gt; y = torch.randn(10, 1)\n        &gt;&gt;&gt; loader = DataLoader(TensorDataset(X, y), batch_size=2)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 2. Create model\n        &gt;&gt;&gt; model = nn.Linear(2, 1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 3. Mock Objective context\n        &gt;&gt;&gt; exp = MagicMock()\n        &gt;&gt;&gt; exp.loss_function = nn.MSELoss()\n        &gt;&gt;&gt; exp.epochs = 1\n        &gt;&gt;&gt; exp.torch_device = \"cpu\"\n        &gt;&gt;&gt; objective = TorchObjective(exp)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # 4. Train\n        &gt;&gt;&gt; params = {'lr': 1e-2, 'optimizer': 'Adam'}\n        &gt;&gt;&gt; metrics = objective.train_model(model, loader, None, params)\n        &gt;&gt;&gt; print(f\"Train Loss: {metrics['train_loss']:.4f}\")\n        &gt;&gt;&gt; print(f\"Epochs: {metrics['epochs']}\")\n    \"\"\"\n    # Optimizer\n    lr = params.get(\"lr\", 1e-3)\n    optimizer_name = params.get(\"optimizer\", \"Adam\")\n\n    if hasattr(model, \"get_optimizer\"):\n        optimizer = model.get_optimizer(optimizer_name, lr=lr)\n    else:\n        opt_class = getattr(optim, optimizer_name, optim.Adam)\n        optimizer = opt_class(model.parameters(), lr=lr)\n\n    criterion = self.experiment.loss_function or nn.MSELoss()\n\n    base_epochs = self.experiment.epochs\n    if \"epochs\" in params:\n        epochs = int(params[\"epochs\"])\n    elif base_epochs is not None:\n        epochs = int(base_epochs)\n    else:\n        epochs = 100\n\n    model.to(self.device)\n\n    min_val_loss = float(\"inf\")\n    final_train_loss = 0.0\n\n    # Track actual epochs trained (if early stopping implementation added later)\n    # For now, we train for exact 'epochs'\n    trained_epochs = epochs\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n        steps = 0\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            steps += 1\n\n        final_train_loss = train_loss / (steps if steps &gt; 0 else 1)\n\n        # Validation\n        if val_loader:\n            model.eval()\n            val_loss = 0.0\n            val_steps = 0\n            with torch.no_grad():\n                for X_batch, y_batch in val_loader:\n                    X_batch, y_batch = X_batch.to(self.device), y_batch.to(\n                        self.device\n                    )\n                    outputs = model(X_batch)\n                    loss = criterion(outputs, y_batch)\n                    val_loss += loss.item()\n                    val_steps += 1\n\n            if val_steps &gt; 0:\n                val_loss /= val_steps\n                if val_loss &lt; min_val_loss:\n                    min_val_loss = val_loss\n        else:\n            # If no validation set, min_val_loss tracks train loss\n            if final_train_loss &lt; min_val_loss:\n                min_val_loss = final_train_loss\n\n    # Collect metrics\n    metrics_out = {}\n\n    # Default MSE / val_loss\n    metrics_out[\"val_loss\"] = min_val_loss\n    metrics_out[\"train_loss\"] = final_train_loss\n    metrics_out[\"mse\"] = min_val_loss  # Alias\n    metrics_out[\"epochs\"] = float(trained_epochs)\n\n    return metrics_out\n</code></pre>"},{"location":"reference/spotoptim/hyperparameters/parameters/","title":"parameters","text":""},{"location":"reference/spotoptim/hyperparameters/parameters/#spotoptim.hyperparameters.parameters.ParameterSet","title":"<code>ParameterSet</code>","text":"<p>User-friendly interface for defining hyperparameters.</p> <p>This class allows for the definition of a set of hyperparameters including their types, bounds, default values, and transformations. It supports float, integer, and categorical parameters and provides a fluent interface for chaining parameter definitions.</p> <p>Attributes:</p> Name Type Description <code>_parameters</code> <code>List[Dict]</code> <p>List of parameter definitions.</p> <code>_var_names</code> <code>List[str]</code> <p>List of parameter names.</p> <code>_var_types</code> <code>List[str]</code> <p>List of parameter types (\u2018float\u2019, \u2018int\u2019, \u2018factor\u2019).</p> <code>_bounds</code> <code>List[Union[Tuple, List]]</code> <p>List of bounds for each parameter.</p> <code>_defaults</code> <code>Dict[str, Any]</code> <p>Dictionary of default values.</p> <code>_var_trans</code> <code>List[Optional[str]]</code> <p>List of variable transformations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ps = ParameterSet()\n&gt;&gt;&gt; ps.add_float(\"max_depth\", 1, 10, default=3)\nParameterSet(\n    max_depth=Parameter(name='max_depth', type='float', check_on_set=True, bounds=(1, 10), default=3),\n)\n</code></pre> Source code in <code>src/spotoptim/hyperparameters/parameters.py</code> <pre><code>class ParameterSet:\n    \"\"\"\n    User-friendly interface for defining hyperparameters.\n\n    This class allows for the definition of a set of hyperparameters including their types,\n    bounds, default values, and transformations. It supports float, integer, and categorical\n    parameters and provides a fluent interface for chaining parameter definitions.\n\n    Attributes:\n        _parameters (List[Dict]): List of parameter definitions.\n        _var_names (List[str]): List of parameter names.\n        _var_types (List[str]): List of parameter types ('float', 'int', 'factor').\n        _bounds (List[Union[Tuple, List]]): List of bounds for each parameter.\n        _defaults (Dict[str, Any]): Dictionary of default values.\n        _var_trans (List[Optional[str]]): List of variable transformations.\n\n    Examples:\n        &gt;&gt;&gt; ps = ParameterSet()\n        &gt;&gt;&gt; ps.add_float(\"max_depth\", 1, 10, default=3)\n        ParameterSet(\n            max_depth=Parameter(name='max_depth', type='float', check_on_set=True, bounds=(1, 10), default=3),\n        )\n    \"\"\"\n\n    def __init__(self):\n        self._parameters = []\n        self._var_names = []\n        self._var_types = []\n        self._bounds = []\n        self._defaults = {}\n        self._var_trans = []\n\n    def add_float(\n        self,\n        name: str,\n        low: float,\n        high: float,\n        default: Optional[float] = None,\n        transform: Optional[str] = None,\n    ) -&gt; \"ParameterSet\":\n        \"\"\"Add a float hyperparameter.\n\n        Args:\n            name: Name of the parameter.\n            low: Lower bound of the parameter range.\n            high: Upper bound of the parameter range.\n            default: Default value for the parameter.\n            transform: Transformation string, e.g., \"log\", \"log(x)\", \"pow(x, 2)\".\n\n        Returns:\n            ParameterSet: The instance itself to allow method chaining.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n            &gt;&gt;&gt; ps = ParameterSet()\n            &gt;&gt;&gt; ps.add_float(\"learning_rate\", 0.0001, 0.1, default=0.01, transform=\"log\")\n        \"\"\"\n        self._parameters.append(\n            {\n                \"name\": name,\n                \"type\": \"float\",\n                \"low\": low,\n                \"high\": high,\n                \"default\": default,\n                \"transform\": transform,\n            }\n        )\n        self._var_names.append(name)\n        self._var_types.append(\"float\")\n        self._bounds.append((low, high))\n        self._var_trans.append(transform)\n        if default is not None:\n            self._defaults[name] = default\n        return self\n\n    def add_int(\n        self,\n        name: str,\n        low: int,\n        high: int,\n        default: Optional[int] = None,\n        transform: Optional[str] = None,\n    ) -&gt; \"ParameterSet\":\n        \"\"\"Add an integer hyperparameter.\n\n        Args:\n            name: Name of the parameter.\n            low: Lower bound of the parameter range (inclusive).\n            high: Upper bound of the parameter range (inclusive).\n            default: Default value for the parameter.\n            transform: Transformation string, e.g., \"log\", \"log(x)\", \"pow(x, 2)\".\n\n        Returns:\n            ParameterSet: The instance itself to allow method chaining.\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n            &gt;&gt;&gt; ps = ParameterSet()\n            &gt;&gt;&gt; ps.add_int(\"n_estimators\", 10, 100, default=50)\n            &gt;&gt;&gt; ps.add_int(\"epochs\", 2, 5, transform=\"log\")\n        \"\"\"\n        self._parameters.append(\n            {\n                \"name\": name,\n                \"type\": \"int\",\n                \"low\": low,\n                \"high\": high,\n                \"default\": default,\n                \"transform\": transform,\n            }\n        )\n        self._var_names.append(name)\n        self._var_types.append(\"int\")\n        self._bounds.append((low, high))\n        self._var_trans.append(transform)\n        if default is not None:\n            self._defaults[name] = default\n        return self\n\n    def add_factor(\n        self, name: str, choices: List[str], default: Optional[str] = None\n    ) -&gt; \"ParameterSet\":\n        \"\"\"Add a factor (categorical) hyperparameter.\n\n        Args:\n            name: Name of the parameter.\n            choices: List of possible values for the parameter.\n            default: Default value for the parameter.\n\n        Returns:\n            ParameterSet: The instance itself to allow method chaining.\n\n\n        Examples:\n            &gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n            &gt;&gt;&gt; ps = ParameterSet()\n            &gt;&gt;&gt; ps.add_factor(\"optimizer\", [\"adam\", \"sgd\"], default=\"adam\")\n        \"\"\"\n        self._parameters.append(\n            {\"name\": name, \"type\": \"factor\", \"choices\": choices, \"default\": default}\n        )\n        self._var_names.append(name)\n        self._var_types.append(\"factor\")\n        # For SpotOptim, categorical bounds are the list of choices (for factor detection)\n        # SpotOptim checks if bound is tuple/list of strings.\n        self._bounds.append(choices)\n        self._var_trans.append(None)\n        if default is not None:\n            self._defaults[name] = default\n        return self\n\n    @property\n    def bounds(\n        self,\n    ) -&gt; List[Union[Tuple[float, float], Tuple[int, int], Tuple[str, ...]]]:\n        \"\"\"Returns bounds formatted for SpotOptim.\n\n        Returns:\n            List[Union[Tuple[float, float], Tuple[int, int], Tuple[str, ...]]]:\n            A list of bounds where each element corresponds to a parameter.\n        \"\"\"\n        return self._bounds\n\n    @property\n    def var_type(self) -&gt; List[str]:\n        \"\"\"Returns variable types formatted for SpotOptim.\n\n        Returns:\n            List[str]: A list of strings representing the types of parameters\n            ('float', 'int', 'factor').\n        \"\"\"\n        return self._var_types\n\n    @property\n    def var_name(self) -&gt; List[str]:\n        \"\"\"Returns variable names.\n\n        Returns:\n            List[str]: A list of parameter names.\n        \"\"\"\n        return self._var_names\n\n    @property\n    def var_trans(self) -&gt; List[Optional[str]]:\n        \"\"\"Returns variable transformations.\n\n        Returns:\n            List[Optional[str]]: A list of transformation strings or None for each parameter.\n        \"\"\"\n        return self._var_trans\n\n    def sample_default(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns the default configuration.\n\n        Returns:\n            Dict[str, Any]: A dictionary mapping parameter names to their default values.\n\n        Examples:\n            &gt;&gt;&gt; ps = ParameterSet()\n            &gt;&gt;&gt; ps.add_int(\"x\", 1, 10, default=5)\n            &gt;&gt;&gt; ps.sample_default()\n            {'x': 5}\n        \"\"\"\n        return self._defaults.copy()\n\n    def names(self) -&gt; List[str]:\n        \"\"\"Returns a list of parameter names.\n\n        Returns:\n            List[str]: The names of the parameters.\n        \"\"\"\n        return self._var_names\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of the ParameterSet.\n\n        Returns:\n            str: A formatted string showing the parameters in the set.\n        \"\"\"\n        lines = [\"ParameterSet(\"]\n        for p in self._parameters:\n            name = p[\"name\"]\n\n            # Construct Bounds based on type\n            if p[\"type\"] == \"factor\":\n                # For factor, bounds are simply the list of choices\n                choices = p[\"choices\"]\n                bounds = choices\n            else:\n                b = Bounds(low=p[\"low\"], high=p[\"high\"])\n                bounds = b\n\n            param_obj = Parameter(\n                name=name,\n                var_name=name,\n                bounds=bounds,\n                default=p.get(\"default\"),\n                transform=p.get(\"transform\"),\n                type=p[\"type\"],\n            )\n\n            # Indent parameter repr\n            p_str = repr(param_obj).replace(\"\\n\", \"\\n    \")\n            lines.append(f\"    {name}={p_str},\")\n\n        lines.append(\")\")\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/spotoptim/hyperparameters/parameters/#spotoptim.hyperparameters.parameters.ParameterSet.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Returns bounds formatted for SpotOptim.</p> <p>Returns:</p> Type Description <code>List[Union[Tuple[float, float], Tuple[int, int], Tuple[str, ...]]]</code> <p>List[Union[Tuple[float, float], Tuple[int, int], Tuple[str, \u2026]]]:</p> <code>List[Union[Tuple[float, float], Tuple[int, int], Tuple[str, ...]]]</code> <p>A list of bounds where each element corresponds to a parameter.</p>"},{"location":"reference/spotoptim/hyperparameters/parameters/#spotoptim.hyperparameters.parameters.ParameterSet.var_name","title":"<code>var_name</code>  <code>property</code>","text":"<p>Returns variable names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of parameter names.</p>"},{"location":"reference/spotoptim/hyperparameters/parameters/#spotoptim.hyperparameters.parameters.ParameterSet.var_trans","title":"<code>var_trans</code>  <code>property</code>","text":"<p>Returns variable transformations.</p> <p>Returns:</p> Type Description <code>List[Optional[str]]</code> <p>List[Optional[str]]: A list of transformation strings or None for each parameter.</p>"},{"location":"reference/spotoptim/hyperparameters/parameters/#spotoptim.hyperparameters.parameters.ParameterSet.var_type","title":"<code>var_type</code>  <code>property</code>","text":"<p>Returns variable types formatted for SpotOptim.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings representing the types of parameters</p> <code>List[str]</code> <p>(\u2018float\u2019, \u2018int\u2019, \u2018factor\u2019).</p>"},{"location":"reference/spotoptim/hyperparameters/parameters/#spotoptim.hyperparameters.parameters.ParameterSet.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of the ParameterSet.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string showing the parameters in the set.</p> Source code in <code>src/spotoptim/hyperparameters/parameters.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the ParameterSet.\n\n    Returns:\n        str: A formatted string showing the parameters in the set.\n    \"\"\"\n    lines = [\"ParameterSet(\"]\n    for p in self._parameters:\n        name = p[\"name\"]\n\n        # Construct Bounds based on type\n        if p[\"type\"] == \"factor\":\n            # For factor, bounds are simply the list of choices\n            choices = p[\"choices\"]\n            bounds = choices\n        else:\n            b = Bounds(low=p[\"low\"], high=p[\"high\"])\n            bounds = b\n\n        param_obj = Parameter(\n            name=name,\n            var_name=name,\n            bounds=bounds,\n            default=p.get(\"default\"),\n            transform=p.get(\"transform\"),\n            type=p[\"type\"],\n        )\n\n        # Indent parameter repr\n        p_str = repr(param_obj).replace(\"\\n\", \"\\n    \")\n        lines.append(f\"    {name}={p_str},\")\n\n    lines.append(\")\")\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/spotoptim/hyperparameters/parameters/#spotoptim.hyperparameters.parameters.ParameterSet.add_factor","title":"<code>add_factor(name, choices, default=None)</code>","text":"<p>Add a factor (categorical) hyperparameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the parameter.</p> required <code>choices</code> <code>List[str]</code> <p>List of possible values for the parameter.</p> required <code>default</code> <code>Optional[str]</code> <p>Default value for the parameter.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ParameterSet</code> <code>ParameterSet</code> <p>The instance itself to allow method chaining.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n&gt;&gt;&gt; ps = ParameterSet()\n&gt;&gt;&gt; ps.add_factor(\"optimizer\", [\"adam\", \"sgd\"], default=\"adam\")\n</code></pre> Source code in <code>src/spotoptim/hyperparameters/parameters.py</code> <pre><code>def add_factor(\n    self, name: str, choices: List[str], default: Optional[str] = None\n) -&gt; \"ParameterSet\":\n    \"\"\"Add a factor (categorical) hyperparameter.\n\n    Args:\n        name: Name of the parameter.\n        choices: List of possible values for the parameter.\n        default: Default value for the parameter.\n\n    Returns:\n        ParameterSet: The instance itself to allow method chaining.\n\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n        &gt;&gt;&gt; ps = ParameterSet()\n        &gt;&gt;&gt; ps.add_factor(\"optimizer\", [\"adam\", \"sgd\"], default=\"adam\")\n    \"\"\"\n    self._parameters.append(\n        {\"name\": name, \"type\": \"factor\", \"choices\": choices, \"default\": default}\n    )\n    self._var_names.append(name)\n    self._var_types.append(\"factor\")\n    # For SpotOptim, categorical bounds are the list of choices (for factor detection)\n    # SpotOptim checks if bound is tuple/list of strings.\n    self._bounds.append(choices)\n    self._var_trans.append(None)\n    if default is not None:\n        self._defaults[name] = default\n    return self\n</code></pre>"},{"location":"reference/spotoptim/hyperparameters/parameters/#spotoptim.hyperparameters.parameters.ParameterSet.add_float","title":"<code>add_float(name, low, high, default=None, transform=None)</code>","text":"<p>Add a float hyperparameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the parameter.</p> required <code>low</code> <code>float</code> <p>Lower bound of the parameter range.</p> required <code>high</code> <code>float</code> <p>Upper bound of the parameter range.</p> required <code>default</code> <code>Optional[float]</code> <p>Default value for the parameter.</p> <code>None</code> <code>transform</code> <code>Optional[str]</code> <p>Transformation string, e.g., \u201clog\u201d, \u201clog(x)\u201d, \u201cpow(x, 2)\u201d.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ParameterSet</code> <code>ParameterSet</code> <p>The instance itself to allow method chaining.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n&gt;&gt;&gt; ps = ParameterSet()\n&gt;&gt;&gt; ps.add_float(\"learning_rate\", 0.0001, 0.1, default=0.01, transform=\"log\")\n</code></pre> Source code in <code>src/spotoptim/hyperparameters/parameters.py</code> <pre><code>def add_float(\n    self,\n    name: str,\n    low: float,\n    high: float,\n    default: Optional[float] = None,\n    transform: Optional[str] = None,\n) -&gt; \"ParameterSet\":\n    \"\"\"Add a float hyperparameter.\n\n    Args:\n        name: Name of the parameter.\n        low: Lower bound of the parameter range.\n        high: Upper bound of the parameter range.\n        default: Default value for the parameter.\n        transform: Transformation string, e.g., \"log\", \"log(x)\", \"pow(x, 2)\".\n\n    Returns:\n        ParameterSet: The instance itself to allow method chaining.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n        &gt;&gt;&gt; ps = ParameterSet()\n        &gt;&gt;&gt; ps.add_float(\"learning_rate\", 0.0001, 0.1, default=0.01, transform=\"log\")\n    \"\"\"\n    self._parameters.append(\n        {\n            \"name\": name,\n            \"type\": \"float\",\n            \"low\": low,\n            \"high\": high,\n            \"default\": default,\n            \"transform\": transform,\n        }\n    )\n    self._var_names.append(name)\n    self._var_types.append(\"float\")\n    self._bounds.append((low, high))\n    self._var_trans.append(transform)\n    if default is not None:\n        self._defaults[name] = default\n    return self\n</code></pre>"},{"location":"reference/spotoptim/hyperparameters/parameters/#spotoptim.hyperparameters.parameters.ParameterSet.add_int","title":"<code>add_int(name, low, high, default=None, transform=None)</code>","text":"<p>Add an integer hyperparameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the parameter.</p> required <code>low</code> <code>int</code> <p>Lower bound of the parameter range (inclusive).</p> required <code>high</code> <code>int</code> <p>Upper bound of the parameter range (inclusive).</p> required <code>default</code> <code>Optional[int]</code> <p>Default value for the parameter.</p> <code>None</code> <code>transform</code> <code>Optional[str]</code> <p>Transformation string, e.g., \u201clog\u201d, \u201clog(x)\u201d, \u201cpow(x, 2)\u201d.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ParameterSet</code> <code>ParameterSet</code> <p>The instance itself to allow method chaining.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n&gt;&gt;&gt; ps = ParameterSet()\n&gt;&gt;&gt; ps.add_int(\"n_estimators\", 10, 100, default=50)\n&gt;&gt;&gt; ps.add_int(\"epochs\", 2, 5, transform=\"log\")\n</code></pre> Source code in <code>src/spotoptim/hyperparameters/parameters.py</code> <pre><code>def add_int(\n    self,\n    name: str,\n    low: int,\n    high: int,\n    default: Optional[int] = None,\n    transform: Optional[str] = None,\n) -&gt; \"ParameterSet\":\n    \"\"\"Add an integer hyperparameter.\n\n    Args:\n        name: Name of the parameter.\n        low: Lower bound of the parameter range (inclusive).\n        high: Upper bound of the parameter range (inclusive).\n        default: Default value for the parameter.\n        transform: Transformation string, e.g., \"log\", \"log(x)\", \"pow(x, 2)\".\n\n    Returns:\n        ParameterSet: The instance itself to allow method chaining.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.hyperparameters import ParameterSet\n        &gt;&gt;&gt; ps = ParameterSet()\n        &gt;&gt;&gt; ps.add_int(\"n_estimators\", 10, 100, default=50)\n        &gt;&gt;&gt; ps.add_int(\"epochs\", 2, 5, transform=\"log\")\n    \"\"\"\n    self._parameters.append(\n        {\n            \"name\": name,\n            \"type\": \"int\",\n            \"low\": low,\n            \"high\": high,\n            \"default\": default,\n            \"transform\": transform,\n        }\n    )\n    self._var_names.append(name)\n    self._var_types.append(\"int\")\n    self._bounds.append((low, high))\n    self._var_trans.append(transform)\n    if default is not None:\n        self._defaults[name] = default\n    return self\n</code></pre>"},{"location":"reference/spotoptim/hyperparameters/parameters/#spotoptim.hyperparameters.parameters.ParameterSet.names","title":"<code>names()</code>","text":"<p>Returns a list of parameter names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The names of the parameters.</p> Source code in <code>src/spotoptim/hyperparameters/parameters.py</code> <pre><code>def names(self) -&gt; List[str]:\n    \"\"\"Returns a list of parameter names.\n\n    Returns:\n        List[str]: The names of the parameters.\n    \"\"\"\n    return self._var_names\n</code></pre>"},{"location":"reference/spotoptim/hyperparameters/parameters/#spotoptim.hyperparameters.parameters.ParameterSet.sample_default","title":"<code>sample_default()</code>","text":"<p>Returns the default configuration.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary mapping parameter names to their default values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ps = ParameterSet()\n&gt;&gt;&gt; ps.add_int(\"x\", 1, 10, default=5)\n&gt;&gt;&gt; ps.sample_default()\n{'x': 5}\n</code></pre> Source code in <code>src/spotoptim/hyperparameters/parameters.py</code> <pre><code>def sample_default(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns the default configuration.\n\n    Returns:\n        Dict[str, Any]: A dictionary mapping parameter names to their default values.\n\n    Examples:\n        &gt;&gt;&gt; ps = ParameterSet()\n        &gt;&gt;&gt; ps.add_int(\"x\", 1, 10, default=5)\n        &gt;&gt;&gt; ps.sample_default()\n        {'x': 5}\n    \"\"\"\n    return self._defaults.copy()\n</code></pre>"},{"location":"reference/spotoptim/hyperparameters/repr_helpers/","title":"repr_helpers","text":""},{"location":"reference/spotoptim/inspection/importance/","title":"importance","text":""},{"location":"reference/spotoptim/inspection/importance/#spotoptim.inspection.importance.generate_imp","title":"<code>generate_imp(X_train, X_test, y_train, y_test, random_state=42, n_repeats=10, use_test=True)</code>","text":"<p>Generates permutation importances from a RandomForestRegressor.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>DataFrame or ndarray</code> <p>The training feature set.</p> required <code>X_test</code> <code>DataFrame or ndarray</code> <p>The test feature set.</p> required <code>y_train</code> <code>Series or ndarray</code> <p>The training target variable.</p> required <code>y_test</code> <code>Series or ndarray</code> <p>The test target variable.</p> required <code>random_state</code> <code>int</code> <p>Random state for the RandomForestRegressor. Defaults to 42.</p> <code>42</code> <code>n_repeats</code> <code>int</code> <p>Number of repeats for permutation importance. Defaults to 10.</p> <code>10</code> <code>use_test</code> <code>bool</code> <p>If True, computes permutation importance on the test set. If False, uses the training set. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>permutation_importance</code> <code>permutation_importance</code> <p>Permutation importances object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.sensitivity.importance import generate_imp\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; X_train, X_test = X[:80], X[80:]\n&gt;&gt;&gt; y_train, y_test = y[:80], y[80:]\n&gt;&gt;&gt; X_train_df = pd.DataFrame(X_train)\n&gt;&gt;&gt; X_test_df = pd.DataFrame(X_test)\n&gt;&gt;&gt; y_train_series = pd.Series(y_train)\n&gt;&gt;&gt; y_test_series = pd.Series(y_test)\n&gt;&gt;&gt; perm_imp = generate_imp(X_train_df, X_test_df, y_train_series, y_test_series)\n&gt;&gt;&gt; print(perm_imp)\n</code></pre> Source code in <code>src/spotoptim/inspection/importance.py</code> <pre><code>def generate_imp(\n    X_train, X_test, y_train, y_test, random_state=42, n_repeats=10, use_test=True\n) -&gt; permutation_importance:\n    \"\"\"\n    Generates permutation importances from a RandomForestRegressor.\n\n    Args:\n        X_train (pd.DataFrame or np.ndarray): The training feature set.\n        X_test (pd.DataFrame or np.ndarray): The test feature set.\n        y_train (pd.Series or np.ndarray): The training target variable.\n        y_test (pd.Series or np.ndarray): The test target variable.\n        random_state (int, optional): Random state for the RandomForestRegressor. Defaults to 42.\n        n_repeats (int, optional): Number of repeats for permutation importance. Defaults to 10.\n        use_test (bool, optional): If True, computes permutation importance on the test set. If False, uses the training set. Defaults to True.\n\n    Returns:\n        permutation_importance: Permutation importances object.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.sensitivity.importance import generate_imp\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.datasets import make_regression\n        &gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n        &gt;&gt;&gt; X_train, X_test = X[:80], X[80:]\n        &gt;&gt;&gt; y_train, y_test = y[:80], y[80:]\n        &gt;&gt;&gt; X_train_df = pd.DataFrame(X_train)\n        &gt;&gt;&gt; X_test_df = pd.DataFrame(X_test)\n        &gt;&gt;&gt; y_train_series = pd.Series(y_train)\n        &gt;&gt;&gt; y_test_series = pd.Series(y_test)\n        &gt;&gt;&gt; perm_imp = generate_imp(X_train_df, X_test_df, y_train_series, y_test_series)\n        &gt;&gt;&gt; print(perm_imp)\n    \"\"\"\n    # Convert inputs to pandas DataFrames/Series if they are not already\n    if not isinstance(X_train, pd.DataFrame):\n        X_train = pd.DataFrame(X_train)\n    if not isinstance(X_test, pd.DataFrame):\n        X_test = pd.DataFrame(X_test)\n    if not isinstance(y_train, pd.Series):\n        y_train = pd.Series(np.ravel(y_train))  # Use np.ravel instead of flatten\n    if not isinstance(y_test, pd.Series):\n        y_test = pd.Series(np.ravel(y_test))  # Use np.ravel instead of flatten\n\n    # Train a Random Forest Regressor\n    rf = RandomForestRegressor(random_state=random_state)\n    rf.fit(X_train, y_train)\n\n    # Select the dataset for permutation importance\n    X_eval = X_test if use_test else X_train\n    y_eval = y_test if use_test else y_train\n\n    # Calculate permutation importances\n    perm_imp = permutation_importance(\n        rf, X_eval, y_eval, n_repeats=n_repeats, random_state=random_state\n    )\n\n    return perm_imp\n</code></pre>"},{"location":"reference/spotoptim/inspection/importance/#spotoptim.inspection.importance.generate_mdi","title":"<code>generate_mdi(X, y, feature_names=None, random_state=42)</code>","text":"<p>Generates a DataFrame with Gini importances from a RandomForestRegressor.</p> Notes <p>There are two limitations of impurity-based feature importances:    - impurity-based importances are biased towards high cardinality features;    - impurity-based importances are computed on training set statistics    and therefore do not reflect the ability of feature to be useful to    make predictions that generalize to the test set. Permutation    importances can mitigate the last limitation, because ti can be computed on the    test set.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame or ndarray</code> <p>The feature set.</p> required <code>y</code> <code>Series or ndarray</code> <p>The target variable.</p> required <code>feature_names</code> <code>list</code> <p>List of feature names for labeling. Defaults to None.</p> <code>None</code> <code>random_state</code> <code>int</code> <p>Random state for the RandomForestRegressor. Defaults to 42.</p> <code>42</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with \u2018Feature\u2019 and \u2018Importance\u2019 columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.sensitivity.importance import generate_mdi\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; X_df = pd.DataFrame(X)\n&gt;&gt;&gt; y_series = pd.Series(y)\n&gt;&gt;&gt; result = generate_mdi(X_df, y_series)\n&gt;&gt;&gt; print(result)\n</code></pre> Source code in <code>src/spotoptim/inspection/importance.py</code> <pre><code>def generate_mdi(X, y, feature_names=None, random_state=42) -&gt; pd.DataFrame:\n    \"\"\"\n    Generates a DataFrame with Gini importances from a RandomForestRegressor.\n\n    Notes:\n     There are two limitations of impurity-based feature importances:\n        - impurity-based importances are biased towards high cardinality features;\n        - impurity-based importances are computed on training set statistics\n        and therefore do not reflect the ability of feature to be useful to\n        make predictions that generalize to the test set. Permutation\n        importances can mitigate the last limitation, because ti can be computed on the\n        test set.\n\n    Args:\n        X (pd.DataFrame or np.ndarray): The feature set.\n        y (pd.Series or np.ndarray): The target variable.\n        feature_names (list, optional): List of feature names for labeling. Defaults to None.\n        random_state (int, optional): Random state for the RandomForestRegressor. Defaults to 42.\n\n    Returns:\n        pd.DataFrame: DataFrame with 'Feature' and 'Importance' columns.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.sensitivity.importance import generate_mdi\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.datasets import make_regression\n        &gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n        &gt;&gt;&gt; X_df = pd.DataFrame(X)\n        &gt;&gt;&gt; y_series = pd.Series(y)\n        &gt;&gt;&gt; result = generate_mdi(X_df, y_series)\n        &gt;&gt;&gt; print(result)\n\n    \"\"\"\n    # Convert X and y to pandas DataFrames if they are not already\n    if not isinstance(X, pd.DataFrame):\n        X = pd.DataFrame(X)\n    if not isinstance(y, pd.Series):\n        y = pd.Series(np.ravel(y))  # Use np.ravel instead of flatten\n\n    # Train a Random Forest Regressor\n    rf = RandomForestRegressor(random_state=random_state)\n    rf.fit(X, y)\n\n    # Get feature importances\n    importances = rf.feature_importances_\n\n    # Create a DataFrame\n    if feature_names is None:\n        df_mdi = pd.DataFrame({\"Feature\": X.columns, \"Importance\": importances})\n    else:\n        df_mdi = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n    df_mdi = df_mdi.sort_values(\"Importance\", ascending=False).reset_index(drop=True)\n\n    return df_mdi\n</code></pre>"},{"location":"reference/spotoptim/inspection/importance/#spotoptim.inspection.importance.plot_feature_importances","title":"<code>plot_feature_importances(X, y, feature_names, target_names, target_index, n_top_features=10, figsize=(6, 6))</code>","text":"<p>Generate and plot feature importances using MDI and permutation importance.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input features array</p> required <code>y</code> <code>ndarray</code> <p>Target array</p> required <code>feature_names</code> <code>list</code> <p>List of feature names</p> required <code>target_names</code> <code>list</code> <p>List of target names</p> required <code>target_index</code> <code>int</code> <p>Index of target variable to analyze</p> required <code>n_top_features</code> <code>int</code> <p>Number of top features to show</p> <code>10</code> <code>figsize</code> <code>tuple</code> <p>Size of the figure</p> <code>(6, 6)</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>(top_features, importance_df)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.sensitivity import plot_feature_importances\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n&gt;&gt;&gt; target_names = [\"target\"]\n&gt;&gt;&gt; top_features, imp_df = plot_feature_importances(X, y, feature_names, target_names, target_index=0)\n&gt;&gt;&gt; print(\"Top features:\", top_features)\n</code></pre> Source code in <code>src/spotoptim/inspection/importance.py</code> <pre><code>def plot_feature_importances(\n    X: np.ndarray,\n    y: np.ndarray,\n    feature_names: list,\n    target_names: list,\n    target_index: int,\n    n_top_features: int = 10,\n    figsize: tuple = (6, 6),\n) -&gt; tuple:\n    \"\"\"\n    Generate and plot feature importances using MDI and permutation importance.\n\n    Args:\n        X: Input features array\n        y: Target array\n        feature_names: List of feature names\n        target_names: List of target names\n        target_index: Index of target variable to analyze\n        n_top_features: Number of top features to show\n        figsize: Size of the figure\n\n    Returns:\n        tuple: (top_features, importance_df)\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.sensitivity import plot_feature_importances\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.datasets import make_regression\n        &gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n        &gt;&gt;&gt; feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n        &gt;&gt;&gt; target_names = [\"target\"]\n        &gt;&gt;&gt; top_features, imp_df = plot_feature_importances(X, y, feature_names, target_names, target_index=0)\n        &gt;&gt;&gt; print(\"Top features:\", top_features)\n    \"\"\"\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, shuffle=True\n    )\n\n    # Convert y_train and y_test to numpy arrays if they're DataFrames\n    if isinstance(y_train, pd.DataFrame):\n        y_train = y_train.iloc[:, target_index].values\n    if isinstance(y_test, pd.DataFrame):\n        y_test = y_test.iloc[:, target_index].values\n\n    # Generate feature importances\n    df_mdi = generate_mdi(\n        X_train, y_train, feature_names=feature_names, random_state=42\n    )\n    perm_imp = generate_imp(\n        X_train, X_test, y_train, y_test, random_state=42, n_repeats=10, use_test=True\n    )\n\n    # Plot importances\n    plot_importances(\n        df_mdi,\n        perm_imp,\n        X_test,\n        target_name=target_names[target_index],\n        feature_names=feature_names,\n        k=n_top_features,\n        figsize=figsize,\n        show=True,\n    )\n\n    # Convert permutation importance to DataFrame and get top features\n    imp_df = pd.DataFrame(\n        {\"feature\": feature_names, \"importance\": perm_imp.importances_mean}\n    )\n    top_features = imp_df.nlargest(n_top_features, \"importance\")[\"feature\"].tolist()\n\n    return top_features, imp_df\n</code></pre>"},{"location":"reference/spotoptim/inspection/importance/#spotoptim.inspection.importance.plot_feature_scatter_matrix","title":"<code>plot_feature_scatter_matrix(X, y, feature_names, target_names, top_features, target_index, figsize=(6, 6))</code>","text":"<p>Generate scatter plot matrix for the most important features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input features array</p> required <code>y</code> <code>ndarray</code> <p>Target array</p> required <code>feature_names</code> <code>list</code> <p>List of feature names</p> required <code>target_names</code> <code>list</code> <p>List of target names</p> required <code>top_features</code> <code>list</code> <p>List of top feature names to include</p> required <code>target_index</code> <code>int</code> <p>Index of target variable to analyze</p> required <code>figsize</code> <code>tuple</code> <p>Size of the figure</p> <code>(6, 6)</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.sensitivity import plot_feature_scatter_matrix\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n&gt;&gt;&gt; target_names = [\"target\"]\n&gt;&gt;&gt; top_features = [\"feature_0\", \"feature_1\", \"feature_2\"]\n&gt;&gt;&gt; plot_feature_scatter_matrix(X, y, feature_names, target_names, top_features, target_index=0)\n</code></pre> Source code in <code>src/spotoptim/inspection/importance.py</code> <pre><code>def plot_feature_scatter_matrix(\n    X: np.ndarray,\n    y: np.ndarray,\n    feature_names: list,\n    target_names: list,\n    top_features: list,\n    target_index: int,\n    figsize: tuple = (6, 6),\n) -&gt; None:\n    \"\"\"\n    Generate scatter plot matrix for the most important features.\n\n    Args:\n        X: Input features array\n        y: Target array\n        feature_names: List of feature names\n        target_names: List of target names\n        top_features: List of top feature names to include\n        target_index: Index of target variable to analyze\n        figsize: Size of the figure\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.sensitivity import plot_feature_scatter_matrix\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.datasets import make_regression\n        &gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n        &gt;&gt;&gt; feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n        &gt;&gt;&gt; target_names = [\"target\"]\n        &gt;&gt;&gt; top_features = [\"feature_0\", \"feature_1\", \"feature_2\"]\n        &gt;&gt;&gt; plot_feature_scatter_matrix(X, y, feature_names, target_names, top_features, target_index=0)\n    \"\"\"\n    # Create DataFrame with top features and target\n    X_top = pd.DataFrame(X, columns=feature_names)[top_features]\n    y_df = pd.DataFrame(y).iloc[:, target_index]\n    # Exclude empty entries before concatenation to avoid FutureWarning\n    to_concat = [X_top, y_df]\n    to_concat = [df for df in to_concat if not df.empty]\n    plot_df = pd.concat(to_concat, axis=1)\n\n    # Create scatter plot matrix\n    plt.figure(figsize=figsize)\n    sns.set_style(\"ticks\")\n    scatter_matrix = sns.pairplot(plot_df, diag_kind=\"kde\", plot_kws={\"alpha\": 0.6})\n\n    # Customize the plot\n    scatter_matrix.fig.suptitle(\n        f\"Scatter Plot Matrix of Top {len(top_features)} Features and Target {target_names[target_index]}\",\n        y=1.02,\n        size=12,\n    )\n\n    # Add gridlines\n    for ax in scatter_matrix.axes.flat:\n        ax.grid(True, linestyle=\"--\", alpha=0.7)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/spotoptim/inspection/importance/#spotoptim.inspection.importance.plot_importances","title":"<code>plot_importances(df_mdi, perm_imp, X_test, target_name=None, feature_names=None, k=10, figsize=(12, 8), show=True)</code>","text":"<p>Plots the impurity-based and permutation-based feature importances for a given classifier.</p> <p>Parameters:</p> Name Type Description Default <code>df_mdi</code> <code>DataFrame</code> <p>DataFrame with Gini importances.</p> required <code>perm_imp</code> <code>object</code> <p>Permutation importances object.</p> required <code>X_test</code> <code>DataFrame</code> <p>The test feature set for permutation importance.</p> required <code>target_name</code> <code>str</code> <p>Name of the target variable for labeling. Defaults to None.</p> <code>None</code> <code>feature_names</code> <code>list</code> <p>List of feature names for labeling. Defaults to None.</p> <code>None</code> <code>k</code> <code>int</code> <p>Number of top features to display based on importance. Default is 10.</p> <code>10</code> <code>figsize</code> <code>tuple</code> <p>Size of the figure (width, height) in inches. Default is (12, 8).</p> <code>(12, 8)</code> <code>show</code> <code>bool</code> <p>If True, displays the plot immediately. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.sensitivity.importance import generate_mdi, generate_imp, plot_importances\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n&gt;&gt;&gt; X_train, X_test = X[:80], X[80:]\n&gt;&gt;&gt; y_train, y_test = y[:80], y[80:]\n&gt;&gt;&gt; X_train_df = pd.DataFrame(X_train)\n&gt;&gt;&gt; X_test_df = pd.DataFrame(X_test)\n&gt;&gt;&gt; y_train_series = pd.Series(y_train)\n&gt;&gt;&gt; y_test_series = pd.Series(y_test)\n&gt;&gt;&gt; df_mdi = generate_mdi(X_train_df, y_train_series)\n&gt;&gt;&gt; perm_imp = generate_imp(X_train_df, X_test_df, y_train_series, y_test_series)\n&gt;&gt;&gt; plot_importances(df_mdi, perm_imp, X_test_df, figsize=(15, 10))\n</code></pre> Source code in <code>src/spotoptim/inspection/importance.py</code> <pre><code>def plot_importances(\n    df_mdi,\n    perm_imp,\n    X_test,\n    target_name=None,\n    feature_names=None,\n    k=10,\n    figsize=(12, 8),\n    show=True,\n) -&gt; None:\n    \"\"\"\n    Plots the impurity-based and permutation-based feature importances for a given classifier.\n\n    Args:\n        df_mdi (pd.DataFrame):\n            DataFrame with Gini importances.\n        perm_imp (object):\n            Permutation importances object.\n        X_test (pd.DataFrame):\n            The test feature set for permutation importance.\n        target_name (str, optional):\n            Name of the target variable for labeling. Defaults to None.\n        feature_names (list, optional):\n            List of feature names for labeling. Defaults to None.\n        k (int, optional):\n            Number of top features to display based on importance. Default is 10.\n        figsize (tuple, optional):\n            Size of the figure (width, height) in inches. Default is (12, 8).\n        show (bool, optional):\n            If True, displays the plot immediately. Default is True.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.sensitivity.importance import generate_mdi, generate_imp, plot_importances\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from sklearn.datasets import make_regression\n        &gt;&gt;&gt; X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n        &gt;&gt;&gt; X_train, X_test = X[:80], X[80:]\n        &gt;&gt;&gt; y_train, y_test = y[:80], y[80:]\n        &gt;&gt;&gt; X_train_df = pd.DataFrame(X_train)\n        &gt;&gt;&gt; X_test_df = pd.DataFrame(X_test)\n        &gt;&gt;&gt; y_train_series = pd.Series(y_train)\n        &gt;&gt;&gt; y_test_series = pd.Series(y_test)\n        &gt;&gt;&gt; df_mdi = generate_mdi(X_train_df, y_train_series)\n        &gt;&gt;&gt; perm_imp = generate_imp(X_train_df, X_test_df, y_train_series, y_test_series)\n        &gt;&gt;&gt; plot_importances(df_mdi, perm_imp, X_test_df, figsize=(15, 10))\n    \"\"\"\n\n    # Plot impurity-based importances for top-k features\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n\n    sorted_mdi_importances = df_mdi.set_index(\"Feature\")[\"Importance\"]\n    sorted_mdi_importances[:k].sort_values().plot.barh(ax=ax1)\n    ax1.set_xlabel(\"Gini importance\")\n    if target_name:\n        ax1.set_title(f\"Impurity-based feature importances for target: {target_name}\")\n    else:\n        ax1.set_title(\"Impurity-based feature importances\")\n\n    # Ensure X_test is a DataFrame\n    if not isinstance(X_test, pd.DataFrame):\n        X_test = pd.DataFrame(X_test)\n\n    perm_sorted_idx = perm_imp.importances_mean.argsort()[-k:]\n    if feature_names is not None:\n        ax2.boxplot(\n            perm_imp.importances[perm_sorted_idx].T,\n            orientation=\"horizontal\",\n            tick_labels=np.array(feature_names)[perm_sorted_idx],\n        )\n    else:\n        ax2.boxplot(\n            perm_imp.importances[perm_sorted_idx].T,\n            orientation=\"horizontal\",\n            tick_labels=X_test.columns[perm_sorted_idx],\n        )\n    ax2.axvline(x=0, color=\"k\", linestyle=\"--\")\n    if target_name:\n        ax2.set_xlabel(f\"Decrease in mse for target: {target_name}\")\n    else:\n        ax2.set_xlabel(\"Decrease in mse\")\n    ax2.set_title(\"Permutation-based feature importances\")\n\n    # fig.suptitle(\"Impurity-based vs. permutation importances\")\n    fig.tight_layout()\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotoptim/inspection/predictions/","title":"predictions","text":""},{"location":"reference/spotoptim/inspection/predictions/#spotoptim.inspection.predictions.plot_actual_vs_predicted","title":"<code>plot_actual_vs_predicted(y_test, y_pred, title=None, show=True, filename=None)</code>","text":"<p>Plot actual vs. predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y_test</code> <code>ndarray</code> <p>True values.</p> required <code>y_pred</code> <code>ndarray</code> <p>Predicted values.</p> required <code>title</code> <code>str</code> <p>Title of the plot. Defaults to None.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is shown. Defaults to True.</p> <code>True</code> <code>filename</code> <code>str</code> <p>Name of the file to save the plot. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NoneType</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n    from sklearn.linear_model import LinearRegression\n    from spotoptim.inspection import plot_actual_vs_predicted\n    X, y = load_diabetes(return_X_y=True)\n    lr = LinearRegression()\n    lr.fit(X, y)\n    y_pred = lr.predict(X)\n    plot_actual_vs_predicted(y, y_pred)\n</code></pre> Source code in <code>src/spotoptim/inspection/predictions.py</code> <pre><code>def plot_actual_vs_predicted(\n    y_test, y_pred, title=None, show=True, filename=None\n) -&gt; None:\n    \"\"\"Plot actual vs. predicted values.\n\n    Args:\n        y_test (np.ndarray):\n            True values.\n        y_pred (np.ndarray):\n            Predicted values.\n        title (str, optional):\n            Title of the plot. Defaults to None.\n        show (bool, optional):\n            If True, the plot is shown. Defaults to True.\n        filename (str, optional):\n            Name of the file to save the plot. Defaults to None.\n\n    Returns:\n        (NoneType): None\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n            from sklearn.linear_model import LinearRegression\n            from spotoptim.inspection import plot_actual_vs_predicted\n            X, y = load_diabetes(return_X_y=True)\n            lr = LinearRegression()\n            lr.fit(X, y)\n            y_pred = lr.predict(X)\n            plot_actual_vs_predicted(y, y_pred)\n    \"\"\"\n    fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"actual_vs_predicted\",\n        subsample=100,\n        ax=axs[0],\n        random_state=0,\n        scatter_kwargs={\"alpha\": 0.5},\n    )\n    axs[0].set_title(\"Actual vs. Predicted values\")\n    PredictionErrorDisplay.from_predictions(\n        y_test,\n        y_pred=y_pred,\n        kind=\"residual_vs_predicted\",\n        subsample=100,\n        ax=axs[1],\n        random_state=0,\n    )\n    axs[1].set_title(\"Residuals vs. Predicted Values\")\n    if title is not None:\n        fig.suptitle(title)\n    plt.tight_layout()\n    if filename is not None:\n        plt.savefig(filename)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotoptim/mo/mo_mm/","title":"mo_mm","text":""},{"location":"reference/spotoptim/mo/mo_mm/#spotoptim.mo.mo_mm.mo_mm_desirability_function","title":"<code>mo_mm_desirability_function(x, models, X_base, J_base, d_base, phi_base, D_overall, mm_objective=True, verbose=False)</code>","text":"<p>Calculates the negative combined desirability for a candidate point x. Can be used by the mo_mm_desirability_optimizer. For each objective, a model is used to predict the objective value at x. If mm_objective is True, the Morris-Mitchell improvement is also calculated and included as an additional objective. The combined desirability, which uses the predictions from the models and optionally the Morris-Mitchell improvement, is then computed using the provided DOverall object.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Candidate point (1D array).</p> required <code>models</code> <code>list</code> <p>List of trained models. One model per objective.</p> required <code>X_base</code> <code>ndarray</code> <p>Existing design points. Used for computing Morris-Mitchell improvement.</p> required <code>J_base</code> <code>ndarray</code> <p>Multiplicities of distances for X_base. Used for Morris-Mitchell improvement.</p> required <code>d_base</code> <code>ndarray</code> <p>Unique distances for X_base. Used for Morris-Mitchell improvement.</p> required <code>phi_base</code> <code>float</code> <p>Base Morris-Mitchell metric for X_base. Used for Morris-Mitchell improvement.</p> required <code>D_overall</code> <code>DOverall</code> <p>The overall desirability function. Must include desirability functions for each objective and optionally for Morris-Mitchell.</p> required <code>mm_objective</code> <code>bool</code> <p>Whether to include space-filling improvement as an objective. Defaults to True.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Whether to print Morris-Mitchell improvement values. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[float, List[float]]</code> <p>Tuple[float, List[float]]: A tuple containing:     - Negative geometric mean of desirabilities (for minimization).     - List of individual objective values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.mo import mo_mm_desirability_function\n&gt;&gt;&gt; from spotdesirability import DOverall, DMax\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_max\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive\n&gt;&gt;&gt; # X_base in the range [0,1]\n&gt;&gt;&gt; X_base = np.random.rand(500, 2)\n&gt;&gt;&gt; y = mo_conv2_max(X_base)\n&gt;&gt;&gt; models = []\n&gt;&gt;&gt; for i in range(y.shape[1]):\n...     model = RandomForestRegressor(n_estimators=100, random_state=42)\n...     model.fit(X_base, y[:, i])\n...     models.append(model)\n&gt;&gt;&gt; # calculate base Morris-Mitchell stats\n&gt;&gt;&gt; phi_base, J_base, d_base = mmphi_intensive(X_base, q=2, p=2)\n&gt;&gt;&gt; d_funcs = []\n&gt;&gt;&gt; for i in range(y.shape[1]):\n...     d_func = DMax(low=np.min(y[:, i]), high=np.max(y[:, i]))\n...     d_funcs.append(d_func)\n&gt;&gt;&gt; D_overall = DOverall(*d_funcs)\n&gt;&gt;&gt; x_test = np.random.rand(2)  # Example test point\n&gt;&gt;&gt; neg_D, objectives = mo_mm_desirability_function(x_test, models, X_base, J_base, d_base, phi_base, D_overall, mm_objective=False)\n&gt;&gt;&gt; print(f\"Negative Desirability: {neg_D}\")\nNegative Desirability: ...\n&gt;&gt;&gt; print(f\"Objectives: {objectives}\")\nObjectives: ...\n</code></pre> Source code in <code>src/spotoptim/mo/mo_mm.py</code> <pre><code>def mo_mm_desirability_function(\n    x,\n    models,\n    X_base,\n    J_base,\n    d_base,\n    phi_base,\n    D_overall,\n    mm_objective=True,\n    verbose=False,\n) -&gt; Tuple[float, List[float]]:\n    \"\"\"\n    Calculates the negative combined desirability for a candidate point x. Can be used by the mo_mm_desirability_optimizer.\n    For each objective, a model is used to predict the objective value at x.\n    If mm_objective is True, the Morris-Mitchell improvement is also calculated and included as an additional objective.\n    The combined desirability, which uses the predictions from the models and optionally the Morris-Mitchell improvement,\n    is then computed using the provided DOverall object.\n\n    Args:\n        x (np.ndarray):\n            Candidate point (1D array).\n        models (list):\n            List of trained models. One model per objective.\n        X_base (np.ndarray):\n            Existing design points. Used for computing Morris-Mitchell improvement.\n        J_base (np.ndarray):\n            Multiplicities of distances for X_base. Used for Morris-Mitchell improvement.\n        d_base (np.ndarray):\n            Unique distances for X_base. Used for Morris-Mitchell improvement.\n        phi_base (float):\n            Base Morris-Mitchell metric for X_base. Used for Morris-Mitchell improvement.\n        D_overall (DOverall):\n            The overall desirability function. Must include desirability functions for each objective and optionally for Morris-Mitchell.\n        mm_objective (bool):\n            Whether to include space-filling improvement as an objective. Defaults to True.\n        verbose (bool):\n            Whether to print Morris-Mitchell improvement values. Defaults to False.\n\n    Returns:\n        Tuple[float, List[float]]:\n            A tuple containing:\n                - Negative geometric mean of desirabilities (for minimization).\n                - List of individual objective values.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.mo import mo_mm_desirability_function\n        &gt;&gt;&gt; from spotdesirability import DOverall, DMax\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_max\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive\n        &gt;&gt;&gt; # X_base in the range [0,1]\n        &gt;&gt;&gt; X_base = np.random.rand(500, 2)\n        &gt;&gt;&gt; y = mo_conv2_max(X_base)\n        &gt;&gt;&gt; models = []\n        &gt;&gt;&gt; for i in range(y.shape[1]):\n        ...     model = RandomForestRegressor(n_estimators=100, random_state=42)\n        ...     model.fit(X_base, y[:, i])\n        ...     models.append(model)\n        &gt;&gt;&gt; # calculate base Morris-Mitchell stats\n        &gt;&gt;&gt; phi_base, J_base, d_base = mmphi_intensive(X_base, q=2, p=2)\n        &gt;&gt;&gt; d_funcs = []\n        &gt;&gt;&gt; for i in range(y.shape[1]):\n        ...     d_func = DMax(low=np.min(y[:, i]), high=np.max(y[:, i]))\n        ...     d_funcs.append(d_func)\n        &gt;&gt;&gt; D_overall = DOverall(*d_funcs)\n        &gt;&gt;&gt; x_test = np.random.rand(2)  # Example test point\n        &gt;&gt;&gt; neg_D, objectives = mo_mm_desirability_function(x_test, models, X_base, J_base, d_base, phi_base, D_overall, mm_objective=False)\n        &gt;&gt;&gt; print(f\"Negative Desirability: {neg_D}\")\n        Negative Desirability: ...\n        &gt;&gt;&gt; print(f\"Objectives: {objectives}\")\n        Objectives: ...\n    \"\"\"\n    # 1. Predict for all models\n    x_reshaped = x.reshape(1, -1)\n    predictions = [model.predict(x_reshaped)[0] for model in models]\n\n    # 2. Compute y_mm (Space-filling improvement) if requested\n    if mm_objective:\n        y_mm = mm_improvement(x, X_base, phi_base, J_base, d_base, verbose=verbose)\n        predictions.append(y_mm)\n\n    # 3. Calculate combined desirability\n    D = D_overall.predict(predictions)\n\n    # Ensure D is a scalar\n    if isinstance(D, np.ndarray):\n        D = D.item()\n\n    return -D, predictions\n</code></pre>"},{"location":"reference/spotoptim/mo/mo_mm/#spotoptim.mo.mo_mm.mo_mm_desirability_optimizer","title":"<code>mo_mm_desirability_optimizer(X_base, models, bounds, obj_func, **kwargs)</code>","text":"<p>Optimizes the multi-objective function to find the next best point. Returns the best point, its desirability, and the history of objective values.</p> <p>Parameters:</p> Name Type Description Default <code>X_base</code> <code>ndarray</code> <p>Existing design points.</p> required <code>models</code> <code>list</code> <p>List of trained surrogate models for each objective.</p> required <code>bounds</code> <code>list</code> <p>Bounds for each dimension.</p> required <code>obj_func</code> <code>callable</code> <p>Objective function to compute desirability and objectives.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments for the objective function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, float, ndarray]</code> <p>Tuple[np.ndarray, float, np.ndarray]: A tuple containing:     - Best point (np.ndarray)     - Best desirability (float)     - History of objective values (np.ndarray)</p> Source code in <code>src/spotoptim/mo/mo_mm.py</code> <pre><code>def mo_mm_desirability_optimizer(\n    X_base, models, bounds, obj_func, **kwargs: Any\n) -&gt; Tuple[np.ndarray, float, np.ndarray]:\n    \"\"\"\n    Optimizes the multi-objective function to find the next best point.\n    Returns the best point, its desirability, and the history of objective values.\n\n    Args:\n        X_base (np.ndarray):\n            Existing design points.\n        models (list):\n            List of trained surrogate models for each objective.\n        bounds (list):\n            Bounds for each dimension.\n        obj_func (callable):\n            Objective function to compute desirability and objectives.\n        **kwargs (Any):\n            Additional arguments for the objective function.\n\n    Returns:\n        Tuple[np.ndarray, float, np.ndarray]:\n            A tuple containing:\n                - Best point (np.ndarray)\n                - Best desirability (float)\n                - History of objective values (np.ndarray)\n\n    \"\"\"\n    # Pre-calculate base MM stats\n    phi_base, J_base, d_base = mmphi_intensive(X_base, q=2, p=2)\n\n    # List to store callback values\n    callback_values = []\n\n    # Define the objective wrapper\n    def func(x):\n        neg_D, objectives = obj_func(\n            x, models, X_base, J_base, d_base, phi_base, **kwargs\n        )\n        callback_values.append(objectives)\n        return neg_D\n\n    # Run optimization\n    result = dual_annealing(func, bounds=bounds, maxiter=100, seed=42)\n\n    return result.x, -result.fun, np.array(callback_values)\n</code></pre>"},{"location":"reference/spotoptim/mo/mo_mm/#spotoptim.mo.mo_mm.mo_xy_desirability_plot","title":"<code>mo_xy_desirability_plot(models, X_base, J_base, d_base, phi_base, D_overall, bounds=None, mm_objective=True, resolution=50, feature_names=None, **kwargs)</code>","text":"<p>Generates a plot of the desirability landscape. Plots the 2-dim X values as points in the plane and colors them according to their desirability values. For each pair of inputs, x_i and x_j (with i &lt; j), one plot is generated.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list</code> <p>List of trained models (one per objective).</p> required <code>X_base</code> <code>ndarray</code> <p>Existing design points.</p> required <code>J_base</code> <code>ndarray</code> <p>Multiplicities of distances for X_base.</p> required <code>d_base</code> <code>ndarray</code> <p>Unique distances for X_base.</p> required <code>phi_base</code> <code>float</code> <p>Base Morris-Mitchell metric.</p> required <code>D_overall</code> <code>DOverall</code> <p>The overall desirability function.</p> required <code>bounds</code> <code>list</code> <p>List of tuples (min, max) for each dimension. If None, derived from X_base.</p> <code>None</code> <code>mm_objective</code> <code>bool</code> <p>Whether to include space-filling improvement. Defaults to True.</p> <code>True</code> <code>resolution</code> <code>int</code> <p>Grid resolution for the plot. Defaults to 50.</p> <code>50</code> <code>feature_names</code> <code>list</code> <p>List of names for the input variables. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for plt.subplots (e.g., figsize).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.mo.mo_mm import mo_xy_desirability_plot\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_max\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive\n&gt;&gt;&gt; # X_base in the range [0,1]\n&gt;&gt;&gt; X_base = np.random.rand(500, 2)\n&gt;&gt;&gt; y = mo_conv2_max(X_base)\n&gt;&gt;&gt; models = []\n&gt;&gt;&gt; for i in range(y.shape[1]):\n...     model = RandomForestRegressor(n_estimators=100, random_state=42)\n...     model.fit(X_base, y[:, i])\n...     models.append(model)\n&gt;&gt;&gt; # calculate base Morris-Mitchell stats\n&gt;&gt;&gt; phi_base, J_base, d_base = mmphi_intensive(X_base, q=2, p=2)\n&gt;&gt;&gt; d_funcs = []\n&gt;&gt;&gt; for i in range(y.shape[1]):\n...     d_func = DMax(low=np.min(y[:, i]), high=np.max(y[:, i]))\n...     d_funcs.append(d_func)\n&gt;&gt;&gt; D_overall = DOverall(*d_funcs)\n&gt;&gt;&gt; mo_xy_desirability_plot(models, X_base, J_base, d_base, phi_base, D_overall)\n</code></pre> Source code in <code>src/spotoptim/mo/mo_mm.py</code> <pre><code>def mo_xy_desirability_plot(\n    models: list,\n    X_base: np.ndarray,\n    J_base: np.ndarray,\n    d_base: np.ndarray,\n    phi_base: float,\n    D_overall,\n    bounds: list = None,\n    mm_objective: bool = True,\n    resolution: int = 50,\n    feature_names: list = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Generates a plot of the desirability landscape.\n    Plots the 2-dim X values as points in the plane and colors them according to their desirability values.\n    For each pair of inputs, x_i and x_j (with i &lt; j), one plot is generated.\n\n    Args:\n        models (list):\n            List of trained models (one per objective).\n        X_base (np.ndarray):\n            Existing design points.\n        J_base (np.ndarray):\n            Multiplicities of distances for X_base.\n        d_base (np.ndarray):\n            Unique distances for X_base.\n        phi_base (float):\n            Base Morris-Mitchell metric.\n        D_overall (DOverall):\n            The overall desirability function.\n        bounds (list, optional):\n            List of tuples (min, max) for each dimension. If None, derived from X_base.\n        mm_objective (bool, optional):\n            Whether to include space-filling improvement. Defaults to True.\n        resolution (int, optional):\n            Grid resolution for the plot. Defaults to 50.\n        feature_names (list, optional):\n            List of names for the input variables. Defaults to None.\n        **kwargs:\n            Additional arguments for plt.subplots (e.g., figsize).\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.mo.mo_mm import mo_xy_desirability_plot\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.function.mo import mo_conv2_max\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive\n        &gt;&gt;&gt; # X_base in the range [0,1]\n        &gt;&gt;&gt; X_base = np.random.rand(500, 2)\n        &gt;&gt;&gt; y = mo_conv2_max(X_base)\n        &gt;&gt;&gt; models = []\n        &gt;&gt;&gt; for i in range(y.shape[1]):\n        ...     model = RandomForestRegressor(n_estimators=100, random_state=42)\n        ...     model.fit(X_base, y[:, i])\n        ...     models.append(model)\n        &gt;&gt;&gt; # calculate base Morris-Mitchell stats\n        &gt;&gt;&gt; phi_base, J_base, d_base = mmphi_intensive(X_base, q=2, p=2)\n        &gt;&gt;&gt; d_funcs = []\n        &gt;&gt;&gt; for i in range(y.shape[1]):\n        ...     d_func = DMax(low=np.min(y[:, i]), high=np.max(y[:, i]))\n        ...     d_funcs.append(d_func)\n        &gt;&gt;&gt; D_overall = DOverall(*d_funcs)\n        &gt;&gt;&gt; mo_xy_desirability_plot(models, X_base, J_base, d_base, phi_base, D_overall)\n    \"\"\"\n    import itertools\n    import matplotlib.pyplot as plt\n\n    n_points, n_features = X_base.shape\n\n    if bounds is None:\n        bounds = [\n            (np.min(X_base[:, i]), np.max(X_base[:, i])) for i in range(n_features)\n        ]\n\n    if feature_names is None:\n        feature_names = [f\"x{i}\" for i in range(n_features)]\n\n    # 1. Identify input pairs (i, j) with i &lt; j\n    feature_pairs = list(itertools.combinations(range(n_features), 2))\n    n_pairs = len(feature_pairs)\n\n    if n_pairs == 0:\n        print(\"Need at least 2 features to generate plots.\")\n        return\n\n    # 2. Setup Plot Grid\n    # One plot per pair.\n    # We can arrange them in a grid roughly square? Or just rows?\n    # User example: \"2 x 2 grid should be used\".\n    # Let's do a simple grid calculation.\n    import math\n\n    cols = int(math.ceil(math.sqrt(n_pairs)))\n    rows = int(math.ceil(n_pairs / cols))\n\n    if \"figsize\" not in kwargs:\n        kwargs[\"figsize\"] = (5 * cols, 4 * rows)\n\n    fig, axes = plt.subplots(rows, cols, **kwargs)\n\n    # Flatten axes for easy iteration\n    if n_pairs == 1:\n        axes = [axes]\n    else:\n        axes = axes.flatten()\n\n    # Base point for fixed dimensions (midpoint)\n    x_mid = np.array([(b[0] + b[1]) / 2.0 for b in bounds])\n\n    for idx, (i, j) in enumerate(feature_pairs):\n        ax = axes[idx]\n\n        # Create grid\n        xi = np.linspace(bounds[i][0], bounds[i][1], resolution)\n        xj = np.linspace(bounds[j][0], bounds[j][1], resolution)\n        Xi, Xj = np.meshgrid(xi, xj)\n        D_values = np.zeros(Xi.shape)\n\n        # Compute Desirability\n        for r in range(Xi.shape[0]):\n            for c in range(Xi.shape[1]):\n                # Construct candidate point\n                x_point = x_mid.copy()\n                x_point[i] = Xi[r, c]\n                x_point[j] = Xj[r, c]\n\n                neg_D, _ = mo_mm_desirability_function(\n                    x_point,\n                    models,\n                    X_base,\n                    J_base,\n                    d_base,\n                    phi_base,\n                    D_overall,\n                    mm_objective=mm_objective,\n                )\n                D_values[r, c] = -neg_D  # Store positive desirability\n\n        # Plot\n        contour = ax.contourf(Xi, Xj, D_values, levels=20, cmap=\"viridis\")\n        fig.colorbar(contour, ax=ax)\n        ax.set_xlabel(feature_names[i])\n        ax.set_ylabel(feature_names[j])\n        ax.set_title(f\"Desirability (x{i} vs x{j})\")\n\n    # Hide unused subplots\n    for idx in range(n_pairs, len(axes)):\n        axes[idx].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/spotoptim/mo/pareto/","title":"pareto","text":""},{"location":"reference/spotoptim/mo/pareto/#spotoptim.mo.pareto.is_pareto_efficient","title":"<code>is_pareto_efficient(costs, minimize=True)</code>","text":"<p>Find the Pareto-efficient points from a set of points.</p> <p>A point is Pareto-efficient if no other point exists that is better in all objectives. This function assumes that lower values are preferred for each objective when <code>minimize=True</code>, and higher values are preferred when <code>minimize=False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>costs</code> <code>ndarray</code> <p>An (N,M) array-like object of points, where N is the number of points and M is the number of objectives.</p> required <code>minimize</code> <code>bool</code> <p>If True, the function finds Pareto-efficient points assuming lower values are better. If False, it assumes higher values are better. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A boolean mask of length N, where True indicates that the corresponding point is Pareto-efficient.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.mo.pareto import is_pareto_efficient\n&gt;&gt;&gt; points = np.array([[1, 2], [2, 1], [1.5, 1.5], [3, 3]])\n&gt;&gt;&gt; pareto_mask = is_pareto_efficient(points, minimize=True)\n&gt;&gt;&gt; print(pareto_mask)\n[ True  True  True False]\n</code></pre> Source code in <code>src/spotoptim/mo/pareto.py</code> <pre><code>def is_pareto_efficient(costs: np.ndarray, minimize: bool = True) -&gt; np.ndarray:\n    \"\"\"\n    Find the Pareto-efficient points from a set of points.\n\n    A point is Pareto-efficient if no other point exists that is better in all objectives.\n    This function assumes that lower values are preferred for each objective when `minimize=True`,\n    and higher values are preferred when `minimize=False`.\n\n    Args:\n        costs (np.ndarray):\n            An (N,M) array-like object of points, where N is the number of points and M is the number of objectives.\n        minimize (bool, optional):\n            If True, the function finds Pareto-efficient points assuming\n            lower values are better. If False, it assumes higher values are better.\n            Defaults to True.\n\n    Returns:\n        np.ndarray:\n            A boolean mask of length N, where True indicates that the corresponding point is Pareto-efficient.\n\n    Examples:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.mo.pareto import is_pareto_efficient\n        &gt;&gt;&gt; points = np.array([[1, 2], [2, 1], [1.5, 1.5], [3, 3]])\n        &gt;&gt;&gt; pareto_mask = is_pareto_efficient(points, minimize=True)\n        &gt;&gt;&gt; print(pareto_mask)\n        [ True  True  True False]\n    \"\"\"\n    is_efficient = np.ones(costs.shape[0], dtype=bool)\n    for i, cost in enumerate(costs):\n        if is_efficient[i]:\n            if minimize:\n                is_efficient[is_efficient] = np.any(costs[is_efficient] &lt; cost, axis=1)\n            else:\n                is_efficient[is_efficient] = np.any(costs[is_efficient] &gt; cost, axis=1)\n            is_efficient[i] = True\n    return is_efficient\n</code></pre>"},{"location":"reference/spotoptim/mo/pareto/#spotoptim.mo.pareto.mo_pareto_optx_plot","title":"<code>mo_pareto_optx_plot(X, Y, minimize=True, feature_names=None, target_names=None, **kwargs)</code>","text":"<p>Visualizes the Pareto-optimal points in the input space for each pair of inputs x_i and x_j (with i &lt; j) and each objective f_k.</p> <p>Plots are placed on a grid where rows correspond to input pairs and columns correspond to objectives.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>An (N,D) array of input points, where N is the number of points and D is the number of variables (dimensions).</p> required <code>Y</code> <code>ndarray</code> <p>An (N,M) array of objective values, where N is the number of points and M is the number of objectives.</p> required <code>minimize</code> <code>bool</code> <p>If True, assumes minimization of objectives. Defaults to True.</p> <code>True</code> <code>feature_names</code> <code>list</code> <p>List of names for the input variables. Defaults to None.</p> <code>None</code> <code>target_names</code> <code>list</code> <p>List of names for the objectives. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to plt.subplots (e.g., figsize).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.mo.pareto import mo_pareto_optx_plot\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; Y = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; mo_pareto_optx_plot(X, Y)\n</code></pre> Source code in <code>src/spotoptim/mo/pareto.py</code> <pre><code>def mo_pareto_optx_plot(\n    X: np.ndarray,\n    Y: np.ndarray,\n    minimize: bool = True,\n    feature_names: list = None,\n    target_names: list = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Visualizes the Pareto-optimal points in the input space for each pair of inputs\n    x_i and x_j (with i &lt; j) and each objective f_k.\n\n    Plots are placed on a grid where rows correspond to input pairs and columns\n    correspond to objectives.\n\n    Args:\n        X (np.ndarray):\n            An (N,D) array of input points, where N is the number of points and\n            D is the number of variables (dimensions).\n        Y (np.ndarray):\n            An (N,M) array of objective values, where N is the number of points and\n            M is the number of objectives.\n        minimize (bool, optional):\n            If True, assumes minimization of objectives. Defaults to True.\n        feature_names (list, optional):\n            List of names for the input variables. Defaults to None.\n        target_names (list, optional):\n            List of names for the objectives. Defaults to None.\n        **kwargs (Any):\n            Additional arguments passed to plt.subplots (e.g., figsize).\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.mo.pareto import mo_pareto_optx_plot\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; Y = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; mo_pareto_optx_plot(X, Y)\n    \"\"\"\n    n_points, n_features = X.shape\n    _, n_objectives = Y.shape\n\n    if feature_names is None:\n        feature_names = [f\"x{i}\" for i in range(n_features)]\n    if target_names is None:\n        target_names = [f\"f{i}\" for i in range(n_objectives)]\n\n    # 1. Determine Pareto-efficient points\n    pareto_mask = is_pareto_efficient(Y, minimize=minimize)\n\n    # 2. Identify input pairs (i, j) with i &lt; j\n    feature_pairs = list(itertools.combinations(range(n_features), 2))\n    n_pairs = len(feature_pairs)\n\n    if n_pairs == 0:\n        print(\"Need at least 2 features to generate plots.\")\n        return\n\n    # 3. Create grid of plots\n    # Grid: Rows = Feature Pairs, Cols = Objectives\n    rows = n_pairs\n    cols = n_objectives\n\n    # Handle figsize in kwargs or default\n    if \"figsize\" not in kwargs:\n        # Default size estimation: 4 inches per row, 5 inches per col\n        kwargs[\"figsize\"] = (5 * cols, 4 * rows)\n\n    fig, axes = plt.subplots(rows, cols, **kwargs)\n\n    # Ensure axes is always a 2D array for consistent indexing\n    if rows == 1 and cols == 1:\n        axes = np.array([[axes]])\n    elif rows == 1:\n        axes = axes.reshape(1, -1)\n    elif cols == 1:\n        axes = axes.reshape(-1, 1)\n\n    # 4. Iterate and plot\n    for r, (i, j) in enumerate(feature_pairs):\n        for c in range(n_objectives):\n            ax = axes[r, c]\n\n            # Scatter all points (not pareto) - maybe faint?\n            # User request: \"visualize the Pareto-optimal points\"\n            # \"points in the input space that correspond to the Pareto front\"\n            # Let's plot all points first for context, then highlight Pareto.\n\n            # Plot non-Pareto points\n            ax.scatter(\n                X[~pareto_mask, i],\n                X[~pareto_mask, j],\n                c=\"lightgray\",\n                label=\"Non-Pareto\",\n                alpha=0.5,\n                s=20,\n            )\n\n            # Plot Pareto points\n            # Color by objective value f_k (Y[:, c])\n            sc = ax.scatter(\n                X[pareto_mask, i],\n                X[pareto_mask, j],\n                c=Y[pareto_mask, c],\n                cmap=\"viridis\",\n                label=\"Pareto\",\n                s=50,\n                edgecolor=\"k\",\n            )\n\n            ax.set_xlabel(feature_names[i])\n            ax.set_ylabel(feature_names[j])\n            ax.set_title(f\"{target_names[c]} (Color)\")\n\n            # Add colorbar for this subplot\n            cbar = plt.colorbar(sc, ax=ax)\n            cbar.set_label(target_names[c])\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/spotoptim/mo/pareto/#spotoptim.mo.pareto.mo_xy_contour","title":"<code>mo_xy_contour(models, bounds, target_names=None, feature_names=None, resolution=50, feature_pairs=None, **kwargs)</code>","text":"<p>Generates contour plots of every combination of two input variables x_i and x_j (where i &lt; j) and for each of the multiple objectives f_k.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list</code> <p>List of trained models (one per objective).</p> required <code>bounds</code> <code>list</code> <p>List of tuples (min, max) for each input variable.</p> required <code>target_names</code> <code>list</code> <p>List of names for the objectives. Defaults to None.</p> <code>None</code> <code>feature_names</code> <code>list</code> <p>List of names for the input variables. Defaults to None.</p> <code>None</code> <code>resolution</code> <code>int</code> <p>Grid resolution for the contour plot. Defaults to 50.</p> <code>50</code> <code>feature_pairs</code> <code>list</code> <p>List of tuples (i, j) specifying which feature pairs to plot. If None, all combinations are plotted. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to plt.subplots (e.g., figsize).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotoptim.mo.pareto import mo_xy_contour\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Train dummy models\n&gt;&gt;&gt; X = np.random.rand(10, 2)\n&gt;&gt;&gt; y1 = X[:, 0] + X[:, 1]\n&gt;&gt;&gt; y2 = X[:, 0] * X[:, 1]\n&gt;&gt;&gt; m1 = RandomForestRegressor().fit(X, y1)\n&gt;&gt;&gt; m2 = RandomForestRegressor().fit(X, y2)\n&gt;&gt;&gt; # Plot\n&gt;&gt;&gt; mo_xy_contour([m1, m2], bounds=[(0, 1), (0, 1)], target_names=[\"Sum\", \"Prod\"])\n</code></pre> Source code in <code>src/spotoptim/mo/pareto.py</code> <pre><code>def mo_xy_contour(\n    models: list,\n    bounds: list,\n    target_names: list = None,\n    feature_names: list = None,\n    resolution: int = 50,\n    feature_pairs: list = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Generates contour plots of every combination of two input variables x_i and x_j\n    (where i &lt; j) and for each of the multiple objectives f_k.\n\n    Args:\n        models (list):\n            List of trained models (one per objective).\n        bounds (list):\n            List of tuples (min, max) for each input variable.\n        target_names (list, optional):\n            List of names for the objectives. Defaults to None.\n        feature_names (list, optional):\n            List of names for the input variables. Defaults to None.\n        resolution (int, optional):\n            Grid resolution for the contour plot. Defaults to 50.\n        feature_pairs (list, optional):\n            List of tuples (i, j) specifying which feature pairs to plot.\n            If None, all combinations are plotted. Defaults to None.\n        **kwargs (Any):\n            Additional keyword arguments passed to plt.subplots (e.g., figsize).\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; from spotoptim.mo.pareto import mo_xy_contour\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; # Train dummy models\n        &gt;&gt;&gt; X = np.random.rand(10, 2)\n        &gt;&gt;&gt; y1 = X[:, 0] + X[:, 1]\n        &gt;&gt;&gt; y2 = X[:, 0] * X[:, 1]\n        &gt;&gt;&gt; m1 = RandomForestRegressor().fit(X, y1)\n        &gt;&gt;&gt; m2 = RandomForestRegressor().fit(X, y2)\n        &gt;&gt;&gt; # Plot\n        &gt;&gt;&gt; mo_xy_contour([m1, m2], bounds=[(0, 1), (0, 1)], target_names=[\"Sum\", \"Prod\"])\n    \"\"\"\n    # Validate bounds\n    for i, b in enumerate(bounds):\n        if not (np.isscalar(b[0]) and np.isscalar(b[1])):\n            raise ValueError(\n                f\"Bounds for feature {i} must be scalars, but got {b}. \"\n                \"Please ensure that bounds are a list of (min, max) tuples with scalar values.\"\n            )\n\n    n_features = len(bounds)\n    n_objectives = len(models)\n\n    # Check if models have n_features_in_ (sklearn API) and validate against bounds\n    for m_idx, model in enumerate(models):\n        if hasattr(model, \"n_features_in_\") and model.n_features_in_ != n_features:\n            raise ValueError(\n                f\"Model {m_idx} expects {model.n_features_in_} features, but {n_features} bounds were provided. \"\n                \"The number of bounds must match the number of features the model was trained on.\"\n            )\n\n    if feature_names is None:\n        feature_names = [f\"x{i}\" for i in range(n_features)]\n    if target_names is None:\n        target_names = [f\"f{i}\" for i in range(n_objectives)]\n\n    # Generate all pairs of features (i, j) with i &lt; j\n    if feature_pairs is None:\n        feature_pairs = list(itertools.combinations(range(n_features), 2))\n    n_pairs = len(feature_pairs)\n\n    if n_pairs == 0:\n        print(\"Need at least 2 features to generate contour plots.\")\n        return\n\n    # Validate feature pairs\n    for i, j in feature_pairs:\n        if not (0 &lt;= i &lt; n_features and 0 &lt;= j &lt; n_features):\n            raise ValueError(\n                f\"Invalid feature pair ({i}, {j}). Indices must be between 0 and {n_features-1}.\"\n            )\n\n    # Create a grid of subplots\n    rows = n_pairs\n    cols = n_objectives\n\n    # Handle figsize in kwargs or default\n    if \"figsize\" not in kwargs:\n        kwargs[\"figsize\"] = (5 * cols, 4 * rows)\n\n    fig, axes = plt.subplots(rows, cols, **kwargs)\n\n    # Ensure axes is always iterable (handle 1x1, 1xN, Nx1 cases)\n    if rows * cols == 1:\n        axes = np.array([axes])\n    axes = axes.flatten()\n\n    # Base point (midpoint of bounds)\n    x_base = np.array([(b[0] + b[1]) / 2.0 for b in bounds])\n\n    plot_idx = 0\n    for i, j in feature_pairs:\n        for k, model in enumerate(models):\n            ax = axes[plot_idx]\n\n            Xi, Xj, Z = _get_mo_plot_data(model, bounds, i, j, resolution, x_base)\n\n            # Plot contour\n            cp = ax.contourf(Xi, Xj, Z, cmap=\"viridis\", alpha=0.8, levels=20)\n            fig.colorbar(cp, ax=ax)  # Add colorbar to each plot\n\n            ax.set_xlabel(feature_names[i])\n            ax.set_ylabel(feature_names[j])\n            ax.set_title(f\"{target_names[k]}\")\n\n            plot_idx += 1\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/spotoptim/mo/pareto/#spotoptim.mo.pareto.mo_xy_surface","title":"<code>mo_xy_surface(models, bounds, target_names=None, feature_names=None, resolution=50, feature_pairs=None, **kwargs)</code>","text":"<p>Generates surface plots of every combination of two input variables x_i and x_j (where i &lt; j) and for each of the multiple objectives f_k.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list</code> <p>List of trained models (one per objective).</p> required <code>bounds</code> <code>list</code> <p>List of tuples (min, max) for each input variable.</p> required <code>target_names</code> <code>list</code> <p>List of names for the objectives. Defaults to None.</p> <code>None</code> <code>feature_names</code> <code>list</code> <p>List of names for the input variables. Defaults to None.</p> <code>None</code> <code>resolution</code> <code>int</code> <p>Grid resolution for the surface plot. Defaults to 50.</p> <code>50</code> <code>feature_pairs</code> <code>list</code> <p>List of tuples (i, j) specifying which feature pairs to plot. If None, all combinations are plotted. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to plt.subplots (e.g., figsize).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n&gt;&gt;&gt; from spotoptim.mo.pareto import mo_xy_surface\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Train dummy models\n&gt;&gt;&gt; X = np.random.rand(10, 2)\n&gt;&gt;&gt; y1 = X[:, 0] + X[:, 1]\n&gt;&gt;&gt; y2 = X[:, 0] * X[:, 1]\n&gt;&gt;&gt; m1 = RandomForestRegressor().fit(X, y1)\n&gt;&gt;&gt; m2 = RandomForestRegressor().fit(X, y2)\n&gt;&gt;&gt; # Plot\n&gt;&gt;&gt; mo_xy_surface([m1, m2], bounds=[(0, 1), (0, 1)], target_names=[\"Sum\", \"Prod\"])\n</code></pre> Source code in <code>src/spotoptim/mo/pareto.py</code> <pre><code>def mo_xy_surface(\n    models: list,\n    bounds: list,\n    target_names: list = None,\n    feature_names: list = None,\n    resolution: int = 50,\n    feature_pairs: list = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Generates surface plots of every combination of two input variables x_i and x_j\n    (where i &lt; j) and for each of the multiple objectives f_k.\n\n    Args:\n        models (list):\n            List of trained models (one per objective).\n        bounds (list):\n            List of tuples (min, max) for each input variable.\n        target_names (list, optional):\n            List of names for the objectives. Defaults to None.\n        feature_names (list, optional):\n            List of names for the input variables. Defaults to None.\n        resolution (int, optional):\n            Grid resolution for the surface plot. Defaults to 50.\n        feature_pairs (list, optional):\n            List of tuples (i, j) specifying which feature pairs to plot.\n            If None, all combinations are plotted. Defaults to None.\n        **kwargs (Any):\n            Additional keyword arguments passed to plt.subplots (e.g., figsize).\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor\n        &gt;&gt;&gt; from spotoptim.mo.pareto import mo_xy_surface\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; # Train dummy models\n        &gt;&gt;&gt; X = np.random.rand(10, 2)\n        &gt;&gt;&gt; y1 = X[:, 0] + X[:, 1]\n        &gt;&gt;&gt; y2 = X[:, 0] * X[:, 1]\n        &gt;&gt;&gt; m1 = RandomForestRegressor().fit(X, y1)\n        &gt;&gt;&gt; m2 = RandomForestRegressor().fit(X, y2)\n        &gt;&gt;&gt; # Plot\n        &gt;&gt;&gt; mo_xy_surface([m1, m2], bounds=[(0, 1), (0, 1)], target_names=[\"Sum\", \"Prod\"])\n    \"\"\"\n    # Validate bounds\n    for i, b in enumerate(bounds):\n        if not (np.isscalar(b[0]) and np.isscalar(b[1])):\n            raise ValueError(\n                f\"Bounds for feature {i} must be scalars, but got {b}. \"\n                \"Please ensure that bounds are a list of (min, max) tuples with scalar values.\"\n            )\n\n    n_features = len(bounds)\n    n_objectives = len(models)\n\n    # Check if models have n_features_in_ (sklearn API) and validate against bounds\n    for m_idx, model in enumerate(models):\n        if hasattr(model, \"n_features_in_\") and model.n_features_in_ != n_features:\n            raise ValueError(\n                f\"Model {m_idx} expects {model.n_features_in_} features, but {n_features} bounds were provided. \"\n                \"The number of bounds must match the number of features the model was trained on.\"\n            )\n\n    if feature_names is None:\n        feature_names = [f\"x{i}\" for i in range(n_features)]\n    if target_names is None:\n        target_names = [f\"f{i}\" for i in range(n_objectives)]\n\n    # Generate all pairs of features (i, j) with i &lt; j\n    if feature_pairs is None:\n        feature_pairs = list(itertools.combinations(range(n_features), 2))\n    n_pairs = len(feature_pairs)\n\n    if n_pairs == 0:\n        print(\"Need at least 2 features to generate surface plots.\")\n        return\n\n    # Validate feature pairs\n    for i, j in feature_pairs:\n        if not (0 &lt;= i &lt; n_features and 0 &lt;= j &lt; n_features):\n            raise ValueError(\n                f\"Invalid feature pair ({i}, {j}). Indices must be between 0 and {n_features-1}.\"\n            )\n\n    # Create a grid of subplots\n    rows = n_pairs\n    cols = n_objectives\n\n    # Handle figsize in kwargs or default\n    if \"figsize\" not in kwargs:\n        kwargs[\"figsize\"] = (5 * cols, 4 * rows)\n\n    fig = plt.figure(**kwargs)\n\n    # Base point (midpoint of bounds)\n    x_base = np.array([(b[0] + b[1]) / 2.0 for b in bounds])\n\n    plot_idx = 1\n    for i, j in feature_pairs:\n        for k, model in enumerate(models):\n            ax = fig.add_subplot(rows, cols, plot_idx, projection=\"3d\")\n\n            Xi, Xj, Z = _get_mo_plot_data(model, bounds, i, j, resolution, x_base)\n\n            # Plot surface\n            ax.plot_surface(Xi, Xj, Z, cmap=\"viridis\", edgecolor=\"none\", alpha=0.8)\n\n            ax.set_xlabel(feature_names[i])\n            ax.set_ylabel(feature_names[j])\n            ax.set_title(f\"{target_names[k]}\")\n\n            plot_idx += 1\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/spotoptim/nn/linear_regressor/","title":"linear_regressor","text":""},{"location":"reference/spotoptim/nn/linear_regressor/#spotoptim.nn.linear_regressor.LinearRegressor","title":"<code>LinearRegressor</code>","text":"<p>               Bases: <code>Module</code></p> <p>PyTorch neural network for regression with configurable architecture.</p> <p>A flexible regression model that supports: - Pure linear regression (no hidden layers) - Deep neural networks with multiple hidden layers - Various activation functions (ReLU, Tanh, Sigmoid, etc.) - Easy optimizer selection (Adam, SGD, RMSprop, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>output_dim</code> <code>int</code> <p>Number of output features/targets.</p> required <code>l1</code> <code>int</code> <p>Number of neurons in each hidden layer. Defaults to 64.</p> <code>64</code> <code>num_hidden_layers</code> <code>int</code> <p>Number of hidden layers. Set to 0 for pure linear regression. Defaults to 0.</p> <code>0</code> <code>activation</code> <code>str</code> <p>Name of activation function from torch.nn to use between layers. Common options: \u201cReLU\u201d, \u201cSigmoid\u201d, \u201cTanh\u201d, \u201cLeakyReLU\u201d, \u201cELU\u201d, \u201cSELU\u201d, \u201cGELU\u201d, \u201cSoftplus\u201d, \u201cSoftsign\u201d, \u201cMish\u201d. Defaults to \u201cReLU\u201d.</p> <code>'ReLU'</code> <code>lr</code> <code>float</code> <p>Unified learning rate multiplier. This value is automatically scaled to optimizer-specific learning rates using the map_lr() function. A value of 1.0 corresponds to the optimizer\u2019s default learning rate. For example, lr=1.0 gives 0.001 for Adam and 0.01 for SGD. Typical range: [0.001, 100.0]. Defaults to 1.0.</p> <code>1.0</code> <p>Attributes:</p> Name Type Description <code>input_dim</code> <code>int</code> <p>Number of input features.</p> <code>output_dim</code> <code>int</code> <p>Number of output features.</p> <code>l1</code> <code>int</code> <p>Number of neurons per hidden layer.</p> <code>num_hidden_layers</code> <code>int</code> <p>Number of hidden layers in the network.</p> <code>activation_name</code> <code>str</code> <p>Name of the activation function.</p> <code>activation</code> <code>Module</code> <p>Instance of the activation function.</p> <code>lr</code> <code>float</code> <p>Unified learning rate multiplier.</p> <code>network</code> <code>Sequential</code> <p>The complete neural network architecture.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified activation function is not found in torch.nn.</p> <p>Examples:</p> <p>Basic usage with pure linear regression:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.nn.linear_regressor import LinearRegressor\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Pure linear regression (no hidden layers)\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1)\n&gt;&gt;&gt; x = torch.randn(32, 10)  # Batch of 32 samples\n&gt;&gt;&gt; y_pred = model(x)\n&gt;&gt;&gt; print(y_pred.shape)\ntorch.Size([32, 1])\n</code></pre> <p>Single hidden layer with custom neurons:</p> <pre><code>&gt;&gt;&gt; # Single hidden layer with 64 neurons and ReLU activation\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=64, num_hidden_layers=1)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.001)\n</code></pre> <p>Deep network with custom activation:</p> <pre><code>&gt;&gt;&gt; # Three hidden layers with 128 neurons each and Tanh activation\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=128,\n...                         num_hidden_layers=3, activation=\"Tanh\")\n</code></pre> <p>Complete example using diabetes dataset:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Split and scale data\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.2, random_state=42\n... )\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n&gt;&gt;&gt; X_test = scaler.transform(X_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Convert to PyTorch tensors\n&gt;&gt;&gt; X_train = torch.FloatTensor(X_train)\n&gt;&gt;&gt; y_train = torch.FloatTensor(y_train)\n&gt;&gt;&gt; X_test = torch.FloatTensor(X_test)\n&gt;&gt;&gt; y_test = torch.FloatTensor(y_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model with 2 hidden layers\n&gt;&gt;&gt; model = LinearRegressor(\n...     input_dim=10,  # diabetes dataset has 10 features\n...     output_dim=1,\n...     l1=32,\n...     num_hidden_layers=2,\n...     activation=\"ReLU\"\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get optimizer and loss function\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n&gt;&gt;&gt; criterion = nn.MSELoss()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training loop\n&gt;&gt;&gt; for epoch in range(100):\n...     # Forward pass\n...     y_pred = model(X_train)\n...     loss = criterion(y_pred, y_train)\n...\n...     # Backward pass and optimization\n...     optimizer.zero_grad()\n...     loss.backward()\n...     optimizer.step()\n...\n...     if (epoch + 1) % 20 == 0:\n...         print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Evaluate on test set\n&gt;&gt;&gt; model.eval()\n&gt;&gt;&gt; with torch.no_grad():\n...     y_pred = model(X_test)\n...     test_loss = criterion(y_pred, y_test)\n...     print(f'Test Loss: {test_loss.item():.4f}')\n</code></pre> <p>Using PyTorch Dataset and DataLoader (recommended for larger datasets):</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch.utils.data import Dataset, DataLoader\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Custom Dataset class for diabetes data\n&gt;&gt;&gt; class DiabetesDataset(Dataset):\n...     def __init__(self, X, y):\n...         self.X = torch.FloatTensor(X)\n...         self.y = torch.FloatTensor(y)\n...\n...     def __len__(self):\n...         return len(self.X)\n...\n...     def __getitem__(self, idx):\n...         return self.X[idx], self.y[idx]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.2, random_state=42\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Scale data\n&gt;&gt;&gt; scaler = StandardScaler()\n&gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n&gt;&gt;&gt; X_test = scaler.transform(X_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create Dataset and DataLoader\n&gt;&gt;&gt; train_dataset = DiabetesDataset(X_train, y_train)\n&gt;&gt;&gt; test_dataset = DiabetesDataset(X_test, y_test)\n&gt;&gt;&gt; train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n&gt;&gt;&gt; test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=32,\n...                         num_hidden_layers=2, activation=\"ReLU\")\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n&gt;&gt;&gt; criterion = nn.MSELoss()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training loop with DataLoader\n&gt;&gt;&gt; for epoch in range(100):\n...     model.train()\n...     for batch_X, batch_y in train_loader:\n...         # Forward pass\n...         predictions = model(batch_X)\n...         loss = criterion(predictions, batch_y)\n...\n...         # Backward pass\n...         optimizer.zero_grad()\n...         loss.backward()\n...         optimizer.step()\n...\n...     # Validation\n...     if (epoch + 1) % 20 == 0:\n...         model.eval()\n...         val_loss = 0.0\n...         with torch.no_grad():\n...             for batch_X, batch_y in test_loader:\n...                 predictions = model(batch_X)\n...                 val_loss += criterion(predictions, batch_y).item()\n...         val_loss /= len(test_loader)\n...         print(f'Epoch [{epoch+1}/100], Val Loss: {val_loss:.4f}')\n</code></pre> Note <ul> <li>When num_hidden_layers=0, the model performs pure linear regression</li> <li>Activation functions are only applied between hidden layers, not on output</li> <li>Use get_optimizer() method for convenient optimizer instantiation</li> <li>For large datasets, use PyTorch Dataset and DataLoader for efficient batch processing</li> </ul> Source code in <code>src/spotoptim/nn/linear_regressor.py</code> <pre><code>class LinearRegressor(nn.Module):\n    \"\"\"PyTorch neural network for regression with configurable architecture.\n\n    A flexible regression model that supports:\n    - Pure linear regression (no hidden layers)\n    - Deep neural networks with multiple hidden layers\n    - Various activation functions (ReLU, Tanh, Sigmoid, etc.)\n    - Easy optimizer selection (Adam, SGD, RMSprop, etc.)\n\n    Args:\n        input_dim (int): Number of input features.\n        output_dim (int): Number of output features/targets.\n        l1 (int, optional): Number of neurons in each hidden layer. Defaults to 64.\n        num_hidden_layers (int, optional): Number of hidden layers. Set to 0 for pure\n            linear regression. Defaults to 0.\n        activation (str, optional): Name of activation function from torch.nn to use\n            between layers. Common options: \"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\",\n            \"ELU\", \"SELU\", \"GELU\", \"Softplus\", \"Softsign\", \"Mish\". Defaults to \"ReLU\".\n        lr (float, optional): Unified learning rate multiplier. This value is automatically\n            scaled to optimizer-specific learning rates using the map_lr() function.\n            A value of 1.0 corresponds to the optimizer's default learning rate.\n            For example, lr=1.0 gives 0.001 for Adam and 0.01 for SGD. Typical range:\n            [0.001, 100.0]. Defaults to 1.0.\n\n\n    Attributes:\n        input_dim (int): Number of input features.\n        output_dim (int): Number of output features.\n        l1 (int): Number of neurons per hidden layer.\n        num_hidden_layers (int): Number of hidden layers in the network.\n        activation_name (str): Name of the activation function.\n        activation (nn.Module): Instance of the activation function.\n        lr (float): Unified learning rate multiplier.\n        network (nn.Sequential): The complete neural network architecture.\n\n\n    Raises:\n        ValueError: If the specified activation function is not found in torch.nn.\n\n    Examples:\n        Basic usage with pure linear regression:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from spotoptim.nn.linear_regressor import LinearRegressor\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Pure linear regression (no hidden layers)\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1)\n        &gt;&gt;&gt; x = torch.randn(32, 10)  # Batch of 32 samples\n        &gt;&gt;&gt; y_pred = model(x)\n        &gt;&gt;&gt; print(y_pred.shape)\n        torch.Size([32, 1])\n\n        Single hidden layer with custom neurons:\n\n        &gt;&gt;&gt; # Single hidden layer with 64 neurons and ReLU activation\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=64, num_hidden_layers=1)\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.001)\n\n        Deep network with custom activation:\n\n        &gt;&gt;&gt; # Three hidden layers with 128 neurons each and Tanh activation\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=128,\n        ...                         num_hidden_layers=3, activation=\"Tanh\")\n\n        Complete example using diabetes dataset:\n\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; from sklearn.model_selection import train_test_split\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load and prepare data\n        &gt;&gt;&gt; diabetes = load_diabetes()\n        &gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Split and scale data\n        &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, test_size=0.2, random_state=42\n        ... )\n        &gt;&gt;&gt; scaler = StandardScaler()\n        &gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n        &gt;&gt;&gt; X_test = scaler.transform(X_test)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Convert to PyTorch tensors\n        &gt;&gt;&gt; X_train = torch.FloatTensor(X_train)\n        &gt;&gt;&gt; y_train = torch.FloatTensor(y_train)\n        &gt;&gt;&gt; X_test = torch.FloatTensor(X_test)\n        &gt;&gt;&gt; y_test = torch.FloatTensor(y_test)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create model with 2 hidden layers\n        &gt;&gt;&gt; model = LinearRegressor(\n        ...     input_dim=10,  # diabetes dataset has 10 features\n        ...     output_dim=1,\n        ...     l1=32,\n        ...     num_hidden_layers=2,\n        ...     activation=\"ReLU\"\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get optimizer and loss function\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n        &gt;&gt;&gt; criterion = nn.MSELoss()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Training loop\n        &gt;&gt;&gt; for epoch in range(100):\n        ...     # Forward pass\n        ...     y_pred = model(X_train)\n        ...     loss = criterion(y_pred, y_train)\n        ...\n        ...     # Backward pass and optimization\n        ...     optimizer.zero_grad()\n        ...     loss.backward()\n        ...     optimizer.step()\n        ...\n        ...     if (epoch + 1) % 20 == 0:\n        ...         print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Evaluate on test set\n        &gt;&gt;&gt; model.eval()\n        &gt;&gt;&gt; with torch.no_grad():\n        ...     y_pred = model(X_test)\n        ...     test_loss = criterion(y_pred, y_test)\n        ...     print(f'Test Loss: {test_loss.item():.4f}')\n\n        Using PyTorch Dataset and DataLoader (recommended for larger datasets):\n\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; from sklearn.model_selection import train_test_split\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from torch.utils.data import Dataset, DataLoader\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Custom Dataset class for diabetes data\n        &gt;&gt;&gt; class DiabetesDataset(Dataset):\n        ...     def __init__(self, X, y):\n        ...         self.X = torch.FloatTensor(X)\n        ...         self.y = torch.FloatTensor(y)\n        ...\n        ...     def __len__(self):\n        ...         return len(self.X)\n        ...\n        ...     def __getitem__(self, idx):\n        ...         return self.X[idx], self.y[idx]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load and prepare data\n        &gt;&gt;&gt; diabetes = load_diabetes()\n        &gt;&gt;&gt; X, y = diabetes.data, diabetes.target.reshape(-1, 1)\n        &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, test_size=0.2, random_state=42\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Scale data\n        &gt;&gt;&gt; scaler = StandardScaler()\n        &gt;&gt;&gt; X_train = scaler.fit_transform(X_train)\n        &gt;&gt;&gt; X_test = scaler.transform(X_test)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create Dataset and DataLoader\n        &gt;&gt;&gt; train_dataset = DiabetesDataset(X_train, y_train)\n        &gt;&gt;&gt; test_dataset = DiabetesDataset(X_test, y_test)\n        &gt;&gt;&gt; train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n        &gt;&gt;&gt; test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create model\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=32,\n        ...                         num_hidden_layers=2, activation=\"ReLU\")\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=0.01)\n        &gt;&gt;&gt; criterion = nn.MSELoss()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Training loop with DataLoader\n        &gt;&gt;&gt; for epoch in range(100):\n        ...     model.train()\n        ...     for batch_X, batch_y in train_loader:\n        ...         # Forward pass\n        ...         predictions = model(batch_X)\n        ...         loss = criterion(predictions, batch_y)\n        ...\n        ...         # Backward pass\n        ...         optimizer.zero_grad()\n        ...         loss.backward()\n        ...         optimizer.step()\n        ...\n        ...     # Validation\n        ...     if (epoch + 1) % 20 == 0:\n        ...         model.eval()\n        ...         val_loss = 0.0\n        ...         with torch.no_grad():\n        ...             for batch_X, batch_y in test_loader:\n        ...                 predictions = model(batch_X)\n        ...                 val_loss += criterion(predictions, batch_y).item()\n        ...         val_loss /= len(test_loader)\n        ...         print(f'Epoch [{epoch+1}/100], Val Loss: {val_loss:.4f}')\n\n    Note:\n        - When num_hidden_layers=0, the model performs pure linear regression\n        - Activation functions are only applied between hidden layers, not on output\n        - Use get_optimizer() method for convenient optimizer instantiation\n        - For large datasets, use PyTorch Dataset and DataLoader for efficient batch processing\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        l1=64,\n        num_hidden_layers=0,\n        activation=\"ReLU\",\n        lr=1.0,\n    ):\n        super(LinearRegressor, self).__init__()\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.l1 = l1\n        self.num_hidden_layers = num_hidden_layers\n        self.activation_name = activation\n        self.lr = lr\n\n        # Get activation function class from string\n        if hasattr(nn, activation):\n            activation_class = getattr(nn, activation)\n            self.activation = activation_class()\n        else:\n            raise ValueError(\n                f\"Activation function '{activation}' not found in torch.nn. \"\n                f\"Please use a valid PyTorch activation function name like 'ReLU', 'Sigmoid', 'Tanh', etc.\"\n            )\n\n        # Build the network layers\n        layers = []\n\n        if num_hidden_layers == 0:\n            # Pure linear regression (no hidden layers)\n            layers.append(nn.Linear(input_dim, output_dim))\n        else:\n            # Input layer to first hidden layer\n            layers.append(nn.Linear(input_dim, l1))\n            layers.append(self.activation)\n\n            # Additional hidden layers\n            for _ in range(num_hidden_layers - 1):\n                layers.append(nn.Linear(l1, l1))\n                # Create a new instance of the activation for each layer\n                layers.append(getattr(nn, self.activation_name)())\n\n            # Final hidden layer to output\n            layers.append(nn.Linear(l1, output_dim))\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x) -&gt; \"torch.Tensor\":\n        \"\"\"Forward pass through the network.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n\n        Returns:\n            torch.Tensor: Output predictions of shape (batch_size, output_dim).\n\n        Examples:\n            &gt;&gt;&gt; model = LinearRegressor(input_dim=5, output_dim=1)\n            &gt;&gt;&gt; x = torch.randn(10, 5)  # 10 samples, 5 features\n            &gt;&gt;&gt; output = model(x)\n            &gt;&gt;&gt; print(output.shape)\n            torch.Size([10, 1])\n        \"\"\"\n        return self.network(x)\n\n    def get_optimizer(\n        self, optimizer_name: str = \"Adam\", lr: float = None, **kwargs: Any\n    ) -&gt; \"optim.Optimizer\":\n        \"\"\"Get a PyTorch optimizer configured for this model.\n\n        Convenience method to instantiate optimizers using string names instead of\n        importing optimizer classes. Automatically configures the optimizer with the\n        model's parameters and applies learning rate mapping for unified interface.\n\n        If lr is not specified, uses the model's lr attribute (default 1.0) which\n        is automatically mapped to optimizer-specific learning rates using map_lr().\n        For example, lr=1.0 gives 0.001 for Adam, 0.01 for SGD, etc.\n\n        Args:\n            optimizer_name (str, optional): Name of the optimizer from torch.optim.\n                Common options: \"Adam\", \"AdamW\", \"Adamax\", \"SGD\", \"RMSprop\", \"Adagrad\",\n                \"Adadelta\", \"NAdam\", \"RAdam\", \"ASGD\", \"LBFGS\", \"Rprop\".\n                Defaults to \"Adam\".\n            lr (float, optional): Unified learning rate multiplier. If None, uses self.lr.\n                This value is automatically scaled to optimizer-specific learning rates.\n                A value of 1.0 corresponds to the optimizer's default learning rate.\n                Typical range: [0.001, 100.0]. Defaults to None (uses self.lr).\n            **kwargs: Additional optimizer-specific parameters (e.g., momentum for SGD,\n                weight_decay for AdamW, alpha for RMSprop).\n\n        Returns:\n            optim.Optimizer: Configured optimizer instance ready for training.\n\n        Raises:\n            ValueError: If the specified optimizer name is not found in torch.optim or\n                not supported by map_lr().\n\n        Examples:\n            Basic usage with model's default unified lr (1.0):\n\n            &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, lr=1.0)\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\")  # Uses 1.0 * 0.001 = 0.001\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\")   # Uses 1.0 * 0.01 = 0.01\n\n            Using custom unified learning rate in model:\n\n            &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, lr=0.5)\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\")  # Uses 0.5 * 0.001 = 0.0005\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\")   # Uses 0.5 * 0.01 = 0.005\n\n            Override model's lr with method parameter:\n\n            &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, lr=1.0)\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=2.0)  # Uses 2.0 * 0.001 = 0.002\n\n            SGD with momentum and unified learning rate:\n\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\", lr=0.5, momentum=0.9)\n\n            AdamW with weight decay:\n\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"AdamW\", lr=1.0, weight_decay=0.01)\n\n            Complete training example with diabetes dataset:\n\n            &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n            &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; import torch.nn as nn\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Prepare data\n            &gt;&gt;&gt; diabetes = load_diabetes()\n            &gt;&gt;&gt; X = StandardScaler().fit_transform(diabetes.data)\n            &gt;&gt;&gt; y = diabetes.target.reshape(-1, 1)\n            &gt;&gt;&gt; X_tensor = torch.FloatTensor(X)\n            &gt;&gt;&gt; y_tensor = torch.FloatTensor(y)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Create model and optimizer with unified learning rate\n            &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=16,\n            ...                         num_hidden_layers=1, lr=10.0)\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\")  # Uses 10.0 * 0.001 = 0.01\n            &gt;&gt;&gt; criterion = nn.MSELoss()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Training\n            &gt;&gt;&gt; for epoch in range(50):\n            ...     optimizer.zero_grad()\n            ...     predictions = model(X_tensor)\n            ...     loss = criterion(predictions, y_tensor)\n            ...     loss.backward()\n            ...     optimizer.step()\n\n            Using with DataLoader for mini-batch training:\n\n            &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n            &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n            &gt;&gt;&gt; from torch.utils.data import Dataset, DataLoader\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; import torch.nn as nn\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Custom Dataset\n            &gt;&gt;&gt; class DiabetesDataset(Dataset):\n            ...     def __init__(self, X, y):\n            ...         self.X = torch.FloatTensor(X)\n            ...         self.y = torch.FloatTensor(y)\n            ...\n            ...     def __len__(self):\n            ...         return len(self.X)\n            ...\n            ...     def __getitem__(self, idx):\n            ...         return self.X[idx], self.y[idx]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Prepare data\n            &gt;&gt;&gt; diabetes = load_diabetes()\n            &gt;&gt;&gt; X = StandardScaler().fit_transform(diabetes.data)\n            &gt;&gt;&gt; y = diabetes.target.reshape(-1, 1)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Create DataLoader\n            &gt;&gt;&gt; dataset = DiabetesDataset(X, y)\n            &gt;&gt;&gt; dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Create model and optimizer with unified learning rate\n            &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=16,\n            ...                         num_hidden_layers=1, lr=1.0)\n            &gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\", momentum=0.9)  # Uses 1.0 * 0.01 = 0.01\n            &gt;&gt;&gt; criterion = nn.MSELoss()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Training with mini-batches\n            &gt;&gt;&gt; for epoch in range(100):\n            ...     for batch_X, batch_y in dataloader:\n            ...         optimizer.zero_grad()\n            ...         predictions = model(batch_X)\n            ...         loss = criterion(predictions, batch_y)\n            ...         loss.backward()\n            ...         optimizer.step()\n\n            Hyperparameter optimization across optimizers:\n\n            &gt;&gt;&gt; from spotoptim import SpotOptim\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; def optimize_model(X):\n            ...     results = []\n            ...     for params in X:\n            ...         lr_unified = 10 ** params[0]  # Log scale: [-2, 2]\n            ...         optimizer_name = params[1]     # Factor: \"Adam\", \"SGD\", \"RMSprop\"\n            ...\n            ...         # Create model with unified lr - automatically scaled per optimizer\n            ...         model = LinearRegressor(input_dim=10, output_dim=1, lr=lr_unified)\n            ...         optimizer = model.get_optimizer(optimizer_name)\n            ...\n            ...         # Train and evaluate\n            ...         # ... training code ...\n            ...         results.append(test_loss)\n            ...     return np.array(results)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; spot = SpotOptim(\n            ...     fun=optimize_model,\n            ...     bounds=[(-2, 2), (\"Adam\", \"SGD\", \"RMSprop\")],\n            ...     var_type=[\"num\", \"factor\"]\n            ... )\n\n        Note:\n            - The optimizer uses self.parameters() automatically\n            - Learning rates are mapped using spotoptim.utils.mapping.map_lr()\n            - Unified lr interface enables fair comparison across optimizers\n            - A unified lr of 1.0 always corresponds to optimizer's PyTorch default\n            - DataLoader enables efficient mini-batch training and data shuffling\n        \"\"\"\n        from spotoptim.utils.mapping import map_lr\n\n        # Use model's lr if not specified\n        if lr is None:\n            lr = self.lr\n\n        # Map unified learning rate to optimizer-specific learning rate\n        try:\n            lr_actual = map_lr(lr, optimizer_name)\n        except ValueError:\n            # If optimizer not in map_lr, try to use it directly with torch.optim\n            if not hasattr(optim, optimizer_name):\n                raise ValueError(\n                    f\"Optimizer '{optimizer_name}' not found in torch.optim and not supported by map_lr(). \"\n                    f\"Please use a valid PyTorch optimizer name like 'Adam', 'SGD', 'AdamW', etc.\"\n                )\n            # Use unified lr directly if optimizer not in mapping\n            lr_actual = lr\n\n        # Check if optimizer exists in torch.optim\n        if hasattr(optim, optimizer_name):\n            optimizer_class = getattr(optim, optimizer_name)\n            # Create optimizer with model parameters, mapped learning rate, and additional kwargs\n            return optimizer_class(self.parameters(), lr=lr_actual, **kwargs)\n        else:\n            raise ValueError(\n                f\"Optimizer '{optimizer_name}' not found in torch.optim. \"\n                f\"Please use a valid PyTorch optimizer name like 'Adam', 'SGD', 'AdamW', etc.\"\n            )\n\n    @staticmethod\n    def get_default_parameters() -&gt; \"ParameterSet\":\n        \"\"\"\n        Returns a ParameterSet populated with default hyperparameters for this model.\n        Users can modify bounds and defaults as needed.\n\n        Returns:\n            ParameterSet: Default hyperparameters.\n\n        Examples:\n            &gt;&gt;&gt; params = LinearRegressor.get_default_parameters()\n            &gt;&gt;&gt; print(params)\n            ParameterSet(\n                l1=Parameter(\n                    name='l1',\n                    var_name='l1',\n                    bounds=Bounds(low=16, high=128),\n                    default=64,\n                    log=False,\n                    type='int'\n                ),\n                num_hidden_layers=Parameter(\n                    name='num_hidden_layers',\n                    var_name='num_hidden_layers',\n                    bounds=Bounds(low=0, high=3),\n                    default=0,\n                    log=False,\n                    type='int'\n                ),\n                activation=Parameter(\n                    name='activation',\n                    var_name='activation',\n                    bounds=Bounds(low='ReLU', high='Tanh'),\n                    default='ReLU',\n                    log=False,\n                    type='str'\n                ),\n                lr=Parameter(\n                    name='lr',\n                    var_name='lr',\n                    bounds=Bounds(low=1e-4, high=1e-1),\n                    default=1e-3,\n                    log=True,\n                    type='float'\n                ),\n                optimizer=Parameter(\n                    name='optimizer',\n                    var_name='optimizer',\n                    bounds=Bounds(low='Adam', high='SGD'),\n                    default='Adam',\n                    log=False,\n                    type='str'\n                )\n            )\n        \"\"\"\n        from spotoptim.hyperparameters.parameters import ParameterSet\n\n        params = ParameterSet()\n\n        # l1: neurons in hidden layer\n        params.add_int(name=\"l1\", low=16, high=128, default=64)\n\n        # num_hidden_layers: depth\n        params.add_int(name=\"num_hidden_layers\", low=0, high=3, default=0)\n\n        # activation: function name\n        params.add_factor(\n            name=\"activation\",\n            choices=[\"ReLU\", \"Tanh\", \"Sigmoid\", \"LeakyReLU\", \"ELU\"],\n            default=\"ReLU\",\n        )\n        # lr: Unified learning rate\n        params.add_float(name=\"lr\", low=1e-4, high=10.0, default=1.0, transform=\"log\")\n\n        # optimizer\n        params.add_factor(\n            \"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\"], default=\"Adam\"\n        )\n\n        return params\n</code></pre>"},{"location":"reference/spotoptim/nn/linear_regressor/#spotoptim.nn.linear_regressor.LinearRegressor.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output predictions of shape (batch_size, output_dim).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = LinearRegressor(input_dim=5, output_dim=1)\n&gt;&gt;&gt; x = torch.randn(10, 5)  # 10 samples, 5 features\n&gt;&gt;&gt; output = model(x)\n&gt;&gt;&gt; print(output.shape)\ntorch.Size([10, 1])\n</code></pre> Source code in <code>src/spotoptim/nn/linear_regressor.py</code> <pre><code>def forward(self, x) -&gt; \"torch.Tensor\":\n    \"\"\"Forward pass through the network.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n\n    Returns:\n        torch.Tensor: Output predictions of shape (batch_size, output_dim).\n\n    Examples:\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=5, output_dim=1)\n        &gt;&gt;&gt; x = torch.randn(10, 5)  # 10 samples, 5 features\n        &gt;&gt;&gt; output = model(x)\n        &gt;&gt;&gt; print(output.shape)\n        torch.Size([10, 1])\n    \"\"\"\n    return self.network(x)\n</code></pre>"},{"location":"reference/spotoptim/nn/linear_regressor/#spotoptim.nn.linear_regressor.LinearRegressor.get_default_parameters","title":"<code>get_default_parameters()</code>  <code>staticmethod</code>","text":"<p>Returns a ParameterSet populated with default hyperparameters for this model. Users can modify bounds and defaults as needed.</p> <p>Returns:</p> Name Type Description <code>ParameterSet</code> <code>ParameterSet</code> <p>Default hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; params = LinearRegressor.get_default_parameters()\n&gt;&gt;&gt; print(params)\nParameterSet(\n    l1=Parameter(\n        name='l1',\n        var_name='l1',\n        bounds=Bounds(low=16, high=128),\n        default=64,\n        log=False,\n        type='int'\n    ),\n    num_hidden_layers=Parameter(\n        name='num_hidden_layers',\n        var_name='num_hidden_layers',\n        bounds=Bounds(low=0, high=3),\n        default=0,\n        log=False,\n        type='int'\n    ),\n    activation=Parameter(\n        name='activation',\n        var_name='activation',\n        bounds=Bounds(low='ReLU', high='Tanh'),\n        default='ReLU',\n        log=False,\n        type='str'\n    ),\n    lr=Parameter(\n        name='lr',\n        var_name='lr',\n        bounds=Bounds(low=1e-4, high=1e-1),\n        default=1e-3,\n        log=True,\n        type='float'\n    ),\n    optimizer=Parameter(\n        name='optimizer',\n        var_name='optimizer',\n        bounds=Bounds(low='Adam', high='SGD'),\n        default='Adam',\n        log=False,\n        type='str'\n    )\n)\n</code></pre> Source code in <code>src/spotoptim/nn/linear_regressor.py</code> <pre><code>@staticmethod\ndef get_default_parameters() -&gt; \"ParameterSet\":\n    \"\"\"\n    Returns a ParameterSet populated with default hyperparameters for this model.\n    Users can modify bounds and defaults as needed.\n\n    Returns:\n        ParameterSet: Default hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; params = LinearRegressor.get_default_parameters()\n        &gt;&gt;&gt; print(params)\n        ParameterSet(\n            l1=Parameter(\n                name='l1',\n                var_name='l1',\n                bounds=Bounds(low=16, high=128),\n                default=64,\n                log=False,\n                type='int'\n            ),\n            num_hidden_layers=Parameter(\n                name='num_hidden_layers',\n                var_name='num_hidden_layers',\n                bounds=Bounds(low=0, high=3),\n                default=0,\n                log=False,\n                type='int'\n            ),\n            activation=Parameter(\n                name='activation',\n                var_name='activation',\n                bounds=Bounds(low='ReLU', high='Tanh'),\n                default='ReLU',\n                log=False,\n                type='str'\n            ),\n            lr=Parameter(\n                name='lr',\n                var_name='lr',\n                bounds=Bounds(low=1e-4, high=1e-1),\n                default=1e-3,\n                log=True,\n                type='float'\n            ),\n            optimizer=Parameter(\n                name='optimizer',\n                var_name='optimizer',\n                bounds=Bounds(low='Adam', high='SGD'),\n                default='Adam',\n                log=False,\n                type='str'\n            )\n        )\n    \"\"\"\n    from spotoptim.hyperparameters.parameters import ParameterSet\n\n    params = ParameterSet()\n\n    # l1: neurons in hidden layer\n    params.add_int(name=\"l1\", low=16, high=128, default=64)\n\n    # num_hidden_layers: depth\n    params.add_int(name=\"num_hidden_layers\", low=0, high=3, default=0)\n\n    # activation: function name\n    params.add_factor(\n        name=\"activation\",\n        choices=[\"ReLU\", \"Tanh\", \"Sigmoid\", \"LeakyReLU\", \"ELU\"],\n        default=\"ReLU\",\n    )\n    # lr: Unified learning rate\n    params.add_float(name=\"lr\", low=1e-4, high=10.0, default=1.0, transform=\"log\")\n\n    # optimizer\n    params.add_factor(\n        \"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\"], default=\"Adam\"\n    )\n\n    return params\n</code></pre>"},{"location":"reference/spotoptim/nn/linear_regressor/#spotoptim.nn.linear_regressor.LinearRegressor.get_optimizer","title":"<code>get_optimizer(optimizer_name='Adam', lr=None, **kwargs)</code>","text":"<p>Get a PyTorch optimizer configured for this model.</p> <p>Convenience method to instantiate optimizers using string names instead of importing optimizer classes. Automatically configures the optimizer with the model\u2019s parameters and applies learning rate mapping for unified interface.</p> <p>If lr is not specified, uses the model\u2019s lr attribute (default 1.0) which is automatically mapped to optimizer-specific learning rates using map_lr(). For example, lr=1.0 gives 0.001 for Adam, 0.01 for SGD, etc.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_name</code> <code>str</code> <p>Name of the optimizer from torch.optim. Common options: \u201cAdam\u201d, \u201cAdamW\u201d, \u201cAdamax\u201d, \u201cSGD\u201d, \u201cRMSprop\u201d, \u201cAdagrad\u201d, \u201cAdadelta\u201d, \u201cNAdam\u201d, \u201cRAdam\u201d, \u201cASGD\u201d, \u201cLBFGS\u201d, \u201cRprop\u201d. Defaults to \u201cAdam\u201d.</p> <code>'Adam'</code> <code>lr</code> <code>float</code> <p>Unified learning rate multiplier. If None, uses self.lr. This value is automatically scaled to optimizer-specific learning rates. A value of 1.0 corresponds to the optimizer\u2019s default learning rate. Typical range: [0.001, 100.0]. Defaults to None (uses self.lr).</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional optimizer-specific parameters (e.g., momentum for SGD, weight_decay for AdamW, alpha for RMSprop).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optimizer</code> <p>optim.Optimizer: Configured optimizer instance ready for training.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified optimizer name is not found in torch.optim or not supported by map_lr().</p> <p>Examples:</p> <p>Basic usage with model\u2019s default unified lr (1.0):</p> <pre><code>&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, lr=1.0)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\")  # Uses 1.0 * 0.001 = 0.001\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\")   # Uses 1.0 * 0.01 = 0.01\n</code></pre> <p>Using custom unified learning rate in model:</p> <pre><code>&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, lr=0.5)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\")  # Uses 0.5 * 0.001 = 0.0005\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\")   # Uses 0.5 * 0.01 = 0.005\n</code></pre> <p>Override model\u2019s lr with method parameter:</p> <pre><code>&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, lr=1.0)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=2.0)  # Uses 2.0 * 0.001 = 0.002\n</code></pre> <p>SGD with momentum and unified learning rate:</p> <pre><code>&gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\", lr=0.5, momentum=0.9)\n</code></pre> <p>AdamW with weight decay:</p> <pre><code>&gt;&gt;&gt; optimizer = model.get_optimizer(\"AdamW\", lr=1.0, weight_decay=0.01)\n</code></pre> <p>Complete training example with diabetes dataset:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Prepare data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X = StandardScaler().fit_transform(diabetes.data)\n&gt;&gt;&gt; y = diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt; X_tensor = torch.FloatTensor(X)\n&gt;&gt;&gt; y_tensor = torch.FloatTensor(y)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model and optimizer with unified learning rate\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=16,\n...                         num_hidden_layers=1, lr=10.0)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\")  # Uses 10.0 * 0.001 = 0.01\n&gt;&gt;&gt; criterion = nn.MSELoss()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training\n&gt;&gt;&gt; for epoch in range(50):\n...     optimizer.zero_grad()\n...     predictions = model(X_tensor)\n...     loss = criterion(predictions, y_tensor)\n...     loss.backward()\n...     optimizer.step()\n</code></pre> <p>Using with DataLoader for mini-batch training:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n&gt;&gt;&gt; from torch.utils.data import Dataset, DataLoader\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Custom Dataset\n&gt;&gt;&gt; class DiabetesDataset(Dataset):\n...     def __init__(self, X, y):\n...         self.X = torch.FloatTensor(X)\n...         self.y = torch.FloatTensor(y)\n...\n...     def __len__(self):\n...         return len(self.X)\n...\n...     def __getitem__(self, idx):\n...         return self.X[idx], self.y[idx]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Prepare data\n&gt;&gt;&gt; diabetes = load_diabetes()\n&gt;&gt;&gt; X = StandardScaler().fit_transform(diabetes.data)\n&gt;&gt;&gt; y = diabetes.target.reshape(-1, 1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create DataLoader\n&gt;&gt;&gt; dataset = DiabetesDataset(X, y)\n&gt;&gt;&gt; dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create model and optimizer with unified learning rate\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=16,\n...                         num_hidden_layers=1, lr=1.0)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\", momentum=0.9)  # Uses 1.0 * 0.01 = 0.01\n&gt;&gt;&gt; criterion = nn.MSELoss()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training with mini-batches\n&gt;&gt;&gt; for epoch in range(100):\n...     for batch_X, batch_y in dataloader:\n...         optimizer.zero_grad()\n...         predictions = model(batch_X)\n...         loss = criterion(predictions, batch_y)\n...         loss.backward()\n...         optimizer.step()\n</code></pre> <p>Hyperparameter optimization across optimizers:</p> <pre><code>&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; def optimize_model(X):\n...     results = []\n...     for params in X:\n...         lr_unified = 10 ** params[0]  # Log scale: [-2, 2]\n...         optimizer_name = params[1]     # Factor: \"Adam\", \"SGD\", \"RMSprop\"\n...\n...         # Create model with unified lr - automatically scaled per optimizer\n...         model = LinearRegressor(input_dim=10, output_dim=1, lr=lr_unified)\n...         optimizer = model.get_optimizer(optimizer_name)\n...\n...         # Train and evaluate\n...         # ... training code ...\n...         results.append(test_loss)\n...     return np.array(results)\n&gt;&gt;&gt;\n&gt;&gt;&gt; spot = SpotOptim(\n...     fun=optimize_model,\n...     bounds=[(-2, 2), (\"Adam\", \"SGD\", \"RMSprop\")],\n...     var_type=[\"num\", \"factor\"]\n... )\n</code></pre> Note <ul> <li>The optimizer uses self.parameters() automatically</li> <li>Learning rates are mapped using spotoptim.utils.mapping.map_lr()</li> <li>Unified lr interface enables fair comparison across optimizers</li> <li>A unified lr of 1.0 always corresponds to optimizer\u2019s PyTorch default</li> <li>DataLoader enables efficient mini-batch training and data shuffling</li> </ul> Source code in <code>src/spotoptim/nn/linear_regressor.py</code> <pre><code>def get_optimizer(\n    self, optimizer_name: str = \"Adam\", lr: float = None, **kwargs: Any\n) -&gt; \"optim.Optimizer\":\n    \"\"\"Get a PyTorch optimizer configured for this model.\n\n    Convenience method to instantiate optimizers using string names instead of\n    importing optimizer classes. Automatically configures the optimizer with the\n    model's parameters and applies learning rate mapping for unified interface.\n\n    If lr is not specified, uses the model's lr attribute (default 1.0) which\n    is automatically mapped to optimizer-specific learning rates using map_lr().\n    For example, lr=1.0 gives 0.001 for Adam, 0.01 for SGD, etc.\n\n    Args:\n        optimizer_name (str, optional): Name of the optimizer from torch.optim.\n            Common options: \"Adam\", \"AdamW\", \"Adamax\", \"SGD\", \"RMSprop\", \"Adagrad\",\n            \"Adadelta\", \"NAdam\", \"RAdam\", \"ASGD\", \"LBFGS\", \"Rprop\".\n            Defaults to \"Adam\".\n        lr (float, optional): Unified learning rate multiplier. If None, uses self.lr.\n            This value is automatically scaled to optimizer-specific learning rates.\n            A value of 1.0 corresponds to the optimizer's default learning rate.\n            Typical range: [0.001, 100.0]. Defaults to None (uses self.lr).\n        **kwargs: Additional optimizer-specific parameters (e.g., momentum for SGD,\n            weight_decay for AdamW, alpha for RMSprop).\n\n    Returns:\n        optim.Optimizer: Configured optimizer instance ready for training.\n\n    Raises:\n        ValueError: If the specified optimizer name is not found in torch.optim or\n            not supported by map_lr().\n\n    Examples:\n        Basic usage with model's default unified lr (1.0):\n\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, lr=1.0)\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\")  # Uses 1.0 * 0.001 = 0.001\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\")   # Uses 1.0 * 0.01 = 0.01\n\n        Using custom unified learning rate in model:\n\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, lr=0.5)\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\")  # Uses 0.5 * 0.001 = 0.0005\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\")   # Uses 0.5 * 0.01 = 0.005\n\n        Override model's lr with method parameter:\n\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, lr=1.0)\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\", lr=2.0)  # Uses 2.0 * 0.001 = 0.002\n\n        SGD with momentum and unified learning rate:\n\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\", lr=0.5, momentum=0.9)\n\n        AdamW with weight decay:\n\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"AdamW\", lr=1.0, weight_decay=0.01)\n\n        Complete training example with diabetes dataset:\n\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Prepare data\n        &gt;&gt;&gt; diabetes = load_diabetes()\n        &gt;&gt;&gt; X = StandardScaler().fit_transform(diabetes.data)\n        &gt;&gt;&gt; y = diabetes.target.reshape(-1, 1)\n        &gt;&gt;&gt; X_tensor = torch.FloatTensor(X)\n        &gt;&gt;&gt; y_tensor = torch.FloatTensor(y)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create model and optimizer with unified learning rate\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=16,\n        ...                         num_hidden_layers=1, lr=10.0)\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\")  # Uses 10.0 * 0.001 = 0.01\n        &gt;&gt;&gt; criterion = nn.MSELoss()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Training\n        &gt;&gt;&gt; for epoch in range(50):\n        ...     optimizer.zero_grad()\n        ...     predictions = model(X_tensor)\n        ...     loss = criterion(predictions, y_tensor)\n        ...     loss.backward()\n        ...     optimizer.step()\n\n        Using with DataLoader for mini-batch training:\n\n        &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n        &gt;&gt;&gt; from sklearn.preprocessing import StandardScaler\n        &gt;&gt;&gt; from torch.utils.data import Dataset, DataLoader\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Custom Dataset\n        &gt;&gt;&gt; class DiabetesDataset(Dataset):\n        ...     def __init__(self, X, y):\n        ...         self.X = torch.FloatTensor(X)\n        ...         self.y = torch.FloatTensor(y)\n        ...\n        ...     def __len__(self):\n        ...         return len(self.X)\n        ...\n        ...     def __getitem__(self, idx):\n        ...         return self.X[idx], self.y[idx]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Prepare data\n        &gt;&gt;&gt; diabetes = load_diabetes()\n        &gt;&gt;&gt; X = StandardScaler().fit_transform(diabetes.data)\n        &gt;&gt;&gt; y = diabetes.target.reshape(-1, 1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create DataLoader\n        &gt;&gt;&gt; dataset = DiabetesDataset(X, y)\n        &gt;&gt;&gt; dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create model and optimizer with unified learning rate\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1, l1=16,\n        ...                         num_hidden_layers=1, lr=1.0)\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"SGD\", momentum=0.9)  # Uses 1.0 * 0.01 = 0.01\n        &gt;&gt;&gt; criterion = nn.MSELoss()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Training with mini-batches\n        &gt;&gt;&gt; for epoch in range(100):\n        ...     for batch_X, batch_y in dataloader:\n        ...         optimizer.zero_grad()\n        ...         predictions = model(batch_X)\n        ...         loss = criterion(predictions, batch_y)\n        ...         loss.backward()\n        ...         optimizer.step()\n\n        Hyperparameter optimization across optimizers:\n\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; def optimize_model(X):\n        ...     results = []\n        ...     for params in X:\n        ...         lr_unified = 10 ** params[0]  # Log scale: [-2, 2]\n        ...         optimizer_name = params[1]     # Factor: \"Adam\", \"SGD\", \"RMSprop\"\n        ...\n        ...         # Create model with unified lr - automatically scaled per optimizer\n        ...         model = LinearRegressor(input_dim=10, output_dim=1, lr=lr_unified)\n        ...         optimizer = model.get_optimizer(optimizer_name)\n        ...\n        ...         # Train and evaluate\n        ...         # ... training code ...\n        ...         results.append(test_loss)\n        ...     return np.array(results)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; spot = SpotOptim(\n        ...     fun=optimize_model,\n        ...     bounds=[(-2, 2), (\"Adam\", \"SGD\", \"RMSprop\")],\n        ...     var_type=[\"num\", \"factor\"]\n        ... )\n\n    Note:\n        - The optimizer uses self.parameters() automatically\n        - Learning rates are mapped using spotoptim.utils.mapping.map_lr()\n        - Unified lr interface enables fair comparison across optimizers\n        - A unified lr of 1.0 always corresponds to optimizer's PyTorch default\n        - DataLoader enables efficient mini-batch training and data shuffling\n    \"\"\"\n    from spotoptim.utils.mapping import map_lr\n\n    # Use model's lr if not specified\n    if lr is None:\n        lr = self.lr\n\n    # Map unified learning rate to optimizer-specific learning rate\n    try:\n        lr_actual = map_lr(lr, optimizer_name)\n    except ValueError:\n        # If optimizer not in map_lr, try to use it directly with torch.optim\n        if not hasattr(optim, optimizer_name):\n            raise ValueError(\n                f\"Optimizer '{optimizer_name}' not found in torch.optim and not supported by map_lr(). \"\n                f\"Please use a valid PyTorch optimizer name like 'Adam', 'SGD', 'AdamW', etc.\"\n            )\n        # Use unified lr directly if optimizer not in mapping\n        lr_actual = lr\n\n    # Check if optimizer exists in torch.optim\n    if hasattr(optim, optimizer_name):\n        optimizer_class = getattr(optim, optimizer_name)\n        # Create optimizer with model parameters, mapped learning rate, and additional kwargs\n        return optimizer_class(self.parameters(), lr=lr_actual, **kwargs)\n    else:\n        raise ValueError(\n            f\"Optimizer '{optimizer_name}' not found in torch.optim. \"\n            f\"Please use a valid PyTorch optimizer name like 'Adam', 'SGD', 'AdamW', etc.\"\n        )\n</code></pre>"},{"location":"reference/spotoptim/nn/mlp/","title":"mlp","text":""},{"location":"reference/spotoptim/nn/mlp/#spotoptim.nn.mlp.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Sequential</code></p> <p>This block implements the multi-layer perceptron (MLP) module.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels of the input</p> required <code>hidden_channels</code> <code>List[int]</code> <p>List of the hidden channel dimensions. Note that the last element of this list is the output dimension of the network.</p> <code>None</code> <code>norm_layer</code> <code>Callable[..., Module]</code> <p>Norm layer that will be stacked on top of the linear layer. If <code>None</code> this layer won\u2019t be used. Default: <code>None</code></p> <code>None</code> <code>activation_layer</code> <code>Callable[..., Module]</code> <p>Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If <code>None</code> this layer won\u2019t be used. Default: <code>torch.nn.ReLU</code></p> <code>ReLU</code> <code>inplace</code> <code>bool</code> <p>Parameter for the activation layer, which can optionally do the operation in-place. Default is <code>None</code>, which uses the respective default values of the <code>activation_layer</code> and Dropout layer.</p> <code>None</code> <code>bias</code> <code>bool</code> <p>Whether to use bias in the linear layer. Default <code>True</code></p> <code>True</code> <code>dropout</code> <code>float</code> <p>The probability for the dropout layer. Default: 0.0</p> <code>0.0</code> <code>lr</code> <code>float</code> <p>Unified learning rate multiplier. This value is automatically scaled to optimizer-specific learning rates using the map_lr() function. A value of 1.0 corresponds to the optimizer\u2019s default learning rate. Default: 1.0.</p> <code>1.0</code> <code>l1</code> <code>int</code> <p>Number of neurons in each hidden layer. Will only be used if hidden_channels is None. Default: 64</p> <code>64</code> <code>num_hidden_layers</code> <code>int</code> <p>Number of hidden layers. Will only be used if hidden_channels is None. Default: 2</p> <code>2</code> Note <p>Parameter Definitions:</p> <ul> <li> <p>hidden_channels: This defines the explicit architecture of the MLP. It is a list where each element is the size of a layer.     The last element is the output dimension.     Example: <code>[32, 32, 1]</code> means two hidden layers of size 32 and an output layer of size 1.</p> </li> <li> <p>l1 and num_hidden_layers: These are helper parameters often used in hyperparameter optimization (see <code>get_default_parameters()</code>).     They will only be used if hidden_channels is None.</p> <ul> <li><code>l1</code>: The number of neurons in each hidden layer.</li> <li><code>num_hidden_layers</code>: The number of hidden layers before the output layer.</li> </ul> <p>They describe the architecture in a more compact way but are less flexible than <code>hidden_channels</code>. Relationship: To convert <code>l1</code> and <code>num_hidden_layers</code> to <code>hidden_channels</code> for a given <code>output_dim</code>: <code>hidden_channels = [l1] * num_hidden_layers + [output_dim]</code></p> </li> </ul> <p>Examples:</p> <p>Basic usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.nn.mlp import MLP\n&gt;&gt;&gt; # Input: 10 features. Output (is considered a hidden layer): 30 features. Hidden layer: 20 neurons.\n&gt;&gt;&gt; mlp = MLP(in_channels=10, hidden_channels=[20, 30])\n&gt;&gt;&gt; x = torch.randn(5, 10)\n&gt;&gt;&gt; output = mlp(x)\n&gt;&gt;&gt; print(output.shape)\ntorch.Size([5, 30])\n</code></pre> <p>Using get_optimizer:</p> <pre><code>&gt;&gt;&gt; model = MLP(in_channels=10, hidden_channels=[32, 1], lr=0.5)\n&gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\")  # Uses 0.5 * 0.001\n&gt;&gt;&gt; print(optimizer)\nAdam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0005\n    maximize: False\n    weight_decay: 0\n)\n</code></pre> <p>Using l1 and num_hidden_layers parameters: This example shows how to use the hyperparameters suggested by <code>get_default_parameters()</code> to construct the <code>hidden_channels</code> list.</p> <pre><code>&gt;&gt;&gt; input_dim = 10\n&gt;&gt;&gt; output_dim = 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Hyperparameters (e.g., from spotoptim tuning)\n&gt;&gt;&gt; l1 = 64\n&gt;&gt;&gt; num_hidden_layers = 2\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Construct hidden_channels list\n&gt;&gt;&gt; # [64, 64, 1] -&gt; 2 hidden layers of 64, output layer of 1\n&gt;&gt;&gt; # Relationship: To convert l1 and num_hidden_layers to hidden_channels for a given output_dim:\n&gt;&gt;&gt; # hidden_channels = [l1] * num_hidden_layers + [output_dim]\n&gt;&gt;&gt; # but we can pass l1 and num_hidden_layers directly to the constructor\n&gt;&gt;&gt; model = MLP(in_channels=input_dim, l1=l1, num_hidden_layers=num_hidden_layers, output_dim=output_dim)\n&gt;&gt;&gt; print(model)\nMLP(\n  (0): Linear(in_features=10, out_features=64, bias=True)\n  (1): ReLU()\n  (2): Dropout(p=0.0, inplace=False)\n  (3): Linear(in_features=64, out_features=64, bias=True)\n  (4): ReLU()\n  (5): Dropout(p=0.0, inplace=False)\n  (6): Linear(in_features=64, out_features=1, bias=True)\n  (7): Dropout(p=0.0, inplace=False)\n)\n</code></pre> <p>Getting default parameters for tuning:</p> <pre><code>&gt;&gt;&gt; params = MLP.get_default_parameters()\n&gt;&gt;&gt; print(params.names())\n['l1', 'num_hidden_layers', 'activation', 'lr', 'optimizer']\n</code></pre> Source code in <code>src/spotoptim/nn/mlp.py</code> <pre><code>class MLP(torch.nn.Sequential):\n    \"\"\"This block implements the multi-layer perceptron (MLP) module.\n\n    Args:\n        in_channels (int):\n            Number of channels of the input\n        hidden_channels (List[int]):\n            List of the hidden channel dimensions. Note that the last element of this list is the output dimension of the network.\n        norm_layer (Callable[..., torch.nn.Module], optional):\n            Norm layer that will be stacked on top of the linear layer. If ``None`` this layer won't be used. Default: ``None``\n        activation_layer (Callable[..., torch.nn.Module], optional):\n            Activation function which will be stacked on top of the normalization layer (if not None),\n            otherwise on top of the linear layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU``\n        inplace (bool, optional):\n            Parameter for the activation layer, which can optionally do the operation in-place.\n            Default is ``None``, which uses the respective default values of the ``activation_layer`` and Dropout layer.\n        bias (bool):\n            Whether to use bias in the linear layer. Default ``True``\n        dropout (float):\n            The probability for the dropout layer. Default: 0.0\n        lr (float, optional):\n            Unified learning rate multiplier. This value is automatically scaled to optimizer-specific learning rates using the map_lr() function.\n            A value of 1.0 corresponds to the optimizer's default learning rate. Default: 1.0.\n        l1 (int, optional):\n            Number of neurons in each hidden layer. Will only be used if hidden_channels is None. Default: 64\n        num_hidden_layers (int, optional):\n            Number of hidden layers. Will only be used if hidden_channels is None. Default: 2\n\n    Note:\n        **Parameter Definitions:**\n\n        *   **hidden_channels**: This defines the explicit architecture of the MLP. It is a list where each element is the size of a layer.\n            The last element is the output dimension.\n            Example: ``[32, 32, 1]`` means two hidden layers of size 32 and an output layer of size 1.\n\n        *   **l1** and **num_hidden_layers**: These are helper parameters often used in hyperparameter optimization (see ``get_default_parameters()``).\n            They will only be used if hidden_channels is None.\n            *   ``l1``: The number of neurons in each hidden layer.\n            *   ``num_hidden_layers``: The number of hidden layers *before* the output layer.\n\n            They describe the architecture in a more compact way but are less flexible than ``hidden_channels``.\n            Relationship: To convert ``l1`` and ``num_hidden_layers`` to ``hidden_channels`` for a given ``output_dim``:\n            ``hidden_channels = [l1] * num_hidden_layers + [output_dim]``\n\n    Examples:\n        Basic usage:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from spotoptim.nn.mlp import MLP\n        &gt;&gt;&gt; # Input: 10 features. Output (is considered a hidden layer): 30 features. Hidden layer: 20 neurons.\n        &gt;&gt;&gt; mlp = MLP(in_channels=10, hidden_channels=[20, 30])\n        &gt;&gt;&gt; x = torch.randn(5, 10)\n        &gt;&gt;&gt; output = mlp(x)\n        &gt;&gt;&gt; print(output.shape)\n        torch.Size([5, 30])\n\n        Using get_optimizer:\n\n        &gt;&gt;&gt; model = MLP(in_channels=10, hidden_channels=[32, 1], lr=0.5)\n        &gt;&gt;&gt; optimizer = model.get_optimizer(\"Adam\")  # Uses 0.5 * 0.001\n        &gt;&gt;&gt; print(optimizer)\n        Adam (\n        Parameter Group 0\n            amsgrad: False\n            betas: (0.9, 0.999)\n            capturable: False\n            differentiable: False\n            eps: 1e-08\n            foreach: None\n            fused: None\n            lr: 0.0005\n            maximize: False\n            weight_decay: 0\n        )\n\n        **Using l1 and num_hidden_layers parameters:**\n        This example shows how to use the hyperparameters suggested by ``get_default_parameters()``\n        to construct the ``hidden_channels`` list.\n\n        &gt;&gt;&gt; input_dim = 10\n        &gt;&gt;&gt; output_dim = 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Hyperparameters (e.g., from spotoptim tuning)\n        &gt;&gt;&gt; l1 = 64\n        &gt;&gt;&gt; num_hidden_layers = 2\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Construct hidden_channels list\n        &gt;&gt;&gt; # [64, 64, 1] -&gt; 2 hidden layers of 64, output layer of 1\n        &gt;&gt;&gt; # Relationship: To convert l1 and num_hidden_layers to hidden_channels for a given output_dim:\n        &gt;&gt;&gt; # hidden_channels = [l1] * num_hidden_layers + [output_dim]\n        &gt;&gt;&gt; # but we can pass l1 and num_hidden_layers directly to the constructor\n        &gt;&gt;&gt; model = MLP(in_channels=input_dim, l1=l1, num_hidden_layers=num_hidden_layers, output_dim=output_dim)\n        &gt;&gt;&gt; print(model)\n        MLP(\n          (0): Linear(in_features=10, out_features=64, bias=True)\n          (1): ReLU()\n          (2): Dropout(p=0.0, inplace=False)\n          (3): Linear(in_features=64, out_features=64, bias=True)\n          (4): ReLU()\n          (5): Dropout(p=0.0, inplace=False)\n          (6): Linear(in_features=64, out_features=1, bias=True)\n          (7): Dropout(p=0.0, inplace=False)\n        )\n\n        Getting default parameters for tuning:\n\n        &gt;&gt;&gt; params = MLP.get_default_parameters()\n        &gt;&gt;&gt; print(params.names())\n        ['l1', 'num_hidden_layers', 'activation', 'lr', 'optimizer']\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        hidden_channels: List[int] = None,\n        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n        inplace: Optional[bool] = None,\n        bias: bool = True,\n        dropout: float = 0.0,\n        lr: float = 1.0,\n        l1: int = 64,\n        num_hidden_layers: int = 2,\n        output_dim: int = 1,\n    ):\n        self.lr = lr\n        params = {} if inplace is None else {\"inplace\": inplace}\n        layers = []\n        in_dim = in_channels\n\n        if hidden_channels is None:\n            hidden_channels = [l1] * num_hidden_layers + [output_dim]\n\n        # Loop over all hidden layers except the last one\n        for hidden_dim in hidden_channels[:-1]:\n            layers.append(torch.nn.Linear(in_dim, hidden_dim, bias=bias))\n            if norm_layer is not None:\n                layers.append(norm_layer(hidden_dim))\n            if activation_layer is not None:\n                layers.append(activation_layer(**params))\n            layers.append(torch.nn.Dropout(dropout, **params))\n            in_dim = hidden_dim\n\n        # Last layer\n        layers.append(torch.nn.Linear(in_dim, hidden_channels[-1], bias=bias))\n        layers.append(torch.nn.Dropout(dropout, **params))\n\n        super().__init__(*layers)\n\n    def get_optimizer(\n        self, optimizer_name: str = \"Adam\", lr: float = None, **kwargs: Any\n    ) -&gt; \"optim.Optimizer\":\n        \"\"\"Get a PyTorch optimizer configured for this model.\n\n        Args:\n            optimizer_name (str, optional): Name of the optimizer from torch.optim. Defaults to \"Adam\".\n            lr (float, optional): Unified learning rate multiplier. If None, uses self.lr.\n                This value is automatically scaled to optimizer-specific learning rates.\n                A value of 1.0 corresponds to the optimizer's default learning rate.\n                Defaults to None (uses self.lr).\n            **kwargs: Additional optimizer-specific parameters.\n\n        Returns:\n            optim.Optimizer: Configured optimizer instance ready for training.\n        \"\"\"\n        from spotoptim.utils.mapping import map_lr\n\n        # Use model's lr if not specified\n        if lr is None:\n            lr = self.lr\n\n        # Map unified learning rate to optimizer-specific learning rate\n        try:\n            lr_actual = map_lr(lr, optimizer_name)\n        except ValueError:\n            # If optimizer not in map_lr, try to use it directly with torch.optim\n            if (\n                not hasattr(optim, optimizer_name)\n                and optimizer_name != \"AdamWScheduleFree\"\n            ):\n                raise ValueError(\n                    f\"Optimizer '{optimizer_name}' not found in torch.optim and not supported by map_lr(). \"\n                    f\"Please use a valid PyTorch optimizer name like 'Adam', 'SGD', 'AdamW', etc.\"\n                )\n            # Use unified lr directly if optimizer not in mapping\n            lr_actual = lr\n\n        # Check if optimizer exists in torch.optim\n        if hasattr(optim, optimizer_name):\n            optimizer_class = getattr(optim, optimizer_name)\n            # Create optimizer with model parameters, mapped learning rate, and additional kwargs\n            return optimizer_class(self.parameters(), lr=lr_actual, **kwargs)\n\n        # Check for custom schedulue-free optimizer\n        elif optimizer_name == \"AdamWScheduleFree\":\n            try:\n                from spotoptim.optimizer import AdamWScheduleFree\n\n                return AdamWScheduleFree(self.parameters(), lr=lr_actual, **kwargs)\n            except ImportError as e:\n                raise ImportError(f\"Could not import AdamWScheduleFree: {e}\")\n\n        else:\n            raise ValueError(\n                f\"Optimizer '{optimizer_name}' not found in torch.optim. \"\n                f\"Please use a valid PyTorch optimizer name like 'Adam', 'SGD', 'AdamW', etc.\"\n            )\n\n    @staticmethod\n    def get_default_parameters() -&gt; \"ParameterSet\":\n        \"\"\"Returns a ParameterSet populated with default hyperparameters for this model.\n\n        Note:\n            Since MLP structure is generic (list of hidden channels), the default parameters\n            provided here are a starting point assuming a simple structure similar to LinearRegressor\n            (l1 units per layer, num_hidden_layers). This might need adjustment for specific architectures.\n\n        Returns:\n            ParameterSet: Default hyperparameters.\n\n        Examples:\n            &gt;&gt;&gt; params = MLP.get_default_parameters()\n            &gt;&gt;&gt; print(params.names())\n            ['l1', 'num_hidden_layers', 'activation', 'lr', 'optimizer']\n        \"\"\"\n        from spotoptim.hyperparameters.parameters import ParameterSet\n\n        params = ParameterSet()\n\n        # l1: neurons in hidden layer\n        params.add_int(name=\"l1\", low=16, high=128, default=64, transform=\"log(x, 2)\")\n\n        # num_hidden_layers: depth\n        params.add_int(name=\"num_hidden_layers\", low=1, high=5, default=3)\n\n        # activation: function name\n        params.add_factor(\n            name=\"activation\",\n            choices=[\"ReLU\", \"Tanh\", \"Sigmoid\", \"LeakyReLU\", \"ELU\"],\n            default=\"ReLU\",\n        )\n\n        # lr: Unified learning rate\n        params.add_float(name=\"lr\", low=1e-4, high=100.0, default=10.0, transform=\"log\")\n\n        # optimizer\n        params.add_factor(\n            \"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\"], default=\"Adam\"\n        )\n\n        return params\n</code></pre>"},{"location":"reference/spotoptim/nn/mlp/#spotoptim.nn.mlp.MLP.get_default_parameters","title":"<code>get_default_parameters()</code>  <code>staticmethod</code>","text":"<p>Returns a ParameterSet populated with default hyperparameters for this model.</p> Note <p>Since MLP structure is generic (list of hidden channels), the default parameters provided here are a starting point assuming a simple structure similar to LinearRegressor (l1 units per layer, num_hidden_layers). This might need adjustment for specific architectures.</p> <p>Returns:</p> Name Type Description <code>ParameterSet</code> <code>ParameterSet</code> <p>Default hyperparameters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; params = MLP.get_default_parameters()\n&gt;&gt;&gt; print(params.names())\n['l1', 'num_hidden_layers', 'activation', 'lr', 'optimizer']\n</code></pre> Source code in <code>src/spotoptim/nn/mlp.py</code> <pre><code>@staticmethod\ndef get_default_parameters() -&gt; \"ParameterSet\":\n    \"\"\"Returns a ParameterSet populated with default hyperparameters for this model.\n\n    Note:\n        Since MLP structure is generic (list of hidden channels), the default parameters\n        provided here are a starting point assuming a simple structure similar to LinearRegressor\n        (l1 units per layer, num_hidden_layers). This might need adjustment for specific architectures.\n\n    Returns:\n        ParameterSet: Default hyperparameters.\n\n    Examples:\n        &gt;&gt;&gt; params = MLP.get_default_parameters()\n        &gt;&gt;&gt; print(params.names())\n        ['l1', 'num_hidden_layers', 'activation', 'lr', 'optimizer']\n    \"\"\"\n    from spotoptim.hyperparameters.parameters import ParameterSet\n\n    params = ParameterSet()\n\n    # l1: neurons in hidden layer\n    params.add_int(name=\"l1\", low=16, high=128, default=64, transform=\"log(x, 2)\")\n\n    # num_hidden_layers: depth\n    params.add_int(name=\"num_hidden_layers\", low=1, high=5, default=3)\n\n    # activation: function name\n    params.add_factor(\n        name=\"activation\",\n        choices=[\"ReLU\", \"Tanh\", \"Sigmoid\", \"LeakyReLU\", \"ELU\"],\n        default=\"ReLU\",\n    )\n\n    # lr: Unified learning rate\n    params.add_float(name=\"lr\", low=1e-4, high=100.0, default=10.0, transform=\"log\")\n\n    # optimizer\n    params.add_factor(\n        \"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\", \"AdamW\"], default=\"Adam\"\n    )\n\n    return params\n</code></pre>"},{"location":"reference/spotoptim/nn/mlp/#spotoptim.nn.mlp.MLP.get_optimizer","title":"<code>get_optimizer(optimizer_name='Adam', lr=None, **kwargs)</code>","text":"<p>Get a PyTorch optimizer configured for this model.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_name</code> <code>str</code> <p>Name of the optimizer from torch.optim. Defaults to \u201cAdam\u201d.</p> <code>'Adam'</code> <code>lr</code> <code>float</code> <p>Unified learning rate multiplier. If None, uses self.lr. This value is automatically scaled to optimizer-specific learning rates. A value of 1.0 corresponds to the optimizer\u2019s default learning rate. Defaults to None (uses self.lr).</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional optimizer-specific parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optimizer</code> <p>optim.Optimizer: Configured optimizer instance ready for training.</p> Source code in <code>src/spotoptim/nn/mlp.py</code> <pre><code>def get_optimizer(\n    self, optimizer_name: str = \"Adam\", lr: float = None, **kwargs: Any\n) -&gt; \"optim.Optimizer\":\n    \"\"\"Get a PyTorch optimizer configured for this model.\n\n    Args:\n        optimizer_name (str, optional): Name of the optimizer from torch.optim. Defaults to \"Adam\".\n        lr (float, optional): Unified learning rate multiplier. If None, uses self.lr.\n            This value is automatically scaled to optimizer-specific learning rates.\n            A value of 1.0 corresponds to the optimizer's default learning rate.\n            Defaults to None (uses self.lr).\n        **kwargs: Additional optimizer-specific parameters.\n\n    Returns:\n        optim.Optimizer: Configured optimizer instance ready for training.\n    \"\"\"\n    from spotoptim.utils.mapping import map_lr\n\n    # Use model's lr if not specified\n    if lr is None:\n        lr = self.lr\n\n    # Map unified learning rate to optimizer-specific learning rate\n    try:\n        lr_actual = map_lr(lr, optimizer_name)\n    except ValueError:\n        # If optimizer not in map_lr, try to use it directly with torch.optim\n        if (\n            not hasattr(optim, optimizer_name)\n            and optimizer_name != \"AdamWScheduleFree\"\n        ):\n            raise ValueError(\n                f\"Optimizer '{optimizer_name}' not found in torch.optim and not supported by map_lr(). \"\n                f\"Please use a valid PyTorch optimizer name like 'Adam', 'SGD', 'AdamW', etc.\"\n            )\n        # Use unified lr directly if optimizer not in mapping\n        lr_actual = lr\n\n    # Check if optimizer exists in torch.optim\n    if hasattr(optim, optimizer_name):\n        optimizer_class = getattr(optim, optimizer_name)\n        # Create optimizer with model parameters, mapped learning rate, and additional kwargs\n        return optimizer_class(self.parameters(), lr=lr_actual, **kwargs)\n\n    # Check for custom schedulue-free optimizer\n    elif optimizer_name == \"AdamWScheduleFree\":\n        try:\n            from spotoptim.optimizer import AdamWScheduleFree\n\n            return AdamWScheduleFree(self.parameters(), lr=lr_actual, **kwargs)\n        except ImportError as e:\n            raise ImportError(f\"Could not import AdamWScheduleFree: {e}\")\n\n    else:\n        raise ValueError(\n            f\"Optimizer '{optimizer_name}' not found in torch.optim. \"\n            f\"Please use a valid PyTorch optimizer name like 'Adam', 'SGD', 'AdamW', etc.\"\n        )\n</code></pre>"},{"location":"reference/spotoptim/optimizer/schedule_free/","title":"schedule_free","text":""},{"location":"reference/spotoptim/optimizer/schedule_free/#spotoptim.optimizer.schedule_free.AdamWScheduleFree","title":"<code>AdamWScheduleFree</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>Schedule-Free AdamW in PyTorch.</p> <p>As the name suggests, no scheduler is needed with this optimizer. To add warmup, rather than using a learning rate schedule you can just set the warmup_steps parameter.</p> <p>This optimizer requires that .train() and .eval() be called on the optimizer before the beginning of training and evaluation respectively.</p> <p>Reference: https://github.com/facebookresearch/schedule_free</p> Source code in <code>src/spotoptim/optimizer/schedule_free.py</code> <pre><code>class AdamWScheduleFree(Optimizer):\n    \"\"\"\n    Schedule-Free AdamW in PyTorch.\n\n    As the name suggests, no scheduler is needed with this optimizer.\n    To add warmup, rather than using a learning rate schedule you can just\n    set the warmup_steps parameter.\n\n    This optimizer requires that .train() and .eval() be called on the optimizer\n    before the beginning of training and evaluation respectively.\n\n    Reference: https://github.com/facebookresearch/schedule_free\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr: float = 0.0025,\n        betas: tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 0,\n        warmup_steps: int = 0,\n        r: float = 0.0,\n        weight_lr_power: float = 2.0,\n    ):\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            warmup_steps=warmup_steps,\n            r=r,\n            weight_lr_power=weight_lr_power,\n        )\n        super().__init__(params, defaults)\n\n    def eval(self):\n        \"\"\"Switch model parameters to evaluation mode (using averaged parameters 'x').\"\"\"\n        for group in self.param_groups:\n            # Check if we are already in eval mode for this group?\n            # The original implementation used a self.train_mode flag.\n            # We can store it in the group or self.\n            if not group.get(\"train_mode\", True):\n                continue\n\n            beta1, _ = group[\"betas\"]\n            for p in group[\"params\"]:\n                state = self.state[p]\n                if \"z\" in state:\n                    z = state[\"z\"]\n                    # p.data currently holds 'y'\n                    # x = (1 - 1/beta1) * y + (1/beta1) * z\n                    # Using in-place ops to save memory where possible.\n                    # x = p + (1.0/beta1 - 1.0) * (z - p) ???\n                    # Wait, formula from original code was:\n                    # new_p = p + w * (z - p) where w = 1.0 - 1.0/beta1\n\n                    w = 1.0 - 1.0 / beta1\n                    # x = y + w * (z - y)\n                    p.data.add_(z - p.data, alpha=w)\n\n            group[\"train_mode\"] = False\n\n    def train(self):\n        \"\"\"Switch model parameters to training mode (using current iterate 'y').\"\"\"\n        for group in self.param_groups:\n            if group.get(\"train_mode\", True):\n                continue\n\n            beta1, _ = group[\"betas\"]\n            for p in group[\"params\"]:\n                state = self.state[p]\n                if \"z\" in state:\n                    z = state[\"z\"]\n                    # p.data currently holds 'x'\n                    # We need to recover 'y'.\n                    # Reverse of eval:\n                    # x = y + w * (z - y)\n                    # x - w*z = y * (1-w)\n                    # y = (x - w*z) / (1-w)\n                    # This seems unstable to invert?\n                    #\n                    # Alternative from reference implementation:\n                    # y = x + (1 - beta1) * (z - x) ? No, let's check basic algebra.\n                    # w = 1 - 1/beta1\n                    # x = y + (1 - 1/beta1)(z - y)\n                    #\n                    # Actually, the reference implementation uses:\n                    # y = x + (1 - beta1) * (z - x)\n                    # Let's verify compatibility.\n                    # If x = (1 - 1/beta1) * y + (1/beta1) * z\n                    # Let's try substituting y = x + (1 - beta1)(z - x)\n                    # y = x + (z - x) - beta1(z - x) = z - beta1(z - x)\n                    # This inversion logic is tricky.\n                    #\n                    # Let's rely on the previous MLX implementation logic:\n                    # train(): new_p = p + (1.0 - beta1) * (z - p)\n                    # where p was 'x' and becomes 'y'.\n\n                    w = 1.0 - beta1\n                    p.data.add_(z - p.data, alpha=w)\n\n            group[\"train_mode\"] = True\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            if not group.get(\"train_mode\", True):\n                raise RuntimeError(\n                    \"Optimizer step() called in eval mode. Use optimizer.train() first.\"\n                )\n\n            lr = group[\"lr\"]\n            beta1, beta2 = group[\"betas\"]\n            eps = group[\"eps\"]\n            weight_decay = group[\"weight_decay\"]\n            warmup_steps = group[\"warmup_steps\"]\n            r = group[\"r\"]\n            weight_lr_power = group[\"weight_lr_power\"]\n\n            # Initialize training state for group if needed\n            if \"k\" not in group:\n                group[\"k\"] = 0\n                group[\"weight_sum\"] = 0.0\n                group[\"lr_max\"] = -1.0\n                group[\"train_mode\"] = True\n\n            k = group[\"k\"]\n\n            # Schedule setup\n            if k &lt; warmup_steps:\n                sched = (k + 1) / warmup_steps\n            else:\n                sched = 1.0\n\n            bias_correction2 = 1 - beta2 ** (k + 1)\n            current_lr = lr * sched\n\n            group[\"lr_max\"] = max(current_lr, group[\"lr_max\"])\n\n            weight = ((k + 1) ** r) * (group[\"lr_max\"] ** weight_lr_power)\n            group[\"weight_sum\"] += weight\n\n            try:\n                ckp1 = weight / group[\"weight_sum\"]\n            except ZeroDivisionError:\n                ckp1 = 0\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n\n                grad = p.grad\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"z\"] = p.clone()\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n\n                z = state[\"z\"]\n                exp_avg_sq = state[\"exp_avg_sq\"]\n\n                # Decay the first and second moment running average coefficient\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                denom = exp_avg_sq.div(bias_correction2).sqrt_().add_(eps)\n\n                # Normalized gradient\n                grad_normalized = grad.div(denom)\n\n                # Weight decay (on y)\n                if weight_decay != 0:\n                    grad_normalized.add_(p, alpha=weight_decay)\n\n                # Update y (p.data)\n                # y_{k+1} = (1 - c_{k+1}) y_k + c_{k+1} z_k\n                # y = y + ckp1 * (z - y)\n                p.data.lerp_(z, ckp1)\n\n                # y_{k+1} = y_{k+1} - \\gamma \\hat{\\nabla} f(y_k)\n                # alpha = lr * (beta1 * (1 - ckp1) - 1)\n                # But wait, original code:\n                # alpha = lr * (beta1 * (1 - ckp1) - 1)\n                # y_new = y_new + grad_normalized * alpha\n\n                alpha = current_lr * (beta1 * (1 - ckp1) - 1)\n                p.data.add_(grad_normalized, alpha=alpha)\n\n                # Update z\n                # z_{k+1} = z_k - \\eta \\hat{\\nabla} f(y_k)\n                z.add_(grad_normalized, alpha=-current_lr)\n\n            group[\"k\"] += 1\n\n        return loss\n</code></pre>"},{"location":"reference/spotoptim/optimizer/schedule_free/#spotoptim.optimizer.schedule_free.AdamWScheduleFree.eval","title":"<code>eval()</code>","text":"<p>Switch model parameters to evaluation mode (using averaged parameters \u2018x\u2019).</p> Source code in <code>src/spotoptim/optimizer/schedule_free.py</code> <pre><code>def eval(self):\n    \"\"\"Switch model parameters to evaluation mode (using averaged parameters 'x').\"\"\"\n    for group in self.param_groups:\n        # Check if we are already in eval mode for this group?\n        # The original implementation used a self.train_mode flag.\n        # We can store it in the group or self.\n        if not group.get(\"train_mode\", True):\n            continue\n\n        beta1, _ = group[\"betas\"]\n        for p in group[\"params\"]:\n            state = self.state[p]\n            if \"z\" in state:\n                z = state[\"z\"]\n                # p.data currently holds 'y'\n                # x = (1 - 1/beta1) * y + (1/beta1) * z\n                # Using in-place ops to save memory where possible.\n                # x = p + (1.0/beta1 - 1.0) * (z - p) ???\n                # Wait, formula from original code was:\n                # new_p = p + w * (z - p) where w = 1.0 - 1.0/beta1\n\n                w = 1.0 - 1.0 / beta1\n                # x = y + w * (z - y)\n                p.data.add_(z - p.data, alpha=w)\n\n        group[\"train_mode\"] = False\n</code></pre>"},{"location":"reference/spotoptim/optimizer/schedule_free/#spotoptim.optimizer.schedule_free.AdamWScheduleFree.step","title":"<code>step(closure=None)</code>","text":"<p>Performs a single optimization step.</p> Source code in <code>src/spotoptim/optimizer/schedule_free.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure=None):\n    \"\"\"Performs a single optimization step.\"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n\n    for group in self.param_groups:\n        if not group.get(\"train_mode\", True):\n            raise RuntimeError(\n                \"Optimizer step() called in eval mode. Use optimizer.train() first.\"\n            )\n\n        lr = group[\"lr\"]\n        beta1, beta2 = group[\"betas\"]\n        eps = group[\"eps\"]\n        weight_decay = group[\"weight_decay\"]\n        warmup_steps = group[\"warmup_steps\"]\n        r = group[\"r\"]\n        weight_lr_power = group[\"weight_lr_power\"]\n\n        # Initialize training state for group if needed\n        if \"k\" not in group:\n            group[\"k\"] = 0\n            group[\"weight_sum\"] = 0.0\n            group[\"lr_max\"] = -1.0\n            group[\"train_mode\"] = True\n\n        k = group[\"k\"]\n\n        # Schedule setup\n        if k &lt; warmup_steps:\n            sched = (k + 1) / warmup_steps\n        else:\n            sched = 1.0\n\n        bias_correction2 = 1 - beta2 ** (k + 1)\n        current_lr = lr * sched\n\n        group[\"lr_max\"] = max(current_lr, group[\"lr_max\"])\n\n        weight = ((k + 1) ** r) * (group[\"lr_max\"] ** weight_lr_power)\n        group[\"weight_sum\"] += weight\n\n        try:\n            ckp1 = weight / group[\"weight_sum\"]\n        except ZeroDivisionError:\n            ckp1 = 0\n\n        for p in group[\"params\"]:\n            if p.grad is None:\n                continue\n\n            grad = p.grad\n            state = self.state[p]\n\n            # State initialization\n            if len(state) == 0:\n                state[\"z\"] = p.clone()\n                state[\"exp_avg_sq\"] = torch.zeros_like(p)\n\n            z = state[\"z\"]\n            exp_avg_sq = state[\"exp_avg_sq\"]\n\n            # Decay the first and second moment running average coefficient\n            exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n            denom = exp_avg_sq.div(bias_correction2).sqrt_().add_(eps)\n\n            # Normalized gradient\n            grad_normalized = grad.div(denom)\n\n            # Weight decay (on y)\n            if weight_decay != 0:\n                grad_normalized.add_(p, alpha=weight_decay)\n\n            # Update y (p.data)\n            # y_{k+1} = (1 - c_{k+1}) y_k + c_{k+1} z_k\n            # y = y + ckp1 * (z - y)\n            p.data.lerp_(z, ckp1)\n\n            # y_{k+1} = y_{k+1} - \\gamma \\hat{\\nabla} f(y_k)\n            # alpha = lr * (beta1 * (1 - ckp1) - 1)\n            # But wait, original code:\n            # alpha = lr * (beta1 * (1 - ckp1) - 1)\n            # y_new = y_new + grad_normalized * alpha\n\n            alpha = current_lr * (beta1 * (1 - ckp1) - 1)\n            p.data.add_(grad_normalized, alpha=alpha)\n\n            # Update z\n            # z_{k+1} = z_k - \\eta \\hat{\\nabla} f(y_k)\n            z.add_(grad_normalized, alpha=-current_lr)\n\n        group[\"k\"] += 1\n\n    return loss\n</code></pre>"},{"location":"reference/spotoptim/optimizer/schedule_free/#spotoptim.optimizer.schedule_free.AdamWScheduleFree.train","title":"<code>train()</code>","text":"<p>Switch model parameters to training mode (using current iterate \u2018y\u2019).</p> Source code in <code>src/spotoptim/optimizer/schedule_free.py</code> <pre><code>def train(self):\n    \"\"\"Switch model parameters to training mode (using current iterate 'y').\"\"\"\n    for group in self.param_groups:\n        if group.get(\"train_mode\", True):\n            continue\n\n        beta1, _ = group[\"betas\"]\n        for p in group[\"params\"]:\n            state = self.state[p]\n            if \"z\" in state:\n                z = state[\"z\"]\n                # p.data currently holds 'x'\n                # We need to recover 'y'.\n                # Reverse of eval:\n                # x = y + w * (z - y)\n                # x - w*z = y * (1-w)\n                # y = (x - w*z) / (1-w)\n                # This seems unstable to invert?\n                #\n                # Alternative from reference implementation:\n                # y = x + (1 - beta1) * (z - x) ? No, let's check basic algebra.\n                # w = 1 - 1/beta1\n                # x = y + (1 - 1/beta1)(z - y)\n                #\n                # Actually, the reference implementation uses:\n                # y = x + (1 - beta1) * (z - x)\n                # Let's verify compatibility.\n                # If x = (1 - 1/beta1) * y + (1/beta1) * z\n                # Let's try substituting y = x + (1 - beta1)(z - x)\n                # y = x + (z - x) - beta1(z - x) = z - beta1(z - x)\n                # This inversion logic is tricky.\n                #\n                # Let's rely on the previous MLX implementation logic:\n                # train(): new_p = p + (1.0 - beta1) * (z - p)\n                # where p was 'x' and becomes 'y'.\n\n                w = 1.0 - beta1\n                p.data.add_(z - p.data, alpha=w)\n\n        group[\"train_mode\"] = True\n</code></pre>"},{"location":"reference/spotoptim/plot/contour/","title":"contour","text":""},{"location":"reference/spotoptim/plot/contour/#spotoptim.plot.contour.contourf_plot","title":"<code>contourf_plot(data, x_col, y_col, z_col, facet_col=None, aspect=1, as_table=True, figsize=(4, 4), levels=10, cmap='viridis', show_contour_lines=True, contour_line_color='black', contour_line_width=0.5, colorbar_orientation='vertical', wspace=0.4, hspace=0.4, highlight_point=None, highlight_color='red', highlight_marker='x', highlight_size=100)</code>","text":"<p>Creates contour plots (single or faceted) using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Data for plotting.</p> required <code>x_col</code> <code>str</code> <p>Column name for x-axis.</p> required <code>y_col</code> <code>str</code> <p>Column name for y-axis.</p> required <code>z_col</code> <code>str</code> <p>Column name for z-axis (values).</p> required <code>facet_col</code> <code>str</code> <p>Column name for faceting.</p> <code>None</code> <code>aspect</code> <code>float</code> <p>Aspect ratio.</p> <code>1</code> <code>as_table</code> <code>bool</code> <p>If True, arranges facets in a table.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size per plot key (if not faceted) or base.</p> <code>(4, 4)</code> <code>levels</code> <code>int</code> <p>Number of contour levels.</p> <code>10</code> <code>cmap</code> <code>str</code> <p>Colormap.</p> <code>'viridis'</code> <code>show_contour_lines</code> <code>bool</code> <p>Whether to show contour lines.</p> <code>True</code> <code>contour_line_color</code> <code>str</code> <p>Color of contour lines.</p> <code>'black'</code> <code>contour_line_width</code> <code>float</code> <p>Width of contour lines.</p> <code>0.5</code> <code>colorbar_orientation</code> <code>str</code> <p>\u2018vertical\u2019 or \u2018horizontal\u2019.</p> <code>'vertical'</code> <code>wspace</code> <code>float</code> <p>Width reserved for space between subplots.</p> <code>0.4</code> <code>hspace</code> <code>float</code> <p>Height reserved for space between subplots.</p> <code>0.4</code> <code>highlight_point</code> <code>dict - like</code> <p>Point to highlight. Must contain x_col, y_col keys.                                    If faceted, must contain facet_col key.</p> <code>None</code> <code>highlight_color</code> <code>str</code> <p>Color of the highlight point.</p> <code>'red'</code> <code>highlight_marker</code> <code>str</code> <p>Marker style for the highlight point.</p> <code>'x'</code> <code>highlight_size</code> <code>int</code> <p>Size of the highlight point.</p> <code>100</code> Source code in <code>src/spotoptim/plot/contour.py</code> <pre><code>def contourf_plot(\n    data,\n    x_col,\n    y_col,\n    z_col,\n    facet_col=None,\n    aspect=1,\n    as_table=True,\n    figsize=(4, 4),\n    levels=10,\n    cmap=\"viridis\",\n    show_contour_lines=True,\n    contour_line_color=\"black\",\n    contour_line_width=0.5,\n    colorbar_orientation=\"vertical\",\n    wspace=0.4,\n    hspace=0.4,\n    highlight_point=None,\n    highlight_color=\"red\",\n    highlight_marker=\"x\",\n    highlight_size=100,\n) -&gt; None:\n    \"\"\"\n    Creates contour plots (single or faceted) using matplotlib.\n\n    Args:\n        data (pd.DataFrame): Data for plotting.\n        x_col (str): Column name for x-axis.\n        y_col (str): Column name for y-axis.\n        z_col (str): Column name for z-axis (values).\n        facet_col (str, optional): Column name for faceting.\n        aspect (float, optional): Aspect ratio.\n        as_table (bool, optional): If True, arranges facets in a table.\n        figsize (tuple, optional): Figure size per plot key (if not faceted) or base.\n        levels (int, optional): Number of contour levels.\n        cmap (str, optional): Colormap.\n        show_contour_lines (bool, optional): Whether to show contour lines.\n        contour_line_color (str, optional): Color of contour lines.\n        contour_line_width (float, optional): Width of contour lines.\n        colorbar_orientation (str, optional): 'vertical' or 'horizontal'.\n        wspace (float, optional): Width reserved for space between subplots.\n        hspace (float, optional): Height reserved for space between subplots.\n        highlight_point (dict-like, optional): Point to highlight. Must contain x_col, y_col keys.\n                                               If faceted, must contain facet_col key.\n        highlight_color (str, optional): Color of the highlight point.\n        highlight_marker (str, optional): Marker style for the highlight point.\n        highlight_size (int, optional): Size of the highlight point.\n    \"\"\"\n    if facet_col:\n        facet_values = data[facet_col].unique()\n        num_facets = len(facet_values)\n\n        # Determine subplot layout\n        if as_table:\n            num_cols = int(np.ceil(np.sqrt(num_facets)))\n            num_rows = int(np.ceil(num_facets / num_cols))\n        else:\n            num_cols = num_facets\n            num_rows = 1\n\n        fig, axes = plt.subplots(\n            num_rows,\n            num_cols,\n            figsize=(figsize[0] * num_cols, figsize[1] * num_rows),\n            layout=\"constrained\",\n        )\n        axes = np.array(axes).flatten()  # Flatten the axes array for easy indexing\n\n        for i, facet_value in enumerate(facet_values):\n            ax = axes[i]\n            facet_data = data[data[facet_col] == facet_value]\n\n            # Create grid for contour plot\n            x = np.unique(facet_data[x_col])\n            y = np.unique(facet_data[y_col])\n            X, Y = np.meshgrid(x, y)\n            Z = facet_data.pivot_table(index=y_col, columns=x_col, values=z_col).values\n\n            # Plot contour\n            contour = ax.contourf(X, Y, Z, levels=levels, cmap=cmap)\n            if show_contour_lines:\n                ax.contour(\n                    X,\n                    Y,\n                    Z,\n                    levels=levels,\n                    colors=contour_line_color,\n                    linewidths=contour_line_width,\n                )\n\n            # Highlight point if it belongs to this facet\n            if highlight_point is not None:\n                # Check if highlight_point belongs to current facet\n                # Use approximate comparison for floats\n                hp_facet_val = highlight_point.get(facet_col)\n                if hp_facet_val is not None and np.isclose(hp_facet_val, facet_value):\n                    ax.scatter(\n                        highlight_point[x_col],\n                        highlight_point[y_col],\n                        color=highlight_color,\n                        marker=highlight_marker,\n                        s=highlight_size,\n                        zorder=10,\n                        label=\"Best\",\n                    )\n\n            # Set labels and title\n            ax.set_xlabel(x_col)\n            ax.set_ylabel(y_col)\n            ax.set_title(f\"{facet_col} = {np.round(facet_value, 2)}\")\n            ax.set_aspect(aspect)\n\n        # Remove empty subplots\n        for i in range(num_facets, len(axes)):\n            fig.delaxes(axes[i])\n\n        # Add colorbar\n        fig.colorbar(\n            contour,\n            ax=axes.ravel().tolist(),\n            orientation=colorbar_orientation,\n            shrink=0.8,\n            pad=0.05,\n        )\n\n        # fig.subplots_adjust(wspace=wspace, hspace=hspace)  # Incompatible with constrained_layout\n        plt.show()\n\n    else:\n        # Create grid for contour plot\n        x = np.unique(data[x_col])\n        y = np.unique(data[y_col])\n        X, Y = np.meshgrid(x, y)\n        Z = data.pivot_table(index=y_col, columns=x_col, values=z_col).values\n\n        # Plot contour\n        fig, ax = plt.subplots(figsize=figsize)\n        contour = ax.contourf(X, Y, Z, levels=levels, cmap=cmap)\n        if show_contour_lines:\n            ax.contour(\n                X,\n                Y,\n                Z,\n                levels=levels,\n                colors=contour_line_color,\n                linewidths=contour_line_width,\n            )\n\n        # Highlight point\n        if highlight_point is not None:\n            ax.scatter(\n                highlight_point[x_col],\n                highlight_point[y_col],\n                color=highlight_color,\n                marker=highlight_marker,\n                s=highlight_size,\n                zorder=10,\n                label=\"Best\",\n            )\n\n        # Set labels and title\n        ax.set_xlabel(x_col)\n        ax.set_ylabel(y_col)\n        ax.set_title(f\"Contour Plot of {z_col}\")\n        ax.set_aspect(aspect)\n\n        # Add colorbar using make_axes_locatable to ensure it's outside\n        divider = make_axes_locatable(ax)\n        if colorbar_orientation == \"vertical\":\n            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n        else:\n            cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.1)\n\n        fig.colorbar(\n            contour,\n            cax=cax,\n            orientation=colorbar_orientation,\n        )\n\n        plt.show()\n</code></pre>"},{"location":"reference/spotoptim/plot/contour/#spotoptim.plot.contour.mo_generate_plot_grid","title":"<code>mo_generate_plot_grid(variables, resolutions, functions)</code>","text":"<p>Generate a grid of input variables and apply objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>dict</code> <p>A dictionary where keys are variable names (e.g., \u201ctime\u201d, \u201ctemperature\u201d)               and values are tuples of (min_value, max_value).</p> required <code>resolutions</code> <code>dict</code> <p>A dictionary where keys are variable names and values are the number of points.</p> required <code>functions</code> <code>dict</code> <p>A dictionary where keys are function names and values are callable functions.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the grid and the results of the objective functions.</p> Source code in <code>src/spotoptim/plot/contour.py</code> <pre><code>def mo_generate_plot_grid(variables, resolutions, functions) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a grid of input variables and apply objective functions.\n\n    Args:\n        variables (dict): A dictionary where keys are variable names (e.g., \"time\", \"temperature\")\n                          and values are tuples of (min_value, max_value).\n        resolutions (dict): A dictionary where keys are variable names and values are the number of points.\n        functions (dict): A dictionary where keys are function names and values are callable functions.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the grid and the results of the objective functions.\n    \"\"\"\n    # Create a meshgrid for all variables\n    grids = [\n        np.linspace(variables[var][0], variables[var][1], resolutions[var])\n        for var in variables\n    ]\n    grid = np.array(np.meshgrid(*grids)).T.reshape(-1, len(variables))\n\n    # Create a DataFrame for the grid\n    plot_grid = pd.DataFrame(grid, columns=variables.keys())\n\n    input_cols = list(variables.keys())\n    # Apply each function to the grid\n    for func_name, func in functions.items():\n        plot_grid[func_name] = plot_grid[input_cols].apply(\n            lambda row: func(row.values), axis=1\n        )\n\n    return plot_grid\n</code></pre>"},{"location":"reference/spotoptim/plot/contour/#spotoptim.plot.contour.plotModel","title":"<code>plotModel(model, lower, upper, i=0, j=1, min_z=None, max_z=None, var_type=None, var_name=None, show=True, filename=None, n_grid=50, contour_levels=10, dpi=200, title='', figsize=(12, 6), use_min=False, use_max=False, aspect_equal=True, legend_fontsize=12, cmap='viridis', X_points=None, y_points=None, plot_points=True, points_color='white', points_size=30, point_color_below='blue', point_color_above='red', atol=1e-06, plot_3d=False)</code>","text":"<p>Generate 2D contour and optionally 3D surface plots for a model\u2019s predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>object</code> <p>A model with a predict method.</p> required <code>lower</code> <code>array_like</code> <p>Lower bounds for each dimension.</p> required <code>upper</code> <code>array_like</code> <p>Upper bounds for each dimension.</p> required <code>i</code> <code>int</code> <p>Index for the x-axis dimension.</p> <code>0</code> <code>j</code> <code>int</code> <p>Index for the y-axis dimension.</p> <code>1</code> <code>min_z</code> <code>float</code> <p>Min value for color scaling. Defaults to None.</p> <code>None</code> <code>max_z</code> <code>float</code> <p>Max value for color scaling. Defaults to None.</p> <code>None</code> <code>var_type</code> <code>list</code> <p>Variable types for each dimension. Defaults to None.</p> <code>None</code> <code>var_name</code> <code>list</code> <p>Variable names for labeling axes. Defaults to None.</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>filename</code> <code>str</code> <p>File path to save the figure. Defaults to None.</p> <code>None</code> <code>n_grid</code> <code>int</code> <p>Resolution for each axis. Defaults to 50.</p> <code>50</code> <code>contour_levels</code> <code>int</code> <p>Number of contour levels. Defaults to 10.</p> <code>10</code> <code>dpi</code> <code>int</code> <p>DPI for saving. Defaults to 200.</p> <code>200</code> <code>title</code> <code>str</code> <p>Title for the figure. Defaults to \u201c\u201d.</p> <code>''</code> <code>figsize</code> <code>tuple</code> <p>Figure size. Defaults to (12, 6).</p> <code>(12, 6)</code> <code>use_min</code> <code>bool</code> <p>If True, leftover dims are set to lower bounds.</p> <code>False</code> <code>use_max</code> <code>bool</code> <p>If True, leftover dims are set to upper bounds.</p> <code>False</code> <code>aspect_equal</code> <code>bool</code> <p>Whether axes have equal scaling. Defaults to True.</p> <code>True</code> <code>legend_fontsize</code> <code>int</code> <p>Font size for labels and legends. Defaults to 12.</p> <code>12</code> <code>cmap</code> <code>str</code> <p>Colormap. Defaults to \u201cviridis\u201d.</p> <code>'viridis'</code> <code>X_points</code> <code>ndarray</code> <p>Original data points. Shape: (N, D).</p> <code>None</code> <code>y_points</code> <code>ndarray</code> <p>Original target values. Shape: (N,).</p> <code>None</code> <code>plot_points</code> <code>bool</code> <p>Whether to plot X_points. Defaults to True.</p> <code>True</code> <code>points_color</code> <code>str</code> <p>Fallback color for data points. Defaults to \u201cwhite\u201d.</p> <code>'white'</code> <code>points_size</code> <code>int</code> <p>Marker size for data points. Defaults to 30.</p> <code>30</code> <code>point_color_below</code> <code>str</code> <p>Color if actual z &lt; predicted z. Defaults to \u201cblue\u201d.</p> <code>'blue'</code> <code>point_color_above</code> <code>str</code> <p>Color if actual z &gt;= predicted z. Defaults to \u201cred\u201d.</p> <code>'red'</code> <code>atol</code> <code>float</code> <p>Absolute tolerance for comparing actual and predicted z-values. Defaults to 1e-6.</p> <code>1e-06</code> <code>plot_3d</code> <code>bool</code> <p>Whether to plot the 3D surface plot. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>(fig, (ax_contour, ax_surface))</code> <p>Figure and axes for the contour and surface plots.</p> Source code in <code>src/spotoptim/plot/contour.py</code> <pre><code>def plotModel(\n    model,\n    lower,\n    upper,\n    i=0,\n    j=1,\n    min_z=None,\n    max_z=None,\n    var_type=None,\n    var_name=None,\n    show=True,\n    filename=None,\n    n_grid=50,\n    contour_levels=10,\n    dpi=200,\n    title=\"\",\n    figsize=(12, 6),\n    use_min=False,\n    use_max=False,\n    aspect_equal=True,\n    legend_fontsize=12,\n    cmap=\"viridis\",\n    X_points=None,\n    y_points=None,\n    plot_points=True,\n    points_color=\"white\",\n    points_size=30,\n    point_color_below=\"blue\",\n    point_color_above=\"red\",\n    atol=1e-6,\n    plot_3d=False,\n) -&gt; None:\n    \"\"\"\n    Generate 2D contour and optionally 3D surface plots for a model's predictions.\n\n    Args:\n        model (object): A model with a predict method.\n        lower (array_like): Lower bounds for each dimension.\n        upper (array_like): Upper bounds for each dimension.\n        i (int): Index for the x-axis dimension.\n        j (int): Index for the y-axis dimension.\n        min_z (float, optional): Min value for color scaling. Defaults to None.\n        max_z (float, optional): Max value for color scaling. Defaults to None.\n        var_type (list, optional): Variable types for each dimension. Defaults to None.\n        var_name (list, optional): Variable names for labeling axes. Defaults to None.\n        show (bool): Whether to display the plot. Defaults to True.\n        filename (str, optional): File path to save the figure. Defaults to None.\n        n_grid (int): Resolution for each axis. Defaults to 50.\n        contour_levels (int): Number of contour levels. Defaults to 10.\n        dpi (int): DPI for saving. Defaults to 200.\n        title (str): Title for the figure. Defaults to \"\".\n        figsize (tuple): Figure size. Defaults to (12, 6).\n        use_min (bool): If True, leftover dims are set to lower bounds.\n        use_max (bool): If True, leftover dims are set to upper bounds.\n        aspect_equal (bool): Whether axes have equal scaling. Defaults to True.\n        legend_fontsize (int): Font size for labels and legends. Defaults to 12.\n        cmap (str): Colormap. Defaults to \"viridis\".\n        X_points (ndarray): Original data points. Shape: (N, D).\n        y_points (ndarray): Original target values. Shape: (N,).\n        plot_points (bool): Whether to plot X_points. Defaults to True.\n        points_color (str): Fallback color for data points. Defaults to \"white\".\n        points_size (int): Marker size for data points. Defaults to 30.\n        point_color_below (str): Color if actual z &lt; predicted z. Defaults to \"blue\".\n        point_color_above (str): Color if actual z &gt;= predicted z. Defaults to \"red\".\n        atol (float): Absolute tolerance for comparing actual and predicted z-values. Defaults to 1e-6.\n        plot_3d (bool): Whether to plot the 3D surface plot. Defaults to False.\n\n    Returns:\n        (fig, (ax_contour, ax_surface)): Figure and axes for the contour and surface plots.\n    \"\"\"\n    # --- Validate inputs ---\n    lower = np.asarray(lower)\n    upper = np.asarray(upper)\n    n_dims = len(lower)\n    if len(upper) != n_dims:\n        raise ValueError(\"Mismatch in dimension count between lower and upper.\")\n    if i &lt; 0 or j &lt; 0 or i &gt;= n_dims or j &gt;= n_dims:\n        raise ValueError(\n            f\"Invalid dimension indices i={i} or j={j} for {n_dims}-dimensional data.\"\n        )\n    if i == j:\n        raise ValueError(\"Dimensions i and j must be different.\")\n\n    if var_name is None:\n        var_name = [f\"x{k}\" for k in range(n_dims)]\n    elif len(var_name) != n_dims:\n        raise ValueError(\"var_name length must match the number of dimensions.\")\n\n    # --- 2D grid for contour/surface ---\n    x_vals = np.linspace(lower[i], upper[i], n_grid)\n    y_vals = np.linspace(lower[j], upper[j], n_grid)\n    X_grid, Y_grid = np.meshgrid(x_vals, y_vals)\n\n    # Helper for leftover dims\n    def hidden_value(dim_index):\n        if use_min:\n            return lower[dim_index]\n        if use_max:\n            return upper[dim_index]\n        return 0.5 * (lower[dim_index] + upper[dim_index])\n\n    # Build all grid points\n    grid_points = []\n    for row in range(n_grid):\n        for col in range(n_grid):\n            p = np.zeros(n_dims)\n            p[i] = X_grid[row, col]\n            p[j] = Y_grid[row, col]\n            for dim in range(n_dims):\n                if dim not in (i, j):\n                    p[dim] = hidden_value(dim)\n            grid_points.append(p)\n    grid_points = np.array(grid_points)\n\n    # Predict for the grid\n    Z_pred = model.predict(grid_points)\n    if isinstance(Z_pred, dict):\n        Z_pred = Z_pred.get(\"mean\", list(Z_pred.values())[0])\n    elif isinstance(Z_pred, tuple):\n        Z_pred = Z_pred[0]\n    Z_pred = Z_pred.reshape(n_grid, n_grid)\n\n    # Determine min/max color scale\n    if min_z is None:\n        min_z = np.min(Z_pred)\n    if max_z is None:\n        max_z = np.max(Z_pred)\n\n    # --- Set up figure ---\n    if plot_3d:\n        fig = plt.figure(figsize=figsize)\n        ax_contour = fig.add_subplot(1, 2, 1)\n        ax_surface = fig.add_subplot(1, 2, 2, projection=\"3d\")\n    else:\n        fig, ax_contour = plt.subplots(figsize=figsize)\n        ax_surface = None\n\n    # --- 2D contour ---\n    cont = ax_contour.contourf(\n        X_grid,\n        Y_grid,\n        Z_pred,\n        levels=contour_levels,\n        cmap=cmap,\n        vmin=min_z,\n        vmax=max_z,\n    )\n    cb1 = plt.colorbar(cont, ax=ax_contour)\n    cb1.ax.tick_params(labelsize=legend_fontsize - 2)\n\n    ax_contour.set_xlabel(var_name[i], fontsize=legend_fontsize)\n    ax_contour.set_ylabel(var_name[j], fontsize=legend_fontsize)\n    ax_contour.tick_params(labelsize=legend_fontsize - 2)\n    if aspect_equal:\n        ax_contour.set_aspect(\"equal\")\n\n    # --- 3D surface ---\n    if plot_3d:\n        surf = ax_surface.plot_surface(\n            X_grid,\n            Y_grid,\n            Z_pred,\n            cmap=cmap,\n            vmin=min_z,\n            vmax=max_z,\n            linewidth=0,\n            antialiased=True,\n            alpha=0.8,\n        )\n        cb2 = fig.colorbar(surf, ax=ax_surface, shrink=0.7, pad=0.1)\n        cb2.ax.tick_params(labelsize=legend_fontsize - 2)\n\n        ax_surface.set_xlabel(var_name[i], fontsize=legend_fontsize)\n        ax_surface.set_ylabel(var_name[j], fontsize=legend_fontsize)\n        ax_surface.set_zlabel(\"f(x)\", fontsize=legend_fontsize)\n        ax_surface.tick_params(labelsize=legend_fontsize - 2)\n\n    # --- Optionally plot points ---\n    if plot_points and X_points is not None:\n        z_pred_for_point = []\n        for row_idx in range(X_points.shape[0]):\n            single_p = np.zeros(n_dims)\n            single_p[i] = X_points[row_idx, i]\n            single_p[j] = X_points[row_idx, j]\n            for dim_idx in range(n_dims):\n                if dim_idx not in (i, j):\n                    single_p[dim_idx] = hidden_value(dim_idx)\n            val = model.predict(single_p.reshape(1, -1))\n            val = np.atleast_1d(val)\n            if isinstance(val, dict):\n                val = val.get(\"mean\", list(val.values())[0])\n            elif isinstance(val, tuple):\n                val = val[0]\n            z_pred_for_point.append(val[0] if hasattr(val, \"__len__\") else val)\n        z_pred_for_point = np.array(z_pred_for_point)\n\n        z_actual = np.array(y_points).flatten()\n\n        on_mask = np.isclose(z_actual, z_pred_for_point, atol=atol)\n        below_mask = z_actual - atol / 2.0 &lt; z_pred_for_point\n        above_mask = z_actual + atol / 2.0 &gt; z_pred_for_point\n        num_correct = np.count_nonzero(on_mask)\n\n        # 2D contour scatter\n        ax_contour.scatter(\n            X_points[below_mask, i],\n            X_points[below_mask, j],\n            c=point_color_below,\n            edgecolor=\"black\",\n            s=points_size,\n            alpha=0.9,\n            zorder=5,\n        )\n        ax_contour.scatter(\n            X_points[above_mask, i],\n            X_points[above_mask, j],\n            c=point_color_above,\n            edgecolor=\"black\",\n            s=points_size,\n            alpha=0.9,\n            zorder=5,\n        )\n        ax_contour.scatter(\n            X_points[on_mask, i],\n            X_points[on_mask, j],\n            c=points_color,\n            edgecolor=\"black\",\n            s=points_size,\n            alpha=0.9,\n            zorder=5,\n        )\n        # 3D plot scatter\n        if plot_3d:\n            ax_surface.scatter(\n                X_points[below_mask, i],\n                X_points[below_mask, j],\n                z_actual[below_mask],\n                c=point_color_below,\n                edgecolor=\"black\",\n                s=points_size,\n                alpha=0.9,\n                zorder=10,\n            )\n            ax_surface.scatter(\n                X_points[above_mask, i],\n                X_points[above_mask, j],\n                z_actual[above_mask],\n                c=point_color_above,\n                edgecolor=\"black\",\n                s=points_size,\n                alpha=0.9,\n                zorder=10,\n            )\n            ax_surface.scatter(\n                X_points[on_mask, i],\n                X_points[on_mask, j],\n                z_actual[on_mask],\n                c=points_color,\n                edgecolor=\"black\",\n                s=points_size,\n                alpha=0.9,\n                zorder=10,\n            )\n\n    # --- Optionally set aspect in 3D ---\n    if plot_3d and aspect_equal:\n        x_range = upper[i] - lower[i]\n        y_range = upper[j] - lower[j]\n        z_range = max_z - min_z if max_z &gt; min_z else 1\n        scale_z = (x_range + y_range) / (2.0 * z_range) if z_range else 1\n        ax_surface.set_box_aspect([1, (y_range / x_range) if x_range else 1, scale_z])\n\n    # --- Title, save, and show ---\n    if title:\n        updated_title = f\"{title}  Correct Points: {num_correct if plot_points and X_points is not None else ''}\"\n        fig.suptitle(updated_title, fontsize=legend_fontsize + 2)\n\n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n\n    if filename:\n        plt.savefig(filename, bbox_inches=\"tight\", dpi=dpi)\n\n    if show:\n        plt.show()\n\n    if plot_3d:\n        return fig, (ax_contour, ax_surface)\n    else:\n        return fig, (ax_contour, None)\n</code></pre>"},{"location":"reference/spotoptim/plot/contour/#spotoptim.plot.contour.simple_contour","title":"<code>simple_contour(fun, min_x=-1, max_x=1, min_y=-1, max_y=1, min_z=None, max_z=None, n_samples=100, n_levels=30)</code>","text":"<p>Simple contour plot</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>_type_</code> <p>description</p> required <code>min_x</code> <code>int</code> <p>description. Defaults to -1.</p> <code>-1</code> <code>max_x</code> <code>int</code> <p>description. Defaults to 1.</p> <code>1</code> <code>min_y</code> <code>int</code> <p>description. Defaults to -1.</p> <code>-1</code> <code>max_y</code> <code>int</code> <p>description. Defaults to 1.</p> <code>1</code> <code>min_z</code> <code>int</code> <p>description. Defaults to 0.</p> <code>None</code> <code>max_z</code> <code>int</code> <p>description. Defaults to 1.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>description. Defaults to 100.</p> <code>100</code> <code>n_levels</code> <code>int</code> <p>description. Defaults to 5.</p> <code>30</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n    import numpy as np\n    # from spotpython.fun.objectivefunctions import analytical\n    # fun = analytical().fun_branin\n    # simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15)\n</code></pre> Source code in <code>src/spotoptim/plot/contour.py</code> <pre><code>def simple_contour(\n    fun,\n    min_x=-1,\n    max_x=1,\n    min_y=-1,\n    max_y=1,\n    min_z=None,\n    max_z=None,\n    n_samples=100,\n    n_levels=30,\n) -&gt; None:\n    \"\"\"\n    Simple contour plot\n\n    Args:\n        fun (_type_): _description_\n        min_x (int, optional): _description_. Defaults to -1.\n        max_x (int, optional): _description_. Defaults to 1.\n        min_y (int, optional): _description_. Defaults to -1.\n        max_y (int, optional): _description_. Defaults to 1.\n        min_z (int, optional): _description_. Defaults to 0.\n        max_z (int, optional): _description_. Defaults to 1.\n        n_samples (int, optional): _description_. Defaults to 100.\n        n_levels (int, optional): _description_. Defaults to 5.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n            import numpy as np\n            # from spotpython.fun.objectivefunctions import analytical\n            # fun = analytical().fun_branin\n            # simple_contour(fun=fun, n_levels=30, min_x=-5, max_x=10, min_y=0, max_y=15)\n\n    \"\"\"\n    XX, YY = np.meshgrid(\n        np.linspace(min_x, max_x, n_samples), np.linspace(min_y, max_y, n_samples)\n    )\n    zz = np.array(\n        [\n            fun(np.array([xi, yi]).reshape(-1, 2))\n            for xi, yi in zip(np.ravel(XX), np.ravel(YY))\n        ]\n    ).reshape(n_samples, n_samples)\n    fig, ax = plt.subplots(figsize=(5, 2.7), layout=\"constrained\")\n    if min_z is None:\n        min_z = np.min(zz)\n    if max_z is None:\n        max_z = np.max(zz)\n    plt.contourf(\n        XX,\n        YY,\n        zz,\n        levels=np.linspace(min_z, max_z, n_levels),\n        zorder=1,\n        cmap=\"jet\",\n        vmin=min_z,\n        vmax=max_z,\n    )\n    plt.colorbar()\n</code></pre>"},{"location":"reference/spotoptim/plot/mo/","title":"mo","text":""},{"location":"reference/spotoptim/plot/mo/#spotoptim.plot.mo.plot_mo","title":"<code>plot_mo(target_names, combinations, pareto, y_rf=None, pareto_front=False, y_best=None, y_add=None, y_add2=None, y_add_color='blue', y_add2_color='green', title='', y_orig=None, pareto_front_orig=False, pareto_label=False, y_rf_color='blue', y_best_color='red', x_axis_transformation='id', y_axis_transformation='id', y_best_label='Best', y_add_label='Add', y_add2_label='Add2', filename=None, figsize=(9, 6))</code>","text":"<p>Generates scatter plots for each combination of two targets from a multi-output prediction while highlighting Pareto optimal points.</p> <p>Parameters:</p> Name Type Description Default <code>y_rf</code> <code>ndarray</code> <p>The predicted target values with shape (n_samples, n_targets).</p> <code>None</code> <code>target_names</code> <code>list</code> <p>A list of target names corresponding to the columns of y_rf.</p> required <code>combinations</code> <code>list</code> <p>A list of tuples, where each tuple contains the indices of the target combinations to plot.</p> required <code>pareto</code> <code>str</code> <p>Specifies whether to compute Pareto front based on \u2018min\u2019 or \u2018max\u2019 criterion.</p> required <code>pareto_front</code> <code>bool</code> <p>If True, connect Pareto optimal points with a red line for y_rf.</p> <code>False</code> <code>y_best</code> <code>ndarray</code> <p>A NumPy array representing the best point to highlight in red. Defaults to None.</p> <code>None</code> <code>y_add</code> <code>ndarray</code> <p>A NumPy array representing the additional points to highlight in blue. Defaults to None.</p> <code>None</code> <code>y_add2</code> <code>ndarray</code> <p>A NumPy array representing the additional points to highlight in green. Defaults to None.</p> <code>None</code> <code>y_add_color</code> <code>str</code> <p>The color of the additional points. Defaults to \u201cblue\u201d.</p> <code>'blue'</code> <code>y_add2_color</code> <code>str</code> <p>The color of the additional points. Defaults to \u201cgreen\u201d.</p> <code>'green'</code> <code>y_best_label</code> <code>str</code> <p>The label for the best point. Defaults to \u201cBest\u201d.</p> <code>'Best'</code> <code>y_add_label</code> <code>str</code> <p>The label for the additional points. Defaults to \u201cAdd\u201d.</p> <code>'Add'</code> <code>y_add2_label</code> <code>str</code> <p>The label for the additional points. Defaults to \u201cAdd2\u201d.</p> <code>'Add2'</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \u201c\u201d (empty string).</p> <code>''</code> <code>y_orig</code> <code>ndarray</code> <p>The original target values with shape (n_samples, n_targets). Defaults to None.</p> <code>None</code> <code>pareto_front_orig</code> <code>bool</code> <p>If True, connect Pareto optimal points with a light blue line for y_orig. Defaults to False.</p> <code>False</code> <code>pareto_label</code> <code>bool</code> <p>If True, label Pareto points with their index. Defaults to False.</p> <code>False</code> <code>y_rf_color</code> <code>str</code> <p>The color of the predicted points. Defaults to \u201cblue\u201d.</p> <code>'blue'</code> <code>y_best_color</code> <code>str</code> <p>The color of the best point. Defaults to \u201cred\u201d.</p> <code>'red'</code> <code>x_axis_transformation</code> <code>str</code> <p>Transformation for the x-axis. Options are \u201cid\u201d (linear), \u201clog\u201d (logarithmic), and \u201cloglog\u201d (log-log). Defaults to \u201cid\u201d.</p> <code>'id'</code> <code>y_axis_transformation</code> <code>str</code> <p>Transformation for the y-axis. Options are \u201cid\u201d (linear), \u201clog\u201d (logarithmic), and \u201cloglog\u201d (log-log). Defaults to \u201cid\u201d.</p> <code>'id'</code> <code>filename</code> <code>str</code> <p>If provided, saves the plot to the specified file. Supports \u201cpdf\u201d and \u201cpng\u201d formats. Defaults to None.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Figure size (width, height) in inches. Default is (9, 6).</p> <code>(9, 6)</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Displays or saves the plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.plot.mo import plot_mo\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; target_names = [\"Target 1\", \"Target 2\"]\n&gt;&gt;&gt; combinations = [(0, 1)]\n&gt;&gt;&gt; pareto = \"min\"\n&gt;&gt;&gt; y_rf = np.random.rand(100, 2)\n&gt;&gt;&gt; y_orig = np.random.rand(100, 2)\n&gt;&gt;&gt; plot_mo(target_names, combinations, pareto, y_rf=y_rf, y_orig=y_orig, filename=\"plot.png\")\n</code></pre> Source code in <code>src/spotoptim/plot/mo.py</code> <pre><code>def plot_mo(\n    target_names: list,\n    combinations: list,\n    pareto: str,\n    y_rf: np.ndarray = None,\n    pareto_front: bool = False,\n    y_best: np.ndarray = None,\n    y_add: np.ndarray = None,\n    y_add2: np.ndarray = None,\n    y_add_color=\"blue\",\n    y_add2_color=\"green\",\n    title: str = \"\",\n    y_orig: np.ndarray = None,\n    pareto_front_orig: bool = False,\n    pareto_label: bool = False,\n    y_rf_color=\"blue\",\n    y_best_color=\"red\",\n    x_axis_transformation: str = \"id\",\n    y_axis_transformation: str = \"id\",\n    y_best_label=\"Best\",\n    y_add_label=\"Add\",\n    y_add2_label=\"Add2\",\n    filename: str = None,\n    figsize: tuple = (9, 6),\n) -&gt; None:\n    \"\"\"\n    Generates scatter plots for each combination of two targets from a multi-output prediction while highlighting Pareto optimal points.\n\n    Args:\n        y_rf (np.ndarray): The predicted target values with shape (n_samples, n_targets).\n        target_names (list): A list of target names corresponding to the columns of y_rf.\n        combinations (list): A list of tuples, where each tuple contains the indices of the target combinations to plot.\n        pareto (str): Specifies whether to compute Pareto front based on 'min' or 'max' criterion.\n        pareto_front (bool): If True, connect Pareto optimal points with a red line for y_rf.\n        y_best (np.ndarray, optional): A NumPy array representing the best point to highlight in red. Defaults to None.\n        y_add (np.ndarray, optional): A NumPy array representing the additional points to highlight in blue. Defaults to None.\n        y_add2 (np.ndarray, optional): A NumPy array representing the additional points to highlight in green. Defaults to None.\n        y_add_color (str): The color of the additional points. Defaults to \"blue\".\n        y_add2_color (str): The color of the additional points. Defaults to \"green\".\n        y_best_label (str): The label for the best point. Defaults to \"Best\".\n        y_add_label (str): The label for the additional points. Defaults to \"Add\".\n        y_add2_label (str): The label for the additional points. Defaults to \"Add2\".\n        title (str): The title of the plot. Defaults to \"\" (empty string).\n        y_orig (np.ndarray, optional): The original target values with shape (n_samples, n_targets). Defaults to None.\n        pareto_front_orig (bool): If True, connect Pareto optimal points with a light blue line for y_orig. Defaults to False.\n        pareto_label (bool): If True, label Pareto points with their index. Defaults to False.\n        y_rf_color (str): The color of the predicted points. Defaults to \"blue\".\n        y_best_color (str): The color of the best point. Defaults to \"red\".\n        x_axis_transformation (str): Transformation for the x-axis. Options are \"id\" (linear), \"log\" (logarithmic), and \"loglog\" (log-log). Defaults to \"id\".\n        y_axis_transformation (str): Transformation for the y-axis. Options are \"id\" (linear), \"log\" (logarithmic), and \"loglog\" (log-log). Defaults to \"id\".\n        filename (str, optional):\n            If provided, saves the plot to the specified file. Supports \"pdf\" and \"png\" formats. Defaults to None.\n        figsize (tuple):\n            Figure size (width, height) in inches. Default is (9, 6).\n\n    Returns:\n        None: Displays or saves the plot.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.plot.mo import plot_mo\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; target_names = [\"Target 1\", \"Target 2\"]\n        &gt;&gt;&gt; combinations = [(0, 1)]\n        &gt;&gt;&gt; pareto = \"min\"\n        &gt;&gt;&gt; y_rf = np.random.rand(100, 2)\n        &gt;&gt;&gt; y_orig = np.random.rand(100, 2)\n        &gt;&gt;&gt; plot_mo(target_names, combinations, pareto, y_rf=y_rf, y_orig=y_orig, filename=\"plot.png\")\n    \"\"\"\n    # Convert y_rf to numpy array if it's a pandas DataFrame\n    if isinstance(y_rf, pd.DataFrame):\n        y_rf = y_rf.values\n\n    # Convert y_orig to numpy array if it's a pandas DataFrame\n    if isinstance(y_orig, pd.DataFrame):\n        y_orig = y_orig.values\n\n    # Ensure y_best, y_add, and y_add2 are 2D arrays (n_samples, n_targets)\n    # If they are 1D (n_targets,), assume they represent a single sample.\n    if y_best is not None and y_best.ndim == 1:\n        y_best = y_best.reshape(1, -1)\n\n    if y_add is not None and y_add.ndim == 1:\n        y_add = y_add.reshape(1, -1)\n\n    if y_add2 is not None and y_add2.ndim == 1:\n        y_add2 = y_add2.reshape(1, -1)\n\n    for i, j in combinations:\n        # Create figure with specified size\n        plt.figure(figsize=figsize)\n        s = 50  # Base size for points\n        pareto_size = s  # Size for Pareto points\n        if pareto_label:\n            pareto_size = s * 4  # Increase the size for Pareto points\n        a = 0.4\n\n        # Plot original data if provided\n        if y_orig is not None:\n            minimize = pareto == \"min\"\n            pareto_mask_orig = is_pareto_efficient(y_orig[:, [i, j]], minimize)\n            plt.scatter(\n                y_orig[:, i],\n                y_orig[:, j],\n                edgecolor=\"w\",\n                c=\"gray\",\n                s=s,\n                marker=\"o\",\n                alpha=a,\n                label=\"Non-Pareto Points\",\n            )\n            plt.scatter(\n                y_orig[pareto_mask_orig, i],\n                y_orig[pareto_mask_orig, j],\n                edgecolor=\"k\",\n                c=\"gray\",\n                s=pareto_size,\n                marker=\"o\",\n                alpha=a,\n                label=\"Pareto Points\",\n            )\n            if pareto_label:\n                for idx in np.where(pareto_mask_orig)[0]:\n                    plt.text(\n                        y_orig[idx, i],\n                        y_orig[idx, j],\n                        str(idx),\n                        color=\"black\",\n                        fontsize=8,\n                        ha=\"center\",\n                        va=\"center\",\n                    )\n            if pareto_front_orig:\n                sorted_indices_orig = np.argsort(y_orig[pareto_mask_orig, i])\n                plt.plot(\n                    y_orig[pareto_mask_orig, i][sorted_indices_orig],\n                    y_orig[pareto_mask_orig, j][sorted_indices_orig],\n                    \"k-\",\n                    alpha=a,\n                    label=\"Pareto Front\",\n                )\n\n        if y_rf is not None:\n            minimize = pareto == \"min\"\n            pareto_mask = is_pareto_efficient(y_rf[:, [i, j]], minimize)\n            plt.scatter(\n                y_rf[:, i],\n                y_rf[:, j],\n                edgecolor=\"w\",\n                c=y_rf_color,\n                s=s,\n                marker=\"^\",\n                alpha=a,\n                label=\"Predicted Points\",\n            )\n            plt.scatter(\n                y_rf[pareto_mask, i],\n                y_rf[pareto_mask, j],\n                edgecolor=\"k\",\n                c=y_rf_color,\n                s=pareto_size,\n                marker=\"s\",\n                alpha=a,\n                label=\"Predicted Pareto\",\n            )\n            if pareto_label:\n                for idx in np.where(pareto_mask)[0]:\n                    plt.text(\n                        y_rf[idx, i],\n                        y_rf[idx, j],\n                        str(idx),\n                        color=\"black\",\n                        fontsize=8,\n                        ha=\"center\",\n                        va=\"center\",\n                    )\n            if pareto_front:\n                sorted_indices = np.argsort(y_rf[pareto_mask, i])\n                plt.plot(\n                    y_rf[pareto_mask, i][sorted_indices],\n                    y_rf[pareto_mask, j][sorted_indices],\n                    linestyle=\"-\",\n                    color=y_rf_color,\n                    alpha=a,\n                    label=\"Predicted Pareto Front\",\n                )\n\n        if y_best is not None:\n            plt.scatter(\n                y_best[:, i],\n                y_best[:, j],\n                edgecolor=\"k\",\n                c=y_best_color,\n                s=s,\n                marker=\"D\",\n                alpha=1,\n                label=y_best_label,\n            )\n        if y_add is not None:\n            plt.scatter(\n                y_add[:, i],\n                y_add[:, j],\n                edgecolor=\"k\",\n                c=y_add_color,\n                s=s,\n                marker=\"D\",\n                alpha=1,\n                label=y_add_label,\n            )\n        if y_add2 is not None:\n            plt.scatter(\n                y_add2[:, i],\n                y_add2[:, j],\n                edgecolor=\"k\",\n                c=y_add2_color,\n                s=s,\n                marker=\"D\",\n                alpha=1,\n                label=y_add2_label,\n            )\n\n        # Apply axis transformations\n        if x_axis_transformation == \"log\":\n            plt.xscale(\"log\")\n        if y_axis_transformation == \"log\":\n            plt.yscale(\"log\")\n        if x_axis_transformation == \"loglog\" or y_axis_transformation == \"loglog\":\n            plt.xscale(\"log\")\n            plt.yscale(\"log\")\n\n        plt.xlabel(target_names[i])\n        plt.ylabel(target_names[j])\n        plt.grid()\n        plt.title(title)\n        # Move the legend outside the plot\n        if plt.gca().get_legend_handles_labels()[1]:\n            plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n        # Save or show the plot\n        if filename:\n            if filename.endswith(\".pdf\") or filename.endswith(\".png\"):\n                plt.savefig(\n                    filename, format=filename.split(\".\")[-1], bbox_inches=\"tight\"\n                )\n            else:\n                raise ValueError(\"Filename must have a valid suffix: '.pdf' or '.png'.\")\n        else:\n            plt.show()\n</code></pre>"},{"location":"reference/spotoptim/plot/visualization/","title":"visualization","text":""},{"location":"reference/spotoptim/plot/visualization/#spotoptim.plot.visualization.plot_important_hyperparameter_contour","title":"<code>plot_important_hyperparameter_contour(optimizer, max_imp=3, show=True, alpha=0.8, cmap='jet', num=100, add_points=True, grid_visible=True, contour_levels=30, figsize=(12, 10))</code>","text":"<p>Plot surrogate contours for all combinations of the top max_imp important parameters.</p> <p>This method identifies the most important parameters using importance scores, then generates surrogate contour plots for all pairwise combinations of these parameters. Factor (categorical) variables are handled by creating discrete grids and displaying factor level names on the axes.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>object</code> <p>SpotOptim instance containing optimization data.</p> required <code>max_imp</code> <code>int</code> <p>Number of most important parameters to visualize. Defaults to 3. For max_imp=3, creates 3 plots: (0,1), (0,2), (1,2).</p> <code>3</code> <code>show</code> <code>bool</code> <p>If True, displays plots immediately. Defaults to True.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>Transparency of 3D surface plots (0=transparent, 1=opaque). Defaults to 0.8.</p> <code>0.8</code> <code>cmap</code> <code>str</code> <p>Matplotlib colormap name. Defaults to \u2018jet\u2019.</p> <code>'jet'</code> <code>num</code> <code>int</code> <p>Number of grid points per dimension. Defaults to 100. For factor variables, uses the number of unique levels instead.</p> <code>100</code> <code>add_points</code> <code>bool</code> <p>If True, overlay evaluated points on contour plots. Defaults to True.</p> <code>True</code> <code>grid_visible</code> <code>bool</code> <p>If True, show grid lines. Defaults to True.</p> <code>True</code> <code>contour_levels</code> <code>int</code> <p>Number of contour levels. Defaults to 30.</p> <code>30</code> <code>figsize</code> <code>tuple of int</code> <p>Figure size in inches (width, height). Defaults to (12, 10).</p> <code>(12, 10)</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If optimization hasn\u2019t been run yet or max_imp is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; from spotoptim.plot.visualization import plot_important_hyperparameter_contour\n&gt;&gt;&gt;\n&gt;&gt;&gt; def sphere(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize and run optimizer with enough dimensions\n&gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5)]*4,\n...                 max_iter=20, n_initial=10,\n...                 var_name=['x1', 'x2', 'x3', 'x4'])\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Plot contours for top 3 important hyperparameters\n&gt;&gt;&gt; plot_important_hyperparameter_contour(opt, max_imp=3, show=False)\n</code></pre> Source code in <code>src/spotoptim/plot/visualization.py</code> <pre><code>def plot_important_hyperparameter_contour(\n    optimizer: object,\n    max_imp: int = 3,\n    show: bool = True,\n    alpha: float = 0.8,\n    cmap: str = \"jet\",\n    num: int = 100,\n    add_points: bool = True,\n    grid_visible: bool = True,\n    contour_levels: int = 30,\n    figsize: Tuple[int, int] = (12, 10),\n) -&gt; None:\n    \"\"\"Plot surrogate contours for all combinations of the top max_imp important parameters.\n\n    This method identifies the most important parameters using importance scores,\n    then generates surrogate contour plots for all pairwise combinations of these\n    parameters. Factor (categorical) variables are handled by creating discrete grids\n    and displaying factor level names on the axes.\n\n    Args:\n        optimizer: SpotOptim instance containing optimization data.\n        max_imp (int, optional): Number of most important parameters to visualize.\n            Defaults to 3. For max_imp=3, creates 3 plots: (0,1), (0,2), (1,2).\n        show (bool, optional): If True, displays plots immediately. Defaults to True.\n        alpha (float, optional): Transparency of 3D surface plots (0=transparent, 1=opaque).\n            Defaults to 0.8.\n        cmap (str, optional): Matplotlib colormap name. Defaults to 'jet'.\n        num (int, optional): Number of grid points per dimension. Defaults to 100.\n            For factor variables, uses the number of unique levels instead.\n        add_points (bool, optional): If True, overlay evaluated points on contour plots.\n            Defaults to True.\n        grid_visible (bool, optional): If True, show grid lines. Defaults to True.\n        contour_levels (int, optional): Number of contour levels. Defaults to 30.\n        figsize (tuple of int, optional): Figure size in inches (width, height).\n            Defaults to (12, 10).\n\n    Raises:\n        ValueError: If optimization hasn't been run yet or max_imp is invalid.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; from spotoptim.plot.visualization import plot_important_hyperparameter_contour\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; def sphere(X):\n        ...     return np.sum(X**2, axis=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Initialize and run optimizer with enough dimensions\n        &gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5)]*4,\n        ...                 max_iter=20, n_initial=10,\n        ...                 var_name=['x1', 'x2', 'x3', 'x4'])\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Plot contours for top 3 important hyperparameters\n        &gt;&gt;&gt; plot_important_hyperparameter_contour(opt, max_imp=3, show=False)\n    \"\"\"\n    from itertools import combinations\n\n    if optimizer.X_ is None or optimizer.y_ is None:\n        raise ValueError(\"No optimization data available. Run optimize() first.\")\n\n    if max_imp &lt; 2:\n        raise ValueError(\"max_imp must be at least 2 to generate pairwise plots.\")\n\n    if max_imp &gt; optimizer.n_dim:\n        raise ValueError(\n            f\"max_imp ({max_imp}) cannot exceed number of dimensions ({optimizer.n_dim}).\"\n        )\n\n    # Get importance scores\n    importance = optimizer.get_importance()\n\n    # Get indices of most important parameters (sorted by importance, descending)\n    importance_array = np.array(importance)\n    top_indices = np.argsort(importance_array)[::-1][:max_imp]\n\n    # Get parameter names for informative output\n    param_names = (\n        optimizer.var_name\n        if optimizer.var_name is not None\n        else [f\"x{i}\" for i in range(len(importance))]\n    )\n\n    print(f\"Plotting surrogate contours for top {max_imp} most important parameters:\")\n    for idx in top_indices:\n        param_type = optimizer.var_type[idx] if optimizer.var_type else \"float\"\n        print(\n            f\"  {param_names[idx]}: importance = {importance[idx]:.2f}% (type: {param_type})\"\n        )\n\n    # Generate all pairwise combinations\n    pairs = list(combinations(top_indices, 2))\n\n    print(f\"\\nGenerating {len(pairs)} surrogate plots...\")\n\n    # Plot each combination\n    for i, j in pairs:\n        print(f\"  Plotting {param_names[i]} vs {param_names[j]}\")\n        _plot_surrogate_with_factors(\n            optimizer,\n            i=int(i),\n            j=int(j),\n            show=show,\n            alpha=alpha,\n            cmap=cmap,\n            num=num,\n            add_points=add_points,\n            grid_visible=grid_visible,\n            contour_levels=contour_levels,\n            figsize=figsize,\n        )\n</code></pre>"},{"location":"reference/spotoptim/plot/visualization/#spotoptim.plot.visualization.plot_progress","title":"<code>plot_progress(optimizer, show=True, log_y=False, figsize=(10, 6), ylabel='Objective Value', mo=False)</code>","text":"<p>Plot optimization progress showing all evaluations and best-so-far curve.</p> <p>This method visualizes the optimization history, displaying both individual function evaluations and the cumulative best value found. Initial design points are shown as individual scatter points with a light grey background region, while sequential optimization iterations are connected with lines.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>object</code> <p>SpotOptim instance containing optimization data.</p> required <code>show</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>log_y</code> <code>bool</code> <p>Whether to use log scale for y-axis. Defaults to False.</p> <code>False</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 6).</p> <code>(10, 6)</code> <code>ylabel</code> <code>str</code> <p>Label for y-axis. Defaults to \u201cObjective Value\u201d.</p> <code>'Objective Value'</code> <code>mo</code> <code>bool</code> <p>Whether to plot individual objectives if available. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If optimization hasn\u2019t been run yet.</p> <code>ImportError</code> <p>If matplotlib is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; from spotoptim.plot.visualization import plot_progress\n&gt;&gt;&gt;\n&gt;&gt;&gt; def sphere(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize and run optimizer\n&gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5)]*2,\n...                 max_iter=20, n_initial=10, seed=42)\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Plot optimization progress (linear scale)\n&gt;&gt;&gt; plot_progress(opt, log_y=False, show=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Plot with log scale\n&gt;&gt;&gt; plot_progress(opt, log_y=True, show=False)\n</code></pre> Source code in <code>src/spotoptim/plot/visualization.py</code> <pre><code>def plot_progress(\n    optimizer: object,\n    show: bool = True,\n    log_y: bool = False,\n    figsize: Tuple[int, int] = (10, 6),\n    ylabel: str = \"Objective Value\",\n    mo: bool = False,\n) -&gt; None:\n    \"\"\"Plot optimization progress showing all evaluations and best-so-far curve.\n\n    This method visualizes the optimization history, displaying both individual\n    function evaluations and the cumulative best value found. Initial design points\n    are shown as individual scatter points with a light grey background region,\n    while sequential optimization iterations are connected with lines.\n\n    Args:\n        optimizer: SpotOptim instance containing optimization data.\n        show (bool, optional): Whether to display the plot. Defaults to True.\n        log_y (bool, optional): Whether to use log scale for y-axis. Defaults to False.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 6).\n        ylabel (str, optional): Label for y-axis. Defaults to \"Objective Value\".\n        mo (bool, optional): Whether to plot individual objectives if available. Defaults to False.\n\n    Raises:\n        ValueError: If optimization hasn't been run yet.\n        ImportError: If matplotlib is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; from spotoptim.plot.visualization import plot_progress\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; def sphere(X):\n        ...     return np.sum(X**2, axis=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Initialize and run optimizer\n        &gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5)]*2,\n        ...                 max_iter=20, n_initial=10, seed=42)\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Plot optimization progress (linear scale)\n        &gt;&gt;&gt; plot_progress(opt, log_y=False, show=False)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Plot with log scale\n        &gt;&gt;&gt; plot_progress(opt, log_y=True, show=False)\n    \"\"\"\n    if plt is None:\n        raise ImportError(\n            \"matplotlib is required for plot_progress(). \"\n            \"Install it with: pip install matplotlib\"\n        )\n\n    if optimizer.y_ is None or len(optimizer.y_) == 0:\n        raise ValueError(\"No optimization data available. Run optimize() first.\")\n\n    history = optimizer.y_\n\n    plt.figure(figsize=figsize)\n\n    # Separate initial design points from sequential evaluations\n    n_initial = min(optimizer.n_initial, len(history))\n    initial_y = history[:n_initial]\n    sequential_y = history[n_initial:]\n\n    # Add light grey background for initial design region\n    if n_initial &gt; 0:\n        plt.axvspan(0, n_initial, alpha=0.15, color=\"gray\", zorder=0)\n\n    # Plot multi-objective values if requested and available\n    if mo and optimizer.y_mo is not None:\n        n_samples, n_obj = optimizer.y_mo.shape\n        x_all = np.arange(1, n_samples + 1)\n\n        # Determine names\n        names = optimizer.objective_names\n        if names is None or len(names) != n_obj:\n            names = [f\"Objective {i+1}\" for i in range(n_obj)]\n\n        # Basic colors (excluding gray/red used for main plot)\n        # Use a colormap or a set list\n        _ = plt.cm.viridis(np.linspace(0, 1, n_obj))\n\n        for i in range(n_obj):\n            plt.plot(\n                x_all,\n                optimizer.y_mo[:, i],\n                linestyle=\"--\",\n                marker=\"x\",\n                alpha=0.7,\n                label=f\"{names[i]}\",\n                zorder=1,\n            )\n\n    # Plot initial design points as scatter (not connected)\n    if n_initial &gt; 0:\n        x_initial = np.arange(1, n_initial + 1)\n        plt.scatter(\n            x_initial,\n            initial_y,\n            alpha=0.6,\n            s=50,\n            label=f\"Initial design (n={n_initial})\",\n            color=\"gray\",\n            edgecolors=\"black\",\n            linewidth=0.5,\n            zorder=2,\n        )\n\n    # Plot sequential evaluations (connected with line)\n    if len(sequential_y) &gt; 0:\n        x_sequential = np.arange(n_initial + 1, len(history) + 1)\n        plt.plot(\n            x_sequential,\n            sequential_y,\n            \"o-\",\n            alpha=0.6,\n            label=\"Sequential evaluations\",\n            markersize=5,\n            zorder=3,\n        )\n\n    # Plot best-so-far curve starting after initial design\n    if len(history) &gt; n_initial:\n        # Best so far across all evaluations\n        best_so_far = np.minimum.accumulate(history)\n        # Start the red line after initial design\n        x_best = np.arange(n_initial + 1, len(history) + 1)\n        y_best = best_so_far[n_initial:]\n        plt.plot(\n            x_best,\n            y_best,\n            \"r-\",\n            linewidth=2,\n            label=\"Best so far\",\n            zorder=4,\n        )\n\n    plt.xlabel(\"Iteration\", fontsize=11)\n    plt.ylabel(ylabel, fontsize=11)\n\n    title = \"Optimization Progress\"\n    if log_y:\n        title += \" (Log Scale)\"\n    plt.title(title, fontsize=12)\n\n    plt.legend(fontsize=10)\n    plt.grid(True, alpha=0.3)\n\n    if log_y:\n        plt.yscale(\"log\")\n\n    plt.tight_layout()\n\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotoptim/plot/visualization/#spotoptim.plot.visualization.plot_surrogate","title":"<code>plot_surrogate(optimizer, i=0, j=1, show=True, alpha=0.8, var_name=None, cmap='jet', num=100, vmin=None, vmax=None, add_points=True, grid_visible=True, contour_levels=30, figsize=(12, 10))</code>","text":"<p>Plot the surrogate model for two dimensions.</p> <p>Creates a 2x2 plot showing: - Top left: 3D surface of predictions - Top right: 3D surface of prediction uncertainty - Bottom left: Contour plot of predictions with evaluated points - Bottom right: Contour plot of prediction uncertainty</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>object</code> <p>SpotOptim instance containing optimization data and surrogate model.</p> required <code>i</code> <code>int</code> <p>Index of the first dimension to plot. Defaults to 0.</p> <code>0</code> <code>j</code> <code>int</code> <p>Index of the second dimension to plot. Defaults to 1.</p> <code>1</code> <code>show</code> <code>bool</code> <p>If True, displays the plot immediately. Defaults to True.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>Transparency of the 3D surface plots (0=transparent, 1=opaque). Defaults to 0.8.</p> <code>0.8</code> <code>var_name</code> <code>list of str</code> <p>Names for each dimension. If None, uses instance var_name. Defaults to None.</p> <code>None</code> <code>cmap</code> <code>str</code> <p>Matplotlib colormap name. Defaults to \u2018jet\u2019.</p> <code>'jet'</code> <code>num</code> <code>int</code> <p>Number of grid points per dimension for mesh grid. Defaults to 100.</p> <code>100</code> <code>vmin</code> <code>float</code> <p>Minimum value for color scale. If None, determined from data. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>Maximum value for color scale. If None, determined from data. Defaults to None.</p> <code>None</code> <code>add_points</code> <code>bool</code> <p>If True, overlay evaluated points on contour plots. Defaults to True.</p> <code>True</code> <code>grid_visible</code> <code>bool</code> <p>If True, show grid lines on contour plots. Defaults to True.</p> <code>True</code> <code>contour_levels</code> <code>int</code> <p>Number of contour levels. Defaults to 30.</p> <code>30</code> <code>figsize</code> <code>tuple of int</code> <p>Figure size in inches (width, height). Defaults to (12, 10).</p> <code>(12, 10)</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If optimization hasn\u2019t been run yet, or if i, j are invalid.</p> <code>ImportError</code> <p>If matplotlib is not installed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; from spotoptim.plot.visualization import plot_surrogate\n&gt;&gt;&gt;\n&gt;&gt;&gt; def sphere(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Initialize and run optimizer\n&gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5), (-5, 5)],\n...                 max_iter=20, n_initial=10, var_name=['x1', 'x2'])\n&gt;&gt;&gt; result = opt.optimize()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Plot surrogate model for dimensions 0 and 1\n&gt;&gt;&gt; plot_surrogate(opt, i=0, j=1, show=False)\n</code></pre> Source code in <code>src/spotoptim/plot/visualization.py</code> <pre><code>def plot_surrogate(\n    optimizer: object,\n    i: int = 0,\n    j: int = 1,\n    show: bool = True,\n    alpha: float = 0.8,\n    var_name: Optional[List[str]] = None,\n    cmap: str = \"jet\",\n    num: int = 100,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    add_points: bool = True,\n    grid_visible: bool = True,\n    contour_levels: int = 30,\n    figsize: Tuple[int, int] = (12, 10),\n) -&gt; None:\n    \"\"\"Plot the surrogate model for two dimensions.\n\n    Creates a 2x2 plot showing:\n    - Top left: 3D surface of predictions\n    - Top right: 3D surface of prediction uncertainty\n    - Bottom left: Contour plot of predictions with evaluated points\n    - Bottom right: Contour plot of prediction uncertainty\n\n    Args:\n        optimizer: SpotOptim instance containing optimization data and surrogate model.\n        i (int, optional): Index of the first dimension to plot. Defaults to 0.\n        j (int, optional): Index of the second dimension to plot. Defaults to 1.\n        show (bool, optional): If True, displays the plot immediately. Defaults to True.\n        alpha (float, optional): Transparency of the 3D surface plots (0=transparent, 1=opaque).\n            Defaults to 0.8.\n        var_name (list of str, optional): Names for each dimension. If None, uses instance var_name.\n            Defaults to None.\n        cmap (str, optional): Matplotlib colormap name. Defaults to 'jet'.\n        num (int, optional): Number of grid points per dimension for mesh grid. Defaults to 100.\n        vmin (float, optional): Minimum value for color scale. If None, determined from data.\n            Defaults to None.\n        vmax (float, optional): Maximum value for color scale. If None, determined from data.\n            Defaults to None.\n        add_points (bool, optional): If True, overlay evaluated points on contour plots.\n            Defaults to True.\n        grid_visible (bool, optional): If True, show grid lines on contour plots. Defaults to True.\n        contour_levels (int, optional): Number of contour levels. Defaults to 30.\n        figsize (tuple of int, optional): Figure size in inches (width, height). Defaults to (12, 10).\n\n    Raises:\n        ValueError: If optimization hasn't been run yet, or if i, j are invalid.\n        ImportError: If matplotlib is not installed.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; from spotoptim.plot.visualization import plot_surrogate\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; def sphere(X):\n        ...     return np.sum(X**2, axis=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Initialize and run optimizer\n        &gt;&gt;&gt; opt = SpotOptim(fun=sphere, bounds=[(-5, 5), (-5, 5)],\n        ...                 max_iter=20, n_initial=10, var_name=['x1', 'x2'])\n        &gt;&gt;&gt; result = opt.optimize()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Plot surrogate model for dimensions 0 and 1\n        &gt;&gt;&gt; plot_surrogate(opt, i=0, j=1, show=False)\n    \"\"\"\n    if plt is None:\n        raise ImportError(\n            \"matplotlib is required for plot_surrogate(). \"\n            \"Install it with: pip install matplotlib\"\n        )\n\n    # Validation\n    if optimizer.X_ is None or optimizer.y_ is None:\n        raise ValueError(\"No optimization data available. Run optimize() first.\")\n\n    k = optimizer.n_dim\n    if i &gt;= k or j &gt;= k:\n        raise ValueError(f\"Dimensions i={i} and j={j} must be less than n_dim={k}.\")\n    if i == j:\n        raise ValueError(\"Dimensions i and j must be different.\")\n\n    # Use instance var_name if not provided\n    if var_name is None:\n        var_name = optimizer.var_name\n\n    # Generate mesh grid\n    X_i, X_j, grid_points = _generate_mesh_grid(optimizer, i, j, num)\n\n    # Predict on grid\n    y_pred, y_std = optimizer._predict_with_uncertainty(grid_points)\n    Z_pred = y_pred.reshape(X_i.shape)\n    Z_std = y_std.reshape(X_i.shape)\n\n    # Create figure\n    fig = plt.figure(figsize=figsize)\n\n    # Plot 1: 3D surface of predictions\n    ax1 = fig.add_subplot(221, projection=\"3d\")\n    ax1.plot_surface(X_i, X_j, Z_pred, cmap=cmap, alpha=alpha, vmin=vmin, vmax=vmax)\n    ax1.set_title(\"Prediction Surface\")\n    ax1.set_xlabel(var_name[i] if var_name else f\"x{i}\")\n    ax1.set_ylabel(var_name[j] if var_name else f\"x{j}\")\n    ax1.set_zlabel(\"Prediction\")\n\n    # Plot 2: 3D surface of prediction uncertainty\n    ax2 = fig.add_subplot(222, projection=\"3d\")\n    ax2.plot_surface(X_i, X_j, Z_std, cmap=cmap, alpha=alpha)\n    ax2.set_title(\"Prediction Uncertainty Surface\")\n    ax2.set_xlabel(var_name[i] if var_name else f\"x{i}\")\n    ax2.set_ylabel(var_name[j] if var_name else f\"x{j}\")\n    ax2.set_zlabel(\"Std. Dev.\")\n\n    # Plot 3: Contour of predictions\n    ax3 = fig.add_subplot(223)\n    contour3 = ax3.contourf(\n        X_i, X_j, Z_pred, levels=contour_levels, cmap=cmap, vmin=vmin, vmax=vmax\n    )\n    plt.colorbar(contour3, ax=ax3)\n    if add_points:\n        ax3.scatter(\n            optimizer.X_[:, i],\n            optimizer.X_[:, j],\n            c=\"red\",\n            s=30,\n            edgecolors=\"black\",\n            zorder=5,\n            label=\"Evaluated points\",\n        )\n        ax3.legend()\n    ax3.set_title(\"Prediction Contour\")\n    ax3.set_xlabel(var_name[i] if var_name else f\"x{i}\")\n    ax3.set_ylabel(var_name[j] if var_name else f\"x{j}\")\n    ax3.grid(visible=grid_visible)\n\n    # Plot 4: Contour of prediction uncertainty\n    ax4 = fig.add_subplot(224)\n    contour4 = ax4.contourf(X_i, X_j, Z_std, levels=contour_levels, cmap=cmap)\n    plt.colorbar(contour4, ax=ax4)\n    if add_points:\n        ax4.scatter(\n            optimizer.X_[:, i],\n            optimizer.X_[:, j],\n            c=\"red\",\n            s=30,\n            edgecolors=\"black\",\n            zorder=5,\n            label=\"Evaluated points\",\n        )\n        ax4.legend()\n    ax4.set_title(\"Uncertainty Contour\")\n    ax4.set_xlabel(var_name[i] if var_name else f\"x{i}\")\n    ax4.set_ylabel(var_name[j] if var_name else f\"x{j}\")\n    ax4.grid(visible=grid_visible)\n\n    plt.tight_layout()\n\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotoptim/sampling/design/","title":"design","text":""},{"location":"reference/spotoptim/sampling/design/#spotoptim.sampling.design.fullfactorial","title":"<code>fullfactorial(q, Edges=1)</code>","text":"<p>Generates a full factorial sampling plan in the unit cube.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>list or ndarray</code> <p>A list or array containing the number of points along each dimension (k-vector).</p> required <code>Edges</code> <code>int</code> <p>Determines spacing of points. If <code>Edges=1</code>, points are equally spaced from edge to edge (default). Otherwise, points will be in the centers of n = q[0]q[1]\u2026*q[k-1] bins filling the unit cube.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Full factorial sampling plan as an array of shape (n, k), where n is the total number of points and k is the number of dimensions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any dimension in <code>q</code> is less than 2.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.sampling import fullfactorial\n&gt;&gt;&gt; q = [3, 2]\n&gt;&gt;&gt; X = fullfactorial(q, Edges=0)\n&gt;&gt;&gt; print(X)\n        [[0.         0.        ]\n        [0.         0.75      ]\n        [0.41666667 0.        ]\n        [0.41666667 0.75      ]\n        [0.83333333 0.        ]\n        [0.83333333 0.75      ]]\n&gt;&gt;&gt; X = fullfactorial(q, Edges=1)\n&gt;&gt;&gt; print(X)\n        [[0.  0. ]\n        [0.  1. ]\n        [0.5 0. ]\n        [0.5 1. ]\n        [1.  0. ]\n        [1.  1. ]]\n</code></pre> Source code in <code>src/spotoptim/sampling/design.py</code> <pre><code>def fullfactorial(q, Edges=1) -&gt; np.ndarray:\n    \"\"\"Generates a full factorial sampling plan in the unit cube.\n\n    Args:\n        q (list or np.ndarray):\n            A list or array containing the number of points along each dimension (k-vector).\n        Edges (int, optional):\n            Determines spacing of points. If `Edges=1`, points are equally spaced from edge to edge (default).\n            Otherwise, points will be in the centers of n = q[0]*q[1]*...*q[k-1] bins filling the unit cube.\n\n    Returns:\n        (np.ndarray): Full factorial sampling plan as an array of shape (n, k), where n is the total number of points and k is the number of dimensions.\n\n    Raises:\n        ValueError: If any dimension in `q` is less than 2.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence.\n        Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.sampling import fullfactorial\n        &gt;&gt;&gt; q = [3, 2]\n        &gt;&gt;&gt; X = fullfactorial(q, Edges=0)\n        &gt;&gt;&gt; print(X)\n                [[0.         0.        ]\n                [0.         0.75      ]\n                [0.41666667 0.        ]\n                [0.41666667 0.75      ]\n                [0.83333333 0.        ]\n                [0.83333333 0.75      ]]\n        &gt;&gt;&gt; X = fullfactorial(q, Edges=1)\n        &gt;&gt;&gt; print(X)\n                [[0.  0. ]\n                [0.  1. ]\n                [0.5 0. ]\n                [0.5 1. ]\n                [1.  0. ]\n                [1.  1. ]]\n\n    \"\"\"\n    q = np.array(q)\n    if np.min(q) &lt; 2:\n        raise ValueError(\"You must have at least two points per dimension.\")\n\n    # Total number of points in the sampling plan\n    n = np.prod(q)\n\n    # Number of dimensions\n    k = len(q)\n\n    # Pre-allocate memory for the sampling plan\n    X = np.zeros((n, k))\n\n    # Additional phantom element\n    q = np.append(q, 1)\n\n    for j in range(k):\n        if Edges == 1:\n            one_d_slice = np.linspace(0, 1, q[j])\n        else:\n            one_d_slice = np.linspace(1 / (2 * q[j]), 1, q[j]) - 1 / (2 * q[j])\n\n        column = np.array([])\n\n        while len(column) &lt; n:\n            for ll in range(q[j]):\n                column = np.append(\n                    column, np.ones(np.prod(q[j + 1 : k])) * one_d_slice[ll]\n                )\n\n        X[:, j] = column\n\n    return X\n</code></pre>"},{"location":"reference/spotoptim/sampling/design/#spotoptim.sampling.design.generate_clustered_design","title":"<code>generate_clustered_design(bounds, n_design, n_clusters, seed=None)</code>","text":"<p>Generates a clustered design.</p> <p>Generates clusters of points using sklearn.datasets.make_blobs. Points are scaled to the provided bounds.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>Union[List[Tuple[float, float]], ndarray]</code> <p>Design space bounds.</p> required <code>n_design</code> <code>int</code> <p>The number of points to generate.</p> required <code>n_clusters</code> <code>int</code> <p>The number of clusters.</p> required <code>seed</code> <code>Optional[Union[int, Generator]]</code> <p>Random seed or generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 2D array of shape (n_design, n_dim) with clustered points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_clustered_design\n&gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n&gt;&gt;&gt; X = generate_clustered_design(bounds, n_design=5, n_clusters=2, seed=42)\n&gt;&gt;&gt; X.shape\n(5, 2)\n&gt;&gt;&gt; np.all((X &gt;= [-5, 0]) &amp; (X &lt;= [5, 10]))\nTrue\n</code></pre> Source code in <code>src/spotoptim/sampling/design.py</code> <pre><code>def generate_clustered_design(\n    bounds: Union[List[Tuple[float, float]], np.ndarray],\n    n_design: int,\n    n_clusters: int,\n    seed: Optional[Union[int, Generator]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Generates a clustered design.\n\n    Generates clusters of points using sklearn.datasets.make_blobs.\n    Points are scaled to the provided bounds.\n\n    Args:\n        bounds (Union[List[Tuple[float, float]], np.ndarray]): Design space bounds.\n        n_design (int): The number of points to generate.\n        n_clusters (int): The number of clusters.\n        seed (Optional[Union[int, Generator]], optional): Random seed or generator.\n\n    Returns:\n        np.ndarray: A 2D array of shape (n_design, n_dim) with clustered points.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.design import generate_clustered_design\n        &gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n        &gt;&gt;&gt; X = generate_clustered_design(bounds, n_design=5, n_clusters=2, seed=42)\n        &gt;&gt;&gt; X.shape\n        (5, 2)\n        &gt;&gt;&gt; np.all((X &gt;= [-5, 0]) &amp; (X &lt;= [5, 10]))\n        True\n    \"\"\"\n    bounds_arr = np.array(bounds)\n    n_dim = len(bounds_arr)\n    lower = bounds_arr[:, 0]\n    upper = bounds_arr[:, 1]\n\n    # Handle seed for make_blobs (expects int or RandomState or None)\n    random_state = seed\n    if isinstance(seed, np.random.Generator):\n        # Generator doesn't map directly to RandomState, but we can get integers\n        random_state = seed.integers(0, 2**32 - 1)\n\n    X_unit, _ = make_blobs(\n        n_samples=n_design,\n        n_features=n_dim,\n        centers=n_clusters,\n        cluster_std=0.05,\n        random_state=random_state,\n        center_box=(0.1, 0.9),\n    )\n\n    # Normalize to [0, 1] if values exceed bounds\n    X_min = X_unit.min(axis=0)\n    X_max = X_unit.max(axis=0)\n\n    if np.any(X_min &lt; 0) or np.any(X_max &gt; 1):\n        X_unit = (X_unit - X_min) / (X_max - X_min + 1e-6)\n\n    # Scale to bounds\n    X = lower + X_unit * (upper - lower)\n\n    return X\n</code></pre>"},{"location":"reference/spotoptim/sampling/design/#spotoptim.sampling.design.generate_collinear_design","title":"<code>generate_collinear_design(bounds, n_design, sigma=0.01, seed=None)</code>","text":"<p>Generates a collinear design (poorly projected).</p> <p>Currently implemented for 2D designs only. Generates points along a line with some Gaussian noise. The points are scaled to the provided bounds.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>Union[List[Tuple[float, float]], ndarray]</code> <p>Design space bounds.</p> required <code>n_design</code> <code>int</code> <p>The number of points to generate.</p> required <code>sigma</code> <code>float</code> <p>The standard deviation of the noise added to the y-coordinate (relative to unit scale). Defaults to 0.01.</p> <code>0.01</code> <code>seed</code> <code>Optional[Union[int, Generator]]</code> <p>Random seed or generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 2D array of shape (n_design, n_dim) with collinear points.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dimension is not 2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_collinear_design\n&gt;&gt;&gt; bounds = [(0, 1), (0, 1)]\n&gt;&gt;&gt; X = generate_collinear_design(bounds, n_design=10, seed=42)\n&gt;&gt;&gt; X.shape\n(10, 2)\n</code></pre> Source code in <code>src/spotoptim/sampling/design.py</code> <pre><code>def generate_collinear_design(\n    bounds: Union[List[Tuple[float, float]], np.ndarray],\n    n_design: int,\n    sigma: float = 0.01,\n    seed: Optional[Union[int, Generator]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Generates a collinear design (poorly projected).\n\n    Currently implemented for 2D designs only. Generates points along a line\n    with some Gaussian noise. The points are scaled to the provided bounds.\n\n    Args:\n        bounds (Union[List[Tuple[float, float]], np.ndarray]): Design space bounds.\n        n_design (int): The number of points to generate.\n        sigma (float): The standard deviation of the noise added to the y-coordinate\n            (relative to unit scale). Defaults to 0.01.\n        seed (Optional[Union[int, Generator]], optional): Random seed or generator.\n\n    Returns:\n        np.ndarray: A 2D array of shape (n_design, n_dim) with collinear points.\n\n    Raises:\n        ValueError: If dimension is not 2.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.design import generate_collinear_design\n        &gt;&gt;&gt; bounds = [(0, 1), (0, 1)]\n        &gt;&gt;&gt; X = generate_collinear_design(bounds, n_design=10, seed=42)\n        &gt;&gt;&gt; X.shape\n        (10, 2)\n    \"\"\"\n    # Initialize random number generator\n    rng = default_rng(seed)\n\n    bounds_arr = np.array(bounds)\n    n_dim = len(bounds_arr)\n\n    if n_dim != 2:\n        raise ValueError(\"Collinear design currently implemented for 2D only.\")\n\n    lower = bounds_arr[:, 0]\n    upper = bounds_arr[:, 1]\n\n    # Generate points in [0, 1] range first\n    x_coords = np.linspace(0.1, 0.9, n_design)\n\n    # Linear relationship: y = 0.2*x + 0.3 + noise\n    # This keeps points roughly in [0.3, 0.5] range for y (before noise)\n    y_coords = 0.2 * x_coords + 0.3\n\n    # Add noise\n    noise = rng.normal(0, sigma, n_design)\n    y_coords = y_coords + noise\n\n    # Clip to [0, 1] to be safe\n    y_coords = np.clip(y_coords, 0.0, 1.0)\n\n    # Stack unit coordinates\n    X_unit = np.vstack([x_coords, y_coords]).T\n\n    # Scale to bounds\n    X = lower + X_unit * (upper - lower)\n\n    return X\n</code></pre>"},{"location":"reference/spotoptim/sampling/design/#spotoptim.sampling.design.generate_grid_design","title":"<code>generate_grid_design(bounds, n_design, seed=None)</code>","text":"<p>Generates a regular grid design.</p> <p>Points are generated by creating a regular grid where the number of points per dimension is derived from n_design (floor(n_design^(1/n_dim))).</p> <p>Note: The actual number of points returned might be less than n_design if n_design is not a perfect power of n_dim.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>Union[List[Tuple[float, float]], ndarray]</code> <p>Design space bounds.</p> required <code>n_design</code> <code>int</code> <p>The target number of points. Used to determine points per dimension.</p> required <code>seed</code> <code>Optional[Union[int, Generator]]</code> <p>Unused, kept for API consistency.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 2D array of shape (points_per_dim^n_dim, n_dim) with grid points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_grid_design\n&gt;&gt;&gt; bounds = [(0, 1), (0, 1)]\n&gt;&gt;&gt; X = generate_grid_design(bounds, n_design=25) # 5^2 = 25\n&gt;&gt;&gt; X.shape\n(25, 2)\n</code></pre> Source code in <code>src/spotoptim/sampling/design.py</code> <pre><code>def generate_grid_design(\n    bounds: Union[List[Tuple[float, float]], np.ndarray],\n    n_design: int,\n    seed: Optional[Union[int, Generator]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Generates a regular grid design.\n\n    Points are generated by creating a regular grid where the number of points\n    per dimension is derived from n_design (floor(n_design^(1/n_dim))).\n\n    Note: The actual number of points returned might be less than n_design\n    if n_design is not a perfect power of n_dim.\n\n    Args:\n        bounds (Union[List[Tuple[float, float]], np.ndarray]): Design space bounds.\n        n_design (int): The target number of points. Used to determine points per dimension.\n        seed (Optional[Union[int, Generator]], optional): Unused, kept for API consistency.\n\n    Returns:\n        np.ndarray: A 2D array of shape (points_per_dim^n_dim, n_dim) with grid points.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.design import generate_grid_design\n        &gt;&gt;&gt; bounds = [(0, 1), (0, 1)]\n        &gt;&gt;&gt; X = generate_grid_design(bounds, n_design=25) # 5^2 = 25\n        &gt;&gt;&gt; X.shape\n        (25, 2)\n    \"\"\"\n    bounds_arr = np.array(bounds)\n    n_dim = len(bounds_arr)\n    lower = bounds_arr[:, 0]\n    upper = bounds_arr[:, 1]\n\n    if n_dim != 2:\n        # Check if we want to support &gt; 2D. The original code raised error.\n        # But meshgrid supports N-D. Let's try to support it or keep restriction.\n        # Original code: if self.k != 2: raise ValueError...\n        # Let's keep restriction for now as specifically requested to fix, but maybe lift it?\n        # User said \"improve the code\". Supporting N-D is an improvement.\n        pass\n\n    # Calculate points per dimension\n    points_per_dim = int(np.floor(n_design ** (1 / n_dim)))\n    if points_per_dim &lt; 2:\n        points_per_dim = 2  # Minimum 2 points per dim to have a grid?\n\n    # Create grid in [0, 1]\n    ticks = np.linspace(0, 1, points_per_dim, endpoint=True)\n\n    if n_dim == 2:\n        x, y = np.meshgrid(ticks, ticks)\n        X_unit = np.vstack([x.ravel(), y.ravel()]).T\n    else:\n        # General N-D grid\n        grid = np.meshgrid(*([ticks] * n_dim))\n        X_unit = np.vstack([g.ravel() for g in grid]).T\n\n    # Scale to bounds\n    X = lower + X_unit * (upper - lower)\n\n    return X\n</code></pre>"},{"location":"reference/spotoptim/sampling/design/#spotoptim.sampling.design.generate_qmc_lhs_design","title":"<code>generate_qmc_lhs_design(bounds, n_design, seed=None)</code>","text":"<p>Generates a Latin Hypercube Sampling design using QMC.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>Union[List[Tuple[float, float]], ndarray]</code> <p>Design space bounds.</p> required <code>n_design</code> <code>int</code> <p>The number of points to generate.</p> required <code>seed</code> <code>Optional[Union[int, Generator]]</code> <p>Random seed or generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of shape (n_design, n_dim) containing the generated LHS points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_qmc_lhs_design\n&gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n&gt;&gt;&gt; X = generate_qmc_lhs_design(bounds, n_design=5, seed=42)\n&gt;&gt;&gt; X.shape\n(5, 2)\n</code></pre> Source code in <code>src/spotoptim/sampling/design.py</code> <pre><code>def generate_qmc_lhs_design(\n    bounds: Union[List[Tuple[float, float]], np.ndarray],\n    n_design: int,\n    seed: Optional[Union[int, Generator]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Generates a Latin Hypercube Sampling design using QMC.\n\n    Args:\n        bounds (Union[List[Tuple[float, float]], np.ndarray]): Design space bounds.\n        n_design (int): The number of points to generate.\n        seed (Optional[Union[int, Generator]], optional): Random seed or generator.\n\n    Returns:\n        np.ndarray: An array of shape (n_design, n_dim) containing the generated LHS points.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.design import generate_qmc_lhs_design\n        &gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n        &gt;&gt;&gt; X = generate_qmc_lhs_design(bounds, n_design=5, seed=42)\n        &gt;&gt;&gt; X.shape\n        (5, 2)\n    \"\"\"\n    bounds_arr = np.array(bounds)\n    n_dim = len(bounds_arr)\n    lower = bounds_arr[:, 0]\n    upper = bounds_arr[:, 1]\n\n    # Handle seed\n    random_state = seed\n    if isinstance(seed, np.random.Generator):\n        random_state = seed.integers(0, 2**32 - 1)\n\n    sampler = qmc.LatinHypercube(d=n_dim, seed=random_state)\n    X_unit = sampler.random(n=n_design)\n\n    # Scale to bounds\n    X = lower + X_unit * (upper - lower)\n\n    return X\n</code></pre>"},{"location":"reference/spotoptim/sampling/design/#spotoptim.sampling.design.generate_sobol_design","title":"<code>generate_sobol_design(bounds, n_design, seed=None)</code>","text":"<p>Generates a Sobol sequence design.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>Union[List[Tuple[float, float]], ndarray]</code> <p>Design space bounds.</p> required <code>n_design</code> <code>int</code> <p>The number of points to generate.</p> required <code>seed</code> <code>Optional[Union[int, Generator]]</code> <p>Random seed or generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of shape (n_design, n_dim) containing the generated Sobol sequence points.</p> Notes <ul> <li>The Sobol sequence is generated with a length that is a power of 2.</li> <li>Scrambling is enabled for improved uniformity.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_sobol_design\n&gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n&gt;&gt;&gt; X = generate_sobol_design(bounds, n_design=5, seed=42)\n&gt;&gt;&gt; X.shape\n(5, 2)\n</code></pre> Source code in <code>src/spotoptim/sampling/design.py</code> <pre><code>def generate_sobol_design(\n    bounds: Union[List[Tuple[float, float]], np.ndarray],\n    n_design: int,\n    seed: Optional[Union[int, Generator]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Generates a Sobol sequence design.\n\n    Args:\n        bounds (Union[List[Tuple[float, float]], np.ndarray]): Design space bounds.\n        n_design (int): The number of points to generate.\n        seed (Optional[Union[int, Generator]], optional): Random seed or generator.\n\n    Returns:\n        np.ndarray: An array of shape (n_design, n_dim) containing the generated Sobol sequence points.\n\n    Notes:\n        - The Sobol sequence is generated with a length that is a power of 2.\n        - Scrambling is enabled for improved uniformity.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.design import generate_sobol_design\n        &gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n        &gt;&gt;&gt; X = generate_sobol_design(bounds, n_design=5, seed=42)\n        &gt;&gt;&gt; X.shape\n        (5, 2)\n    \"\"\"\n    bounds_arr = np.array(bounds)\n    n_dim = len(bounds_arr)\n    lower = bounds_arr[:, 0]\n    upper = bounds_arr[:, 1]\n\n    # Handle seed\n    random_state = seed\n    if isinstance(seed, np.random.Generator):\n        random_state = seed.integers(0, 2**32 - 1)\n\n    sampler = qmc.Sobol(d=n_dim, scramble=True, seed=random_state)\n    m = int(np.ceil(np.log2(n_design)))\n    X_unit = sampler.random_base2(m=m)[:n_design, :]\n\n    # Scale to bounds\n    X = lower + X_unit * (upper - lower)\n\n    return X\n</code></pre>"},{"location":"reference/spotoptim/sampling/design/#spotoptim.sampling.design.generate_uniform_design","title":"<code>generate_uniform_design(bounds, n_design, seed=None)</code>","text":"<p>Generate a uniform random experimental design.</p> <p>Generates n_design points uniformly distributed within the specified bounds. This function is compatible with SpotOptim\u2019s random number handling.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>Union[List[Tuple[float, float]], ndarray]</code> <p>Design space bounds. List of (lower, upper) tuples for each dimension.</p> required <code>n_design</code> <code>int</code> <p>Number of design points to generate.</p> required <code>seed</code> <code>Optional[Union[int, Generator]]</code> <p>Random seed or generator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Generated design points of shape (n_design, n_dim).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.design import generate_uniform_design\n&gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n&gt;&gt;&gt; X = generate_uniform_design(bounds, n_design=5, seed=42)\n&gt;&gt;&gt; X.shape\n(5, 2)\n&gt;&gt;&gt; np.all((X &gt;= [-5, 0]) &amp; (X &lt;= [5, 10]))\nTrue\n</code></pre> Source code in <code>src/spotoptim/sampling/design.py</code> <pre><code>def generate_uniform_design(\n    bounds: Union[List[Tuple[float, float]], np.ndarray],\n    n_design: int,\n    seed: Optional[Union[int, Generator]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Generate a uniform random experimental design.\n\n    Generates n_design points uniformly distributed within the specified bounds.\n    This function is compatible with SpotOptim's random number handling.\n\n    Args:\n        bounds (Union[List[Tuple[float, float]], np.ndarray]): Design space bounds.\n            List of (lower, upper) tuples for each dimension.\n        n_design (int): Number of design points to generate.\n        seed (Optional[Union[int, Generator]], optional): Random seed or generator.\n            Defaults to None.\n\n    Returns:\n        np.ndarray: Generated design points of shape (n_design, n_dim).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.design import generate_uniform_design\n        &gt;&gt;&gt; bounds = [(-5, 5), (0, 10)]\n        &gt;&gt;&gt; X = generate_uniform_design(bounds, n_design=5, seed=42)\n        &gt;&gt;&gt; X.shape\n        (5, 2)\n        &gt;&gt;&gt; np.all((X &gt;= [-5, 0]) &amp; (X &lt;= [5, 10]))\n        True\n    \"\"\"\n    # Initialize random number generator\n    rng = default_rng(seed)\n\n    # Convert bounds to numpy array for easier handling\n    bounds_arr = np.array(bounds)\n    n_dim = len(bounds_arr)\n\n    lower = bounds_arr[:, 0]\n    upper = bounds_arr[:, 1]\n\n    # Generate random points in [0, 1] range\n    X_unit = rng.random(size=(n_design, n_dim))\n\n    # Scale to bounds\n    X = lower + X_unit * (upper - lower)\n\n    return X\n</code></pre>"},{"location":"reference/spotoptim/sampling/effects/","title":"effects","text":""},{"location":"reference/spotoptim/sampling/effects/#spotoptim.sampling.effects.plot_all_partial_dependence","title":"<code>plot_all_partial_dependence(df, df_target, model='GradientBoostingRegressor', nrows=5, ncols=6, figsize=(20, 15), title='')</code>","text":"<p>Generates Partial Dependence Plots (PDPs) for every feature in a DataFrame against a target variable, arranged in a grid.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the features.</p> required <code>df_target</code> <code>Series</code> <p>Series containing the target variable.</p> required <code>model</code> <code>str</code> <p>Name of the model class to use (e.g., \u201cGradientBoostingRegressor\u201d).                    Defaults to \u201cGradientBoostingRegressor\u201d.</p> <code>'GradientBoostingRegressor'</code> <code>nrows</code> <code>int</code> <p>Number of rows in the grid of subplots. Defaults to 5.</p> <code>5</code> <code>ncols</code> <code>int</code> <p>Number of columns in the grid of subplots. Defaults to 6.</p> <code>6</code> <code>figsize</code> <code>tuple</code> <p>Figure size (width, height) in inches. Defaults to (20, 15).</p> <code>(20, 15)</code> <code>title</code> <code>str</code> <p>Title for the subplots. Defaults to \u201c\u201d.</p> <code>''</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; form spotpython.utils.effects import plot_all_partial_dependence\n&gt;&gt;&gt; from sklearn.datasets import load_boston\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = load_boston()\n&gt;&gt;&gt; df = pd.DataFrame(data.data, columns=data.feature_names)\n&gt;&gt;&gt; df_target = pd.Series(data.target, name=\"target\")\n&gt;&gt;&gt; plot_all_partial_dependence(df, df_target, model=\"GradientBoostingRegressor\", nrows=5, ncols=6, figsize=(20, 15))\n</code></pre> Source code in <code>src/spotoptim/sampling/effects.py</code> <pre><code>def plot_all_partial_dependence(\n    df,\n    df_target,\n    model=\"GradientBoostingRegressor\",\n    nrows=5,\n    ncols=6,\n    figsize=(20, 15),\n    title=\"\",\n) -&gt; None:\n    \"\"\"\n    Generates Partial Dependence Plots (PDPs) for every feature in a DataFrame against a target variable,\n    arranged in a grid.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the features.\n        df_target (pd.Series): Series containing the target variable.\n        model (str, optional): Name of the model class to use (e.g., \"GradientBoostingRegressor\").\n                               Defaults to \"GradientBoostingRegressor\".\n        nrows (int, optional): Number of rows in the grid of subplots. Defaults to 5.\n        ncols (int, optional): Number of columns in the grid of subplots. Defaults to 6.\n        figsize (tuple, optional): Figure size (width, height) in inches. Defaults to (20, 15).\n        title (str, optional): Title for the subplots. Defaults to \"\".\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; form spotpython.utils.effects import plot_all_partial_dependence\n        &gt;&gt;&gt; from sklearn.datasets import load_boston\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = load_boston()\n        &gt;&gt;&gt; df = pd.DataFrame(data.data, columns=data.feature_names)\n        &gt;&gt;&gt; df_target = pd.Series(data.target, name=\"target\")\n        &gt;&gt;&gt; plot_all_partial_dependence(df, df_target, model=\"GradientBoostingRegressor\", nrows=5, ncols=6, figsize=(20, 15))\n\n    \"\"\"\n\n    # Separate features and target\n    X = df\n    y = df_target  # Target variable is now a Series\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    # Instantiate the model\n    if model == \"GradientBoostingRegressor\":\n        gb_model = GradientBoostingRegressor(random_state=42)\n    elif model == \"RandomForestRegressor\":\n        from sklearn.ensemble import RandomForestRegressor\n\n        gb_model = RandomForestRegressor(random_state=42)\n    elif model == \"DecisionTreeRegressor\":\n        from sklearn.tree import DecisionTreeRegressor\n\n        gb_model = DecisionTreeRegressor(random_state=42)\n    else:\n        raise ValueError(f\"Unsupported model: {model}\")\n\n    # Train model\n    gb_model.fit(X_train, y_train)\n\n    # Create subplots\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n    axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n\n    # Generate PDP for each feature\n    features = X.columns\n    for i, feature in enumerate(features):\n        ax = axes[i]  # Select the axis for the current feature\n        PartialDependenceDisplay.from_estimator(gb_model, X_train, [feature], ax=ax)\n        ax.set_title(title)  # Set the title of the subplot to the feature name\n\n    # Remove empty subplots if the number of features is less than nrows * ncols\n    for i in range(len(features), nrows * ncols):\n        fig.delaxes(axes[i])\n\n    plt.tight_layout()  # Adjust subplot parameters for a tight layout\n    plt.show()\n</code></pre>"},{"location":"reference/spotoptim/sampling/effects/#spotoptim.sampling.effects.randorient","title":"<code>randorient(k, p, xi, seed=None)</code>","text":"<p>Generates a random orientation of a sampling matrix. This function creates a random sampling matrix for a given number of dimensions (k), number of levels (p), and step length (xi). The resulting matrix is used for screening designs in the context of experimental design.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of dimensions.</p> required <code>p</code> <code>int</code> <p>Number of levels.</p> required <code>xi</code> <code>float</code> <p>Step length.</p> required <code>seed</code> <code>int</code> <p>Seed for the random number generator. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A random sampling matrix of shape (k+1, k).</p> Example <p>randorient(k=2, p=3, xi=0.5) array([[0. , 0. ],        [0.5, 0.5],        [1. , 1. ]])</p> Source code in <code>src/spotoptim/sampling/effects.py</code> <pre><code>def randorient(k, p, xi, seed=None) -&gt; np.ndarray:\n    \"\"\"Generates a random orientation of a sampling matrix.\n    This function creates a random sampling matrix for a given number of\n    dimensions (k), number of levels (p), and step length (xi). The\n    resulting matrix is used for screening designs in the context of\n    experimental design.\n\n    Args:\n        k (int): Number of dimensions.\n        p (int): Number of levels.\n        xi (float): Step length.\n        seed (int, optional): Seed for the random number generator.\n            Defaults to None.\n\n    Returns:\n        np.ndarray: A random sampling matrix of shape (k+1, k).\n\n    Example:\n        &gt;&gt;&gt; randorient(k=2, p=3, xi=0.5)\n        array([[0. , 0. ],\n               [0.5, 0.5],\n               [1. , 1. ]])\n    \"\"\"\n    # Initialize random number generator with the provided seed\n    if seed is not None:\n        rng = np.random.default_rng(seed)\n    else:\n        rng = np.random.default_rng()\n\n    # Step length\n    Delta = xi / (p - 1)\n\n    m = k + 1\n\n    # A truncated p-level grid in one dimension\n    xs = np.arange(0, 1 - Delta, 1 / (p - 1))\n    xsl = len(xs)\n    if xsl &lt; 1:\n        print(f\"xi = {xi}.\")\n        print(f\"p = {p}.\")\n        print(f\"Delta = {Delta}.\")\n        print(f\"p - 1 = {p - 1}.\")\n        raise ValueError(\n            f\"The number of levels xsl is {xsl}, but it must be greater than 0.\"\n        )\n\n    # Basic sampling matrix\n    B = np.vstack((np.zeros((1, k)), np.tril(np.ones((k, k)))))\n\n    # Randomization\n\n    # Matrix with +1s and -1s on the diagonal with equal probability\n    Dstar = np.diag(2 * rng.integers(0, 2, size=k) - 1)\n\n    # Random base value\n    xstar = xs[rng.integers(0, xsl, size=k)]\n\n    # Permutation matrix\n    Pstar = np.zeros((k, k))\n    rp = rng.permutation(k)\n    for i in range(k):\n        Pstar[i, rp[i]] = 1\n\n    # A random orientation of the sampling matrix\n    Bstar = (\n        np.ones((m, 1)) @ xstar.reshape(1, -1)\n        + (Delta / 2) * ((2 * B - np.ones((m, k))) @ Dstar + np.ones((m, k)))\n    ) @ Pstar\n\n    return Bstar\n</code></pre>"},{"location":"reference/spotoptim/sampling/effects/#spotoptim.sampling.effects.screening_plot","title":"<code>screening_plot(X, fun, xi, p, labels, bounds=None, show=True)</code>","text":"<p>Generates a plot with elementary effect screening metrics.</p> <p>This function calculates the mean and standard deviation of the elementary effects for a given set of design variables and plots the results.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The screening plan matrix, typically structured within a [0,1]^k box.</p> required <code>fun</code> <code>object</code> <p>The objective function to evaluate at each design point in the screening plan.</p> required <code>xi</code> <code>float</code> <p>The elementary effect step length factor.</p> required <code>p</code> <code>int</code> <p>Number of discrete levels along each dimension.</p> required <code>labels</code> <code>list of str</code> <p>A list of variable names corresponding to the design variables.</p> required <code>bounds</code> <code>ndarray</code> <p>A 2xk matrix where the first row contains lower bounds and the second row contains upper bounds for each variable.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If True, the plot is displayed. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The function generates a plot of the results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.effects import screening, screeningplan\n    from spotpython.fun.objectivefunctions import Analytical\n    fun = Analytical()\n    k = 10\n    p = 10\n    xi = 1\n    r = 25\n    X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n    # Provide real-world bounds from the wing weight docs (2 x 10).\n    value_range = np.array([\n        [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n        [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n    ])\n    labels = [\n        \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n        \"q\",   \"lambda\", \"tc\", \"N_z\",\n        \"W_dg\", \"W_p\"\n    ]\n    screening(\n        X=X,\n        fun=fun.fun_wingwt,\n        bounds=value_range,\n        xi=xi,\n        p=p,\n        labels=labels,\n        print=False,\n    )\n</code></pre> Source code in <code>src/spotoptim/sampling/effects.py</code> <pre><code>def screening_plot(X, fun, xi, p, labels, bounds=None, show=True) -&gt; None:\n    \"\"\"Generates a plot with elementary effect screening metrics.\n\n    This function calculates the mean and standard deviation of the\n    elementary effects for a given set of design variables and plots\n    the results.\n\n    Args:\n        X (np.ndarray):\n            The screening plan matrix, typically structured within a [0,1]^k box.\n        fun (object):\n            The objective function to evaluate at each design point in the screening plan.\n        xi (float):\n            The elementary effect step length factor.\n        p (int):\n            Number of discrete levels along each dimension.\n        labels (list of str):\n            A list of variable names corresponding to the design variables.\n        bounds (np.ndarray):\n            A 2xk matrix where the first row contains lower bounds and\n            the second row contains upper bounds for each variable.\n        show (bool):\n            If True, the plot is displayed. Defaults to True.\n\n    Returns:\n        None: The function generates a plot of the results.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.utils.effects import screening, screeningplan\n            from spotpython.fun.objectivefunctions import Analytical\n            fun = Analytical()\n            k = 10\n            p = 10\n            xi = 1\n            r = 25\n            X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n            # Provide real-world bounds from the wing weight docs (2 x 10).\n            value_range = np.array([\n                [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n                [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n            ])\n            labels = [\n                \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n                \"q\",   \"lambda\", \"tc\", \"N_z\",\n                \"W_dg\", \"W_p\"\n            ]\n            screening(\n                X=X,\n                fun=fun.fun_wingwt,\n                bounds=value_range,\n                xi=xi,\n                p=p,\n                labels=labels,\n                print=False,\n            )\n    \"\"\"\n    k = X.shape[1]\n    sm, ssd = _screening(X=X, fun=fun, xi=xi, p=p, labels=labels, bounds=bounds)\n    plt.figure()\n    for i in range(k):\n        plt.text(sm[i], ssd[i], labels[i], fontsize=10)\n    plt.axis([min(sm), 1.1 * max(sm), min(ssd), 1.1 * max(ssd)])\n    plt.xlabel(\"Sample means\")\n    plt.ylabel(\"Sample standard deviations\")\n    plt.gca().tick_params(labelsize=10)\n    plt.grid(True)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotoptim/sampling/effects/#spotoptim.sampling.effects.screening_print","title":"<code>screening_print(X, fun, xi, p, labels, bounds=None)</code>","text":"<p>Generates a DataFrame with elementary effect screening metrics.</p> <p>This function calculates the mean and standard deviation of the elementary effects for a given set of design variables and returns the results as a Pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The screening plan matrix, typically structured within a [0,1]^k box.</p> required <code>fun</code> <code>object</code> <p>The objective function to evaluate at each design point in the screening plan.</p> required <code>xi</code> <code>float</code> <p>The elementary effect step length factor.</p> required <code>p</code> <code>int</code> <p>Number of discrete levels along each dimension.</p> required <code>labels</code> <code>list of str</code> <p>A list of variable names corresponding to the design variables.</p> required <code>bounds</code> <code>ndarray</code> <p>A 2xk matrix where the first row contains lower bounds and the second row contains upper bounds for each variable.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing three columns: - \u2018varname\u2019: The name of each variable. - \u2018mean\u2019: The mean of the elementary effects for each variable. - \u2018sd\u2019: The standard deviation of the elementary effects for each variable.</p> <code>DataFrame</code> <p>or None: If print is set to False, a plot of the results is generated instead of returning a DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotpython.utils.effects import screening, screeningplan\n    from spotpython.fun.objectivefunctions import Analytical\n    fun = Analytical()\n    k = 10\n    p = 10\n    xi = 1\n    r = 25\n    X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n    # Provide real-world bounds from the wing weight docs (2 x 10).\n    value_range = np.array([\n        [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n        [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n    ])\n    labels = [\n        \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n        \"q\",   \"lambda\", \"tc\", \"N_z\",\n        \"W_dg\", \"W_p\"\n    ]\n    screening(\n        X=X,\n        fun=fun.fun_wingwt,\n        bounds=value_range,\n        xi=xi,\n        p=p,\n        labels=labels,\n        print=False,\n    )\n</code></pre> Source code in <code>src/spotoptim/sampling/effects.py</code> <pre><code>def screening_print(X, fun, xi, p, labels, bounds=None) -&gt; pd.DataFrame:\n    \"\"\"Generates a DataFrame with elementary effect screening metrics.\n\n    This function calculates the mean and standard deviation of the\n    elementary effects for a given set of design variables and returns\n    the results as a Pandas DataFrame.\n\n    Args:\n        X (np.ndarray): The screening plan matrix, typically structured\n            within a [0,1]^k box.\n        fun (object): The objective function to evaluate at each\n            design point in the screening plan.\n        xi (float): The elementary effect step length factor.\n        p (int): Number of discrete levels along each dimension.\n        labels (list of str): A list of variable names corresponding to\n            the design variables.\n        bounds (np.ndarray): A 2xk matrix where the first row contains\n            lower bounds and the second row contains upper bounds for\n            each variable.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing three columns:\n            - 'varname': The name of each variable.\n            - 'mean': The mean of the elementary effects for each variable.\n            - 'sd': The standard deviation of the elementary effects for\n            each variable.\n        or None: If print is set to False, a plot of the results is\n            generated instead of returning a DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotpython.utils.effects import screening, screeningplan\n            from spotpython.fun.objectivefunctions import Analytical\n            fun = Analytical()\n            k = 10\n            p = 10\n            xi = 1\n            r = 25\n            X = screeningplan(k=k, p=p, xi=xi, r=r)  # shape (r x (k+1), k)\n            # Provide real-world bounds from the wing weight docs (2 x 10).\n            value_range = np.array([\n                [150, 220,   6, -10, 16, 0.5, 0.08, 2.5, 1700, 0.025],\n                [200, 300,  10,  10, 45, 1.0, 0.18, 6.0, 2500, 0.08 ],\n            ])\n            labels = [\n                \"S_W\", \"W_fw\", \"A\", \"Lambda\",\n                \"q\",   \"lambda\", \"tc\", \"N_z\",\n                \"W_dg\", \"W_p\"\n            ]\n            screening(\n                X=X,\n                fun=fun.fun_wingwt,\n                bounds=value_range,\n                xi=xi,\n                p=p,\n                labels=labels,\n                print=False,\n            )\n    \"\"\"\n    sm, ssd = _screening(X=X, fun=fun, xi=xi, p=p, labels=labels, bounds=bounds)\n    idx = np.argsort(-np.abs(sm))\n    sorted_labels = [labels[i] for i in idx]\n    sm = sm[idx]\n    ssd = ssd[idx]\n    df = pd.DataFrame({\"varname\": sorted_labels, \"mean\": sm, \"sd\": ssd})\n    return df\n</code></pre>"},{"location":"reference/spotoptim/sampling/lhs/","title":"lhs","text":""},{"location":"reference/spotoptim/sampling/lhs/#spotoptim.sampling.lhs.rlh","title":"<code>rlh(n, k, edges=0, seed=None)</code>","text":"<p>Generates a random Latin hypercube within the [0,1]^k hypercube.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Desired number of points.</p> required <code>k</code> <code>int</code> <p>Number of design variables (dimensions).</p> required <code>edges</code> <code>int</code> <p>If 1, places centers of the extreme bins at the domain edges ([0,1]). Otherwise, bins are fully contained within the domain, i.e. midpoints. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A Latin hypercube sampling plan of n points in k dimensions,         with each coordinate in the range [0,1].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.sampling.lhs import rlh\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Generate a 2D Latin hypercube with 5 points and edges=0\n&gt;&gt;&gt; X = rlh(n=5, k=2, edges=0)\n&gt;&gt;&gt; print(X)\n# Example output (values vary due to randomness):\n# [[0.1  0.5 ]\n#  [0.7  0.1 ]\n#  [0.9  0.7 ]\n#  [0.3  0.9 ]\n#  [0.5  0.3 ]]\n</code></pre> Source code in <code>src/spotoptim/sampling/lhs.py</code> <pre><code>def rlh(\n    n: int, k: int, edges: int = 0, seed: Optional[Union[int, Generator]] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Generates a random Latin hypercube within the [0,1]^k hypercube.\n\n    Args:\n        n (int): Desired number of points.\n        k (int): Number of design variables (dimensions).\n        edges (int, optional):\n            If 1, places centers of the extreme bins at the domain edges ([0,1]).\n            Otherwise, bins are fully contained within the domain, i.e. midpoints.\n            Defaults to 0.\n\n    Returns:\n        np.ndarray: A Latin hypercube sampling plan of n points in k dimensions,\n                    with each coordinate in the range [0,1].\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.sampling.lhs import rlh\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; # Generate a 2D Latin hypercube with 5 points and edges=0\n        &gt;&gt;&gt; X = rlh(n=5, k=2, edges=0)\n        &gt;&gt;&gt; print(X)\n        # Example output (values vary due to randomness):\n        # [[0.1  0.5 ]\n        #  [0.7  0.1 ]\n        #  [0.9  0.7 ]\n        #  [0.3  0.9 ]\n        #  [0.5  0.3 ]]\n    \"\"\"\n    # Validate inputs\n    if n &lt; 1:\n        raise ValueError(\"n must be &gt;= 1\")\n    if k &lt; 1:\n        raise ValueError(\"k must be &gt;= 1\")\n    if edges not in (0, 1):\n        raise ValueError(\"edges must be 0 or 1\")\n\n    # Initialize array\n    X = np.zeros((n, k), dtype=float)\n\n    # Initialize rng\n    rng = default_rng(seed)\n\n    # Fill with random permutations\n    for i in range(k):\n        X[:, i] = rng.permutation(n)\n\n    # Adjust normalization based on the edges flag\n    if edges == 1:\n        # [X=0..n-1] -&gt; [0..1]\n        if n == 1:\n            # Avoid division by zero; for a single point place at 0\n            X[:, :] = 0.0\n        else:\n            X = X / (n - 1)\n    else:\n        # Points at true midpoints\n        # [X=0..n-1] -&gt; [0.5/n..(n-0.5)/n]\n        X = (X + 0.5) / n\n\n    return X\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/","title":"mm","text":""},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.bestlh","title":"<code>bestlh(n, k, population, iterations, p=1, plot=False, verbosity=0, edges=0, q_list=[1, 2, 5, 10, 20, 50, 100])</code>","text":"<p>Generates an optimized Latin hypercube by evolving the Morris-Mitchell criterion across multiple exponents (q values) and selecting the best plan.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of points required in the Latin hypercube.</p> required <code>k</code> <code>int</code> <p>Number of design variables (dimensions).</p> required <code>population</code> <code>int</code> <p>Number of offspring in each generation of the evolutionary search.</p> required <code>iterations</code> <code>int</code> <p>Number of generations for the evolutionary search.</p> required <code>p</code> <code>int</code> <p>The distance norm to use. p=1 for Manhattan (L1), p=2 for Euclidean (L2). Defaults to 1 (faster than 2).</p> <code>1</code> <code>plot</code> <code>bool</code> <p>If True, a scatter plot of the optimized plan in the first two dimensions will be displayed. Only if k&gt;=2. Defaults to False.</p> <code>False</code> <code>verbosity</code> <code>int</code> <p>Verbosity level. 0 is silent, 1 prints the best q value found. Defaults to 0.</p> <code>0</code> <code>edges</code> <code>int</code> <p>If 1, places centers of the extreme bins at the domain edges ([0,1]). Otherwise, bins are fully contained within the domain, i.e. midpoints. Defaults to 0.</p> <code>0</code> <code>q_list</code> <code>list</code> <p>A list of q values to optimize. Defaults to [1, 2, 5, 10, 20, 50, 100]. These values are used to evaluate the space-fillingness of the Latin hypercube. The best plan is selected based on the lowest mmphi value.</p> <code>[1, 2, 5, 10, 20, 50, 100]</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 2D array of shape (n, k) representing an optimized Latin hypercube.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import bestlh\n# Generate a 5-point, 2-dimensional Latin hypercube\n&gt;&gt;&gt; X = bestlh(n=5, k=2, population=5, iterations=10)\n&gt;&gt;&gt; print(X.shape)\n(5, 2)\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def bestlh(\n    n: int,\n    k: int,\n    population: int,\n    iterations: int,\n    p=1,\n    plot=False,\n    verbosity=0,\n    edges=0,\n    q_list=[1, 2, 5, 10, 20, 50, 100],\n) -&gt; np.ndarray:\n    \"\"\"\n    Generates an optimized Latin hypercube by evolving the Morris-Mitchell\n    criterion across multiple exponents (q values) and selecting the best plan.\n\n    Args:\n        n (int):\n            Number of points required in the Latin hypercube.\n        k (int):\n            Number of design variables (dimensions).\n        population (int):\n            Number of offspring in each generation of the evolutionary search.\n        iterations (int):\n            Number of generations for the evolutionary search.\n        p (int, optional):\n            The distance norm to use. p=1 for Manhattan (L1), p=2 for Euclidean (L2).\n            Defaults to 1 (faster than 2).\n        plot (bool, optional):\n            If True, a scatter plot of the optimized plan in the first two dimensions\n            will be displayed. Only if k&gt;=2. Defaults to False.\n        verbosity (int, optional):\n            Verbosity level. 0 is silent, 1 prints the best q value found. Defaults to 0.\n        edges (int, optional):\n            If 1, places centers of the extreme bins at the domain edges ([0,1]).\n            Otherwise, bins are fully contained within the domain, i.e. midpoints.\n            Defaults to 0.\n        q_list (list, optional):\n            A list of q values to optimize. Defaults to [1, 2, 5, 10, 20, 50, 100].\n            These values are used to evaluate the space-fillingness of the Latin\n            hypercube. The best plan is selected based on the lowest mmphi value.\n\n    Returns:\n        np.ndarray:\n            A 2D array of shape (n, k) representing an optimized Latin hypercube.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.mm import bestlh\n        # Generate a 5-point, 2-dimensional Latin hypercube\n        &gt;&gt;&gt; X = bestlh(n=5, k=2, population=5, iterations=10)\n        &gt;&gt;&gt; print(X.shape)\n        (5, 2)\n    \"\"\"\n    if n &lt; 2:\n        raise ValueError(\"Latin hypercubes require at least 2 points\")\n    if k &lt; 2:\n        raise ValueError(\"Latin hypercubes are not defined for dim k &lt; 2\")\n\n    # Start with a random Latin hypercube\n    X_start = rlh(n, k, edges=edges)\n\n    # Allocate a 3D array to store the results for each q\n    # (shape: (n, k, number_of_q_values))\n    X3D = np.zeros((n, k, len(q_list)))\n\n    # Evolve the plan for each q in q_list\n    for i, q_val in enumerate(q_list):\n        if verbosity &gt; 0:\n            print(f\"Now optimizing for q={q_val}...\")\n        X3D[:, :, i] = mmlhs(X_start, population, iterations, q_val)\n\n    # Sort the set of evolved plans according to the Morris-Mitchell criterion\n    index_order = mmsort(X3D, p=p)\n\n    # index_order is a 1-based array of plan indices; the first element is the best\n    best_idx = index_order[0] - 1\n    if verbosity &gt; 0:\n        print(f\"Best lh found using q={q_list[best_idx]}...\")\n\n    # The best plan in 3D array order\n    X = X3D[:, :, best_idx]\n\n    # Plot the first two dimensions\n    if plot and (k &gt;= 2):\n        plt.scatter(X[:, 0], X[:, 1], c=\"r\", marker=\"o\")\n        plt.title(f\"Morris-Mitchell optimum plan found using q={q_list[best_idx]}\")\n        plt.xlabel(\"x_1\")\n        plt.ylabel(\"x_2\")\n        plt.grid(True)\n        plt.show()\n\n    return X\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.jd","title":"<code>jd(X, p=1.0)</code>","text":"<p>Computes and counts the distinct p-norm distances between all pairs of points in X. It returns: 1) A list of distinct distances (sorted), and 2) A corresponding multiplicity array that indicates how often each distance occurs.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D array of shape (n, d) representing n points in d-dimensional space.</p> required <code>p</code> <code>float</code> <p>The distance norm to use. p=1 uses the Manhattan (L1) norm, while p=2 uses the Euclidean (L2) norm. Defaults to 1.0 (Manhattan norm).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>A tuple (J, distinct_d), where: - distinct_d is a 1D float array of unique, sorted distances between points. - J is a 1D integer array that provides the multiplicity (occurrence count)   of each distance in distinct_d.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import jd\n&gt;&gt;&gt; # A small 3-point set in 2D\n&gt;&gt;&gt; X = np.array([[0.0, 0.0],\n...               [1.0, 1.0],\n...               [2.0, 2.0]])\n&gt;&gt;&gt; J, distinct_d = jd(X, p=2.0)\n&gt;&gt;&gt; print(\"Distinct distances:\", distinct_d)\n&gt;&gt;&gt; print(\"Occurrences:\", J)\n# Possible output (using Euclidean norm):\n# Distinct distances: [1.41421356 2.82842712]\n# Occurrences: [1 1]\n# Explanation: Distances are sqrt(2) between consecutive points and 2*sqrt(2) for the farthest pair.\n    Distinct distances: [1.41421356 2.82842712]\n    Occurrences: [2 1]\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def jd(X: np.ndarray, p: float = 1.0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Computes and counts the distinct p-norm distances between all pairs of points in X.\n    It returns:\n    1) A list of distinct distances (sorted), and\n    2) A corresponding multiplicity array that indicates how often each distance occurs.\n\n    Args:\n        X (np.ndarray):\n            A 2D array of shape (n, d) representing n points in d-dimensional space.\n        p (float, optional):\n            The distance norm to use. p=1 uses the Manhattan (L1) norm, while p=2 uses the\n            Euclidean (L2) norm. Defaults to 1.0 (Manhattan norm).\n\n    Returns:\n        (np.ndarray, np.ndarray):\n            A tuple (J, distinct_d), where:\n            - distinct_d is a 1D float array of unique, sorted distances between points.\n            - J is a 1D integer array that provides the multiplicity (occurrence count)\n              of each distance in distinct_d.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code\n        under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.mm import jd\n        &gt;&gt;&gt; # A small 3-point set in 2D\n        &gt;&gt;&gt; X = np.array([[0.0, 0.0],\n        ...               [1.0, 1.0],\n        ...               [2.0, 2.0]])\n        &gt;&gt;&gt; J, distinct_d = jd(X, p=2.0)\n        &gt;&gt;&gt; print(\"Distinct distances:\", distinct_d)\n        &gt;&gt;&gt; print(\"Occurrences:\", J)\n        # Possible output (using Euclidean norm):\n        # Distinct distances: [1.41421356 2.82842712]\n        # Occurrences: [1 1]\n        # Explanation: Distances are sqrt(2) between consecutive points and 2*sqrt(2) for the farthest pair.\n            Distinct distances: [1.41421356 2.82842712]\n            Occurrences: [2 1]\n    \"\"\"\n    n = X.shape[0]\n\n    # Allocate enough space for all pairwise distances\n    # (n*(n-1))/2 pairs for an n-point set\n    pair_count = n * (n - 1) // 2\n    d = np.zeros(pair_count, dtype=float)\n\n    # Fill the distance array\n    idx = 0\n    for i in range(n - 1):\n        for j in range(i + 1, n):\n            # Compute the p-norm distance\n            d[idx] = np.linalg.norm(X[i] - X[j], ord=p)\n            idx += 1\n\n    # Find unique distances and their multiplicities\n    distinct_d = np.unique(d)\n    J = np.zeros_like(distinct_d, dtype=int)\n    for i, val in enumerate(distinct_d):\n        J[i] = np.sum(d == val)\n\n    return J, distinct_d\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.mm","title":"<code>mm(X1, X2, p=1.0)</code>","text":"<p>Determines which of two sampling plans has better space-filling properties according to the Morris-Mitchell criterion.</p> <p>Parameters:</p> Name Type Description Default <code>X1</code> <code>ndarray</code> <p>A 2D array representing the first sampling plan.</p> required <code>X2</code> <code>ndarray</code> <p>A 2D array representing the second sampling plan.</p> required <code>p</code> <code>float</code> <p>The distance metric. p=1 uses Manhattan (L1) distance, while p=2 uses Euclidean (L2). Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <ul> <li>0 if both plans are identical or equally space-filling</li> <li>1 if X1 is more space-filling</li> <li>2 if X2 is more space-filling</li> </ul> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mm\n&gt;&gt;&gt; # Create two 3-point sampling plans in 2D\n&gt;&gt;&gt; X1 = np.array([[0.0, 0.0],\n...                [0.5, 0.5],\n...                [0.0, 1.0]])\n&gt;&gt;&gt; X2 = np.array([[0.1, 0.1],\n...                [0.4, 0.6],\n...                [0.1, 0.9]])\n&gt;&gt;&gt; # Compare which plan has better space-filling (Morris-Mitchell)\n&gt;&gt;&gt; better = mm(X1, X2, p=2.0)\n&gt;&gt;&gt; print(better)\n# Prints either 0, 1, or 2 depending on which plan is more space-filling.\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def mm(X1: np.ndarray, X2: np.ndarray, p: Optional[float] = 1.0) -&gt; int:\n    \"\"\"\n    Determines which of two sampling plans has better space-filling properties\n    according to the Morris-Mitchell criterion.\n\n    Args:\n        X1 (np.ndarray): A 2D array representing the first sampling plan.\n        X2 (np.ndarray): A 2D array representing the second sampling plan.\n        p (float, optional): The distance metric. p=1 uses Manhattan (L1) distance,\n            while p=2 uses Euclidean (L2). Defaults to 1.0.\n\n    Returns:\n        int:\n            - 0 if both plans are identical or equally space-filling\n            - 1 if X1 is more space-filling\n            - 2 if X2 is more space-filling\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code\n        under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.mm import mm\n        &gt;&gt;&gt; # Create two 3-point sampling plans in 2D\n        &gt;&gt;&gt; X1 = np.array([[0.0, 0.0],\n        ...                [0.5, 0.5],\n        ...                [0.0, 1.0]])\n        &gt;&gt;&gt; X2 = np.array([[0.1, 0.1],\n        ...                [0.4, 0.6],\n        ...                [0.1, 0.9]])\n        &gt;&gt;&gt; # Compare which plan has better space-filling (Morris-Mitchell)\n        &gt;&gt;&gt; better = mm(X1, X2, p=2.0)\n        &gt;&gt;&gt; print(better)\n        # Prints either 0, 1, or 2 depending on which plan is more space-filling.\n    \"\"\"\n    # Quick check if the sorted sets of points are identical\n    # (mimicking MATLAB's sortrows check)\n    X1_sorted = X1[np.lexsort(np.rot90(X1))]\n    X2_sorted = X2[np.lexsort(np.rot90(X2))]\n    if np.array_equal(X1_sorted, X2_sorted):\n        return 0  # Identical sampling plans\n\n    # Compute distance multiplicities for each plan\n    J1, d1 = jd(X1, p)\n    J2, d2 = jd(X2, p)\n    m1, m2 = len(d1), len(d2)\n\n    # Construct V1 and V2: alternate distance and negative multiplicity\n    V1 = np.zeros(2 * m1)\n    V1[0::2] = d1\n    V1[1::2] = -J1\n\n    V2 = np.zeros(2 * m2)\n    V2[0::2] = d2\n    V2[1::2] = -J2\n\n    # Trim the longer vector to match the size of the shorter\n    m = min(m1, m2)\n    V1 = V1[:m]\n    V2 = V2[:m]\n\n    # Compare element-by-element:\n    # c[i] = 1 if V1[i] &gt; V2[i], 2 if V1[i] &lt; V2[i], 0 otherwise.\n    c = (V1 &gt; V2).astype(int) + 2 * (V1 &lt; V2).astype(int)\n\n    if np.sum(c) == 0:\n        # Equally space-filling\n        return 0\n    else:\n        # The first non-zero entry indicates which plan is better\n        idx = np.argmax(c != 0)\n        return c[idx]\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.mm_improvement","title":"<code>mm_improvement(x, X_base, phi_base=None, J_base=None, d_base=None, q=2, p=2, normalize_flag=False, verbose=False, exponential=True)</code>","text":"<p>Calculates the Morris-Mitchell improvement for a candidate point x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Candidate point (1D array).</p> required <code>X_base</code> <code>ndarray</code> <p>Existing design points.</p> required <code>J_base</code> <code>ndarray</code> <p>Multiplicities of distances for X_base.</p> <code>None</code> <code>d_base</code> <code>ndarray</code> <p>Unique distances for X_base.</p> <code>None</code> <code>q</code> <code>int</code> <p>Number of nearest neighbors for MM metric.</p> <code>2</code> <code>p</code> <code>int</code> <p>Power for MM metric.</p> <code>2</code> <code>normalize_flag</code> <code>bool</code> <p>If True, normalizes the X array and candidate point before computing distances. Defaults to False.</p> <code>False</code> <code>exponential</code> <code>bool</code> <p>If True, the exponential is applied.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Morris-Mitchell improvement.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mm_improvement\n&gt;&gt;&gt; X_base = np.array([[0.1, 0.2], [0.4, 0.5], [0.7, 0.8]])\n&gt;&gt;&gt; x = np.array([0.5, 0.5])\n&gt;&gt;&gt; improvement = mm_improvement(x, X_base, q=2, p=2)\n&gt;&gt;&gt; print(improvement)\n0.123456789\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def mm_improvement(\n    x,\n    X_base,\n    phi_base=None,\n    J_base=None,\n    d_base=None,\n    q=2,\n    p=2,\n    normalize_flag=False,\n    verbose=False,\n    exponential=True,\n) -&gt; float:\n    \"\"\"\n    Calculates the Morris-Mitchell improvement for a candidate point x.\n\n    Args:\n        x (np.ndarray): Candidate point (1D array).\n        X_base (np.ndarray): Existing design points.\n        J_base (np.ndarray): Multiplicities of distances for X_base.\n        d_base (np.ndarray): Unique distances for X_base.\n        q (int): Number of nearest neighbors for MM metric.\n        p (int): Power for MM metric.\n        normalize_flag (bool): If True, normalizes the X array and candidate point before computing distances. Defaults to False.\n        exponential (bool): If True, the exponential is applied.\n\n    Returns:\n        float: Morris-Mitchell improvement.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.mm import mm_improvement\n        &gt;&gt;&gt; X_base = np.array([[0.1, 0.2], [0.4, 0.5], [0.7, 0.8]])\n        &gt;&gt;&gt; x = np.array([0.5, 0.5])\n        &gt;&gt;&gt; improvement = mm_improvement(x, X_base, q=2, p=2)\n        &gt;&gt;&gt; print(improvement)\n        0.123456789\n    \"\"\"\n    if phi_base is None or J_base is None or d_base is None:\n        phi_base, J_base, d_base = mmphi_intensive(\n            X_base, q=q, p=p, normalize_flag=normalize_flag\n        )\n    phi_new, _, _ = mmphi_intensive_update(\n        X_base, x, J_base, d_base, q=q, p=p, normalize_flag=normalize_flag\n    )\n    if exponential:\n        y_mm = np.exp(phi_base - phi_new)\n    else:\n        y_mm = phi_base - phi_new\n    if verbose:\n        print(f\"Morris-Mitchell base: {phi_base}\")\n        print(f\"Morris-Mitchell new: {phi_new}\")\n        print(f\"Morris-Mitchell improvement: {y_mm}\")\n    return float(y_mm)\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.mm_improvement_contour","title":"<code>mm_improvement_contour(X_base, x1=np.linspace(0, 1, 100), x2=np.linspace(0, 1, 100), q=2, p=2)</code>","text":"<p>Generates a contour plot of the Morris-Mitchell improvement over a grid defined by x1 and x2.</p> <p>Parameters:</p> Name Type Description Default <code>X_base</code> <code>ndarray</code> <p>Base design points.</p> required <code>x1</code> <code>ndarray</code> <p>Grid values for the first dimension. Default is np.linspace(0, 1, 100).</p> <code>linspace(0, 1, 100)</code> <code>x2</code> <code>ndarray</code> <p>Grid values for the second dimension. Default is np.linspace(0, 1, 100).</p> <code>linspace(0, 1, 100)</code> <code>q</code> <code>int</code> <p>Morris-Mitchell metric parameter. Default is 2.</p> <code>2</code> <code>p</code> <code>int</code> <p>Morris-Mitchell metric parameter. Default is 2.</p> <code>2</code> <p>Returns:     None: Displays a contour plot of the Morris-Mitchell improvement.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotoptim.sampling.mm import mm_improvement_contour\n    X_base = np.array([[0.1, 0.1], [0.2, 0.2], [0.7, 0.7]])\n    mm_improvement_contour(X_base)\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def mm_improvement_contour(\n    X_base, x1=np.linspace(0, 1, 100), x2=np.linspace(0, 1, 100), q=2, p=2\n):\n    \"\"\"\n    Generates a contour plot of the Morris-Mitchell improvement over a grid defined by x1 and x2.\n\n    Args:\n        X_base (np.ndarray):\n            Base design points.\n        x1 (np.ndarray):\n            Grid values for the first dimension. Default is np.linspace(0, 1, 100).\n        x2 (np.ndarray): Grid values for the second dimension. Default is np.linspace(0, 1, 100).\n        q (int):\n            Morris-Mitchell metric parameter. Default is 2.\n        p (int):\n            Morris-Mitchell metric parameter. Default is 2.\n    Returns:\n        None: Displays a contour plot of the Morris-Mitchell improvement.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotoptim.sampling.mm import mm_improvement_contour\n            X_base = np.array([[0.1, 0.1], [0.2, 0.2], [0.7, 0.7]])\n            mm_improvement_contour(X_base)\n    \"\"\"\n\n    _, J_base, d_base = mmphi_intensive(X_base, q=2, p=2)\n    X1, X2 = np.meshgrid(x1, x2)\n    improvement_grid = np.zeros(X1.shape)\n    for i in range(X1.shape[0]):\n        for j in range(X1.shape[1]):\n            x = np.array([X1[i, j], X2[i, j]])\n            improvement_grid[i, j] = mm_improvement(x, X_base, J_base, d_base, q=2, p=2)\n    plt.contourf(X1, X2, improvement_grid, levels=30, cmap=\"viridis\")\n    plt.colorbar(label=\"Morris-Mitchell Improvement\")\n    plt.scatter(X_base[:, 0], X_base[:, 1], color=\"red\", label=\"X_base\")\n    plt.title(\"Morris-Mitchell Improvement Contour Plot\")\n    plt.xlabel(\"X1\")\n    plt.ylabel(\"X2\")\n    plt.legend()\n    plt.grid()\n    plt.show()\n    plt.close()\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.mmlhs","title":"<code>mmlhs(X_start, population, iterations, q=2.0, plot=False)</code>","text":"<p>Performs an evolutionary search (using perturbations) to find a Morris-Mitchell optimal Latin hypercube, starting from an initial plan X_start.</p> This function does the following <ol> <li>Initializes a \u201cbest\u201d Latin hypercube (X_best) from the provided X_start.</li> <li>Iteratively perturbs X_best to create offspring.</li> <li>Evaluates the space-fillingness of each offspring via the Morris-Mitchell    metric (using mmphi).</li> <li>Updates the best plan whenever a better offspring is found.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>X_start</code> <code>ndarray</code> <p>A 2D array of shape (n, k) providing the initial Latin hypercube (n points in k dimensions).</p> required <code>population</code> <code>int</code> <p>Number of offspring to create in each generation.</p> required <code>iterations</code> <code>int</code> <p>Total number of generations to run the evolutionary search.</p> required <code>q</code> <code>float</code> <p>The exponent used by the Morris-Mitchell space-filling criterion. Defaults to 2.0.</p> <code>2.0</code> <code>plot</code> <code>bool</code> <p>If True, a simple scatter plot of the first two dimensions will be displayed at each iteration. Only if k &gt;= 2. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 2D array representing the most space-filling Latin hypercube found after all iterations, of the same shape as X_start.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmlhs\n&gt;&gt;&gt; # Suppose we have an initial 4x2 plan\n&gt;&gt;&gt; X_start = np.array([\n...     [0, 0],\n...     [1, 3],\n...     [2, 1],\n...     [3, 2]\n... ])\n&gt;&gt;&gt; # Search for a more space-filling plan\n&gt;&gt;&gt; X_opt = mmlhs(X_start, population=5, iterations=10, q=2)\n&gt;&gt;&gt; print(\"Optimized plan:\")\n&gt;&gt;&gt; print(X_opt)\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def mmlhs(\n    X_start: np.ndarray,\n    population: int,\n    iterations: int,\n    q: Optional[float] = 2.0,\n    plot=False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Performs an evolutionary search (using perturbations) to find a Morris-Mitchell\n    optimal Latin hypercube, starting from an initial plan X_start.\n\n    This function does the following:\n      1. Initializes a \"best\" Latin hypercube (X_best) from the provided X_start.\n      2. Iteratively perturbs X_best to create offspring.\n      3. Evaluates the space-fillingness of each offspring via the Morris-Mitchell\n         metric (using mmphi).\n      4. Updates the best plan whenever a better offspring is found.\n\n    Args:\n        X_start (np.ndarray):\n            A 2D array of shape (n, k) providing the initial Latin hypercube\n            (n points in k dimensions).\n        population (int):\n            Number of offspring to create in each generation.\n        iterations (int):\n            Total number of generations to run the evolutionary search.\n        q (float, optional):\n            The exponent used by the Morris-Mitchell space-filling criterion.\n            Defaults to 2.0.\n        plot (bool, optional):\n            If True, a simple scatter plot of the first two dimensions will be\n            displayed at each iteration. Only if k &gt;= 2. Defaults to False.\n\n    Returns:\n        np.ndarray:\n            A 2D array representing the most space-filling Latin hypercube found\n            after all iterations, of the same shape as X_start.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code\n        under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.mm import mmlhs\n        &gt;&gt;&gt; # Suppose we have an initial 4x2 plan\n        &gt;&gt;&gt; X_start = np.array([\n        ...     [0, 0],\n        ...     [1, 3],\n        ...     [2, 1],\n        ...     [3, 2]\n        ... ])\n        &gt;&gt;&gt; # Search for a more space-filling plan\n        &gt;&gt;&gt; X_opt = mmlhs(X_start, population=5, iterations=10, q=2)\n        &gt;&gt;&gt; print(\"Optimized plan:\")\n        &gt;&gt;&gt; print(X_opt)\n    \"\"\"\n    n = X_start.shape[0]\n    if n &lt; 2:\n        raise ValueError(\"Latin hypercubes require at least 2 points\")\n    k = X_start.shape[1]\n    if k &lt; 2:\n        raise ValueError(\"Latin hypercubes are not defined for dim k &lt; 2\")\n\n    # Initialize best plan and its metric\n    X_best = X_start.copy()\n    Phi_best = mmphi(X_best, q=q)\n\n    # After 85% of iterations, reduce the mutation rate to 1\n    leveloff = int(np.floor(0.85 * iterations))\n\n    for it in range(1, iterations + 1):\n        # Decrease number of mutations over time\n        if it &lt; leveloff:\n            mutations = int(round(1 + (0.5 * n - 1) * (leveloff - it) / (leveloff - 1)))\n        else:\n            mutations = 1\n\n        X_improved = X_best.copy()\n        Phi_improved = Phi_best\n\n        # Create offspring, evaluate, and keep the best\n        for _ in range(population):\n            X_try = perturb(X_best.copy(), mutations)\n            Phi_try = mmphi(X_try, q=q)\n\n            if Phi_try &lt; Phi_improved:\n                X_improved = X_try\n                Phi_improved = Phi_try\n\n        # Update the global best if we found a better plan\n        if Phi_improved &lt; Phi_best:\n            X_best = X_improved\n            Phi_best = Phi_improved\n\n        # Simple visualization of the first two dimensions\n        if plot and (X_best.shape[1] &gt;= 2):\n            plt.clf()\n            plt.scatter(X_best[:, 0], X_best[:, 1], marker=\"o\")\n            plt.grid(True)\n            plt.title(f\"Iteration {it} - Current Best Plan\")\n            plt.pause(0.01)\n\n    return X_best\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.mmphi","title":"<code>mmphi(X, q=2.0, p=1.0, verbosity=0)</code>","text":"<p>Calculates the Morris-Mitchell sampling plan quality criterion.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D array representing the sampling plan, where each row is a point in d-dimensional space (shape: (n, d)).</p> required <code>q</code> <code>float</code> <p>Exponent used in the computation of the metric. Defaults to 2.0.</p> <code>2.0</code> <code>p</code> <code>float</code> <p>The distance norm to use. For example, p=1 is Manhattan (L1), p=2 is Euclidean (L2). Defaults to 1.0.</p> <code>1.0</code> <code>verbosity</code> <code>int</code> <p>If set to 1, prints additional information about the computation. Defaults to 0 (no additional output).</p> <code>0</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The space-fillingness metric Phiq. Larger values typically indicate a more space-filling plan according to the Morris-Mitchell criterion.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi\n&gt;&gt;&gt; # Simple 3-point sampling plan in 2D\n&gt;&gt;&gt; X = np.array([\n...     [0.0, 0.0],\n...     [0.5, 0.5],\n...     [1.0, 1.0]\n... ])\n&gt;&gt;&gt; # Calculate the space-fillingness metric with q=2, using Euclidean distances (p=2)\n&gt;&gt;&gt; quality = mmphi(X, q=2, p=2)\n&gt;&gt;&gt; print(quality)\n# This value indicates how well points are spread out, with smaller being better.\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def mmphi(\n    X: np.ndarray, q: Optional[float] = 2.0, p: Optional[float] = 1.0, verbosity=0\n) -&gt; float:\n    \"\"\"\n    Calculates the Morris-Mitchell sampling plan quality criterion.\n\n    Args:\n        X (np.ndarray):\n            A 2D array representing the sampling plan, where each row is a point in\n            d-dimensional space (shape: (n, d)).\n        q (float, optional):\n            Exponent used in the computation of the metric. Defaults to 2.0.\n        p (float, optional):\n            The distance norm to use. For example, p=1 is Manhattan (L1),\n            p=2 is Euclidean (L2). Defaults to 1.0.\n        verbosity (int, optional):\n            If set to 1, prints additional information about the computation.\n            Defaults to 0 (no additional output).\n\n    Returns:\n        float:\n            The space-fillingness metric Phiq. Larger values typically indicate a more\n            space-filling plan according to the Morris-Mitchell criterion.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code\n        under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.mm import mmphi\n        &gt;&gt;&gt; # Simple 3-point sampling plan in 2D\n        &gt;&gt;&gt; X = np.array([\n        ...     [0.0, 0.0],\n        ...     [0.5, 0.5],\n        ...     [1.0, 1.0]\n        ... ])\n        &gt;&gt;&gt; # Calculate the space-fillingness metric with q=2, using Euclidean distances (p=2)\n        &gt;&gt;&gt; quality = mmphi(X, q=2, p=2)\n        &gt;&gt;&gt; print(quality)\n        # This value indicates how well points are spread out, with smaller being better.\n    \"\"\"\n    # check that X has unique rows\n    if X.shape[0] != len(np.unique(X, axis=0)):\n        # issue a warning if there are duplicate rows\n        print(\n            \"Warning: X contains duplicate rows. This may affect the space-fillingness metric.\"\n        )\n        # make X unique\n        X = np.unique(X, axis=0)\n    # Compute the distance multiplicities: J, and unique distances: d\n    J, d = jd(X, p)\n    print(f\"J: {J}, d: {d}\") if verbosity &gt; 0 else None\n\n    # Summation of J[i] * d[i]^(-q), then raised to 1/q\n    # This follows the Morris-Mitchell definition.\n    Phiq = np.sum(J * (d ** (-q))) ** (1.0 / q)\n    return Phiq\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.mmphi_intensive","title":"<code>mmphi_intensive(X, q=2.0, p=2.0, normalize_flag=False)</code>","text":"<p>Calculates a size-invariant Morris-Mitchell criterion.</p> <p>This \u201cintensive\u201d version of the criterion allows for the comparison of sampling plans with different sample sizes by normalizing for the number of point pairs. A smaller value indicates a better (more space-filling) design.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D array representing the sampling plan (shape: (n, d)).</p> required <code>q</code> <code>float</code> <p>The exponent used in the computation of the metric. Defaults to 2.0.</p> <code>2.0</code> <code>p</code> <code>float</code> <p>The distance norm to use (e.g., p=1 for Manhattan, p=2 for Euclidean). Defaults to 2.0.</p> <code>2.0</code> <code>normalize_flag</code> <code>bool</code> <p>If True, normalizes the X array before computing distances. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[float, ndarray, ndarray]</code> <p>tuple[float, np.ndarray, np.ndarray]: A tuple containing: - intensive_phiq: The intensive space-fillingness metric. - J: Multiplicities of distances. - d: Unique distances.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive\n&gt;&gt;&gt; # Create a simple 3-point sampling plan in 2D\n&gt;&gt;&gt; X = np.array([\n...     [0.0, 0.0],\n...     [0.5, 0.5],\n...     [1.0, 1.0]\n... ])\n&gt;&gt;&gt; # Calculate the intensive space-fillingness metric with q=2, using Euclidean distances (p=2)\n&gt;&gt;&gt; quality, J, d = mmphi_intensive(X, q=2, p=2)\n&gt;&gt;&gt; print(quality)\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def mmphi_intensive(\n    X: np.ndarray,\n    q: Optional[float] = 2.0,\n    p: Optional[float] = 2.0,\n    normalize_flag: bool = False,\n) -&gt; tuple[float, np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculates a size-invariant Morris-Mitchell criterion.\n\n    This \"intensive\" version of the criterion allows for the comparison of\n    sampling plans with different sample sizes by normalizing for the number\n    of point pairs. A smaller value indicates a better (more space-filling)\n    design.\n\n    Args:\n        X (np.ndarray):\n            A 2D array representing the sampling plan (shape: (n, d)).\n        q (float, optional):\n            The exponent used in the computation of the metric. Defaults to 2.0.\n        p (float, optional):\n            The distance norm to use (e.g., p=1 for Manhattan, p=2 for Euclidean).\n            Defaults to 2.0.\n        normalize_flag (bool, optional):\n            If True, normalizes the X array before computing distances.\n            Defaults to False.\n\n    Returns:\n        tuple[float, np.ndarray, np.ndarray]:\n            A tuple containing:\n            - intensive_phiq: The intensive space-fillingness metric.\n            - J: Multiplicities of distances.\n            - d: Unique distances.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive\n        &gt;&gt;&gt; # Create a simple 3-point sampling plan in 2D\n        &gt;&gt;&gt; X = np.array([\n        ...     [0.0, 0.0],\n        ...     [0.5, 0.5],\n        ...     [1.0, 1.0]\n        ... ])\n        &gt;&gt;&gt; # Calculate the intensive space-fillingness metric with q=2, using Euclidean distances (p=2)\n        &gt;&gt;&gt; quality, J, d = mmphi_intensive(X, q=2, p=2)\n        &gt;&gt;&gt; print(quality)\n    \"\"\"\n    # Ensure there are no duplicate points\n    if X.shape[0] != len(np.unique(X, axis=0)):\n        X = np.unique(X, axis=0)\n\n    n_points = X.shape[0]\n\n    # Normalize X to [0, 1] in each dimension if requested\n    if normalize_flag:\n        X = normalize_X(X)\n\n    # The criterion is not well-defined for fewer than 2 points.\n    if n_points &lt; 2:\n        return np.inf, 0, 0\n\n    # Get the unique distances and their multiplicities\n    J, d = jd(X, p=p)\n\n    # If all points are identical, the design is infinitely bad.\n    if d.size == 0:\n        return np.inf, J, d\n\n    # Calculate the number of unique pairs of points\n    M = n_points * (n_points - 1) / 2\n\n    try:\n        # Calculate the sum term of the original mmphi\n        sum_term = np.sum(J * (d ** (-q)))\n        # Normalize the sum by M before taking the final root\n        intensive_phiq = (sum_term / M) ** (1.0 / q)\n    except ZeroDivisionError:\n        return np.inf\n    except FloatingPointError:\n        return np.inf\n    except Exception:\n        return np.inf\n\n    return intensive_phiq, J, d\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.mmphi_intensive_update","title":"<code>mmphi_intensive_update(X, new_point, J, d, q=2.0, p=2.0, normalize_flag=False)</code>","text":"<p>Updates the Morris-Mitchell intensive criterion for n+1 points by adding a new point to the design. This should be more efficient than recalculating the metric from scratch, because it only needs to compute the distances between the new point and the existing points.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Existing sampling plan (shape: (n, d)).</p> required <code>new_point</code> <code>ndarray</code> <p>New point to add (shape: (d,)).</p> required <code>J</code> <code>ndarray</code> <p>Multiplicities of distances for the existing design.</p> required <code>d</code> <code>ndarray</code> <p>Unique distances for the existing design.</p> required <code>q</code> <code>float</code> <p>Exponent used in the computation of the Morris-Mitchell metric. Defaults to 2.0.</p> <code>2.0</code> <code>p</code> <code>float</code> <p>Distance norm to use (e.g., p=1 for Manhattan, p=2 for Euclidean). Defaults to 2.0.</p> <code>2.0</code> <code>normalize_flag</code> <code>bool</code> <p>If True, normalizes the X array and the new_point before computing distances. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[float, ndarray, ndarray]</code> <p>tuple[float, np.ndarray, np.ndarray]: Updated intensive_phiq, updated_J, updated_d.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive_update\n&gt;&gt;&gt; # Existing design with 3 points in 2D\n&gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; phiq, J, d = mmphi_intensive(X, q=2, p=2)\n&gt;&gt;&gt; # New point to add\n&gt;&gt;&gt; new_point = np.array([0.1, 0.1])\n&gt;&gt;&gt; # Update the intensive criterion\n&gt;&gt;&gt; updated_phiq, updated_J, updated_d = mmphi_intensive_update(X, new_point, J, d, q=2, p=2)\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def mmphi_intensive_update(\n    X: np.ndarray,\n    new_point: np.ndarray,\n    J: np.ndarray,\n    d: np.ndarray,\n    q: float = 2.0,\n    p: float = 2.0,\n    normalize_flag: bool = False,\n) -&gt; tuple[float, np.ndarray, np.ndarray]:\n    \"\"\"\n    Updates the Morris-Mitchell intensive criterion for n+1 points by adding a new point to the design.\n    This should be more efficient than recalculating the metric from scratch, because it only needs to\n    compute the distances between the new point and the existing points.\n\n    Args:\n        X (np.ndarray): Existing sampling plan (shape: (n, d)).\n        new_point (np.ndarray): New point to add (shape: (d,)).\n        J (np.ndarray): Multiplicities of distances for the existing design.\n        d (np.ndarray): Unique distances for the existing design.\n        q (float): Exponent used in the computation of the Morris-Mitchell metric. Defaults to 2.0.\n        p (float): Distance norm to use (e.g., p=1 for Manhattan, p=2 for Euclidean). Defaults to 2.0.\n        normalize_flag (bool): If True, normalizes the X array and the new_point before computing distances. Defaults to False.\n\n    Returns:\n        tuple[float, np.ndarray, np.ndarray]: Updated intensive_phiq, updated_J, updated_d.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.mm import mmphi_intensive_update\n        &gt;&gt;&gt; # Existing design with 3 points in 2D\n        &gt;&gt;&gt; X = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n        &gt;&gt;&gt; phiq, J, d = mmphi_intensive(X, q=2, p=2)\n        &gt;&gt;&gt; # New point to add\n        &gt;&gt;&gt; new_point = np.array([0.1, 0.1])\n        &gt;&gt;&gt; # Update the intensive criterion\n        &gt;&gt;&gt; updated_phiq, updated_J, updated_d = mmphi_intensive_update(X, new_point, J, d, q=2, p=2)\n\n    \"\"\"\n    n_points = X.shape[0]\n    if n_points &lt; 1:\n        raise ValueError(\"The existing design must contain at least one point.\")\n\n    # Normalize X and new_point to [0, 1] in each dimension if requested\n    if normalize_flag:\n        X = normalize_X(X)\n        new_point = (new_point - np.min(X, axis=0)) / (\n            np.max(X, axis=0) - np.min(X, axis=0)\n        )\n\n    # Compute distances between the new point and all existing points\n    new_distances = np.array(\n        [np.linalg.norm(new_point - X[i], ord=p) for i in range(n_points)]\n    )\n\n    # Combine old distances and new distances into a single list\n    all_distances = []\n    for dist, count in zip(d, J):\n        all_distances.extend([dist] * count)\n    all_distances.extend(new_distances)\n\n    # Find unique distances and their counts\n    updated_d, updated_J = np.unique(all_distances, return_counts=True)\n\n    # Calculate the number of unique pairs of points\n    M = (n_points + 1) * n_points / 2\n\n    # Compute the updated intensive_phiq\n    sum_term = np.sum(updated_J * (updated_d ** (-q)))\n    intensive_phiq = (sum_term / M) ** (1.0 / q)\n\n    return intensive_phiq, updated_J, updated_d\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.mmsort","title":"<code>mmsort(X3D, p=1.0)</code>","text":"<p>Ranks multiple sampling plans stored in a 3D array according to the Morris-Mitchell criterion, using a simple bubble sort.</p> <p>Parameters:</p> Name Type Description Default <code>X3D</code> <code>ndarray</code> <p>A 3D NumPy array of shape (n, d, m), where m is the number of sampling plans, and each plan is an (n, d) matrix of points.</p> required <code>p</code> <code>float</code> <p>The distance metric to use. p=1 for Manhattan (L1), p=2 for Euclidean (L2). Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D integer array of length m that holds the plan indices in ascending order of space-filling quality. The first index in the returned array corresponds to the most space-filling plan.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import mmsort\n&gt;&gt;&gt; # Suppose we have two 3-point sampling plans in 2D, stored in X3D:\n&gt;&gt;&gt; X1 = np.array([[0.0, 0.0],\n...                [0.5, 0.5],\n...                [1.0, 1.0]])\n&gt;&gt;&gt; X2 = np.array([[0.2, 0.2],\n...                [0.6, 0.4],\n...                [0.9, 0.9]])\n&gt;&gt;&gt; # Stack them along the third dimension: shape will be (3, 2, 2)\n&gt;&gt;&gt; X3D = np.stack([X1, X2], axis=2)\n&gt;&gt;&gt; # Sort them using the Morris-Mitchell criterion with p=2\n&gt;&gt;&gt; ranking = mmsort(X3D, p=2.0)\n&gt;&gt;&gt; print(ranking)\n# It might print [2 1] or [1 2], depending on which plan is more space-filling.\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def mmsort(X3D: np.ndarray, p: Optional[float] = 1.0) -&gt; np.ndarray:\n    \"\"\"\n    Ranks multiple sampling plans stored in a 3D array according to the\n    Morris-Mitchell criterion, using a simple bubble sort.\n\n    Args:\n        X3D (np.ndarray):\n            A 3D NumPy array of shape (n, d, m), where m is the number of\n            sampling plans, and each plan is an (n, d) matrix of points.\n        p (float, optional):\n            The distance metric to use. p=1 for Manhattan (L1), p=2 for\n            Euclidean (L2). Defaults to 1.0.\n\n    Returns:\n        np.ndarray:\n            A 1D integer array of length m that holds the plan indices in\n            ascending order of space-filling quality. The first index in the\n            returned array corresponds to the most space-filling plan.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code\n        under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.mm import mmsort\n        &gt;&gt;&gt; # Suppose we have two 3-point sampling plans in 2D, stored in X3D:\n        &gt;&gt;&gt; X1 = np.array([[0.0, 0.0],\n        ...                [0.5, 0.5],\n        ...                [1.0, 1.0]])\n        &gt;&gt;&gt; X2 = np.array([[0.2, 0.2],\n        ...                [0.6, 0.4],\n        ...                [0.9, 0.9]])\n        &gt;&gt;&gt; # Stack them along the third dimension: shape will be (3, 2, 2)\n        &gt;&gt;&gt; X3D = np.stack([X1, X2], axis=2)\n        &gt;&gt;&gt; # Sort them using the Morris-Mitchell criterion with p=2\n        &gt;&gt;&gt; ranking = mmsort(X3D, p=2.0)\n        &gt;&gt;&gt; print(ranking)\n        # It might print [2 1] or [1 2], depending on which plan is more space-filling.\n    \"\"\"\n    # Number of plans (m)\n    m = X3D.shape[2]\n\n    # Create index array (1-based to match original MATLAB convention)\n    Index = np.arange(1, m + 1)\n\n    swap_flag = True\n    while swap_flag:\n        swap_flag = False\n        i = 0\n        while i &lt; m - 1:\n            # Compare plan at Index[i] vs. Index[i+1] using mm()\n            # Note: subtract 1 from each index to convert to 0-based array indexing\n            if mm(X3D[:, :, Index[i] - 1], X3D[:, :, Index[i + 1] - 1], p) == 2:\n                # Swap indices if the second plan is more space-filling\n                Index[i], Index[i + 1] = Index[i + 1], Index[i]\n                swap_flag = True\n            i += 1\n\n    return Index\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.perturb","title":"<code>perturb(X, PertNum=1)</code>","text":"<p>Performs a specified number of random element swaps on a sampling plan. If the plan is a Latin hypercube, the result remains a valid Latin hypercube.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D array (sampling plan) of shape (n, k), where each row is a point and each column is a dimension.</p> required <code>PertNum</code> <code>int</code> <p>The number of element swaps (perturbations) to perform. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The perturbed sampling plan, identical in shape to the input, with one or more random column swaps executed.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import perturb\n&gt;&gt;&gt; # Create a simple 4x2 sampling plan\n&gt;&gt;&gt; X_original = np.array([\n...     [1, 3],\n...     [2, 4],\n...     [3, 1],\n...     [4, 2]\n... ])\n&gt;&gt;&gt; # Perturb it once\n&gt;&gt;&gt; X_perturbed = perturb(X_original, PertNum=1)\n&gt;&gt;&gt; print(X_perturbed)\n# The output may differ due to random swaps, but each column is still a permutation of [1,2,3,4].\n    [[1 3]\n    [2 2]\n    [3 1]\n    [4 4]]\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def perturb(X: np.ndarray, PertNum: Optional[int] = 1) -&gt; np.ndarray:\n    \"\"\"\n    Performs a specified number of random element swaps on a sampling plan.\n    If the plan is a Latin hypercube, the result remains a valid Latin hypercube.\n\n    Args:\n        X (np.ndarray):\n            A 2D array (sampling plan) of shape (n, k), where each row is a point\n            and each column is a dimension.\n        PertNum (int, optional):\n            The number of element swaps (perturbations) to perform. Defaults to 1.\n\n    Returns:\n        np.ndarray:\n            The perturbed sampling plan, identical in shape to the input, with\n            one or more random column swaps executed.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code\n        under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.mm import perturb\n        &gt;&gt;&gt; # Create a simple 4x2 sampling plan\n        &gt;&gt;&gt; X_original = np.array([\n        ...     [1, 3],\n        ...     [2, 4],\n        ...     [3, 1],\n        ...     [4, 2]\n        ... ])\n        &gt;&gt;&gt; # Perturb it once\n        &gt;&gt;&gt; X_perturbed = perturb(X_original, PertNum=1)\n        &gt;&gt;&gt; print(X_perturbed)\n        # The output may differ due to random swaps, but each column is still a permutation of [1,2,3,4].\n            [[1 3]\n            [2 2]\n            [3 1]\n            [4 4]]\n    \"\"\"\n    # Get dimensions of the plan\n    n, k = X.shape\n    if n &lt; 2 or k &lt; 2:\n        raise ValueError(\"Latin hypercubes require at least 2 points and 2 dimensions\")\n\n    for _ in range(PertNum):\n        # Pick a random column\n        col = int(np.floor(np.random.rand() * k))\n\n        # Pick two distinct row indices\n        el1, el2 = 0, 0\n        while el1 == el2:\n            el1 = int(np.floor(np.random.rand() * n))\n            el2 = int(np.floor(np.random.rand() * n))\n\n        # Swap the two selected elements in the chosen column\n        X[el1, col], X[el2, col] = X[el2, col], X[el1, col]\n\n    return X\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.phisort","title":"<code>phisort(X3D, q=2.0, p=1.0)</code>","text":"<p>Ranks multiple sampling plans stored in a 3D array by the Morris-Mitchell numerical quality metric (mmphi). Uses a simple bubble-sort: sampling plans with smaller mmphi values are placed first in the index array.</p> <p>Parameters:</p> Name Type Description Default <code>X3D</code> <code>ndarray</code> <p>A 3D array of shape (n, d, m), where m is the number of sampling plans.</p> required <code>q</code> <code>float</code> <p>Exponent for the mmphi metric. Defaults to 2.0.</p> <code>2.0</code> <code>p</code> <code>float</code> <p>Distance norm for mmphi. p=1 is Manhattan; p=2 is Euclidean. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D integer array of length m, giving the plan indices in ascending order of mmphi. The first index in the returned array corresponds to the numerically lowest mmphi value.</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotoptim.sampling.mm import phisort\n    from spotoptim.sampling.mm import bestlh\n    X1 = bestlh(n=5, k=2, population=5, iterations=10)\n    X2 = bestlh(n=5, k=2, population=15, iterations=20)\n    X3 = bestlh(n=5, k=2, population=25, iterations=30)\n    # Map X1 and X2 so that X3D has the two sampling plans in X3D[:, :, 0] and X3D[:, :, 1]\n    X3D = np.array([X1, X2])\n    print(phisort(X3D))\n    X3D = np.array([X3, X2])\n    print(phisort(X3D))\n        [2 1]\n        [1 2]\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def phisort(\n    X3D: np.ndarray, q: Optional[float] = 2.0, p: Optional[float] = 1.0\n) -&gt; np.ndarray:\n    \"\"\"\n    Ranks multiple sampling plans stored in a 3D array by the Morris-Mitchell\n    numerical quality metric (mmphi). Uses a simple bubble-sort:\n    sampling plans with smaller mmphi values are placed first in the index array.\n\n    Args:\n        X3D (np.ndarray):\n            A 3D array of shape (n, d, m), where m is the number of sampling plans.\n        q (float, optional):\n            Exponent for the mmphi metric. Defaults to 2.0.\n        p (float, optional):\n            Distance norm for mmphi. p=1 is Manhattan; p=2 is Euclidean. Defaults to 1.0.\n\n    Returns:\n        np.ndarray:\n            A 1D integer array of length m, giving the plan indices in ascending\n            order of mmphi. The first index in the returned array corresponds\n            to the numerically lowest mmphi value.\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code\n        under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotoptim.sampling.mm import phisort\n            from spotoptim.sampling.mm import bestlh\n            X1 = bestlh(n=5, k=2, population=5, iterations=10)\n            X2 = bestlh(n=5, k=2, population=15, iterations=20)\n            X3 = bestlh(n=5, k=2, population=25, iterations=30)\n            # Map X1 and X2 so that X3D has the two sampling plans in X3D[:, :, 0] and X3D[:, :, 1]\n            X3D = np.array([X1, X2])\n            print(phisort(X3D))\n            X3D = np.array([X3, X2])\n            print(phisort(X3D))\n                [2 1]\n                [1 2]\n    \"\"\"\n    # Number of 2D sampling plans\n    m = X3D.shape[2]\n\n    # Create a 1-based index array\n    Index = np.arange(1, m + 1)\n\n    # Bubble-sort: plan with lower mmphi() climbs toward the front\n    swap_flag = True\n    while swap_flag:\n        swap_flag = False\n        for i in range(m - 1):\n            # Retrieve mmphi values for consecutive plans\n            val_i = mmphi(X3D[:, :, Index[i] - 1], q=q, p=p)\n            val_j = mmphi(X3D[:, :, Index[i + 1] - 1], q=q, p=p)\n\n            # Swap if the left plan's mmphi is larger (i.e. 'worse')\n            if val_i &gt; val_j:\n                Index[i], Index[i + 1] = Index[i + 1], Index[i]\n                swap_flag = True\n\n    return Index\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.plot_mmphi_vs_n_lhs","title":"<code>plot_mmphi_vs_n_lhs(k_dim, seed, n_min=10, n_max=100, n_step=5, q_phi=2.0, p_phi=2.0)</code>","text":"<p>Generates LHS designs for varying n, calculates mmphi and mmphi_intensive, and plots them against the number of samples (n).</p> <p>Parameters:</p> Name Type Description Default <code>k_dim</code> <code>int</code> <p>Number of dimensions for the LHS design.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <code>n_min</code> <code>int</code> <p>Minimum number of samples.</p> <code>10</code> <code>n_max</code> <code>int</code> <p>Maximum number of samples.</p> <code>100</code> <code>n_step</code> <code>int</code> <p>Step size for increasing n.</p> <code>5</code> <code>q_phi</code> <code>float</code> <p>Exponent q for the Morris-Mitchell criteria.</p> <code>2.0</code> <code>p_phi</code> <code>float</code> <p>Distance norm p for the Morris-Mitchell criteria.</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Displays a plot of mmphi and mmphi_intensive vs. number of samples (n).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.sampling.mm import plot_mmphi_vs_n_lhs\n&gt;&gt;&gt; plot_mmphi_vs_n_lhs(k_dim=3, seed=42, n_min=10, n_max=50, n_step=5, q_phi=2.0, p_phi=2.0)\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def plot_mmphi_vs_n_lhs(\n    k_dim: int,\n    seed: int,\n    n_min: int = 10,\n    n_max: int = 100,\n    n_step: int = 5,\n    q_phi: float = 2.0,\n    p_phi: float = 2.0,\n) -&gt; None:\n    \"\"\"\n    Generates LHS designs for varying n, calculates mmphi and mmphi_intensive,\n    and plots them against the number of samples (n).\n\n    Args:\n        k_dim (int): Number of dimensions for the LHS design.\n        seed (int): Random seed for reproducibility.\n        n_min (int): Minimum number of samples.\n        n_max (int): Maximum number of samples.\n        n_step (int): Step size for increasing n.\n        q_phi (float): Exponent q for the Morris-Mitchell criteria.\n        p_phi (float): Distance norm p for the Morris-Mitchell criteria.\n\n    Returns:\n        None: Displays a plot of mmphi and mmphi_intensive vs. number of samples (n).\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.sampling.mm import plot_mmphi_vs_n_lhs\n        &gt;&gt;&gt; plot_mmphi_vs_n_lhs(k_dim=3, seed=42, n_min=10, n_max=50, n_step=5, q_phi=2.0, p_phi=2.0)\n    \"\"\"\n    n_values = list(range(n_min, n_max + 1, n_step))\n    if not n_values:\n        print(\"Warning: n_values list is empty. Check n_min, n_max, and n_step.\")\n        return\n    mmphi_results = []\n    mmphi_intensive_results = []\n    lhs_sampler = LatinHypercube(d=k_dim, seed=seed)\n\n    for n_points in n_values:\n        if n_points &lt; 2:  # mmphi requires at least 2 points to calculate distances\n            print(f\"Skipping n={n_points} as it's less than 2.\")\n            mmphi_results.append(np.nan)\n            mmphi_intensive_results.append(np.nan)\n            continue\n        try:\n            X_design = lhs_sampler.random(n=n_points)\n            phi = mmphi(X_design, q=q_phi, p=p_phi)\n            phi_intensive, _, _ = mmphi_intensive(X_design, q=q_phi, p=p_phi)\n            mmphi_results.append(phi)\n            mmphi_intensive_results.append(phi_intensive)\n        except Exception as e:\n            print(f\"Error calculating for n={n_points}: {e}\")\n            mmphi_results.append(np.nan)\n            mmphi_intensive_results.append(np.nan)\n\n    fig, ax1 = plt.subplots(figsize=(9, 6))\n\n    color = \"tab:red\"\n    ax1.set_xlabel(\"Number of Samples (n)\")\n    ax1.set_ylabel(\"mmphi (Phiq)\", color=color)\n    ax1.plot(\n        n_values,\n        mmphi_results,\n        color=color,\n        marker=\"o\",\n        linestyle=\"-\",\n        label=\"mmphi (Phiq)\",\n    )\n    ax1.tick_params(axis=\"y\", labelcolor=color)\n    ax1.grid(True, linestyle=\"--\", alpha=0.7)\n\n    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n    color = \"tab:blue\"\n    ax2.set_ylabel(\n        \"mmphi_intensive (PhiqI)\", color=color\n    )  # we already handled the x-label with ax1\n    ax2.plot(\n        n_values,\n        mmphi_intensive_results,\n        color=color,\n        marker=\"x\",\n        linestyle=\"--\",\n        label=\"mmphi_intensive (PhiqI)\",\n    )\n    ax2.tick_params(axis=\"y\", labelcolor=color)\n\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n    plt.title(\n        f\"Morris-Mitchell Criteria vs. Number of Samples (n)\\nLHS (k={k_dim}, q={q_phi}, p={p_phi})\"\n    )\n    # Add legends\n    lines, labels = ax1.get_legend_handles_labels()\n    lines2, labels2 = ax2.get_legend_handles_labels()\n    ax2.legend(lines + lines2, labels + labels2, loc=\"best\")\n    plt.show()\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.plot_mmphi_vs_points","title":"<code>plot_mmphi_vs_points(X_base, x_min, x_max, p_min=10, p_max=100, p_step=10, n_repeats=5)</code>","text":"<p>Plot the Morris-Mitchell criterion versus the number of added points.</p> <p>Parameters:</p> Name Type Description Default <code>X_base</code> <code>ndarray</code> <p>Base design matrix</p> required <code>x_min</code> <code>ndarray</code> <p>Lower bounds for variables</p> required <code>x_max</code> <code>ndarray</code> <p>Upper bounds for variables</p> required <code>p_min</code> <code>int</code> <p>Minimum number of points to add</p> <code>10</code> <code>p_max</code> <code>int</code> <p>Maximum number of points to add</p> <code>100</code> <code>p_step</code> <code>int</code> <p>Step size for number of points</p> <code>10</code> <code>n_repeats</code> <code>int</code> <p>Number of repetitions for each point count</p> <code>5</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Summary DataFrame with mean and std of mmphi for each number of added points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.sampling.mm import plot_mmphi_vs_points\n&gt;&gt;&gt; # Define base design\n&gt;&gt;&gt; X_base = np.array([[0.1, 0.2], [0.4, 0.5], [0.7, 0.8]])\n&gt;&gt;&gt; # Define variable bounds\n&gt;&gt;&gt; x_min = np.array([0.0, 0.0])\n&gt;&gt;&gt; x_max = np.array([1.0, 1.0])\n&gt;&gt;&gt; # Plot mmphi vs number of added points\n&gt;&gt;&gt; df_summary = plot_mmphi_vs_points(X_base, x_min, x_max, p_min=10, p_max=50, p_step=10, n_repeats=3)\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def plot_mmphi_vs_points(\n    X_base: np.ndarray,\n    x_min: np.ndarray,\n    x_max: np.ndarray,\n    p_min: int = 10,\n    p_max: int = 100,\n    p_step: int = 10,\n    n_repeats: int = 5,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Plot the Morris-Mitchell criterion versus the number of added points.\n\n    Args:\n        X_base (np.ndarray): Base design matrix\n        x_min (np.ndarray): Lower bounds for variables\n        x_max (np.ndarray): Upper bounds for variables\n        p_min (int): Minimum number of points to add\n        p_max (int): Maximum number of points to add\n        p_step (int): Step size for number of points\n        n_repeats (int): Number of repetitions for each point count\n\n    Returns:\n        pd.DataFrame: Summary DataFrame with mean and std of mmphi for each number of added points.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.sampling.mm import plot_mmphi_vs_points\n        &gt;&gt;&gt; # Define base design\n        &gt;&gt;&gt; X_base = np.array([[0.1, 0.2], [0.4, 0.5], [0.7, 0.8]])\n        &gt;&gt;&gt; # Define variable bounds\n        &gt;&gt;&gt; x_min = np.array([0.0, 0.0])\n        &gt;&gt;&gt; x_max = np.array([1.0, 1.0])\n        &gt;&gt;&gt; # Plot mmphi vs number of added points\n        &gt;&gt;&gt; df_summary = plot_mmphi_vs_points(X_base, x_min, x_max, p_min=10, p_max=50, p_step=10, n_repeats=3)\n    \"\"\"\n    n, m = X_base.shape\n    p_values = range(p_min, p_max + 1, p_step)\n\n    # Calculate base mmphi value\n    mmphi_base, _, _ = mmphi_intensive(X=X_base)\n\n    # Store results\n    results = []\n\n    # For each number of points\n    for p in p_values:\n        # Repeat multiple times to get average behavior\n        for _ in range(n_repeats):\n            # Generate random points\n            x_random = np.random.uniform(low=x_min, high=x_max, size=(p, m))\n\n            # Append to base design\n            X_extended = np.append(X_base, x_random, axis=0)\n\n            # Calculate new mmphi value\n            mmphi_extended, _, _ = mmphi_intensive(X=X_extended)\n\n            # Store results\n            results.append({\"n_points\": p, \"mmphi\": mmphi_extended})\n\n    # Convert results to DataFrame for easier plotting\n    df_results = pd.DataFrame(results)\n\n    # Calculate mean and std for each point count\n    df_summary = (\n        df_results.groupby(\"n_points\").agg({\"mmphi\": [\"mean\", \"std\"]}).reset_index()\n    )\n\n    # Create the plot\n    plt.figure(figsize=(10, 6))\n\n    # Plot mean line with error bars\n    plt.errorbar(\n        df_summary[\"n_points\"],\n        df_summary[\"mmphi\"][\"mean\"],\n        yerr=df_summary[\"mmphi\"][\"std\"],\n        fmt=\"bo-\",\n        capsize=5,\n        capthick=1,\n        elinewidth=1,\n        label=\"Mean mmphi \u00b1 std\",\n    )\n\n    # Add error bands (optional - can keep or remove depending on preference)\n    plt.fill_between(\n        df_summary[\"n_points\"],\n        df_summary[\"mmphi\"][\"mean\"] - df_summary[\"mmphi\"][\"std\"],\n        df_summary[\"mmphi\"][\"mean\"] + df_summary[\"mmphi\"][\"std\"],\n        alpha=0.1,\n        color=\"blue\",\n    )\n\n    # Add baseline\n    plt.axhline(\n        y=mmphi_base,\n        color=\"r\",\n        linestyle=\"--\",\n        label=f\"Base design mmphi ({mmphi_base:.4f})\",\n    )\n\n    # Customize plot\n    plt.xlabel(\"Number of Added Points\")\n    plt.ylabel(\"Morris-Mitchell Criterion\")\n    plt.title(\"Morris-Mitchell Criterion vs Number of Added Points\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # Show plot\n    plt.show()\n    plt.close()\n\n    return df_summary\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.propose_mmphi_intensive_minimizing_point","title":"<code>propose_mmphi_intensive_minimizing_point(X, n_candidates=1000, q=2.0, p=2.0, seed=None, lower=None, upper=None, normalize_flag=False)</code>","text":"<p>Propose a new point that, when added to X, minimizes the intensive Morris-Mitchell (mmphi_intensive) criterion.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Existing points, shape (n_points, n_dim).</p> required <code>n_candidates</code> <code>int</code> <p>Number of random candidates to sample.</p> <code>1000</code> <code>q</code> <code>float</code> <p>Exponent for mmphi_intensive.</p> <code>2.0</code> <code>p</code> <code>float</code> <p>Distance norm for mmphi_intensive.</p> <code>2.0</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>None</code> <code>lower</code> <code>ndarray</code> <p>Lower bounds for each dimension (default: 0).</p> <code>None</code> <code>upper</code> <code>ndarray</code> <p>Upper bounds for each dimension (default: 1).</p> <code>None</code> <code>normalize_flag</code> <code>bool</code> <p>If True, normalizes the X array and candidate points before computing distances. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Proposed new point, shape (1, n_dim).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n    from spotoptim.sampling.mm import propose_mmphi_intensive_minimizing_point\n    # Existing design with 3 points in 2D\n    X = np.array([[1.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n    # Propose a new point\n    new_point = propose_mmphi_intensive_minimizing_point(X, n_candidates=500, q=2, p=2, seed=42)\n    print(new_point)\n    # plot the existing points and the new proposed point\n    import matplotlib.pyplot as plt\n    plt.scatter(X[:, 0], X[:, 1], color='blue', label='Existing Points')\n    plt.scatter(new_point[0, 0], new_point[0, 1], color='red', label='Proposed Point')\n    plt.legend()\n    # add grid and labels\n    plt.grid()\n    plt.title('MM-PHI Proposed Point')\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.show()\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def propose_mmphi_intensive_minimizing_point(\n    X: np.ndarray,\n    n_candidates: int = 1000,\n    q: float = 2.0,\n    p: float = 2.0,\n    seed: Optional[int] = None,\n    lower: Optional[np.ndarray] = None,\n    upper: Optional[np.ndarray] = None,\n    normalize_flag: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Propose a new point that, when added to X, minimizes the intensive Morris-Mitchell (mmphi_intensive) criterion.\n\n    Args:\n        X (np.ndarray): Existing points, shape (n_points, n_dim).\n        n_candidates (int): Number of random candidates to sample.\n        q (float): Exponent for mmphi_intensive.\n        p (float): Distance norm for mmphi_intensive.\n        seed (int, optional): Random seed.\n        lower (np.ndarray, optional): Lower bounds for each dimension (default: 0).\n        upper (np.ndarray, optional): Upper bounds for each dimension (default: 1).\n        normalize_flag (bool): If True, normalizes the X array and candidate points before computing distances. Defaults to False.\n\n    Returns:\n        np.ndarray: Proposed new point, shape (1, n_dim).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n            from spotoptim.sampling.mm import propose_mmphi_intensive_minimizing_point\n            # Existing design with 3 points in 2D\n            X = np.array([[1.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n            # Propose a new point\n            new_point = propose_mmphi_intensive_minimizing_point(X, n_candidates=500, q=2, p=2, seed=42)\n            print(new_point)\n            # plot the existing points and the new proposed point\n            import matplotlib.pyplot as plt\n            plt.scatter(X[:, 0], X[:, 1], color='blue', label='Existing Points')\n            plt.scatter(new_point[0, 0], new_point[0, 1], color='red', label='Proposed Point')\n            plt.legend()\n            # add grid and labels\n            plt.grid()\n            plt.title('MM-PHI Proposed Point')\n            plt.xlabel('X1')\n            plt.ylabel('X2')\n            plt.show()\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n_dim = X.shape[1]\n    if lower is None:\n        lower = np.zeros(n_dim)\n    if upper is None:\n        upper = np.ones(n_dim)\n    if np.any(lower &gt;= upper):\n        raise ValueError(\"Lower bounds must be less than upper bounds.\")\n    # Generate candidate points uniformly\n    candidates = rng.uniform(lower, upper, size=(n_candidates, n_dim))\n    if normalize_flag:\n        X = normalize_X(X)\n        candidates = (candidates - lower) / (upper - lower)\n    best_phi = np.inf\n    best_point = None\n    for cand in candidates:\n        X_aug = np.vstack([X, cand])\n        phi, _, _ = mmphi_intensive(X_aug, q=q, p=p)\n        if phi &lt; best_phi:\n            best_phi = phi\n            best_point = cand\n    return best_point.reshape(1, -1)\n</code></pre>"},{"location":"reference/spotoptim/sampling/mm/#spotoptim.sampling.mm.subset","title":"<code>subset(X, ns)</code>","text":"<p>Returns a space-filling subset of a given size from a sampling plan, along with the remainder. It repeatedly attempts to substitute each point in the subset with a point from the remainder if doing so improves the Morris-Mitchell metric.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D array representing the original sampling plan, of shape (n, d).</p> required <code>ns</code> <code>int</code> <p>The size of the desired subset.</p> required <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>A tuple (Xs, Xr) where: - Xs is the chosen subset of size ns, with space-filling properties. - Xr is the remainder (X \\ Xs).</p> Notes <p>Many thanks to the original author of this code, A Sobester, for providing the original Matlab code under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester: \u201cThis program is free software: you can redistribute it and/or modify  it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more details. You should have received a copy of the GNU General Public License and GNU Lesser General Public License along with this program. If not, see http://www.gnu.org/licenses/.\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.sampling.mm import subset, bestlh\n    X = bestlh(n=5, k=3, population=5, iterations=10)\n    Xs, Xr = subset(X, ns=2)\n    print(Xs)\n    print(Xr)\n        [[0.25 0.   0.5 ]\n        [0.5  0.75 0.  ]]\n        [[1.   0.25 0.25]\n        [0.   1.   0.75]\n        [0.75 0.5  1.  ]]\n</code></pre> Source code in <code>src/spotoptim/sampling/mm.py</code> <pre><code>def subset(X: np.ndarray, ns: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns a space-filling subset of a given size from a sampling plan, along with\n    the remainder. It repeatedly attempts to substitute each point in the subset\n    with a point from the remainder if doing so improves the Morris-Mitchell metric.\n\n    Args:\n        X (np.ndarray):\n            A 2D array representing the original sampling plan, of shape (n, d).\n        ns (int):\n            The size of the desired subset.\n\n    Returns:\n        (np.ndarray, np.ndarray):\n            A tuple (Xs, Xr) where:\n            - Xs is the chosen subset of size ns, with space-filling properties.\n            - Xr is the remainder (X \\\\ Xs).\n\n    Notes:\n        Many thanks to the original author of this code, A Sobester, for providing the original Matlab code\n        under the GNU Licence. Original Matlab Code: Copyright 2007 A Sobester:\n        \"This program is free software: you can redistribute it and/or modify  it\n        under the terms of the GNU Lesser General Public License as published by\n        the Free Software Foundation, either version 3 of the License, or any\n        later version.\n        This program is distributed in the hope that it will be useful, but\n        WITHOUT ANY WARRANTY; without even the implied warranty of\n        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser\n        General Public License for more details.\n        You should have received a copy of the GNU General Public License and GNU\n        Lesser General Public License along with this program. If not, see\n        &lt;http://www.gnu.org/licenses/&gt;.\"\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.sampling.mm import subset, bestlh\n            X = bestlh(n=5, k=3, population=5, iterations=10)\n            Xs, Xr = subset(X, ns=2)\n            print(Xs)\n            print(Xr)\n                [[0.25 0.   0.5 ]\n                [0.5  0.75 0.  ]]\n                [[1.   0.25 0.25]\n                [0.   1.   0.75]\n                [0.75 0.5  1.  ]]\n    \"\"\"\n    # Number of total points\n    n = X.shape[0]\n\n    # Morris-Mitchell parameters\n    p = 1\n    q = 5\n\n    # Create a random permutation of row indices\n    r = np.random.permutation(n)\n\n    # Initial subset and remainder\n    Xs = X[r[:ns], :].copy()\n    Xr = X[r[ns:], :].copy()\n\n    # Attempt to improve space-filling by swapping points\n    for j in range(ns):\n        orig_crit = mmphi(Xs, q=q, p=p)\n        orig_point = Xs[j, :].copy()\n\n        # Track best substitution index and metric\n        bestsub = 0\n        bestsubcrit = np.inf\n\n        # Try replacing Xs[j] with each candidate in Xr\n        for i in range(n - ns):\n            Xs[j, :] = Xr[i, :]\n            crit = mmphi(Xs, q=q, p=p)\n            if crit &lt; bestsubcrit:\n                bestsubcrit = crit\n                bestsub = i\n\n        # If a better subset is found, swap permanently\n        if bestsubcrit &lt; orig_crit:\n            Xs[j, :] = Xr[bestsub, :].copy()\n            Xr[bestsub, :] = orig_point\n        else:\n            Xs[j, :] = orig_point\n\n    return Xs, Xr\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kernels/","title":"kernels","text":""},{"location":"reference/spotoptim/surrogate/kernels/#spotoptim.surrogate.kernels.ConstantKernel","title":"<code>ConstantKernel</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>Constant kernel.</p> <p>Can be used as a scaling factor (e.g. 2.0 * RBF()) or as part of a sum (e.g. RBF() + 1.0).</p> <p>Parameters:</p> Name Type Description Default <code>constant_value</code> <code>float</code> <p>The constant value. Defaults to 1.0.</p> <code>1.0</code> <code>constant_value_bounds</code> <code>tuple</code> <p>The lower and upper bound on constant_value. Defaults to (1e-5, 1e5).</p> <code>(1e-05, 100000.0)</code> Source code in <code>src/spotoptim/surrogate/kernels.py</code> <pre><code>class ConstantKernel(Kernel):\n    \"\"\"\n    Constant kernel.\n\n    Can be used as a scaling factor (e.g. 2.0 * RBF()) or as part of a\n    sum (e.g. RBF() + 1.0).\n\n    Args:\n        constant_value (float): The constant value. Defaults to 1.0.\n        constant_value_bounds (tuple): The lower and upper bound on constant_value.\n            Defaults to (1e-5, 1e5).\n    \"\"\"\n\n    def __init__(self, constant_value=1.0, constant_value_bounds=(1e-5, 1e5)):\n        self.constant_value = constant_value\n        self.constant_value_bounds = constant_value_bounds\n\n    def __call__(self, X, Y=None):\n        if Y is None:\n            Y = X\n        return np.full((X.shape[0], Y.shape[0]), self.constant_value)\n\n    def diag(self, X):\n        return np.full(X.shape[0], self.constant_value)\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kernels/#spotoptim.surrogate.kernels.Kernel","title":"<code>Kernel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for Kernels.</p> Source code in <code>src/spotoptim/surrogate/kernels.py</code> <pre><code>class Kernel(ABC):\n    \"\"\"\n    Base class for Kernels.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, X, Y=None) -&gt; np.ndarray:\n        \"\"\"\n        Evaluate the kernel.\n\n        Args:\n            X (np.ndarray): Left argument of the kernel evaluation.\n            Y (np.ndarray, optional): Right argument of the kernel evaluation.\n                If None, defaults to X.\n\n        Returns:\n            np.ndarray: Kernel matrix of shape (n_samples_X, n_samples_Y).\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def diag(self, X) -&gt; np.ndarray:\n        \"\"\"\n        Returns the diagonal of the kernel k(X, X).\n\n        The result of this method is equivalent to np.diag(self(X)); however,\n        it can be evaluated more efficiently.\n\n        Args:\n            X (np.ndarray): Argument of the kernel evaluation.\n\n        Returns:\n            np.ndarray: Diagonal of the kernel matrix, shape (n_samples_X,).\n        \"\"\"\n        pass\n\n    def __add__(self, other):\n        return Sum(self, other)\n\n    def __mul__(self, other):\n        return Product(self, other)\n\n    def __rmul__(self, other):\n        if isinstance(other, (int, float)):\n            return Product(ConstantKernel(other), self)\n        return Product(other, self)\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kernels/#spotoptim.surrogate.kernels.Kernel.__call__","title":"<code>__call__(X, Y=None)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the kernel.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Left argument of the kernel evaluation.</p> required <code>Y</code> <code>ndarray</code> <p>Right argument of the kernel evaluation. If None, defaults to X.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Kernel matrix of shape (n_samples_X, n_samples_Y).</p> Source code in <code>src/spotoptim/surrogate/kernels.py</code> <pre><code>@abstractmethod\ndef __call__(self, X, Y=None) -&gt; np.ndarray:\n    \"\"\"\n    Evaluate the kernel.\n\n    Args:\n        X (np.ndarray): Left argument of the kernel evaluation.\n        Y (np.ndarray, optional): Right argument of the kernel evaluation.\n            If None, defaults to X.\n\n    Returns:\n        np.ndarray: Kernel matrix of shape (n_samples_X, n_samples_Y).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kernels/#spotoptim.surrogate.kernels.Kernel.diag","title":"<code>diag(X)</code>  <code>abstractmethod</code>","text":"<p>Returns the diagonal of the kernel k(X, X).</p> <p>The result of this method is equivalent to np.diag(self(X)); however, it can be evaluated more efficiently.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Argument of the kernel evaluation.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Diagonal of the kernel matrix, shape (n_samples_X,).</p> Source code in <code>src/spotoptim/surrogate/kernels.py</code> <pre><code>@abstractmethod\ndef diag(self, X) -&gt; np.ndarray:\n    \"\"\"\n    Returns the diagonal of the kernel k(X, X).\n\n    The result of this method is equivalent to np.diag(self(X)); however,\n    it can be evaluated more efficiently.\n\n    Args:\n        X (np.ndarray): Argument of the kernel evaluation.\n\n    Returns:\n        np.ndarray: Diagonal of the kernel matrix, shape (n_samples_X,).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kernels/#spotoptim.surrogate.kernels.Product","title":"<code>Product</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>The Product kernel k1 * k2.</p> <p>The kernel value is k1(X, Y) * k2(X, Y).</p> <p>Parameters:</p> Name Type Description Default <code>k1</code> <code>Kernel</code> <p>First kernel.</p> required <code>k2</code> <code>Kernel</code> <p>Second kernel.</p> required Source code in <code>src/spotoptim/surrogate/kernels.py</code> <pre><code>class Product(Kernel):\n    \"\"\"\n    The Product kernel k1 * k2.\n\n    The kernel value is k1(X, Y) * k2(X, Y).\n\n    Args:\n        k1 (Kernel): First kernel.\n        k2 (Kernel): Second kernel.\n    \"\"\"\n\n    def __init__(self, k1, k2):\n        self.k1 = k1\n        self.k2 = k2\n\n    def __call__(self, X, Y=None):\n        return self.k1(X, Y) * self.k2(X, Y)\n\n    def diag(self, X):\n        return self.k1.diag(X) * self.k2.diag(X)\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kernels/#spotoptim.surrogate.kernels.RBF","title":"<code>RBF</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>Radial Basis Function (RBF) kernel.</p> <p>Also known as the \u201csquared exponential\u201d kernel. It is given by: k(x_i, x_j) = exp(-0.5 * d(x_i, x_j)^2 / length_scale^2)</p> <p>Parameters:</p> Name Type Description Default <code>length_scale</code> <code>float or ndarray</code> <p>The length scale of the kernel. If a float, using isotropic distances. If an array, using anisotropic distances. Defaults to 1.0.</p> <code>1.0</code> <code>length_scale_bounds</code> <code>tuple</code> <p>The lower and upper bound on length_scale. Defaults to (1e-5, 1e5).</p> <code>(1e-05, 100000.0)</code> Source code in <code>src/spotoptim/surrogate/kernels.py</code> <pre><code>class RBF(Kernel):\n    \"\"\"\n    Radial Basis Function (RBF) kernel.\n\n    Also known as the \"squared exponential\" kernel. It is given by:\n    k(x_i, x_j) = exp(-0.5 * d(x_i, x_j)^2 / length_scale^2)\n\n    Args:\n        length_scale (float or np.ndarray): The length scale of the kernel.\n            If a float, using isotropic distances. If an array, using anisotropic distances.\n            Defaults to 1.0.\n        length_scale_bounds (tuple): The lower and upper bound on length_scale.\n            Defaults to (1e-5, 1e5).\n    \"\"\"\n\n    def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5)):\n        self.length_scale = length_scale\n        self.length_scale_bounds = length_scale_bounds\n\n    def __call__(self, X, Y=None):\n        X = np.atleast_2d(X)\n        length_scale = np.atleast_1d(self.length_scale)\n        if Y is None:\n            Y = X\n\n        if X.ndim == 2 and Y.ndim == 2 and length_scale.shape[0] == 1:\n            # Isotropic\n            dists = euclidean_distances(X, Y, squared=True)\n            return np.exp(-0.5 * dists / length_scale**2)\n        else:\n            # handle anisotropic if needed or raise error, for now assume isotropic or matching dims\n            # Simple implementation for now to match sklearn RBF basic usage\n            X_scaled = X / length_scale\n            Y_scaled = Y / length_scale\n            dists = euclidean_distances(X_scaled, Y_scaled, squared=True)\n            return np.exp(-0.5 * dists)\n\n    def diag(self, X):\n        return np.ones(X.shape[0])\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kernels/#spotoptim.surrogate.kernels.SpotOptimKernel","title":"<code>SpotOptimKernel</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>Kernel designed for SpotOptim\u2019s Kriging with mixed variable support.</p> <p>It handles continuous (\u2018float\u2019), integer (\u2018int\u2019), and categorical (\u2018factor\u2019) variables similarly to the internal logic of the Kriging class.</p> <p>The correlation function is defined as: Psi = exp(- (D_ordered + D_factor))</p> <p>where: D_ordered = sum_j theta_j * |x_ij - y_lj|^p  (for ordered variables) D_factor  = sum_j theta_j * d(x_ij, y_lj)    (for factor variables, d is metric like Canberra)</p> <p>Parameters:</p> Name Type Description Default <code>theta</code> <code>ndarray</code> <p>The correlation parameters (weights). Note: In standard Kriging usage, this corresponds to <code>10^theta_log</code>. This kernel expects the LINEAR scale theta values (weights), not log.</p> required <code>var_type</code> <code>list of str</code> <p>List of variable types, e.g. [\u2018float\u2019, \u2018int\u2019, \u2018factor\u2019].</p> required <code>p_val</code> <code>float</code> <p>Power parameter for ordered distance. Defaults to 2.0.</p> <code>2.0</code> <code>metric_factorial</code> <code>str</code> <p>Metric for factor distance (passed to cdist/pdist). Defaults to \u2018canberra\u2019.</p> <code>'canberra'</code> Source code in <code>src/spotoptim/surrogate/kernels.py</code> <pre><code>class SpotOptimKernel(Kernel):\n    \"\"\"\n    Kernel designed for SpotOptim's Kriging with mixed variable support.\n\n    It handles continuous ('float'), integer ('int'), and categorical ('factor') variables\n    similarly to the internal logic of the Kriging class.\n\n    The correlation function is defined as:\n    Psi = exp(- (D_ordered + D_factor))\n\n    where:\n    D_ordered = sum_j theta_j * |x_ij - y_lj|^p  (for ordered variables)\n    D_factor  = sum_j theta_j * d(x_ij, y_lj)    (for factor variables, d is metric like Canberra)\n\n    Args:\n        theta (np.ndarray): The correlation parameters (weights).\n            Note: In standard Kriging usage, this corresponds to `10^theta_log`.\n            This kernel expects the LINEAR scale theta values (weights), not log.\n        var_type (list of str): List of variable types, e.g. ['float', 'int', 'factor'].\n        p_val (float, optional): Power parameter for ordered distance. Defaults to 2.0.\n        metric_factorial (str, optional): Metric for factor distance (passed to cdist/pdist).\n            Defaults to 'canberra'.\n    \"\"\"\n\n    def __init__(\n        self,\n        theta,\n        var_type,\n        p_val=2.0,\n        metric_factorial=\"canberra\",\n    ):\n        self.theta = np.asanyarray(theta)\n        self.var_type = var_type\n        self.p_val = p_val\n        self.metric_factorial = metric_factorial\n\n        # Precompute masks\n        var_type_array = np.array(self.var_type)\n        self.ordered_mask = np.isin(var_type_array, [\"int\", \"float\"])\n        self.factor_mask = var_type_array == \"factor\"\n\n        # Validate theta dimension\n        if self.theta.size == 1:\n            # Broadcast if isotropic (size 1) but multiple vars provided\n            # But here we assume we might get a scalar or a vector matching k\n            pass  # Logic handled in call/diag\n        else:\n            if self.theta.shape[0] != len(var_type):\n                # This check might fail if isotropic theta (size 1) is passed with k &gt; 1.\n                # Kriging generally expands it. We will handle broadcasting inside call.\n                pass\n\n    def __call__(self, X, Y=None):\n        X = np.atleast_2d(X)\n        if Y is None:\n            Y = X\n            is_symmetric = True\n        else:\n            Y = np.atleast_2d(Y)\n            is_symmetric = False\n\n        n, k = X.shape\n        m, _ = Y.shape\n\n        # Ensure theta is correct shape\n        theta = self.theta\n        if theta.size == 1 and k &gt; 1:\n            theta = np.full(k, theta.item())\n\n        if theta.shape[0] != k:\n            raise ValueError(\n                f\"theta dimension {theta.shape[0]} does not match input dimension {k}\"\n            )\n\n        D = np.zeros((n, m))\n\n        # Ordered variables\n        if self.ordered_mask.any():\n            X_ordered = X[:, self.ordered_mask]\n            Y_ordered = Y[:, self.ordered_mask]\n            w_ordered = theta[self.ordered_mask]\n\n            if is_symmetric:\n                D_ordered = squareform(\n                    pdist(X_ordered, metric=\"sqeuclidean\", w=w_ordered)\n                )\n            else:\n                D_ordered = cdist(\n                    X_ordered, Y_ordered, metric=\"sqeuclidean\", w=w_ordered\n                )\n            D += D_ordered\n\n        # Factor variables\n        if self.factor_mask.any():\n            X_factor = X[:, self.factor_mask]\n            Y_factor = Y[:, self.factor_mask]\n            w_factor = theta[self.factor_mask]\n\n            if is_symmetric:\n                D_factor = squareform(\n                    pdist(X_factor, metric=self.metric_factorial, w=w_factor)\n                )\n            else:\n                D_factor = cdist(\n                    X_factor, Y_factor, metric=self.metric_factorial, w=w_factor\n                )\n            D += D_factor\n\n        # Final exponential (Gaussian correlation)\n        # Note: Kriging's Psi = exp(-D)\n        if self.p_val != 2.0:\n            # If p != 2, the 'sqeuclidean' above was mathematically sum w * (diff^2).\n            # If we strictly want sum w * |diff|^p, we can't use 'sqeuclidean' directly if p != 2.\n            # But standard implementation often assumes p=2 for efficiency.\n            # Kriging code shows:\n            #    pdist(..., metric=\"sqeuclidean\", ...)\n            #    Psi = np.exp(-Psi)\n            # This implies p=2 is hardcoded effectively in 'sqeuclidean' metric usage in Kriging code provided earlier.\n            # The parameter p_val seems unused in the `build_correlation_matrix` snippet I read earlier\n            # (which used `sqeuclidean`).\n            # I will stick to what the code did: sqeuclidean -&gt; exp(-D).\n            pass\n\n        return np.exp(-D)\n\n    def diag(self, X):\n        # Diagonal of correlation matrix is always 1 (distance 0 -&gt; exp(0) = 1)\n        return np.ones(X.shape[0])\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kernels/#spotoptim.surrogate.kernels.Sum","title":"<code>Sum</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>The Sum kernel k1 + k2.</p> <p>The kernel value is k1(X, Y) + k2(X, Y).</p> <p>Parameters:</p> Name Type Description Default <code>k1</code> <code>Kernel</code> <p>First kernel.</p> required <code>k2</code> <code>Kernel</code> <p>Second kernel.</p> required Source code in <code>src/spotoptim/surrogate/kernels.py</code> <pre><code>class Sum(Kernel):\n    \"\"\"\n    The Sum kernel k1 + k2.\n\n    The kernel value is k1(X, Y) + k2(X, Y).\n\n    Args:\n        k1 (Kernel): First kernel.\n        k2 (Kernel): Second kernel.\n    \"\"\"\n\n    def __init__(self, k1, k2):\n        self.k1 = k1\n        self.k2 = k2\n\n    def __call__(self, X, Y=None):\n        return self.k1(X, Y) + self.k2(X, Y)\n\n    def diag(self, X):\n        return self.k1.diag(X) + self.k2.diag(X)\n\n    def __repr__(self):\n        return f\"{self.k1} + {self.k2}\"\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kernels/#spotoptim.surrogate.kernels.WhiteKernel","title":"<code>WhiteKernel</code>","text":"<p>               Bases: <code>Kernel</code></p> <p>White kernel.</p> <p>The main use case is capturing noise in the signal: k(x_i, x_j) = noise_level if x_i == x_j else 0</p> <p>Parameters:</p> Name Type Description Default <code>noise_level</code> <code>float</code> <p>Parameter controlling the noise level (variance). Defaults to 1.0.</p> <code>1.0</code> <code>noise_level_bounds</code> <code>tuple</code> <p>The lower and upper bound on noise_level. Defaults to (1e-5, 1e5).</p> <code>(1e-05, 100000.0)</code> Source code in <code>src/spotoptim/surrogate/kernels.py</code> <pre><code>class WhiteKernel(Kernel):\n    \"\"\"\n    White kernel.\n\n    The main use case is capturing noise in the signal:\n    k(x_i, x_j) = noise_level if x_i == x_j else 0\n\n    Args:\n        noise_level (float): Parameter controlling the noise level (variance).\n            Defaults to 1.0.\n        noise_level_bounds (tuple): The lower and upper bound on noise_level.\n            Defaults to (1e-5, 1e5).\n    \"\"\"\n\n    def __init__(self, noise_level=1.0, noise_level_bounds=(1e-5, 1e5)):\n        self.noise_level = noise_level\n        self.noise_level_bounds = noise_level_bounds\n\n    def __call__(self, X, Y=None):\n        if Y is not None and Y is not X:  # Different inputs, no correlation\n            return np.zeros((X.shape[0], Y.shape[0]))\n        # Y is None or Y is X -&gt; self-correlation\n        return self.noise_level * np.eye(X.shape[0])\n\n    def diag(self, X):\n        return np.full(X.shape[0], self.noise_level)\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kriging/","title":"kriging","text":"<p>Kriging (Gaussian Process) surrogate model for SpotOptim.</p> <p>Adapted from spotpython.surrogate.kriging_basic for compatibility with SpotOptim. This implementation follows Forrester et al. (2008) \u201cEngineering Design via Surrogate Modelling\u201d.</p> <p>Specific references: - Section 2.4 \u201cKriging\u201d: Core implementation of the Kriging predictor and likelihood. - Section 6 \u201cSurrogate Modeling of Noisy Data\u201d: Implementation of \u201cregression\u201d and \u201creinterpolation\u201d methods. - Validated against the book\u2019s Matlab implementation:     - <code>likelihood.m</code>: Concentrated log-likelihood calculation.     - <code>pred.m</code>: Prediction and error estimation.</p>"},{"location":"reference/spotoptim/surrogate/kriging/#spotoptim.surrogate.kriging.Kriging","title":"<code>Kriging</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>A scikit-learn compatible Kriging model class for regression tasks.</p> <p>This class provides Ordinary Kriging with support for: - Mixed variable types (continuous, integer, factor) - Gaussian/RBF correlation function - Three fitting methods (Forrester (2008), Section 6): - Isotropic or anisotropic length scales</p> <p>Compatible with SpotOptim\u2019s variable type conventions: - \u2018float\u2019: continuous numeric variables - \u2018int\u2019: integer variables - \u2018factor\u2019: categorical/unordered variables</p> <p>Parameters:</p> Name Type Description Default <code>noise</code> <code>float</code> <p>Small regularization term for numerical stability (nugget effect). If None, defaults to sqrt(machine epsilon). Only used for \u201cinterpolation\u201d method. For \u201cregression\u201d and \u201creinterpolation\u201d, this is replaced by the Lambda parameter. Defaults to None.</p> <code>None</code> <code>penalty</code> <code>float</code> <p>Large negative log-likelihood assigned if correlation matrix is not positive-definite. Defaults to 1e4.</p> <code>10000.0</code> <code>method</code> <code>str</code> <p>Fitting method (Forrester (2008), Section 6). Options: - \u201cinterpolation\u201d: Pure Kriging interpolation (Eq 2.X). Fits exact data points.   Uses a small <code>noise</code> (nugget) for numerical stability. - \u201cregression\u201d: Regression Kriging (Section 6.2). Optimizes a regularization parameter   Lambda (nugget) along with theta. Suitable for noisy data. - \u201creinterpolation\u201d: Re-interpolation (Section 6.3). Fits hyperparameters using   regression (with Lambda), but predicts using the \u201cnoise-free\u201d correlation matrix   (removing Lambda). This creates a surrogate that glosses over noise but passes   closer to the underlying trend (interpolating the regression model). Defaults to \u201cregression\u201d.</p> <code>'regression'</code> <code>var_type</code> <code>List[str]</code> <p>Variable types for each dimension. SpotOptim uses: \u2018float\u2019 (continuous), \u2018int\u2019 (integer), \u2018factor\u2019 (categorical). Defaults to [\u201cfloat\u201d].</p> <code>None</code> <code>name</code> <code>str</code> <p>Name of the Kriging instance. Defaults to \u201cKriging\u201d.</p> <code>'Kriging'</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 124.</p> <code>124</code> <code>model_fun_evals</code> <code>int</code> <p>Maximum function evaluations for hyperparameter optimization. Defaults to 100.</p> <code>None</code> <code>n_theta</code> <code>int</code> <p>Number of theta parameters. If None, set during fit. Defaults to None.</p> <code>None</code> <code>min_theta</code> <code>float</code> <p>Minimum log10(theta) bound. Defaults to -3.0.</p> <code>-3.0</code> <code>max_theta</code> <code>float</code> <p>Maximum log10(theta) bound. Defaults to 2.0.</p> <code>2.0</code> <code>theta_init_zero</code> <code>bool</code> <p>Initialize theta to zero. Defaults to False.</p> <code>False</code> <code>p_val</code> <code>float</code> <p>Power parameter for correlation (fixed at 2.0 for Gaussian). Defaults to 2.0.</p> <code>2.0</code> <code>n_p</code> <code>int</code> <p>Number of p parameters (currently not optimized). Defaults to 1.</p> <code>1</code> <code>optim_p</code> <code>bool</code> <p>Optimize p parameters (currently not supported). Defaults to False.</p> <code>False</code> <code>min_Lambda</code> <code>float</code> <p>Minimum log10(Lambda) bound. Defaults to -9.0.</p> <code>-9.0</code> <code>max_Lambda</code> <code>float</code> <p>Maximum log10(Lambda) bound. Defaults to 0.0.</p> <code>0.0</code> <code>metric_factorial</code> <code>str</code> <p>Distance metric for factor variables. Defaults to \u201ccanberra\u201d.</p> <code>'canberra'</code> <code>isotropic</code> <code>bool</code> <p>Use single theta for all dimensions. Defaults to False.</p> <code>False</code> <code>theta</code> <code>ndarray</code> <p>Initial theta values (log10 scale). Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>X_</code> <code>ndarray</code> <p>Training data, shape (n_samples, n_features).</p> <code>y_</code> <code>ndarray</code> <p>Training targets, shape (n_samples,).</p> <code>theta_</code> <code>ndarray</code> <p>Optimized log10(theta) parameters.</p> <code>Lambda_</code> <code>float or None</code> <p>Optimized log10(Lambda) for regression methods.</p> <code>mu_</code> <code>float</code> <p>Mean of Kriging predictor.</p> <code>sigma2_</code> <code>float</code> <p>Variance of Kriging predictor.</p> <code>U_</code> <code>ndarray</code> <p>Cholesky factor of correlation matrix.</p> <code>Psi_</code> <code>ndarray</code> <p>Correlation matrix.</p> <code>negLnLike</code> <code>float</code> <p>Negative log-likelihood value.</p> <p>Examples:</p> <p>Basic usage with SpotOptim:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Define objective\n&gt;&gt;&gt; def objective(X):\n...     return np.sum(X**2, axis=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create Kriging surrogate\n&gt;&gt;&gt; kriging = Kriging(\n...     noise=1e-10,\n...     method='regression',\n...     min_theta=-3.0,\n...     max_theta=2.0,\n...     seed=42\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use with SpotOptim\n&gt;&gt;&gt; opt = SpotOptim(\n...     fun=objective,\n...     bounds=[(-5, 5), (-5, 5)],\n...     surrogate=kriging,\n...     max_iter=30,\n...     n_initial=10,\n...     seed=42\n... )\n&gt;&gt;&gt; result = opt.optimize()\n</code></pre> <p>Direct usage (scikit-learn compatible):</p> <pre><code>&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Training data\n&gt;&gt;&gt; X_train = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n&gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit model\n&gt;&gt;&gt; model = Kriging(method='regression', seed=42)\n&gt;&gt;&gt; model.fit(X_train, y_train)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Predict\n&gt;&gt;&gt; X_test = np.array([[0.25, 0.25], [0.75, 0.75]])\n&gt;&gt;&gt; y_pred = model.predict(X_test)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Predict with uncertainty\n&gt;&gt;&gt; y_pred, std = model.predict(X_test, return_std=True)\n</code></pre> Source code in <code>src/spotoptim/surrogate/kriging.py</code> <pre><code>class Kriging(BaseEstimator, RegressorMixin):\n    \"\"\"\n    A scikit-learn compatible Kriging model class for regression tasks.\n\n    This class provides Ordinary Kriging with support for:\n    - Mixed variable types (continuous, integer, factor)\n    - Gaussian/RBF correlation function\n    - Three fitting methods (Forrester (2008), Section 6):\n    - Isotropic or anisotropic length scales\n\n    Compatible with SpotOptim's variable type conventions:\n    - 'float': continuous numeric variables\n    - 'int': integer variables\n    - 'factor': categorical/unordered variables\n\n    Args:\n        noise (float, optional): Small regularization term for numerical stability (nugget effect).\n            If None, defaults to sqrt(machine epsilon). Only used for \"interpolation\" method.\n            For \"regression\" and \"reinterpolation\", this is replaced by the Lambda parameter.\n            Defaults to None.\n        penalty (float, optional): Large negative log-likelihood assigned if correlation matrix\n            is not positive-definite. Defaults to 1e4.\n        method (str, optional): Fitting method (Forrester (2008), Section 6). Options:\n            - \"interpolation\": Pure Kriging interpolation (Eq 2.X). Fits exact data points.\n              Uses a small `noise` (nugget) for numerical stability.\n            - \"regression\": Regression Kriging (Section 6.2). Optimizes a regularization parameter\n              Lambda (nugget) along with theta. Suitable for noisy data.\n            - \"reinterpolation\": Re-interpolation (Section 6.3). Fits hyperparameters using\n              regression (with Lambda), but predicts using the \"noise-free\" correlation matrix\n              (removing Lambda). This creates a surrogate that glosses over noise but passes\n              closer to the underlying trend (interpolating the regression model).\n            Defaults to \"regression\".\n        var_type (List[str], optional): Variable types for each dimension.\n            SpotOptim uses: 'float' (continuous), 'int' (integer), 'factor' (categorical).\n            Defaults to [\"float\"].\n        name (str, optional): Name of the Kriging instance. Defaults to \"Kriging\".\n        seed (int, optional): Random seed for reproducibility. Defaults to 124.\n        model_fun_evals (int, optional): Maximum function evaluations for hyperparameter\n            optimization. Defaults to 100.\n        n_theta (int, optional): Number of theta parameters. If None, set during fit.\n            Defaults to None.\n        min_theta (float, optional): Minimum log10(theta) bound. Defaults to -3.0.\n        max_theta (float, optional): Maximum log10(theta) bound. Defaults to 2.0.\n        theta_init_zero (bool, optional): Initialize theta to zero. Defaults to False.\n        p_val (float, optional): Power parameter for correlation (fixed at 2.0 for Gaussian).\n            Defaults to 2.0.\n        n_p (int, optional): Number of p parameters (currently not optimized). Defaults to 1.\n        optim_p (bool, optional): Optimize p parameters (currently not supported). Defaults to False.\n        min_Lambda (float, optional): Minimum log10(Lambda) bound. Defaults to -9.0.\n        max_Lambda (float, optional): Maximum log10(Lambda) bound. Defaults to 0.0.\n        metric_factorial (str, optional): Distance metric for factor variables.\n            Defaults to \"canberra\".\n        isotropic (bool, optional): Use single theta for all dimensions. Defaults to False.\n        theta (np.ndarray, optional): Initial theta values (log10 scale). Defaults to None.\n\n    Attributes:\n        X_ (ndarray): Training data, shape (n_samples, n_features).\n        y_ (ndarray): Training targets, shape (n_samples,).\n        theta_ (ndarray): Optimized log10(theta) parameters.\n        Lambda_ (float or None): Optimized log10(Lambda) for regression methods.\n        mu_ (float): Mean of Kriging predictor.\n        sigma2_ (float): Variance of Kriging predictor.\n        U_ (ndarray): Cholesky factor of correlation matrix.\n        Psi_ (ndarray): Correlation matrix.\n        negLnLike (float): Negative log-likelihood value.\n\n    Examples:\n        Basic usage with SpotOptim:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Define objective\n        &gt;&gt;&gt; def objective(X):\n        ...     return np.sum(X**2, axis=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create Kriging surrogate\n        &gt;&gt;&gt; kriging = Kriging(\n        ...     noise=1e-10,\n        ...     method='regression',\n        ...     min_theta=-3.0,\n        ...     max_theta=2.0,\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use with SpotOptim\n        &gt;&gt;&gt; opt = SpotOptim(\n        ...     fun=objective,\n        ...     bounds=[(-5, 5), (-5, 5)],\n        ...     surrogate=kriging,\n        ...     max_iter=30,\n        ...     n_initial=10,\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt; result = opt.optimize()\n\n        Direct usage (scikit-learn compatible):\n\n        &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Training data\n        &gt;&gt;&gt; X_train = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n        &gt;&gt;&gt; y_train = np.array([0.1, 0.2, 0.3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit model\n        &gt;&gt;&gt; model = Kriging(method='regression', seed=42)\n        &gt;&gt;&gt; model.fit(X_train, y_train)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Predict\n        &gt;&gt;&gt; X_test = np.array([[0.25, 0.25], [0.75, 0.75]])\n        &gt;&gt;&gt; y_pred = model.predict(X_test)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Predict with uncertainty\n        &gt;&gt;&gt; y_pred, std = model.predict(X_test, return_std=True)\n    \"\"\"\n\n    def __init__(\n        self,\n        noise: Optional[float] = None,\n        penalty: float = 1e4,\n        method: str = \"regression\",\n        var_type: Optional[List[str]] = None,\n        name: str = \"Kriging\",\n        seed: int = 124,\n        model_fun_evals: Optional[int] = None,\n        n_theta: Optional[int] = None,\n        min_theta: float = -3.0,\n        max_theta: float = 2.0,\n        theta_init_zero: bool = False,\n        p_val: float = 2.0,\n        n_p: int = 1,\n        optim_p: bool = False,\n        min_Lambda: float = -9.0,\n        max_Lambda: float = 0.0,\n        metric_factorial: str = \"canberra\",\n        isotropic: bool = False,\n        theta: Optional[np.ndarray] = None,\n        **kwargs,\n    ):\n        if noise is None:\n            self.noise = self._get_eps()\n        else:\n            if noise &lt;= 0:\n                raise ValueError(\"noise must be positive\")\n            self.noise = noise\n\n        self.penalty = penalty\n        self.var_type = var_type if var_type is not None else [\"float\"]\n        self.name = name\n        self.seed = seed\n        self.metric_factorial = metric_factorial\n        self.min_theta = min_theta\n        self.max_theta = max_theta\n        self.min_Lambda = min_Lambda\n        self.max_Lambda = max_Lambda\n        self.n_theta = n_theta\n        self.isotropic = isotropic\n        self.p_val = p_val\n        self.n_p = n_p\n        self.optim_p = optim_p\n        self.theta_init_zero = theta_init_zero\n        self.theta = theta\n        self.model_fun_evals = model_fun_evals if model_fun_evals is not None else 100\n\n        if method not in [\"interpolation\", \"regression\", \"reinterpolation\"]:\n            raise ValueError(\n                \"method must be 'interpolation', 'regression', or 'reinterpolation'\"\n            )\n        self.method = method\n\n        # Fitted attributes\n        self.X_ = None\n        self.y_ = None\n        self.n = None\n        self.k = None\n        self.theta_ = None\n        self.Lambda_ = None\n        self.mu_ = None\n        self.sigma2_ = None\n        self.U_ = None\n        self.Psi_ = None\n        self.negLnLike = None\n        self.inf_Psi = False\n        self.cnd_Psi = None\n\n    def _get_eps(self) -&gt; float:\n        \"\"\"Get square root of machine epsilon.\"\"\"\n        return np.sqrt(np.finfo(float).eps)\n\n    def _set_variable_types(self) -&gt; None:\n        \"\"\"Set variable type masks for different variable types.\n\n        Creates boolean masks for:\n        - num_mask: 'float' variables\n        - int_mask: 'int' variables\n        - factor_mask: 'factor' variables\n        - ordered_mask: 'float', or 'int' variables (ordered/numeric)\n        \"\"\"\n        # Ensure var_type has appropriate length\n        if len(self.var_type) &lt; self.k:\n            self.var_type = [\"float\"] * self.k\n\n        var_type_array = np.array(self.var_type)\n        # SpotOptim uses 'float' for continuous variables\n        self.num_mask = np.isin(var_type_array, [\"float\"])\n        self.int_mask = var_type_array == \"int\"\n        self.factor_mask = var_type_array == \"factor\"\n        # Ordered variables: numeric (float/num) and integer\n        self.ordered_mask = np.isin(var_type_array, [\"int\", \"float\"])\n\n    def _set_theta(self) -&gt; None:\n        \"\"\"Set number of theta parameters based on isotropic flag.\"\"\"\n        if self.isotropic:\n            self.n_theta = 1\n        else:\n            self.n_theta = self.k\n\n    def _get_theta10_from_logtheta(self) -&gt; np.ndarray:\n        \"\"\"Convert log10(theta) to linear scale and expand if isotropic.\"\"\"\n        theta10 = np.power(10.0, self.theta_)\n        if self.n_theta == 1:\n            theta10 = theta10 * np.ones(self.k)\n        return theta10\n\n    def _reshape_X(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Ensure X has shape (n_samples, n_features).\"\"\"\n        X = np.asarray(X)\n        if X.ndim == 1:\n            X = X.reshape(-1, self.k)\n        else:\n            if X.shape[1] != self.k:\n                if X.shape[0] == self.k:\n                    X = X.T\n                elif self.k == 1:\n                    X = X.reshape(-1, 1)\n                else:\n                    raise ValueError(f\"X has shape {X.shape}, expected (*, {self.k})\")\n        return X\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"Kriging\":\n        \"\"\"Fit the Kriging model to training data.\n\n        Optimizes hyperparameters (theta, Lambda) by maximizing the concentrated\n        log-likelihood using differential evolution.\n\n        Args:\n            X (np.ndarray): Training inputs, shape (n_samples, n_features).\n            y (np.ndarray): Training targets, shape (n_samples,).\n\n        Returns:\n            Kriging: Fitted model instance (self).\n        \"\"\"\n        X = np.asarray(X)\n        y = np.asarray(y).flatten()\n        self.X_ = X\n        self.y_ = y\n        self.n, self.k = self.X_.shape\n\n        self._set_variable_types()\n\n        if self.n_theta is None:\n            self._set_theta()\n\n        # Store data bounds\n        self.min_X = np.min(self.X_, axis=0)\n        self.max_X = np.max(self.X_, axis=0)\n\n        # Setup bounds for optimization\n        bounds = [(self.min_theta, self.max_theta)] * self.n_theta\n\n        if self.method in [\"regression\", \"reinterpolation\"]:\n            bounds += [(self.min_Lambda, self.max_Lambda)]\n\n        if self.optim_p:\n            bounds += [(1.0, 2.0)] * self.n_p\n\n        # Optimize hyperparameters\n        result = differential_evolution(\n            func=self.objective,\n            bounds=bounds,\n            seed=self.seed,\n            maxiter=self.model_fun_evals,\n        )\n\n        params = result.x\n        self.theta_ = params[: self.n_theta]\n\n        if self.method in [\"regression\", \"reinterpolation\"]:\n            self.Lambda_ = params[self.n_theta : self.n_theta + 1][0]\n        else:\n            self.Lambda_ = None\n\n        # Store final likelihood and matrices\n        self.negLnLike, self.Psi_, self.U_ = self.likelihood(params)\n\n        return self\n\n    def objective(self, params: np.ndarray) -&gt; float:\n        \"\"\"Objective function for hyperparameter optimization.\n\n        Args:\n            params (np.ndarray): Hyperparameters to evaluate.\n\n        Returns:\n            float: Negative concentrated log-likelihood.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n            &gt;&gt;&gt; X = np.array([[0.], [1.]])\n            &gt;&gt;&gt; y = np.array([0., 1.])\n            &gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n            &gt;&gt;&gt; # Evaluate objective at optimal parameters\n            &gt;&gt;&gt; val = k.objective(np.concatenate([k.theta_, [k.Lambda_]]))\n        \"\"\"\n        negLnLike, _, _ = self.likelihood(params)\n        return negLnLike\n\n    def predict(self, X: np.ndarray, return_std: bool = False) -&gt; np.ndarray:\n        \"\"\"Predict at new points.\n\n        Args:\n            X (np.ndarray): Test points, shape (n_samples, n_features).\n            return_std (bool, optional): Return standard deviations. Defaults to False.\n\n        Returns:\n            np.ndarray: Predictions, shape (n_samples,).\n            tuple: (predictions, std_devs) if return_std=True.\n        \"\"\"\n        X = self._reshape_X(X)\n\n        if return_std:\n            predictions, stds = zip(*[self.predict_single(x) for x in X])\n            return np.array(predictions), np.array(stds)\n        else:\n            predictions = [self.predict_single(x)[0] for x in X]\n            return np.array(predictions)\n\n    def build_correlation_matrix(self) -&gt; np.ndarray:\n        \"\"\"Build correlation matrix from training data.\n\n        Returns:\n            np.ndarray: Upper triangle of correlation matrix.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n            &gt;&gt;&gt; X = np.array([[0.], [1.]])\n            &gt;&gt;&gt; y = np.array([0., 1.])\n            &gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n            &gt;&gt;&gt; Psi_upper = k.build_correlation_matrix()\n            &gt;&gt;&gt; print(Psi_upper.shape)\n            (2, 2)\n        \"\"\"\n        try:\n            theta10 = self._get_theta10_from_logtheta()\n\n            # Use the new SpotOptimKernel to compute the correlation matrix\n            # Note: We create it on the fly here to use current parameters\n            kernel = SpotOptimKernel(\n                theta=theta10,\n                var_type=self.var_type,\n                p_val=self.p_val,\n                metric_factorial=self.metric_factorial,\n            )\n\n            # Compute full correlation matrix\n            Psi = kernel(self.X_)\n\n            self.inf_Psi = np.isinf(Psi).any()\n            self.cnd_Psi = cond(Psi)\n\n            return np.triu(Psi, k=1)\n\n        except LinAlgError as err:\n            print(f\"Building Psi failed: {err}\")\n            raise\n\n    def likelihood(self, params: np.ndarray) -&gt; Tuple[float, np.ndarray, np.ndarray]:\n        \"\"\"Compute negative concentrated log-likelihood.\n\n        Args:\n            params (np.ndarray): Hyperparameters [log10(theta), log10(Lambda)].\n\n        Returns:\n            tuple: (negLnLike, Psi, U) where U is Cholesky factor.\n\n        Reference:\n            Forrester et al. (2008), Section 2.4.\n            Matches implementation in `likelihood.m` from the book's code.\n            Concentrated Log-Likelihood approx: ln(L) ~ -(n/2)ln(sigma^2) - (1/2)ln|R|\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n            &gt;&gt;&gt; X = np.array([[0.], [1.]])\n            &gt;&gt;&gt; y = np.array([0., 1.])\n            &gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n            &gt;&gt;&gt; params = np.concatenate([k.theta_, [k.Lambda_]])\n            &gt;&gt;&gt; nll, _, _ = k.likelihood(params)\n        \"\"\"\n        # Extract parameters\n        self.theta_ = params[: self.n_theta]\n\n        if self.method in [\"regression\", \"reinterpolation\"]:\n            lambda_log = params[self.n_theta : self.n_theta + 1][0]\n            lambda_ = 10.0**lambda_log\n        else:\n            lambda_ = self.noise\n\n        n = self.n\n        one = np.ones(n)\n\n        # Build correlation matrix\n        Psi_upper = self.build_correlation_matrix()\n        Psi = Psi_upper + Psi_upper.T + np.eye(n) * (1.0 + lambda_)\n\n        # Cholesky factorization\n        try:\n            U = np.linalg.cholesky(Psi)\n        except LinAlgError:\n            return self.penalty, Psi, None\n\n        # log|R|\n        LnDetPsi = 2.0 * np.sum(np.log(np.abs(np.diag(U))))\n\n        # Solve for mu and sigma^2\n        y = self.y_\n        temp_y = np.linalg.solve(U, y)\n        temp_one = np.linalg.solve(U, one)\n        vy = np.linalg.solve(U.T, temp_y)\n        vone = np.linalg.solve(U.T, temp_one)\n\n        mu = (one @ vy) / (one @ vone)\n        resid = y - one * mu\n        tresid = np.linalg.solve(U, resid)\n        tresid = np.linalg.solve(U.T, tresid)\n        SigmaSqr = (resid @ tresid) / n\n\n        # Concentrated negative log-likelihood\n        # Corresponds to `likelihood.m` line:\n        # NegLnLike=-1*(-(n/2)*log(SigmaSqr) - 0.5*LnDetPsi);\n        # We minimize positive NegLnLike, which is equivalent to maximizing likelihood.\n        negLnLike = (n / 2.0) * np.log(SigmaSqr) + 0.5 * LnDetPsi\n\n        return negLnLike, Psi, U\n\n    def build_psi_vector(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Build correlation vector between x and training points.\n\n        Args:\n            x (np.ndarray): Single test point, shape (n_features,).\n\n        Returns:\n            np.ndarray: Correlation vector, shape (n_samples,).\n\n        Reference:\n            Calculates the vector small psi from Eq 2.15 in Forrester (2008).\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n            &gt;&gt;&gt; X = np.array([[0.], [1.]])\n            &gt;&gt;&gt; y = np.array([0., 1.])\n            &gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n            &gt;&gt;&gt; x_new = np.array([0.5])\n            &gt;&gt;&gt; psi = k.build_psi_vector(x_new)\n            &gt;&gt;&gt; print(psi.shape)\n            (2,)\n        \"\"\"\n        theta10 = self._get_theta10_from_logtheta()\n\n        kernel = SpotOptimKernel(\n            theta=theta10,\n            var_type=self.var_type,\n            p_val=self.p_val,\n            metric_factorial=self.metric_factorial,\n        )\n\n        # Calculate correlation between X_ (training) and x (new point)\n        # kernel(X, Y) returns matrix of shape (n_X, n_Y)\n        # We need vector of shape (n,).\n        # kernel(self.X_, x.reshape(1, -1)) returns (n, 1)\n        # flatten to get (n,)\n        psi = kernel(self.X_, x.reshape(1, -1)).flatten()\n\n        return psi\n\n    def predict_single(self, x: np.ndarray) -&gt; Tuple[float, float]:\n        \"\"\"Predict at a single point.\n\n        Args:\n            x (np.ndarray): Test point, shape (n_features,).\n\n        Returns:\n            tuple: (prediction, std_dev).\n\n        Reference:\n            Forrester et al. (2008), Eq 2.15 (Predictor) and Eq 2.19 (Error).\n            Matches implementation in `pred.m` from the book's code.\n\n        Examples:\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n            &gt;&gt;&gt; X = np.array([[0.], [1.]])\n            &gt;&gt;&gt; y = np.array([0., 1.])\n            &gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n            &gt;&gt;&gt; x_new = np.array([0.5])\n            &gt;&gt;&gt; y_pred, y_std = k.predict_single(x_new)\n        \"\"\"\n        if self.method in [\"regression\", \"reinterpolation\"]:\n            lambda_ = 10.0**self.Lambda_\n        else:\n            lambda_ = self.noise\n\n        U = self.U_\n        n = self.n\n        y = self.y_\n        one = np.ones(n)\n\n        # Compute mu\n        y_tilde = np.linalg.solve(U, y)\n        y_tilde = np.linalg.solve(U.T, y_tilde)\n        one_tilde = np.linalg.solve(U, one)\n        one_tilde = np.linalg.solve(U.T, one_tilde)\n        mu = (one @ y_tilde) / (one @ one_tilde)\n\n        # Residual\n        resid = y - one * mu\n        resid_tilde = np.linalg.solve(U, resid)\n        resid_tilde = np.linalg.solve(U.T, resid_tilde)\n\n        # Correlation vector\n        psi = self.build_psi_vector(x)\n\n        # Prediction\n        # Eq 2.15: \\hat{y} = \\hat{\\mu} + \\psi^T \\Psi^{-1} (y - \\mathbf{1}\\hat{\\mu})\n        # Implemented using pre-computed Cholesky factors as in `pred.m`\n        f = mu + psi @ resid_tilde\n\n        # Variance\n        # Eq 2.19 (Mean Squared Error)\n        if self.method in [\"interpolation\", \"regression\"]:\n            SigmaSqr = (resid @ resid_tilde) / n\n            psi_tilde = np.linalg.solve(U, psi)\n            psi_tilde = np.linalg.solve(U.T, psi_tilde)\n            SSqr = SigmaSqr * (1.0 + lambda_ - psi @ psi_tilde)\n        else:  # reinterpolation\n            Psi_adjusted = self.Psi_ - np.eye(n) * lambda_ + np.eye(n) * self.noise\n            SigmaSqr = (\n                resid\n                @ np.linalg.solve(U.T, np.linalg.solve(U, Psi_adjusted @ resid_tilde))\n            ) / n\n            Uint = np.linalg.cholesky(Psi_adjusted)\n            psi_tilde = np.linalg.solve(Uint, psi)\n            psi_tilde = np.linalg.solve(Uint.T, psi_tilde)\n            SSqr = SigmaSqr * (1.0 - psi @ psi_tilde)\n\n        s = np.sqrt(np.abs(SSqr))\n\n        return float(f), float(s)\n\n    def get_params(self, deep: bool = True) -&gt; Dict:\n        \"\"\"Get parameters for this estimator (scikit-learn compatibility).\n\n        Args:\n            deep (bool): Ignored, for compatibility.\n\n        Returns:\n            dict: Parameter names mapped to their values.\n        \"\"\"\n        return {\n            \"noise\": self.noise,\n            \"penalty\": self.penalty,\n            \"method\": self.method,\n            \"var_type\": self.var_type,\n            \"name\": self.name,\n            \"seed\": self.seed,\n            \"model_fun_evals\": self.model_fun_evals,\n            \"n_theta\": self.n_theta,\n            \"min_theta\": self.min_theta,\n            \"max_theta\": self.max_theta,\n            \"theta_init_zero\": self.theta_init_zero,\n            \"p_val\": self.p_val,\n            \"n_p\": self.n_p,\n            \"optim_p\": self.optim_p,\n            \"min_Lambda\": self.min_Lambda,\n            \"max_Lambda\": self.max_Lambda,\n            \"metric_factorial\": self.metric_factorial,\n            \"isotropic\": self.isotropic,\n            \"theta\": self.theta,\n        }\n\n    def set_params(self, **params: \"typing.Any\") -&gt; \"Kriging\":\n        \"\"\"Set parameters (scikit-learn compatibility).\n\n        Args:\n            **params: Parameter names and values.\n\n        Returns:\n            Kriging: Self.\n        \"\"\"\n        for key, value in params.items():\n            setattr(self, key, value)\n        return self\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kriging/#spotoptim.surrogate.kriging.Kriging.build_correlation_matrix","title":"<code>build_correlation_matrix()</code>","text":"<p>Build correlation matrix from training data.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Upper triangle of correlation matrix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt; X = np.array([[0.], [1.]])\n&gt;&gt;&gt; y = np.array([0., 1.])\n&gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n&gt;&gt;&gt; Psi_upper = k.build_correlation_matrix()\n&gt;&gt;&gt; print(Psi_upper.shape)\n(2, 2)\n</code></pre> Source code in <code>src/spotoptim/surrogate/kriging.py</code> <pre><code>def build_correlation_matrix(self) -&gt; np.ndarray:\n    \"\"\"Build correlation matrix from training data.\n\n    Returns:\n        np.ndarray: Upper triangle of correlation matrix.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n        &gt;&gt;&gt; X = np.array([[0.], [1.]])\n        &gt;&gt;&gt; y = np.array([0., 1.])\n        &gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n        &gt;&gt;&gt; Psi_upper = k.build_correlation_matrix()\n        &gt;&gt;&gt; print(Psi_upper.shape)\n        (2, 2)\n    \"\"\"\n    try:\n        theta10 = self._get_theta10_from_logtheta()\n\n        # Use the new SpotOptimKernel to compute the correlation matrix\n        # Note: We create it on the fly here to use current parameters\n        kernel = SpotOptimKernel(\n            theta=theta10,\n            var_type=self.var_type,\n            p_val=self.p_val,\n            metric_factorial=self.metric_factorial,\n        )\n\n        # Compute full correlation matrix\n        Psi = kernel(self.X_)\n\n        self.inf_Psi = np.isinf(Psi).any()\n        self.cnd_Psi = cond(Psi)\n\n        return np.triu(Psi, k=1)\n\n    except LinAlgError as err:\n        print(f\"Building Psi failed: {err}\")\n        raise\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kriging/#spotoptim.surrogate.kriging.Kriging.build_psi_vector","title":"<code>build_psi_vector(x)</code>","text":"<p>Build correlation vector between x and training points.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Single test point, shape (n_features,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Correlation vector, shape (n_samples,).</p> Reference <p>Calculates the vector small psi from Eq 2.15 in Forrester (2008).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt; X = np.array([[0.], [1.]])\n&gt;&gt;&gt; y = np.array([0., 1.])\n&gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n&gt;&gt;&gt; x_new = np.array([0.5])\n&gt;&gt;&gt; psi = k.build_psi_vector(x_new)\n&gt;&gt;&gt; print(psi.shape)\n(2,)\n</code></pre> Source code in <code>src/spotoptim/surrogate/kriging.py</code> <pre><code>def build_psi_vector(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build correlation vector between x and training points.\n\n    Args:\n        x (np.ndarray): Single test point, shape (n_features,).\n\n    Returns:\n        np.ndarray: Correlation vector, shape (n_samples,).\n\n    Reference:\n        Calculates the vector small psi from Eq 2.15 in Forrester (2008).\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n        &gt;&gt;&gt; X = np.array([[0.], [1.]])\n        &gt;&gt;&gt; y = np.array([0., 1.])\n        &gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n        &gt;&gt;&gt; x_new = np.array([0.5])\n        &gt;&gt;&gt; psi = k.build_psi_vector(x_new)\n        &gt;&gt;&gt; print(psi.shape)\n        (2,)\n    \"\"\"\n    theta10 = self._get_theta10_from_logtheta()\n\n    kernel = SpotOptimKernel(\n        theta=theta10,\n        var_type=self.var_type,\n        p_val=self.p_val,\n        metric_factorial=self.metric_factorial,\n    )\n\n    # Calculate correlation between X_ (training) and x (new point)\n    # kernel(X, Y) returns matrix of shape (n_X, n_Y)\n    # We need vector of shape (n,).\n    # kernel(self.X_, x.reshape(1, -1)) returns (n, 1)\n    # flatten to get (n,)\n    psi = kernel(self.X_, x.reshape(1, -1)).flatten()\n\n    return psi\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kriging/#spotoptim.surrogate.kriging.Kriging.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the Kriging model to training data.</p> <p>Optimizes hyperparameters (theta, Lambda) by maximizing the concentrated log-likelihood using differential evolution.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training inputs, shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Training targets, shape (n_samples,).</p> required <p>Returns:</p> Name Type Description <code>Kriging</code> <code>Kriging</code> <p>Fitted model instance (self).</p> Source code in <code>src/spotoptim/surrogate/kriging.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"Kriging\":\n    \"\"\"Fit the Kriging model to training data.\n\n    Optimizes hyperparameters (theta, Lambda) by maximizing the concentrated\n    log-likelihood using differential evolution.\n\n    Args:\n        X (np.ndarray): Training inputs, shape (n_samples, n_features).\n        y (np.ndarray): Training targets, shape (n_samples,).\n\n    Returns:\n        Kriging: Fitted model instance (self).\n    \"\"\"\n    X = np.asarray(X)\n    y = np.asarray(y).flatten()\n    self.X_ = X\n    self.y_ = y\n    self.n, self.k = self.X_.shape\n\n    self._set_variable_types()\n\n    if self.n_theta is None:\n        self._set_theta()\n\n    # Store data bounds\n    self.min_X = np.min(self.X_, axis=0)\n    self.max_X = np.max(self.X_, axis=0)\n\n    # Setup bounds for optimization\n    bounds = [(self.min_theta, self.max_theta)] * self.n_theta\n\n    if self.method in [\"regression\", \"reinterpolation\"]:\n        bounds += [(self.min_Lambda, self.max_Lambda)]\n\n    if self.optim_p:\n        bounds += [(1.0, 2.0)] * self.n_p\n\n    # Optimize hyperparameters\n    result = differential_evolution(\n        func=self.objective,\n        bounds=bounds,\n        seed=self.seed,\n        maxiter=self.model_fun_evals,\n    )\n\n    params = result.x\n    self.theta_ = params[: self.n_theta]\n\n    if self.method in [\"regression\", \"reinterpolation\"]:\n        self.Lambda_ = params[self.n_theta : self.n_theta + 1][0]\n    else:\n        self.Lambda_ = None\n\n    # Store final likelihood and matrices\n    self.negLnLike, self.Psi_, self.U_ = self.likelihood(params)\n\n    return self\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kriging/#spotoptim.surrogate.kriging.Kriging.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Get parameters for this estimator (scikit-learn compatibility).</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>Ignored, for compatibility.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict</code> <p>Parameter names mapped to their values.</p> Source code in <code>src/spotoptim/surrogate/kriging.py</code> <pre><code>def get_params(self, deep: bool = True) -&gt; Dict:\n    \"\"\"Get parameters for this estimator (scikit-learn compatibility).\n\n    Args:\n        deep (bool): Ignored, for compatibility.\n\n    Returns:\n        dict: Parameter names mapped to their values.\n    \"\"\"\n    return {\n        \"noise\": self.noise,\n        \"penalty\": self.penalty,\n        \"method\": self.method,\n        \"var_type\": self.var_type,\n        \"name\": self.name,\n        \"seed\": self.seed,\n        \"model_fun_evals\": self.model_fun_evals,\n        \"n_theta\": self.n_theta,\n        \"min_theta\": self.min_theta,\n        \"max_theta\": self.max_theta,\n        \"theta_init_zero\": self.theta_init_zero,\n        \"p_val\": self.p_val,\n        \"n_p\": self.n_p,\n        \"optim_p\": self.optim_p,\n        \"min_Lambda\": self.min_Lambda,\n        \"max_Lambda\": self.max_Lambda,\n        \"metric_factorial\": self.metric_factorial,\n        \"isotropic\": self.isotropic,\n        \"theta\": self.theta,\n    }\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kriging/#spotoptim.surrogate.kriging.Kriging.likelihood","title":"<code>likelihood(params)</code>","text":"<p>Compute negative concentrated log-likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>ndarray</code> <p>Hyperparameters [log10(theta), log10(Lambda)].</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[float, ndarray, ndarray]</code> <p>(negLnLike, Psi, U) where U is Cholesky factor.</p> Reference <p>Forrester et al. (2008), Section 2.4. Matches implementation in <code>likelihood.m</code> from the book\u2019s code. Concentrated Log-Likelihood approx: ln(L) ~ -(n/2)ln(sigma^2) - (1/2)ln|R|</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt; X = np.array([[0.], [1.]])\n&gt;&gt;&gt; y = np.array([0., 1.])\n&gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n&gt;&gt;&gt; params = np.concatenate([k.theta_, [k.Lambda_]])\n&gt;&gt;&gt; nll, _, _ = k.likelihood(params)\n</code></pre> Source code in <code>src/spotoptim/surrogate/kriging.py</code> <pre><code>def likelihood(self, params: np.ndarray) -&gt; Tuple[float, np.ndarray, np.ndarray]:\n    \"\"\"Compute negative concentrated log-likelihood.\n\n    Args:\n        params (np.ndarray): Hyperparameters [log10(theta), log10(Lambda)].\n\n    Returns:\n        tuple: (negLnLike, Psi, U) where U is Cholesky factor.\n\n    Reference:\n        Forrester et al. (2008), Section 2.4.\n        Matches implementation in `likelihood.m` from the book's code.\n        Concentrated Log-Likelihood approx: ln(L) ~ -(n/2)ln(sigma^2) - (1/2)ln|R|\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n        &gt;&gt;&gt; X = np.array([[0.], [1.]])\n        &gt;&gt;&gt; y = np.array([0., 1.])\n        &gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n        &gt;&gt;&gt; params = np.concatenate([k.theta_, [k.Lambda_]])\n        &gt;&gt;&gt; nll, _, _ = k.likelihood(params)\n    \"\"\"\n    # Extract parameters\n    self.theta_ = params[: self.n_theta]\n\n    if self.method in [\"regression\", \"reinterpolation\"]:\n        lambda_log = params[self.n_theta : self.n_theta + 1][0]\n        lambda_ = 10.0**lambda_log\n    else:\n        lambda_ = self.noise\n\n    n = self.n\n    one = np.ones(n)\n\n    # Build correlation matrix\n    Psi_upper = self.build_correlation_matrix()\n    Psi = Psi_upper + Psi_upper.T + np.eye(n) * (1.0 + lambda_)\n\n    # Cholesky factorization\n    try:\n        U = np.linalg.cholesky(Psi)\n    except LinAlgError:\n        return self.penalty, Psi, None\n\n    # log|R|\n    LnDetPsi = 2.0 * np.sum(np.log(np.abs(np.diag(U))))\n\n    # Solve for mu and sigma^2\n    y = self.y_\n    temp_y = np.linalg.solve(U, y)\n    temp_one = np.linalg.solve(U, one)\n    vy = np.linalg.solve(U.T, temp_y)\n    vone = np.linalg.solve(U.T, temp_one)\n\n    mu = (one @ vy) / (one @ vone)\n    resid = y - one * mu\n    tresid = np.linalg.solve(U, resid)\n    tresid = np.linalg.solve(U.T, tresid)\n    SigmaSqr = (resid @ tresid) / n\n\n    # Concentrated negative log-likelihood\n    # Corresponds to `likelihood.m` line:\n    # NegLnLike=-1*(-(n/2)*log(SigmaSqr) - 0.5*LnDetPsi);\n    # We minimize positive NegLnLike, which is equivalent to maximizing likelihood.\n    negLnLike = (n / 2.0) * np.log(SigmaSqr) + 0.5 * LnDetPsi\n\n    return negLnLike, Psi, U\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kriging/#spotoptim.surrogate.kriging.Kriging.objective","title":"<code>objective(params)</code>","text":"<p>Objective function for hyperparameter optimization.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>ndarray</code> <p>Hyperparameters to evaluate.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Negative concentrated log-likelihood.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt; X = np.array([[0.], [1.]])\n&gt;&gt;&gt; y = np.array([0., 1.])\n&gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n&gt;&gt;&gt; # Evaluate objective at optimal parameters\n&gt;&gt;&gt; val = k.objective(np.concatenate([k.theta_, [k.Lambda_]]))\n</code></pre> Source code in <code>src/spotoptim/surrogate/kriging.py</code> <pre><code>def objective(self, params: np.ndarray) -&gt; float:\n    \"\"\"Objective function for hyperparameter optimization.\n\n    Args:\n        params (np.ndarray): Hyperparameters to evaluate.\n\n    Returns:\n        float: Negative concentrated log-likelihood.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n        &gt;&gt;&gt; X = np.array([[0.], [1.]])\n        &gt;&gt;&gt; y = np.array([0., 1.])\n        &gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n        &gt;&gt;&gt; # Evaluate objective at optimal parameters\n        &gt;&gt;&gt; val = k.objective(np.concatenate([k.theta_, [k.Lambda_]]))\n    \"\"\"\n    negLnLike, _, _ = self.likelihood(params)\n    return negLnLike\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kriging/#spotoptim.surrogate.kriging.Kriging.predict","title":"<code>predict(X, return_std=False)</code>","text":"<p>Predict at new points.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Test points, shape (n_samples, n_features).</p> required <code>return_std</code> <code>bool</code> <p>Return standard deviations. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ndarray</code> <p>np.ndarray: Predictions, shape (n_samples,).</p> <code>tuple</code> <code>ndarray</code> <p>(predictions, std_devs) if return_std=True.</p> Source code in <code>src/spotoptim/surrogate/kriging.py</code> <pre><code>def predict(self, X: np.ndarray, return_std: bool = False) -&gt; np.ndarray:\n    \"\"\"Predict at new points.\n\n    Args:\n        X (np.ndarray): Test points, shape (n_samples, n_features).\n        return_std (bool, optional): Return standard deviations. Defaults to False.\n\n    Returns:\n        np.ndarray: Predictions, shape (n_samples,).\n        tuple: (predictions, std_devs) if return_std=True.\n    \"\"\"\n    X = self._reshape_X(X)\n\n    if return_std:\n        predictions, stds = zip(*[self.predict_single(x) for x in X])\n        return np.array(predictions), np.array(stds)\n    else:\n        predictions = [self.predict_single(x)[0] for x in X]\n        return np.array(predictions)\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kriging/#spotoptim.surrogate.kriging.Kriging.predict_single","title":"<code>predict_single(x)</code>","text":"<p>Predict at a single point.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Test point, shape (n_features,).</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[float, float]</code> <p>(prediction, std_dev).</p> Reference <p>Forrester et al. (2008), Eq 2.15 (Predictor) and Eq 2.19 (Error). Matches implementation in <code>pred.m</code> from the book\u2019s code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.surrogate import Kriging\n&gt;&gt;&gt; X = np.array([[0.], [1.]])\n&gt;&gt;&gt; y = np.array([0., 1.])\n&gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n&gt;&gt;&gt; x_new = np.array([0.5])\n&gt;&gt;&gt; y_pred, y_std = k.predict_single(x_new)\n</code></pre> Source code in <code>src/spotoptim/surrogate/kriging.py</code> <pre><code>def predict_single(self, x: np.ndarray) -&gt; Tuple[float, float]:\n    \"\"\"Predict at a single point.\n\n    Args:\n        x (np.ndarray): Test point, shape (n_features,).\n\n    Returns:\n        tuple: (prediction, std_dev).\n\n    Reference:\n        Forrester et al. (2008), Eq 2.15 (Predictor) and Eq 2.19 (Error).\n        Matches implementation in `pred.m` from the book's code.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.surrogate import Kriging\n        &gt;&gt;&gt; X = np.array([[0.], [1.]])\n        &gt;&gt;&gt; y = np.array([0., 1.])\n        &gt;&gt;&gt; k = Kriging(seed=42).fit(X, y)\n        &gt;&gt;&gt; x_new = np.array([0.5])\n        &gt;&gt;&gt; y_pred, y_std = k.predict_single(x_new)\n    \"\"\"\n    if self.method in [\"regression\", \"reinterpolation\"]:\n        lambda_ = 10.0**self.Lambda_\n    else:\n        lambda_ = self.noise\n\n    U = self.U_\n    n = self.n\n    y = self.y_\n    one = np.ones(n)\n\n    # Compute mu\n    y_tilde = np.linalg.solve(U, y)\n    y_tilde = np.linalg.solve(U.T, y_tilde)\n    one_tilde = np.linalg.solve(U, one)\n    one_tilde = np.linalg.solve(U.T, one_tilde)\n    mu = (one @ y_tilde) / (one @ one_tilde)\n\n    # Residual\n    resid = y - one * mu\n    resid_tilde = np.linalg.solve(U, resid)\n    resid_tilde = np.linalg.solve(U.T, resid_tilde)\n\n    # Correlation vector\n    psi = self.build_psi_vector(x)\n\n    # Prediction\n    # Eq 2.15: \\hat{y} = \\hat{\\mu} + \\psi^T \\Psi^{-1} (y - \\mathbf{1}\\hat{\\mu})\n    # Implemented using pre-computed Cholesky factors as in `pred.m`\n    f = mu + psi @ resid_tilde\n\n    # Variance\n    # Eq 2.19 (Mean Squared Error)\n    if self.method in [\"interpolation\", \"regression\"]:\n        SigmaSqr = (resid @ resid_tilde) / n\n        psi_tilde = np.linalg.solve(U, psi)\n        psi_tilde = np.linalg.solve(U.T, psi_tilde)\n        SSqr = SigmaSqr * (1.0 + lambda_ - psi @ psi_tilde)\n    else:  # reinterpolation\n        Psi_adjusted = self.Psi_ - np.eye(n) * lambda_ + np.eye(n) * self.noise\n        SigmaSqr = (\n            resid\n            @ np.linalg.solve(U.T, np.linalg.solve(U, Psi_adjusted @ resid_tilde))\n        ) / n\n        Uint = np.linalg.cholesky(Psi_adjusted)\n        psi_tilde = np.linalg.solve(Uint, psi)\n        psi_tilde = np.linalg.solve(Uint.T, psi_tilde)\n        SSqr = SigmaSqr * (1.0 - psi @ psi_tilde)\n\n    s = np.sqrt(np.abs(SSqr))\n\n    return float(f), float(s)\n</code></pre>"},{"location":"reference/spotoptim/surrogate/kriging/#spotoptim.surrogate.kriging.Kriging.set_params","title":"<code>set_params(**params)</code>","text":"<p>Set parameters (scikit-learn compatibility).</p> <p>Parameters:</p> Name Type Description Default <code>**params</code> <code>Any</code> <p>Parameter names and values.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Kriging</code> <code>Kriging</code> <p>Self.</p> Source code in <code>src/spotoptim/surrogate/kriging.py</code> <pre><code>def set_params(self, **params: \"typing.Any\") -&gt; \"Kriging\":\n    \"\"\"Set parameters (scikit-learn compatibility).\n\n    Args:\n        **params: Parameter names and values.\n\n    Returns:\n        Kriging: Self.\n    \"\"\"\n    for key, value in params.items():\n        setattr(self, key, value)\n    return self\n</code></pre>"},{"location":"reference/spotoptim/surrogate/mlp_surrogate/","title":"mlp_surrogate","text":"<p>MLP Surrogate model for SpotOptim.</p> <p>This module implements a standard Multi-Layer Perceptron (MLP) surrogate that enables uncertainty estimation via Monte Carlo Dropout (MC Dropout). It is designed to be a drop-in alternative to the Kriging surrogate within the SpotOptim framework.</p>"},{"location":"reference/spotoptim/surrogate/mlp_surrogate/#spotoptim.surrogate.mlp_surrogate.MLPSurrogate","title":"<code>MLPSurrogate</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>A scikit-learn compatible MLP surrogate model with uncertainty estimation.</p> <p>This class wraps a PyTorch MLP (from spotoptim.nn.mlp) and provides the standard fit/predict interface required by SpotOptim. It uses Monte Carlo (MC) Dropout during prediction to estimate uncertainty (standard deviation) which is crucial for acquisition functions.</p> <p>Compatible with SpotOptim\u2019s variable type conventions: - \u2018float\u2019: continuous numeric variables - \u2018int\u2019: integer variables - \u2018factor\u2019: categorical/unordered variables</p> <p>Note: All input variables are currently treated as numeric and standardized. For best performance with categorical variables, this simple treatment may be suboptimal compared to embedding processing, but ensures compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Input dimension. If None, inferred during fit.</p> <code>None</code> <code>hidden_channels</code> <code>List[int]</code> <p>Explicit list of hidden layer sizes.</p> <code>None</code> <code>l1</code> <code>int</code> <p>Neurons per hidden layer (used if hidden_channels is None). Defaults to 64.</p> <code>128</code> <code>num_hidden_layers</code> <code>int</code> <p>Number of hidden layers (used if hidden_channels is None). Defaults to 2.</p> <code>3</code> <code>dropout</code> <code>float</code> <p>Dropout probability. Crucial for uncertainty estimation. Defaults to 0.1.</p> <code>0.0</code> <code>lr</code> <code>float</code> <p>Learning rate. Defaults to 1e-3.</p> <code>0.001</code> <code>activation</code> <code>str</code> <p>Activation function. Defaults to \u201crelu\u201d.</p> <code>'relu'</code> <code>epochs</code> <code>int</code> <p>Number of training epochs. Defaults to 200.</p> <code>200</code> <code>batch_size</code> <code>int</code> <p>Training batch size. Defaults to 32.</p> <code>32</code> <code>optimizer_name</code> <code>str</code> <p>Name of PyTorch optimizer (\u201cAdam\u201d, \u201cSGD\u201d, etc.) or \u201cAdamWScheduleFree\u201d. Defaults to \u201cAdamWScheduleFree\u201d.</p> <code>'AdamWScheduleFree'</code> <code>mc_dropout_passes</code> <code>int</code> <p>Number of forward passes for MC Dropout uncertainty estimation. Defaults to 30.</p> <code>30</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>var_type</code> <code>List[str]</code> <p>Variable types for each dimension. Defaults to None.</p> <code>None</code> <code>name</code> <code>str</code> <p>Name of the surrogate. Defaults to \u201cMLPSurrogate\u201d.</p> <code>'MLPSurrogate'</code> <code>verbose</code> <code>bool</code> <p>Whether to print training progress. Defaults to False.</p> <code>False</code> Source code in <code>src/spotoptim/surrogate/mlp_surrogate.py</code> <pre><code>class MLPSurrogate(BaseEstimator, RegressorMixin):\n    \"\"\"\n    A scikit-learn compatible MLP surrogate model with uncertainty estimation.\n\n    This class wraps a PyTorch MLP (from spotoptim.nn.mlp) and provides\n    the standard fit/predict interface required by SpotOptim.\n    It uses Monte Carlo (MC) Dropout during prediction to estimate\n    uncertainty (standard deviation) which is crucial for acquisition functions.\n\n    Compatible with SpotOptim's variable type conventions:\n    - 'float': continuous numeric variables\n    - 'int': integer variables\n    - 'factor': categorical/unordered variables\n\n    Note: All input variables are currently treated as numeric and standardized.\n    For best performance with categorical variables, this simple treatment\n    may be suboptimal compared to embedding processing, but ensures compatibility.\n\n    Args:\n        in_channels (int, optional): Input dimension. If None, inferred during fit.\n        hidden_channels (List[int], optional): Explicit list of hidden layer sizes.\n        l1 (int, optional): Neurons per hidden layer (used if hidden_channels is None).\n            Defaults to 64.\n        num_hidden_layers (int, optional): Number of hidden layers (used if hidden_channels is None).\n            Defaults to 2.\n        dropout (float, optional): Dropout probability. Crucial for uncertainty estimation.\n            Defaults to 0.1.\n        lr (float, optional): Learning rate. Defaults to 1e-3.\n        activation (str, optional): Activation function. Defaults to \"relu\".\n        epochs (int, optional): Number of training epochs. Defaults to 200.\n        batch_size (int, optional): Training batch size. Defaults to 32.\n        optimizer_name (str, optional): Name of PyTorch optimizer (\"Adam\", \"SGD\", etc.) or\n            \"AdamWScheduleFree\". Defaults to \"AdamWScheduleFree\".\n        mc_dropout_passes (int, optional): Number of forward passes for MC Dropout\n            uncertainty estimation. Defaults to 30.\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n        var_type (List[str], optional): Variable types for each dimension.\n            Defaults to None.\n        name (str, optional): Name of the surrogate. Defaults to \"MLPSurrogate\".\n        verbose (bool, optional): Whether to print training progress. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int = None,\n        hidden_channels: List[int] = None,\n        l1: int = 128,\n        num_hidden_layers: int = 3,\n        dropout: float = 0.0,\n        activation: str = \"relu\",\n        lr: float = 1e-3,\n        epochs: int = 200,\n        batch_size: int = 32,\n        optimizer_name: str = \"AdamWScheduleFree\",\n        mc_dropout_passes: int = 30,\n        seed: int = 42,\n        var_type: Optional[List[str]] = None,\n        name: str = \"MLPSurrogate\",\n        verbose: bool = False,\n    ):\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.l1 = l1\n        self.num_hidden_layers = num_hidden_layers\n        self.dropout = dropout\n        self.activation = activation\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.optimizer_name = optimizer_name\n        self.mc_dropout_passes = mc_dropout_passes\n        self.seed = seed\n        self.var_type = var_type\n        self.name = name\n        self.verbose = verbose\n\n        # State attributes\n        self.model_ = None\n        self.scaler_x_ = None\n        self.scaler_y_ = None\n        self.X_ = None\n        self.y_ = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"MLPSurrogate\":\n        \"\"\"\n        Fit the MLP model to the training data.\n\n        Args:\n            X (np.ndarray): Training inputs, shape (n_samples, n_features).\n            y (np.ndarray): Training targets, shape (n_samples,).\n\n        Returns:\n            MLPSurrogate: The fitted model.\n        \"\"\"\n        # Set seeds for reproducibility\n        torch.manual_seed(self.seed)\n        np.random.seed(self.seed)\n\n        X = np.asarray(X, dtype=np.float32)\n        y = np.asarray(y, dtype=np.float32)\n\n        if y.ndim == 1:\n            y = y.reshape(-1, 1)\n\n        self.X_ = X\n        self.y_ = y.flatten()\n\n        # Handle var_type (store for compatibility, though currently we treat all as float)\n        if self.var_type is None:\n            self.var_type = [\"float\"] * X.shape[1]\n\n        # Scaling\n        self.scaler_x_ = StandardScaler().fit(X)\n        self.scaler_y_ = StandardScaler().fit(y)\n\n        X_scaled = self.scaler_x_.transform(X)\n        y_scaled = self.scaler_y_.transform(y)\n\n        # Dataset\n        dataset = TensorDataset(torch.tensor(X_scaled), torch.tensor(y_scaled))\n        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n        # Initialize MLP\n        input_dim = X.shape[1]\n        output_dim = y.shape[1]\n\n        # Resolve activation function\n        activation_map = {\n            \"relu\": nn.ReLU,\n            \"tanh\": nn.Tanh,\n            \"sigmoid\": nn.Sigmoid,\n            \"leakyrelu\": nn.LeakyReLU,\n            \"elu\": nn.ELU,\n            \"gelu\": nn.GELU,\n            \"silu\": nn.SiLU,\n        }\n        activation_key = self.activation.lower()\n        if activation_key not in activation_map:\n            raise ValueError(\n                f\"Unsupported activation '{self.activation}'. \"\n                f\"Validation options: {list(activation_map.keys())}\"\n            )\n        activation_layer = activation_map[activation_key]\n\n        # If hidden_channels provided, use it, otherwise use l1/num_hidden_layers\n        self._current_hidden_channels = self.hidden_channels\n\n        self.model_ = MLP(\n            in_channels=input_dim,\n            hidden_channels=self._current_hidden_channels,\n            l1=self.l1,\n            num_hidden_layers=self.num_hidden_layers,\n            output_dim=output_dim,\n            activation_layer=activation_layer,\n            dropout=self.dropout,\n            lr=self.lr,\n        )\n\n        optimizer = self.model_.get_optimizer(self.optimizer_name)\n        criterion = nn.MSELoss()\n\n        # Training loop\n        self.model_.train()\n\n        # Schedule-Free Optimizer requires specific train/eval mode switching\n        if hasattr(optimizer, \"train\"):\n            optimizer.train()\n\n        for epoch in range(self.epochs):\n            total_loss = 0.0\n            num_batches = 0\n            for batch_x, batch_y in loader:\n                optimizer.zero_grad()\n                pred = self.model_(batch_x)\n                loss = criterion(pred, batch_y)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n                num_batches += 1\n\n            if self.verbose and (epoch + 1) % (self.epochs // 10 or 1) == 0:\n                print(\n                    f\"Epoch {epoch+1}/{self.epochs}, Loss: {total_loss/num_batches:.6f}\"\n                )\n\n        # Schedule-Free Optimizer finalization\n        if hasattr(optimizer, \"eval\"):\n            optimizer.eval()\n\n        return self\n\n    def predict(\n        self, X: np.ndarray, return_std: bool = False\n    ) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"\n        Predict targets for X.\n\n        Args:\n            X (np.ndarray): Input data, shape (n_samples, n_features).\n            return_std (bool, optional): Whether to return the standard deviation\n                (uncertainty) along with the mean prediction.\n                Defaults to False.\n\n        Returns:\n            np.ndarray: Predicted mean values, shape (n_samples,).\n            tuple: (mean, std) if return_std is True.\n        \"\"\"\n        if self.model_ is None:\n            raise RuntimeError(\"Model is not fitted yet. Call 'fit' first.\")\n\n        X = np.asarray(X, dtype=np.float32)\n        X_scaled = self.scaler_x_.transform(X)\n        X_tensor = torch.tensor(X_scaled)\n\n        if return_std:\n            # MC Dropout Prediction\n            self.model_.train()  # Enable dropout\n            preds_scaled = []\n\n            with torch.no_grad():\n                for _ in range(self.mc_dropout_passes):\n                    out = self.model_(X_tensor)\n                    preds_scaled.append(out.numpy())\n\n            # Stack to shape (passes, n_samples, out_dim)\n            preds_scaled = np.stack(preds_scaled)\n\n            # Compute mean and std in scaled space\n            mean_scaled = np.mean(preds_scaled, axis=0)\n            std_scaled = np.std(preds_scaled, axis=0)\n\n            # Inverse transform mean\n            mean = self.scaler_y_.inverse_transform(mean_scaled)\n\n            # Transform std: std_real = std_scaled * scale_\n            std = std_scaled * self.scaler_y_.scale_\n\n            # Flatten to match Kriging interface (n_samples,)\n            if mean.shape[1] == 1:\n                mean = mean.flatten()\n                std = std.flatten()\n\n            return mean, std\n\n        else:\n            # Single Deterministic Prediction\n            self.model_.eval()  # Disable dropout\n            with torch.no_grad():\n                pred_scaled = self.model_(X_tensor).numpy()\n\n            pred = self.scaler_y_.inverse_transform(pred_scaled)\n\n            if pred.shape[1] == 1:\n                pred = pred.flatten()\n\n            return pred\n</code></pre>"},{"location":"reference/spotoptim/surrogate/mlp_surrogate/#spotoptim.surrogate.mlp_surrogate.MLPSurrogate.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the MLP model to the training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training inputs, shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Training targets, shape (n_samples,).</p> required <p>Returns:</p> Name Type Description <code>MLPSurrogate</code> <code>MLPSurrogate</code> <p>The fitted model.</p> Source code in <code>src/spotoptim/surrogate/mlp_surrogate.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"MLPSurrogate\":\n    \"\"\"\n    Fit the MLP model to the training data.\n\n    Args:\n        X (np.ndarray): Training inputs, shape (n_samples, n_features).\n        y (np.ndarray): Training targets, shape (n_samples,).\n\n    Returns:\n        MLPSurrogate: The fitted model.\n    \"\"\"\n    # Set seeds for reproducibility\n    torch.manual_seed(self.seed)\n    np.random.seed(self.seed)\n\n    X = np.asarray(X, dtype=np.float32)\n    y = np.asarray(y, dtype=np.float32)\n\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n\n    self.X_ = X\n    self.y_ = y.flatten()\n\n    # Handle var_type (store for compatibility, though currently we treat all as float)\n    if self.var_type is None:\n        self.var_type = [\"float\"] * X.shape[1]\n\n    # Scaling\n    self.scaler_x_ = StandardScaler().fit(X)\n    self.scaler_y_ = StandardScaler().fit(y)\n\n    X_scaled = self.scaler_x_.transform(X)\n    y_scaled = self.scaler_y_.transform(y)\n\n    # Dataset\n    dataset = TensorDataset(torch.tensor(X_scaled), torch.tensor(y_scaled))\n    loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n    # Initialize MLP\n    input_dim = X.shape[1]\n    output_dim = y.shape[1]\n\n    # Resolve activation function\n    activation_map = {\n        \"relu\": nn.ReLU,\n        \"tanh\": nn.Tanh,\n        \"sigmoid\": nn.Sigmoid,\n        \"leakyrelu\": nn.LeakyReLU,\n        \"elu\": nn.ELU,\n        \"gelu\": nn.GELU,\n        \"silu\": nn.SiLU,\n    }\n    activation_key = self.activation.lower()\n    if activation_key not in activation_map:\n        raise ValueError(\n            f\"Unsupported activation '{self.activation}'. \"\n            f\"Validation options: {list(activation_map.keys())}\"\n        )\n    activation_layer = activation_map[activation_key]\n\n    # If hidden_channels provided, use it, otherwise use l1/num_hidden_layers\n    self._current_hidden_channels = self.hidden_channels\n\n    self.model_ = MLP(\n        in_channels=input_dim,\n        hidden_channels=self._current_hidden_channels,\n        l1=self.l1,\n        num_hidden_layers=self.num_hidden_layers,\n        output_dim=output_dim,\n        activation_layer=activation_layer,\n        dropout=self.dropout,\n        lr=self.lr,\n    )\n\n    optimizer = self.model_.get_optimizer(self.optimizer_name)\n    criterion = nn.MSELoss()\n\n    # Training loop\n    self.model_.train()\n\n    # Schedule-Free Optimizer requires specific train/eval mode switching\n    if hasattr(optimizer, \"train\"):\n        optimizer.train()\n\n    for epoch in range(self.epochs):\n        total_loss = 0.0\n        num_batches = 0\n        for batch_x, batch_y in loader:\n            optimizer.zero_grad()\n            pred = self.model_(batch_x)\n            loss = criterion(pred, batch_y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            num_batches += 1\n\n        if self.verbose and (epoch + 1) % (self.epochs // 10 or 1) == 0:\n            print(\n                f\"Epoch {epoch+1}/{self.epochs}, Loss: {total_loss/num_batches:.6f}\"\n            )\n\n    # Schedule-Free Optimizer finalization\n    if hasattr(optimizer, \"eval\"):\n        optimizer.eval()\n\n    return self\n</code></pre>"},{"location":"reference/spotoptim/surrogate/mlp_surrogate/#spotoptim.surrogate.mlp_surrogate.MLPSurrogate.predict","title":"<code>predict(X, return_std=False)</code>","text":"<p>Predict targets for X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data, shape (n_samples, n_features).</p> required <code>return_std</code> <code>bool</code> <p>Whether to return the standard deviation (uncertainty) along with the mean prediction. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>np.ndarray: Predicted mean values, shape (n_samples,).</p> <code>tuple</code> <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>(mean, std) if return_std is True.</p> Source code in <code>src/spotoptim/surrogate/mlp_surrogate.py</code> <pre><code>def predict(\n    self, X: np.ndarray, return_std: bool = False\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Predict targets for X.\n\n    Args:\n        X (np.ndarray): Input data, shape (n_samples, n_features).\n        return_std (bool, optional): Whether to return the standard deviation\n            (uncertainty) along with the mean prediction.\n            Defaults to False.\n\n    Returns:\n        np.ndarray: Predicted mean values, shape (n_samples,).\n        tuple: (mean, std) if return_std is True.\n    \"\"\"\n    if self.model_ is None:\n        raise RuntimeError(\"Model is not fitted yet. Call 'fit' first.\")\n\n    X = np.asarray(X, dtype=np.float32)\n    X_scaled = self.scaler_x_.transform(X)\n    X_tensor = torch.tensor(X_scaled)\n\n    if return_std:\n        # MC Dropout Prediction\n        self.model_.train()  # Enable dropout\n        preds_scaled = []\n\n        with torch.no_grad():\n            for _ in range(self.mc_dropout_passes):\n                out = self.model_(X_tensor)\n                preds_scaled.append(out.numpy())\n\n        # Stack to shape (passes, n_samples, out_dim)\n        preds_scaled = np.stack(preds_scaled)\n\n        # Compute mean and std in scaled space\n        mean_scaled = np.mean(preds_scaled, axis=0)\n        std_scaled = np.std(preds_scaled, axis=0)\n\n        # Inverse transform mean\n        mean = self.scaler_y_.inverse_transform(mean_scaled)\n\n        # Transform std: std_real = std_scaled * scale_\n        std = std_scaled * self.scaler_y_.scale_\n\n        # Flatten to match Kriging interface (n_samples,)\n        if mean.shape[1] == 1:\n            mean = mean.flatten()\n            std = std.flatten()\n\n        return mean, std\n\n    else:\n        # Single Deterministic Prediction\n        self.model_.eval()  # Disable dropout\n        with torch.no_grad():\n            pred_scaled = self.model_(X_tensor).numpy()\n\n        pred = self.scaler_y_.inverse_transform(pred_scaled)\n\n        if pred.shape[1] == 1:\n            pred = pred.flatten()\n\n        return pred\n</code></pre>"},{"location":"reference/spotoptim/surrogate/nystroem/","title":"nystroem","text":""},{"location":"reference/spotoptim/surrogate/nystroem/#spotoptim.surrogate.nystroem.Nystroem","title":"<code>Nystroem</code>","text":"<p>Approximate a feature map of a kernel using a subset of data.</p> <p>The Nystroem method approximates a kernel map using a subset of the training data. It constructs an approximate feature map <code>X \\mapsto X'</code> such that <code>X'.dot(X'.T) \\approx K(X, X)</code>.</p> <p>This is particularly useful when: *   n (samples) is moderate/large: The exact kernel method scales as O(n^3).     Nystroem reduces complexity to O(n * n_components^2) for training. *   k (features) is large: By setting <code>n_components</code> such that <code>k &lt; n_components &lt;&lt; n</code>,     it projects high-dimensional data into a manageable feature space where     distance calculations are cheaper (if followed by a linear model).</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>str or callable or Kernel</code> <p>Kernel map to be approximated. Can be a string (e.g., \u2018rbf\u2019), a callable, or a <code>spotoptim.surrogate.kernels.Kernel</code> instance. Defaults to \u2018rbf\u2019.</p> <code>'rbf'</code> <code>n_components</code> <code>int</code> <p>Number of features to construct. This corresponds to the number of samples used to construct the basis. Determines the dimension of the transformed feature space. Defaults to 100.</p> <code>100</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Pseudo-random number generator to control the uniform sampling without replacement of n_components of the training data. Defaults to None.</p> <code>None</code> Source code in <code>src/spotoptim/surrogate/nystroem.py</code> <pre><code>class Nystroem:\n    r\"\"\"\n    Approximate a feature map of a kernel using a subset of data.\n\n    The Nystroem method approximates a kernel map using a subset of the training data.\n    It constructs an approximate feature map `X \\mapsto X'` such that\n    `X'.dot(X'.T) \\approx K(X, X)`.\n\n    This is particularly useful when:\n    *   **n (samples) is moderate/large**: The exact kernel method scales as O(n^3).\n        Nystroem reduces complexity to O(n * n_components^2) for training.\n    *   **k (features) is large**: By setting `n_components` such that `k &lt; n_components &lt;&lt; n`,\n        it projects high-dimensional data into a manageable feature space where\n        distance calculations are cheaper (if followed by a linear model).\n\n    Args:\n        kernel (str or callable or Kernel, optional): Kernel map to be approximated.\n            Can be a string (e.g., 'rbf'), a callable, or a `spotoptim.surrogate.kernels.Kernel`\n            instance. Defaults to 'rbf'.\n        n_components (int, optional): Number of features to construct.\n            This corresponds to the number of samples used to construct the basis.\n            Determines the dimension of the transformed feature space.\n            Defaults to 100.\n        random_state (int, RandomState instance or None, optional):\n            Pseudo-random number generator to control the uniform sampling without replacement\n            of n_components of the training data. Defaults to None.\n    \"\"\"\n\n    def __init__(self, kernel=\"rbf\", n_components=100, random_state=None):\n        self.kernel = kernel\n        self.n_components = n_components\n        self.random_state = random_state\n        self.components_ = None\n        self.component_indices_ = None\n        self.normalization_ = None\n\n    def fit(self, X, y=None) -&gt; \"Nystroem\":\n        \"\"\"\n        Fit estimator to data.\n\n        Samples a subset of `n_components` training points to serve as the basis,\n        computes the kernel matrix on these points, and computes the normalization matrix.\n\n        Args:\n            X (np.ndarray): Training data, shape (n_samples, n_features).\n            y (np.ndarray, optional): Target values (ignored).\n\n        Returns:\n            Nystroem: Returns the instance itself.\n        \"\"\"\n        X = np.atleast_2d(X)\n        rnd = check_random_state(self.random_state)\n        n_samples = X.shape[0]\n\n        # get basis vectors\n        if self.n_components &gt; n_samples:\n            # User requested more components than samples.\n            # We just take all samples.\n            n_components = n_samples\n        else:\n            n_components = self.n_components\n\n        inds = rnd.permutation(n_samples)\n        basis_inds = inds[:n_components]\n        basis = X[basis_inds]\n\n        self.component_indices_ = basis_inds\n        self.components_ = basis\n\n        # Compute basis kernel matrix\n        # If self.kernel is a Kernel object, call it.\n        # If it is a string/callable, use pairwise_kernels.\n        if isinstance(self.kernel, Kernel):\n            K_W = self.kernel(basis, basis)\n        else:\n            # Fallback to sklearn's utility if standard string\n            K_W = pairwise_kernels(basis, metric=self.kernel)\n\n        # Compute normalization (svd decomposition of K_W)\n        # K_W = U S U^T\n        # We need W^(-1/2) for the feature map\n        # But for stability usually use SVD: K_W = U @ S @ V.T (symmetric so U=V)\n        # normalization = U / sqrt(S)\n        U, S, V = np.linalg.svd(K_W)\n        S = np.maximum(S, 1e-12)\n        self.normalization_ = np.dot(U / np.sqrt(S), V)\n\n        return self\n\n    def transform(self, X) -&gt; np.ndarray:\n        \"\"\"\n        Apply feature map to X.\n\n        Computes the kernel between X and the basis vectors, multiplied by the normalization.\n        No sample reduction happens here; the output has the same number of samples as X.\n\n        Args:\n            X (np.ndarray): Data to transform, shape (n_samples, n_features).\n\n        Returns:\n            np.ndarray: Transformed data, shape (n_samples, n_components).\n        \"\"\"\n        X = np.atleast_2d(X)\n        if isinstance(self.kernel, Kernel):\n            K_sum = self.kernel(X, self.components_)\n        else:\n            K_sum = pairwise_kernels(X, self.components_, metric=self.kernel)\n\n        return np.dot(K_sum, self.normalization_.T)\n\n    def fit_transform(self, X, y=None) -&gt; np.ndarray:\n        \"\"\"\n        Fit to data, then transform it.\n\n        Args:\n            X (np.ndarray): Training data.\n            y (np.ndarray, optional): Target values.\n\n        Returns:\n            np.ndarray: Transformed data.\n        \"\"\"\n        return self.fit(X, y).transform(X)\n</code></pre>"},{"location":"reference/spotoptim/surrogate/nystroem/#spotoptim.surrogate.nystroem.Nystroem.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit estimator to data.</p> <p>Samples a subset of <code>n_components</code> training points to serve as the basis, computes the kernel matrix on these points, and computes the normalization matrix.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training data, shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Target values (ignored).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Nystroem</code> <code>Nystroem</code> <p>Returns the instance itself.</p> Source code in <code>src/spotoptim/surrogate/nystroem.py</code> <pre><code>def fit(self, X, y=None) -&gt; \"Nystroem\":\n    \"\"\"\n    Fit estimator to data.\n\n    Samples a subset of `n_components` training points to serve as the basis,\n    computes the kernel matrix on these points, and computes the normalization matrix.\n\n    Args:\n        X (np.ndarray): Training data, shape (n_samples, n_features).\n        y (np.ndarray, optional): Target values (ignored).\n\n    Returns:\n        Nystroem: Returns the instance itself.\n    \"\"\"\n    X = np.atleast_2d(X)\n    rnd = check_random_state(self.random_state)\n    n_samples = X.shape[0]\n\n    # get basis vectors\n    if self.n_components &gt; n_samples:\n        # User requested more components than samples.\n        # We just take all samples.\n        n_components = n_samples\n    else:\n        n_components = self.n_components\n\n    inds = rnd.permutation(n_samples)\n    basis_inds = inds[:n_components]\n    basis = X[basis_inds]\n\n    self.component_indices_ = basis_inds\n    self.components_ = basis\n\n    # Compute basis kernel matrix\n    # If self.kernel is a Kernel object, call it.\n    # If it is a string/callable, use pairwise_kernels.\n    if isinstance(self.kernel, Kernel):\n        K_W = self.kernel(basis, basis)\n    else:\n        # Fallback to sklearn's utility if standard string\n        K_W = pairwise_kernels(basis, metric=self.kernel)\n\n    # Compute normalization (svd decomposition of K_W)\n    # K_W = U S U^T\n    # We need W^(-1/2) for the feature map\n    # But for stability usually use SVD: K_W = U @ S @ V.T (symmetric so U=V)\n    # normalization = U / sqrt(S)\n    U, S, V = np.linalg.svd(K_W)\n    S = np.maximum(S, 1e-12)\n    self.normalization_ = np.dot(U / np.sqrt(S), V)\n\n    return self\n</code></pre>"},{"location":"reference/spotoptim/surrogate/nystroem/#spotoptim.surrogate.nystroem.Nystroem.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Fit to data, then transform it.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training data.</p> required <code>y</code> <code>ndarray</code> <p>Target values.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Transformed data.</p> Source code in <code>src/spotoptim/surrogate/nystroem.py</code> <pre><code>def fit_transform(self, X, y=None) -&gt; np.ndarray:\n    \"\"\"\n    Fit to data, then transform it.\n\n    Args:\n        X (np.ndarray): Training data.\n        y (np.ndarray, optional): Target values.\n\n    Returns:\n        np.ndarray: Transformed data.\n    \"\"\"\n    return self.fit(X, y).transform(X)\n</code></pre>"},{"location":"reference/spotoptim/surrogate/nystroem/#spotoptim.surrogate.nystroem.Nystroem.transform","title":"<code>transform(X)</code>","text":"<p>Apply feature map to X.</p> <p>Computes the kernel between X and the basis vectors, multiplied by the normalization. No sample reduction happens here; the output has the same number of samples as X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Data to transform, shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Transformed data, shape (n_samples, n_components).</p> Source code in <code>src/spotoptim/surrogate/nystroem.py</code> <pre><code>def transform(self, X) -&gt; np.ndarray:\n    \"\"\"\n    Apply feature map to X.\n\n    Computes the kernel between X and the basis vectors, multiplied by the normalization.\n    No sample reduction happens here; the output has the same number of samples as X.\n\n    Args:\n        X (np.ndarray): Data to transform, shape (n_samples, n_features).\n\n    Returns:\n        np.ndarray: Transformed data, shape (n_samples, n_components).\n    \"\"\"\n    X = np.atleast_2d(X)\n    if isinstance(self.kernel, Kernel):\n        K_sum = self.kernel(X, self.components_)\n    else:\n        K_sum = pairwise_kernels(X, self.components_, metric=self.kernel)\n\n    return np.dot(K_sum, self.normalization_.T)\n</code></pre>"},{"location":"reference/spotoptim/surrogate/pipeline/","title":"pipeline","text":""},{"location":"reference/spotoptim/surrogate/pipeline/#spotoptim.surrogate.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>Pipeline of transforms with a final estimator.</p> <p>Sequentially apply a list of transforms and a final estimator. This simple implementation assumes that all steps except the last one are transformers (have <code>fit_transform</code> or <code>fit</code>+<code>transform</code>), and the last step is an estimator (has <code>fit</code> and <code>predict</code>).</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>list</code> <p>List of (name, transform) tuples (implementing fit/transform) that are chained, in the order they are chained, with the last object an estimator.</p> required Source code in <code>src/spotoptim/surrogate/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"\n    Pipeline of transforms with a final estimator.\n\n    Sequentially apply a list of transforms and a final estimator.\n    This simple implementation assumes that all steps except the last one\n    are transformers (have `fit_transform` or `fit`+`transform`), and the last\n    step is an estimator (has `fit` and `predict`).\n\n    Args:\n        steps (list): List of (name, transform) tuples (implementing fit/transform)\n            that are chained, in the order they are chained, with the last object\n            an estimator.\n    \"\"\"\n\n    def __init__(self, steps):\n        self.steps = steps\n\n    @property\n    def _final_estimator(self):\n        \"\"\"Returns the final estimator of the pipeline.\"\"\"\n        return self.steps[-1][1]\n\n    def fit(self, X, y=None, **fit_params) -&gt; \"Pipeline\":\n        \"\"\"\n        Fit the model.\n\n        Fit all the transformers one after the other and transform the\n        data. Finally, fit the transformed data using the final estimator.\n\n        Args:\n            X (iterable): Training data. Must fulfill input requirements of first step of the pipeline.\n            y (iterable, optional): Training targets. Must fulfill label requirements for all steps of the pipeline.\n            **fit_params (Any): Additional parameters passed to the `fit` method of the final estimator.\n\n        Returns:\n            Pipeline: Pipeline with fitted steps.\n        \"\"\"\n        Xt = X\n        for name, transform in self.steps[:-1]:\n            if hasattr(transform, \"fit_transform\"):\n                Xt = transform.fit_transform(Xt, y)\n            else:\n                Xt = transform.fit(Xt, y).transform(Xt)\n\n        if hasattr(self._final_estimator, \"fit\"):\n            self._final_estimator.fit(Xt, y, **fit_params)\n\n        return self\n\n    def predict(self, X, **predict_params) -&gt; Any:\n        \"\"\"\n        Transform the data, and apply `predict` with the final estimator.\n\n        Args:\n            X (iterable): Data to predict on. Must fulfill input requirements of first step of the pipeline.\n            **predict_params (Any): Additional parameters passed to the `predict` method of the final estimator.\n\n        Returns:\n            Any: Result of calling `predict` on the final estimator.\n        \"\"\"\n        Xt = X\n        for name, transform in self.steps[:-1]:\n            if hasattr(transform, \"transform\"):\n                Xt = transform.transform(Xt)\n\n        return self._final_estimator.predict(Xt, **predict_params)\n</code></pre>"},{"location":"reference/spotoptim/surrogate/pipeline/#spotoptim.surrogate.pipeline.Pipeline.fit","title":"<code>fit(X, y=None, **fit_params)</code>","text":"<p>Fit the model.</p> <p>Fit all the transformers one after the other and transform the data. Finally, fit the transformed data using the final estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>iterable</code> <p>Training data. Must fulfill input requirements of first step of the pipeline.</p> required <code>y</code> <code>iterable</code> <p>Training targets. Must fulfill label requirements for all steps of the pipeline.</p> <code>None</code> <code>**fit_params</code> <code>Any</code> <p>Additional parameters passed to the <code>fit</code> method of the final estimator.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Pipeline</code> <code>Pipeline</code> <p>Pipeline with fitted steps.</p> Source code in <code>src/spotoptim/surrogate/pipeline.py</code> <pre><code>def fit(self, X, y=None, **fit_params) -&gt; \"Pipeline\":\n    \"\"\"\n    Fit the model.\n\n    Fit all the transformers one after the other and transform the\n    data. Finally, fit the transformed data using the final estimator.\n\n    Args:\n        X (iterable): Training data. Must fulfill input requirements of first step of the pipeline.\n        y (iterable, optional): Training targets. Must fulfill label requirements for all steps of the pipeline.\n        **fit_params (Any): Additional parameters passed to the `fit` method of the final estimator.\n\n    Returns:\n        Pipeline: Pipeline with fitted steps.\n    \"\"\"\n    Xt = X\n    for name, transform in self.steps[:-1]:\n        if hasattr(transform, \"fit_transform\"):\n            Xt = transform.fit_transform(Xt, y)\n        else:\n            Xt = transform.fit(Xt, y).transform(Xt)\n\n    if hasattr(self._final_estimator, \"fit\"):\n        self._final_estimator.fit(Xt, y, **fit_params)\n\n    return self\n</code></pre>"},{"location":"reference/spotoptim/surrogate/pipeline/#spotoptim.surrogate.pipeline.Pipeline.predict","title":"<code>predict(X, **predict_params)</code>","text":"<p>Transform the data, and apply <code>predict</code> with the final estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>iterable</code> <p>Data to predict on. Must fulfill input requirements of first step of the pipeline.</p> required <code>**predict_params</code> <code>Any</code> <p>Additional parameters passed to the <code>predict</code> method of the final estimator.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of calling <code>predict</code> on the final estimator.</p> Source code in <code>src/spotoptim/surrogate/pipeline.py</code> <pre><code>def predict(self, X, **predict_params) -&gt; Any:\n    \"\"\"\n    Transform the data, and apply `predict` with the final estimator.\n\n    Args:\n        X (iterable): Data to predict on. Must fulfill input requirements of first step of the pipeline.\n        **predict_params (Any): Additional parameters passed to the `predict` method of the final estimator.\n\n    Returns:\n        Any: Result of calling `predict` on the final estimator.\n    \"\"\"\n    Xt = X\n    for name, transform in self.steps[:-1]:\n        if hasattr(transform, \"transform\"):\n            Xt = transform.transform(Xt)\n\n    return self._final_estimator.predict(Xt, **predict_params)\n</code></pre>"},{"location":"reference/spotoptim/surrogate/simple_kriging/","title":"simple_kriging","text":"<p>Simplified SimpleKriging surrogate model for SpotOptim.</p> <p>This is a streamlined version adapted from spotpython.surrogate.kriging for use with the SpotOptim optimizer.</p>"},{"location":"reference/spotoptim/surrogate/simple_kriging/#spotoptim.surrogate.simple_kriging.SimpleKriging","title":"<code>SimpleKriging</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>A simplified Kriging (Gaussian Process) surrogate model for SpotOptim.</p> <p>This class provides a scikit-learn compatible interface with fit() and predict() methods, making it suitable for use as a surrogate in SpotOptim.</p> <p>Parameters:</p> Name Type Description Default <code>noise</code> <code>float</code> <p>Regularization parameter (nugget effect). If None, uses sqrt(eps). Defaults to None.</p> <code>None</code> <code>kernel</code> <code>str</code> <p>Kernel type. Currently only \u2018gauss\u2019 (Gaussian/RBF) is supported. Defaults to \u2018gauss\u2019.</p> <code>'gauss'</code> <code>n_theta</code> <code>int</code> <p>Number of theta parameters. If None, uses k (number of dimensions). Defaults to None.</p> <code>None</code> <code>min_theta</code> <code>float</code> <p>Minimum log10(theta) bound for optimization. Defaults to -3.0.</p> <code>-3.0</code> <code>max_theta</code> <code>float</code> <p>Maximum log10(theta) bound for optimization. Defaults to 2.0.</p> <code>2.0</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>X_</code> <code>ndarray</code> <p>Training data, shape (n_samples, n_features).</p> <code>y_</code> <code>ndarray</code> <p>Training targets, shape (n_samples,).</p> <code>theta_</code> <code>ndarray</code> <p>Optimized theta parameters (log10 scale).</p> <code>mu_</code> <code>float</code> <p>Mean of the SimpleKriging predictor.</p> <code>sigma2_</code> <code>float</code> <p>Variance of the SimpleKriging predictor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.surrogate import SimpleKriging\n&gt;&gt;&gt; X = np.array([[0.0], [0.5], [1.0]])\n&gt;&gt;&gt; y = np.array([0.0, 0.25, 1.0])\n&gt;&gt;&gt; model = SimpleKriging()\n&gt;&gt;&gt; model.fit(X, y)\n&gt;&gt;&gt; predictions = model.predict(np.array([[0.25], [0.75]]))\n</code></pre> Source code in <code>src/spotoptim/surrogate/simple_kriging.py</code> <pre><code>class SimpleKriging(BaseEstimator, RegressorMixin):\n    \"\"\"A simplified Kriging (Gaussian Process) surrogate model for SpotOptim.\n\n    This class provides a scikit-learn compatible interface with fit() and predict()\n    methods, making it suitable for use as a surrogate in SpotOptim.\n\n    Args:\n        noise (float, optional): Regularization parameter (nugget effect). If None, uses sqrt(eps).\n            Defaults to None.\n        kernel (str, optional): Kernel type. Currently only 'gauss' (Gaussian/RBF) is supported.\n            Defaults to 'gauss'.\n        n_theta (int, optional): Number of theta parameters. If None, uses k (number of dimensions).\n            Defaults to None.\n        min_theta (float, optional): Minimum log10(theta) bound for optimization. Defaults to -3.0.\n        max_theta (float, optional): Maximum log10(theta) bound for optimization. Defaults to 2.0.\n        seed (int, optional): Random seed for reproducibility. Defaults to None.\n\n    Attributes:\n        X_ (ndarray): Training data, shape (n_samples, n_features).\n        y_ (ndarray): Training targets, shape (n_samples,).\n        theta_ (ndarray): Optimized theta parameters (log10 scale).\n        mu_ (float): Mean of the SimpleKriging predictor.\n        sigma2_ (float): Variance of the SimpleKriging predictor.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.surrogate import SimpleKriging\n        &gt;&gt;&gt; X = np.array([[0.0], [0.5], [1.0]])\n        &gt;&gt;&gt; y = np.array([0.0, 0.25, 1.0])\n        &gt;&gt;&gt; model = SimpleKriging()\n        &gt;&gt;&gt; model.fit(X, y)\n        &gt;&gt;&gt; predictions = model.predict(np.array([[0.25], [0.75]]))\n    \"\"\"\n\n    def __init__(\n        self,\n        noise: Optional[float] = None,\n        kernel: str = \"gauss\",\n        n_theta: Optional[int] = None,\n        min_theta: float = -3.0,\n        max_theta: float = 2.0,\n        seed: Optional[int] = None,\n    ):\n        self.noise = noise\n        self.kernel = kernel\n        self.n_theta = n_theta\n        self.min_theta = min_theta\n        self.max_theta = max_theta\n        self.seed = seed\n\n        # Fitted attributes\n        self.X_ = None\n        self.y_ = None\n        self.theta_ = None\n        self.mu_ = None\n        self.sigma2_ = None\n        self.U_ = None  # Cholesky factor\n        self.Rinv_one_ = None\n        self.Rinv_r_ = None\n\n    def _get_noise(self) -&gt; float:\n        \"\"\"Get the noise/regularization parameter.\n\n        Returns:\n            float: Noise/regularization value.\n        \"\"\"\n        if self.noise is None:\n            return np.sqrt(np.finfo(float).eps)\n        return self.noise\n\n    def _correlation(self, D: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute correlation from distance matrix using Gaussian kernel.\n\n        Args:\n            D (ndarray): Squared distance matrix.\n\n        Returns:\n            ndarray: Correlation matrix.\n        \"\"\"\n        if self.kernel == \"gauss\":\n            return np.exp(-D)\n        else:\n            raise ValueError(f\"Unsupported kernel: {self.kernel}\")\n\n    def _build_correlation_matrix(self, X: np.ndarray, theta: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Build correlation matrix R for training data.\n\n        Args:\n            X (ndarray): Input data, shape (n, k).\n            theta (ndarray): Theta parameters (10^theta used as weights), shape (k,).\n\n        Returns:\n            ndarray: Correlation matrix with noise on diagonal, shape (n, n).\n        \"\"\"\n        n = X.shape[0]\n        theta10 = 10.0**theta\n\n        # Compute weighted squared distances\n        R = np.zeros((n, n))\n        for i in range(n):\n            for j in range(i + 1, n):\n                diff = X[i] - X[j]\n                dist = np.sum(theta10 * diff**2)\n                R[i, j] = dist\n                R[j, i] = dist\n\n        # Apply correlation function\n        R = self._correlation(R)\n\n        # Add noise to diagonal\n        noise_val = self._get_noise()\n        np.fill_diagonal(R, 1.0 + noise_val)\n\n        return R\n\n    def _build_correlation_vector(\n        self, x: np.ndarray, X: np.ndarray, theta: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"Build correlation vector between new point x and training data X.\n\n        Args:\n            x (ndarray): New point, shape (k,).\n            X (ndarray): Training data, shape (n, k).\n            theta (ndarray): Theta parameters, shape (k,).\n\n        Returns:\n            ndarray: Correlation vector, shape (n,).\n        \"\"\"\n        theta10 = 10.0**theta\n        diff = X - x.reshape(1, -1)\n        D = np.sum(theta10 * diff**2, axis=1)\n        return self._correlation(D)\n\n    def _neg_log_likelihood(self, log_theta: np.ndarray) -&gt; float:\n        \"\"\"Compute negative concentrated log-likelihood.\n\n        Args:\n            log_theta (ndarray): Log10(theta) parameters.\n\n        Returns:\n            float: Negative log-likelihood (to be minimized).\n        \"\"\"\n        try:\n            n = self.X_.shape[0]\n            y = self.y_.flatten()\n            one = np.ones(n)\n\n            # Build correlation matrix\n            R = self._build_correlation_matrix(self.X_, log_theta)\n\n            # Cholesky decomposition\n            try:\n                U = cholesky(R)\n            except LinAlgError:\n                return 1e10  # Penalty for ill-conditioned matrix\n\n            # Solve for mean and variance\n            Uy = solve(U, y)\n            Uone = solve(U, one)\n\n            Rinv_y = solve(U.T, Uy)\n            Rinv_one = solve(U.T, Uone)\n\n            mu = (one @ Rinv_y) / (one @ Rinv_one)\n            r = y - one * mu\n\n            Ur = solve(U, r)\n            Rinv_r = solve(U.T, Ur)\n\n            sigma2 = (r @ Rinv_r) / n\n\n            if sigma2 &lt;= 0:\n                return 1e10\n\n            # Concentrated log-likelihood\n            log_det_R = 2.0 * np.sum(np.log(np.abs(np.diag(U))))\n            neg_log_like = (n / 2.0) * np.log(sigma2) + 0.5 * log_det_R\n\n            return neg_log_like\n\n        except (LinAlgError, ValueError):\n            return 1e10\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"SimpleKriging\":\n        \"\"\"Fit the SimpleKriging model to training data.\n\n        Args:\n            X (ndarray): Training input data, shape (n_samples, n_features).\n            y (ndarray): Training target values, shape (n_samples,).\n\n        Returns:\n            SimpleKriging: Fitted estimator (self).\n        \"\"\"\n        X = np.atleast_2d(X)\n        y = np.asarray(y).flatten()\n\n        if X.ndim != 2:\n            raise ValueError(f\"X must be 2-dimensional, got shape {X.shape}\")\n        if y.ndim != 1:\n            raise ValueError(f\"y must be 1-dimensional, got shape {y.shape}\")\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\"X and y must have same number of samples\")\n\n        self.X_ = X\n        self.y_ = y\n        n, k = X.shape\n\n        # Set number of theta parameters\n        if self.n_theta is None:\n            self.n_theta = k\n\n        # Optimize theta via maximum likelihood\n        bounds = [(self.min_theta, self.max_theta)] * self.n_theta\n\n        result = differential_evolution(\n            func=self._neg_log_likelihood,\n            bounds=bounds,\n            seed=self.seed,\n            maxiter=100,\n            atol=1e-6,\n            tol=0.01,\n        )\n\n        self.theta_ = result.x\n\n        # Compute final model parameters\n        one = np.ones(n)\n        R = self._build_correlation_matrix(X, self.theta_)\n\n        try:\n            self.U_ = cholesky(R)\n        except LinAlgError:\n            # Add more regularization if needed\n            R = self._build_correlation_matrix(X, self.theta_)\n            R += np.eye(n) * 1e-8\n            self.U_ = cholesky(R)\n\n        Uy = solve(self.U_, y)\n        Uone = solve(self.U_, one)\n\n        Rinv_y = solve(self.U_.T, Uy)\n        Rinv_one = solve(self.U_.T, Uone)\n\n        self.mu_ = float((one @ Rinv_y) / (one @ Rinv_one))\n\n        r = y - one * self.mu_\n        Ur = solve(self.U_, r)\n        Rinv_r = solve(self.U_.T, Ur)\n\n        self.sigma2_ = float((r @ Rinv_r) / n)\n\n        # Store for prediction\n        self.Rinv_one_ = Rinv_one\n        self.Rinv_r_ = Rinv_r\n\n        return self\n\n    def predict(self, X: np.ndarray, return_std: bool = False) -&gt; np.ndarray:\n        \"\"\"Predict using the SimpleKriging model.\n\n        Args:\n            X (ndarray): Points to predict at, shape (n_samples, n_features).\n            return_std (bool, optional): If True, return standard deviations as well.\n                Defaults to False.\n\n        Returns:\n            ndarray or tuple: If return_std is False, returns predicted values (n_samples,).\n                If return_std is True, returns tuple of (predictions, std_devs) both shape (n_samples,).\n        \"\"\"\n        X = np.atleast_2d(X)\n\n        if X.ndim == 1:\n            X = X.reshape(1, -1)\n\n        if X.shape[1] != self.X_.shape[1]:\n            raise ValueError(\n                f\"X has {X.shape[1]} features, expected {self.X_.shape[1]}\"\n            )\n\n        n_pred = X.shape[0]\n        predictions = np.zeros(n_pred)\n\n        if return_std:\n            std_devs = np.zeros(n_pred)\n\n        for i, x in enumerate(X):\n            # Build correlation vector\n            psi = self._build_correlation_vector(x, self.X_, self.theta_)\n\n            # Predict mean\n            predictions[i] = self.mu_ + psi @ self.Rinv_r_\n\n            if return_std:\n                # Predict variance\n                Upsi = solve(self.U_, psi)\n                psi_Rinv_psi = psi @ solve(self.U_.T, Upsi)\n\n                variance = self.sigma2_ * (1.0 + self._get_noise() - psi_Rinv_psi)\n                std_devs[i] = np.sqrt(max(0.0, variance))\n\n        if return_std:\n            return predictions, std_devs\n        return predictions\n</code></pre>"},{"location":"reference/spotoptim/surrogate/simple_kriging/#spotoptim.surrogate.simple_kriging.SimpleKriging.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the SimpleKriging model to training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training input data, shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Training target values, shape (n_samples,).</p> required <p>Returns:</p> Name Type Description <code>SimpleKriging</code> <code>SimpleKriging</code> <p>Fitted estimator (self).</p> Source code in <code>src/spotoptim/surrogate/simple_kriging.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"SimpleKriging\":\n    \"\"\"Fit the SimpleKriging model to training data.\n\n    Args:\n        X (ndarray): Training input data, shape (n_samples, n_features).\n        y (ndarray): Training target values, shape (n_samples,).\n\n    Returns:\n        SimpleKriging: Fitted estimator (self).\n    \"\"\"\n    X = np.atleast_2d(X)\n    y = np.asarray(y).flatten()\n\n    if X.ndim != 2:\n        raise ValueError(f\"X must be 2-dimensional, got shape {X.shape}\")\n    if y.ndim != 1:\n        raise ValueError(f\"y must be 1-dimensional, got shape {y.shape}\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have same number of samples\")\n\n    self.X_ = X\n    self.y_ = y\n    n, k = X.shape\n\n    # Set number of theta parameters\n    if self.n_theta is None:\n        self.n_theta = k\n\n    # Optimize theta via maximum likelihood\n    bounds = [(self.min_theta, self.max_theta)] * self.n_theta\n\n    result = differential_evolution(\n        func=self._neg_log_likelihood,\n        bounds=bounds,\n        seed=self.seed,\n        maxiter=100,\n        atol=1e-6,\n        tol=0.01,\n    )\n\n    self.theta_ = result.x\n\n    # Compute final model parameters\n    one = np.ones(n)\n    R = self._build_correlation_matrix(X, self.theta_)\n\n    try:\n        self.U_ = cholesky(R)\n    except LinAlgError:\n        # Add more regularization if needed\n        R = self._build_correlation_matrix(X, self.theta_)\n        R += np.eye(n) * 1e-8\n        self.U_ = cholesky(R)\n\n    Uy = solve(self.U_, y)\n    Uone = solve(self.U_, one)\n\n    Rinv_y = solve(self.U_.T, Uy)\n    Rinv_one = solve(self.U_.T, Uone)\n\n    self.mu_ = float((one @ Rinv_y) / (one @ Rinv_one))\n\n    r = y - one * self.mu_\n    Ur = solve(self.U_, r)\n    Rinv_r = solve(self.U_.T, Ur)\n\n    self.sigma2_ = float((r @ Rinv_r) / n)\n\n    # Store for prediction\n    self.Rinv_one_ = Rinv_one\n    self.Rinv_r_ = Rinv_r\n\n    return self\n</code></pre>"},{"location":"reference/spotoptim/surrogate/simple_kriging/#spotoptim.surrogate.simple_kriging.SimpleKriging.predict","title":"<code>predict(X, return_std=False)</code>","text":"<p>Predict using the SimpleKriging model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Points to predict at, shape (n_samples, n_features).</p> required <code>return_std</code> <code>bool</code> <p>If True, return standard deviations as well. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>ndarray or tuple: If return_std is False, returns predicted values (n_samples,). If return_std is True, returns tuple of (predictions, std_devs) both shape (n_samples,).</p> Source code in <code>src/spotoptim/surrogate/simple_kriging.py</code> <pre><code>def predict(self, X: np.ndarray, return_std: bool = False) -&gt; np.ndarray:\n    \"\"\"Predict using the SimpleKriging model.\n\n    Args:\n        X (ndarray): Points to predict at, shape (n_samples, n_features).\n        return_std (bool, optional): If True, return standard deviations as well.\n            Defaults to False.\n\n    Returns:\n        ndarray or tuple: If return_std is False, returns predicted values (n_samples,).\n            If return_std is True, returns tuple of (predictions, std_devs) both shape (n_samples,).\n    \"\"\"\n    X = np.atleast_2d(X)\n\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n\n    if X.shape[1] != self.X_.shape[1]:\n        raise ValueError(\n            f\"X has {X.shape[1]} features, expected {self.X_.shape[1]}\"\n        )\n\n    n_pred = X.shape[0]\n    predictions = np.zeros(n_pred)\n\n    if return_std:\n        std_devs = np.zeros(n_pred)\n\n    for i, x in enumerate(X):\n        # Build correlation vector\n        psi = self._build_correlation_vector(x, self.X_, self.theta_)\n\n        # Predict mean\n        predictions[i] = self.mu_ + psi @ self.Rinv_r_\n\n        if return_std:\n            # Predict variance\n            Upsi = solve(self.U_, psi)\n            psi_Rinv_psi = psi @ solve(self.U_.T, Upsi)\n\n            variance = self.sigma2_ * (1.0 + self._get_noise() - psi_Rinv_psi)\n            std_devs[i] = np.sqrt(max(0.0, variance))\n\n    if return_std:\n        return predictions, std_devs\n    return predictions\n</code></pre>"},{"location":"reference/spotoptim/tricands/tricands/","title":"tricands","text":""},{"location":"reference/spotoptim/tricands/tricands/#spotoptim.tricands.tricands.tricands","title":"<code>tricands(X, p=0.5, fringe=True, nmax=None, best=None, ordering=None, vis=False, imgname='tricands.pdf', lower=0, upper=1)</code>","text":"<p>Generate Triangulation Candidates for Bayesian Optimization. Assumes a bounding box of [lower, upper]^m.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Design matrix of shape (n_samples, n_features). Each row gives a design point and each column a feature.</p> required <code>p</code> <code>float</code> <p>Distance to the boundary for fringe candidates (0 = on hull, 1 = on boundary). Defaults to 0.5.</p> <code>0.5</code> <code>fringe</code> <code>bool</code> <p>Whether to include fringe points to allow exploration outside the convex hull. Defaults to True.</p> <code>True</code> <code>nmax</code> <code>int</code> <p>Maximum size of candidate set. If output exceeds this, strategic subsetting is employed. Defaults to 100 * n_features.</p> <code>None</code> <code>best</code> <code>int</code> <p>Index of the best (lowest) currently observed point. Used for strategic subsetting in Bayesian optimization. Defaults to None.</p> <code>None</code> <code>ordering</code> <code>ndarray</code> <p>Order of closeness of rows of X to a contour level. Used for contour location subsetting. Defaults to None.</p> <code>None</code> <code>vis</code> <code>bool</code> <p>Whether to visualize the triangulation. Only applicable to 2D designs. Defaults to False.</p> <code>False</code> <code>imgname</code> <code>str</code> <p>File name for saved plot if vis=True. Defaults to \u2018tricands.pdf\u2019.</p> <code>'tricands.pdf'</code> <code>lower</code> <code>float</code> <p>Lower bound of bounding box for all dimensions. Defaults to 0.</p> <code>0</code> <code>upper</code> <code>float</code> <p>Upper bound of bounding box for all dimensions. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of candidate points, shape (n_candidates, n_features).</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If visualization is requested for non-2D data.</p> <code>Exception</code> <p>If number of points is less than n_features + 1.</p> <code>Exception</code> <p>If both \u2018best\u2019 and \u2018ordering\u2019 are provided.</p> <code>Exception</code> <p>If X contains values outside [lower, upper].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.tricands import tricands\n&gt;&gt;&gt; X = np.array([[0.1, 0.1], [0.9, 0.1], [0.5, 0.9], [0.2, 0.5]])\n&gt;&gt;&gt; candidates = tricands(X, fringe=True, p=0.5)\n&gt;&gt;&gt; print(candidates.shape)\n(7, 2)\n</code></pre> Source code in <code>src/spotoptim/tricands/tricands.py</code> <pre><code>def tricands(\n    X: np.ndarray,\n    p: float = 0.5,\n    fringe: bool = True,\n    nmax: int = None,\n    best: int = None,\n    ordering: np.ndarray = None,\n    vis: bool = False,\n    imgname: str = \"tricands.pdf\",\n    lower: float = 0,\n    upper: float = 1,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate Triangulation Candidates for Bayesian Optimization.\n    Assumes a bounding box of [lower, upper]^m.\n\n    Args:\n        X (np.ndarray): Design matrix of shape (n_samples, n_features).\n            Each row gives a design point and each column a feature.\n        p (float, optional): Distance to the boundary for fringe candidates\n            (0 = on hull, 1 = on boundary). Defaults to 0.5.\n        fringe (bool, optional): Whether to include fringe points to allow\n            exploration outside the convex hull. Defaults to True.\n        nmax (int, optional): Maximum size of candidate set. If output exceeds this,\n            strategic subsetting is employed. Defaults to 100 * n_features.\n        best (int, optional): Index of the best (lowest) currently observed point.\n            Used for strategic subsetting in Bayesian optimization.\n            Defaults to None.\n        ordering (np.ndarray, optional): Order of closeness of rows of X to a contour level.\n            Used for contour location subsetting. Defaults to None.\n        vis (bool, optional): Whether to visualize the triangulation.\n            Only applicable to 2D designs. Defaults to False.\n        imgname (str, optional): File name for saved plot if vis=True.\n            Defaults to 'tricands.pdf'.\n        lower (float, optional): Lower bound of bounding box for all dimensions. Defaults to 0.\n        upper (float, optional): Upper bound of bounding box for all dimensions. Defaults to 1.\n\n    Returns:\n        np.ndarray: Array of candidate points, shape (n_candidates, n_features).\n\n    Raises:\n        Exception: If visualization is requested for non-2D data.\n        Exception: If number of points is less than n_features + 1.\n        Exception: If both 'best' and 'ordering' are provided.\n        Exception: If X contains values outside [lower, upper].\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.tricands import tricands\n        &gt;&gt;&gt; X = np.array([[0.1, 0.1], [0.9, 0.1], [0.5, 0.9], [0.2, 0.5]])\n        &gt;&gt;&gt; candidates = tricands(X, fringe=True, p=0.5)\n        &gt;&gt;&gt; print(candidates.shape)\n        (7, 2)\n    \"\"\"\n    # extract dimsions and do sanity checks\n    m = X.shape[1]\n    n = X.shape[0]\n    if nmax is None:\n        nmax = 100 * m\n    if vis and m != 2:\n        raise Exception(\"visuals only possible when ncol(X) = 2\")\n    if n &lt; m + 1:\n        raise Exception(\"must have nrow(X) &gt;= ncol(X) + 1\")\n    if best is not None and ordering is not None:\n        raise Exception(\"can only subset for BO or CL, not both\")\n    if np.min(X) &lt; lower or np.max(X) &gt; upper:\n        raise Exception(\"X outside of lower/upper bounds\")\n\n    # possible visual\n    if vis:\n        plt.figure()\n        plt.scatter(X[:, 0], X[:, 1])\n        plt.xlim(lower, upper)\n        plt.ylim(lower, upper)\n\n    # interior candidates\n    ic = tricands_interior(X)\n    Xcand = ic[\"cand\"]\n    if vis:\n        for i in range(ic[\"tri\"].shape[0]):\n            X[np.append(ic[\"tri\"][i, :], ic[\"tri\"][i, 0]),].T\n            for j in range(ic[\"tri\"].shape[1] + 1):\n                if j &lt; ic[\"tri\"].shape[1]:\n                    xpoints = X[ic[\"tri\"][i, j : (j + 2)], 0]\n                    ypoints = X[ic[\"tri\"][i, j : (j + 2)], 1]\n                else:\n                    xpoints = X[ic[\"tri\"][i, [-1, 0]], 0]\n                    ypoints = X[ic[\"tri\"][i, [-1, 0]], 1]\n                plt.plot(xpoints, ypoints, color=\"black\")\n\n        plt.scatter(Xcand[:, 0], Xcand[:, 1])\n\n    # calculate midpoints of convex hull vectors\n    if fringe:\n        fr = tricands_fringe(X, p, lower=lower, upper=upper)\n        Xcand = np.concatenate([Xcand, fr[\"XF\"]], axis=0)\n        # possibly visualize fringe candidates\n        if vis:\n            for i in range(fr[\"XB\"].shape[0]):\n                plt.arrow(\n                    fr[\"XB\"][i, 0],\n                    fr[\"XB\"][i, 1],\n                    fr[\"XF\"][i, 0] - fr[\"XB\"][i, 0],\n                    fr[\"XF\"][i, 1] - fr[\"XB\"][i, 1],\n                    width=0.005,\n                    color=\"red\",\n                )\n\n    # throw some away?\n    if nmax &lt; Xcand.shape[0]:\n\n        adj = []\n\n        # Bayesian optimization strategic subsetting\n        if best is not None:\n            # find candidates adjacent to best\n            adj = np.where(\n                np.apply_along_axis(lambda x: np.any(x == best), 1, ic[\"tri\"])\n            )[0]\n            if len(adj) &gt; nmax / 10:\n                np.random.choice(adj, round(nmax / 10), replace=False)\n            if vis:\n                plt.scatter(\n                    X[best : (best + 1), 0], X[best : (best + 1), 1], color=\"green\"\n                )\n            if len(adj) &gt;= nmax:\n                raise Exception(\"adjacent to best &gt;= nmax\")\n\n        # Contour location strategic subsetting\n        if ordering is not None:\n            i = 0\n            facets = np.column_stack(\n                (fr[\"qhull\"].simplices, [None] * fr[\"qhull\"].simplices.shape[0])\n            )\n            all_tri = np.vstack((ic[\"tri\"], facets))\n            while len(adj) &lt; nmax / 10:\n                # get all triangles adjacent to the i'th best point\n                adj_tri = np.where(\n                    np.apply_along_axis(lambda x: np.any(x == ordering[i]), 1, all_tri)\n                )[0]\n                if len(adj_tri) == 0 or i == len(ordering) - 1:\n                    break\n                else:\n                    # remove duplicates\n                    duplicates = [i in adj for i in adj_tri]\n                    adj_tri = adj_tri[np.logical_not(duplicates)]\n                    adj.append(np.random.choice(adj_tri))\n                    i = i + 1\n            # need to add optional visual\n\n        # get the rest randomly\n        remain = np.array(list(range(Xcand.shape[0])))\n        if len(adj) &gt; 0:\n            remain = np.delete(remain, adj, 0)\n        rest = np.random.choice(remain, (nmax - len(adj)), replace=False)\n        sel = np.concatenate([adj, rest], axis=0).astype(int)\n        Xcand = Xcand[sel, :]\n\n        # possibly visualize\n        if vis:\n            plt.scatter(Xcand[:, 0], Xcand[:, 1], color=\"green\")\n\n    if vis:\n        plt.savefig(imgname)\n        plt.close()\n    return Xcand\n</code></pre>"},{"location":"reference/spotoptim/tricands/tricands/#spotoptim.tricands.tricands.tricands_fringe","title":"<code>tricands_fringe(X, p=0.5, lower=0, upper=1)</code>","text":"<p>Generate fringe candidates outside the convex hull.</p> <p>Subroutine used by tricands wrapper. Assumes a bounding box of [lower, upper]^m.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input design matrix of shape (n_samples, n_features).</p> required <code>p</code> <code>float</code> <p>Distance to the boundary (0 = on hull, 1 = on boundary). Defaults to 0.5.</p> <code>0.5</code> <code>lower</code> <code>float</code> <p>Lower bound of bounding box for all dimensions. Defaults to 0.</p> <code>0</code> <code>upper</code> <code>float</code> <p>Upper bound of bounding box for all dimensions. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing: - \u2018XF\u2019 (np.ndarray): Fringe candidate points. - \u2018XB\u2019 (np.ndarray): Boundary points (means of external facets). - \u2018qhull\u2019 (scipy.spatial.ConvexHull): The computed convex hull object.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the number of points is less than n_features + 1.</p> Source code in <code>src/spotoptim/tricands/tricands.py</code> <pre><code>def tricands_fringe(\n    X: np.ndarray, p: float = 0.5, lower: float = 0, upper: float = 1\n) -&gt; dict:\n    \"\"\"\n    Generate fringe candidates outside the convex hull.\n\n    Subroutine used by tricands wrapper. Assumes a bounding box of [lower, upper]^m.\n\n    Args:\n        X (np.ndarray): Input design matrix of shape (n_samples, n_features).\n        p (float, optional): Distance to the boundary (0 = on hull, 1 = on boundary). Defaults to 0.5.\n        lower (float, optional): Lower bound of bounding box for all dimensions. Defaults to 0.\n        upper (float, optional): Upper bound of bounding box for all dimensions. Defaults to 1.\n\n    Returns:\n        dict: A dictionary containing:\n            - 'XF' (np.ndarray): Fringe candidate points.\n            - 'XB' (np.ndarray): Boundary points (means of external facets).\n            - 'qhull' (scipy.spatial.ConvexHull): The computed convex hull object.\n\n    Raises:\n        Exception: If the number of points is less than n_features + 1.\n    \"\"\"\n\n    m = X.shape[1]\n    n = X.shape[0]\n    if n &lt; m + 1:\n        raise Exception(\"must have nrow(X) &gt;= ncol(X) + 1\")\n\n    # get midpoints of external (convex hull) facets and normal vectors\n    qhull = ConvexHull(X, qhull_options=\"n\")\n    norms = np.zeros((qhull.simplices.shape[0], m))\n    Xbound = np.zeros((qhull.simplices.shape[0], m))\n    for i in range(qhull.simplices.shape[0]):\n        Xbound[i,] = np.mean(X[qhull.simplices[i,], :], axis=0)\n        norms[i,] = qhull.equations[i, 0:m]\n\n    # norms off of the boundary points to get fringe candidates\n    # p specifies distance between hull and boundary\n    eps = np.sqrt(np.finfo(float).eps)\n    alpha = np.zeros(Xbound.shape[0])\n\n    # Initialize with infinity (for zero/small norms)\n    ai = np.full([Xbound.shape[0], m], np.inf)\n\n    # Positive norms (significant): distance to upper bound\n    pos_mask = norms &gt; eps\n    ai[pos_mask] = (upper - Xbound[pos_mask]) / norms[pos_mask]\n\n    # Negative norms (significant): distance to lower bound\n    neg_mask = norms &lt; -eps\n    ai[neg_mask] = (lower - Xbound[neg_mask]) / norms[neg_mask]\n\n    alpha = np.min(ai, axis=1)\n\n    Xfringe = Xbound + norms * alpha[:, np.newaxis] * p\n\n    return {\"XF\": Xfringe, \"XB\": Xbound, \"qhull\": qhull}\n</code></pre>"},{"location":"reference/spotoptim/tricands/tricands/#spotoptim.tricands.tricands.tricands_interior","title":"<code>tricands_interior(X)</code>","text":"<p>Generate interior candidates using Delaunay triangulation.</p> <p>Subroutine used by tricands wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input design matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing: - \u2018cand\u2019 (np.ndarray): Candidate points (midpoints of triangles). - \u2018tri\u2019 (np.ndarray): Simplicies of the Delaunay triangulation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the number of points is less than n_features + 1.</p> Source code in <code>src/spotoptim/tricands/tricands.py</code> <pre><code>def tricands_interior(X: np.ndarray) -&gt; dict:\n    \"\"\"\n    Generate interior candidates using Delaunay triangulation.\n\n    Subroutine used by tricands wrapper.\n\n    Args:\n        X (np.ndarray): Input design matrix of shape (n_samples, n_features).\n\n    Returns:\n        dict: A dictionary containing:\n            - 'cand' (np.ndarray): Candidate points (midpoints of triangles).\n            - 'tri' (np.ndarray): Simplicies of the Delaunay triangulation.\n\n    Raises:\n        Exception: If the number of points is less than n_features + 1.\n    \"\"\"\n    m = X.shape[1]\n    n = X.shape[0]\n    if n &lt; m + 1:\n        raise Exception(\"must have nrow(X) &gt;= ncol(X) + 1\")\n\n    # possible to further vectorize?\n    # find the middle of triangles\n    tri = Delaunay(X, qhull_options=\"Q12\").simplices\n    Xcand = np.zeros([tri.shape[0], m])\n    for i in range(tri.shape[0]):\n        Xcand[i, :] = np.mean(X[tri[i,],], axis=0)\n\n    return {\"cand\": Xcand, \"tri\": tri}\n</code></pre>"},{"location":"reference/spotoptim/utils/boundaries/","title":"boundaries","text":""},{"location":"reference/spotoptim/utils/boundaries/#spotoptim.utils.boundaries.get_boundaries","title":"<code>get_boundaries(data)</code>","text":"<p>Calculates the minimum and maximum values for each column in a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A NumPy array of shape (n, k), where n is the number of rows and k is the number of columns.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>tuple[np.ndarray, np.ndarray]: A tuple containing two NumPy arrays: - The first array contains the minimum values for each column, with shape (k,). - The second array contains the maximum values for each column, with shape (k,).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input array has shape (1, 0) (empty array).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.utils.boundaries import get_boundaries\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; min_values, max_values = get_boundaries(data)\n&gt;&gt;&gt; print(\"Minimum values:\", min_values)\nMinimum values: [1 2 3]\n&gt;&gt;&gt; print(\"Maximum values:\", max_values)\nMaximum values: [7 8 9]\n</code></pre> Source code in <code>src/spotoptim/utils/boundaries.py</code> <pre><code>def get_boundaries(data: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculates the minimum and maximum values for each column in a NumPy array.\n\n    Args:\n        data (np.ndarray): A NumPy array of shape (n, k), where n is the number of rows\n            and k is the number of columns.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing two NumPy arrays:\n            - The first array contains the minimum values for each column, with shape (k,).\n            - The second array contains the maximum values for each column, with shape (k,).\n\n    Raises:\n        ValueError: If the input array has shape (1, 0) (empty array).\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.utils.boundaries import get_boundaries\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        &gt;&gt;&gt; min_values, max_values = get_boundaries(data)\n        &gt;&gt;&gt; print(\"Minimum values:\", min_values)\n        Minimum values: [1 2 3]\n        &gt;&gt;&gt; print(\"Maximum values:\", max_values)\n        Maximum values: [7 8 9]\n    \"\"\"\n    if data.size == 0:\n        raise ValueError(\"Input array cannot be empty.\")\n    min_values = np.min(data, axis=0)\n    max_values = np.max(data, axis=0)\n    return min_values, max_values\n</code></pre>"},{"location":"reference/spotoptim/utils/boundaries/#spotoptim.utils.boundaries.map_to_original_scale","title":"<code>map_to_original_scale(X_search, x_min, x_max)</code>","text":"<p>Maps the values in X_search from the range [0, 1] to the original scale defined by x_min and x_max.</p> <p>Parameters:</p> Name Type Description Default <code>X_search</code> <code>Union[DataFrame, ndarray]</code> <p>A Pandas DataFrame or NumPy array containing the search points in the range [0, 1].</p> required <code>x_min</code> <code>ndarray</code> <p>A NumPy array containing the minimum values for each feature in the original scale.</p> required <code>x_max</code> <code>ndarray</code> <p>A NumPy array containing the maximum values for each feature in the original scale.</p> required <p>Returns:</p> Type Description <code>Union[DataFrame, ndarray]</code> <p>Union[pd.DataFrame, np.ndarray]: A Pandas DataFrame or NumPy array with the values mapped to the original scale.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.utils.boundaries import map_to_original_scale\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; X_search = pd.DataFrame([[0.5, 0.5], [0.25, 0.75]], columns=['x', 'y'])\n&gt;&gt;&gt; x_min = np.array([0, 0])\n&gt;&gt;&gt; x_max = np.array([10, 20])\n&gt;&gt;&gt; X_search_scaled = map_to_original_scale(X_search, x_min, x_max)\n&gt;&gt;&gt; print(X_search_scaled)\n      x     y\n0   5.0  10.0\n1   2.5  15.0\n</code></pre> Source code in <code>src/spotoptim/utils/boundaries.py</code> <pre><code>def map_to_original_scale(\n    X_search: Union[pd.DataFrame, np.ndarray],\n    x_min: np.ndarray,\n    x_max: np.ndarray,\n) -&gt; Union[pd.DataFrame, np.ndarray]:\n    \"\"\"\n    Maps the values in X_search from the range [0, 1] to the original scale defined by x_min and x_max.\n\n    Args:\n        X_search (Union[pd.DataFrame, np.ndarray]): A Pandas DataFrame or NumPy array containing the search points in the range [0, 1].\n        x_min (np.ndarray): A NumPy array containing the minimum values for each feature in the original scale.\n        x_max (np.ndarray): A NumPy array containing the maximum values for each feature in the original scale.\n\n    Returns:\n        Union[pd.DataFrame, np.ndarray]: A Pandas DataFrame or NumPy array with the values mapped to the original scale.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.utils.boundaries import map_to_original_scale\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; X_search = pd.DataFrame([[0.5, 0.5], [0.25, 0.75]], columns=['x', 'y'])\n        &gt;&gt;&gt; x_min = np.array([0, 0])\n        &gt;&gt;&gt; x_max = np.array([10, 20])\n        &gt;&gt;&gt; X_search_scaled = map_to_original_scale(X_search, x_min, x_max)\n        &gt;&gt;&gt; print(X_search_scaled)\n              x     y\n        0   5.0  10.0\n        1   2.5  15.0\n    \"\"\"\n    if not isinstance(X_search, (pd.DataFrame, np.ndarray)):\n        raise TypeError(\"X_search must be a Pandas DataFrame or a NumPy array.\")\n\n    # if x_min or x_max are not numpy arrays, convert them to numpy arrays\n    if not isinstance(x_min, np.ndarray):\n        x_min = np.array(x_min)\n    if not isinstance(x_max, np.ndarray):\n        x_max = np.array(x_max)\n\n    if len(x_min) != X_search.shape[1]:\n        raise IndexError(\n            f\"x_min and X_search must have the same number of columns. x_min has {len(x_min)} columns and X_search has {X_search.shape[1]} columns.\"\n        )\n    if len(x_max) != X_search.shape[1]:\n        raise IndexError(\n            f\"x_max and X_search must have the same number of columns. x_max has {len(x_max)} columns and X_search has {X_search.shape[1]} columns.\"\n        )\n\n    if isinstance(X_search, pd.DataFrame):\n        X_search_scaled = (\n            X_search.copy()\n        )  # Create a copy to avoid modifying the original DataFrame\n        for i, col in enumerate(X_search.columns):\n            X_search_scaled.loc[:, col] = (\n                X_search[col] * (x_max[i] - x_min[i]) + x_min[i]\n            )\n        return X_search_scaled\n    elif isinstance(X_search, np.ndarray):\n        X_search_scaled = (\n            X_search.copy()\n        )  # Create a copy to avoid modifying the original array\n        for i in range(X_search.shape[1]):\n            X_search_scaled[:, i] = X_search[:, i] * (x_max[i] - x_min[i]) + x_min[i]\n        return X_search_scaled\n</code></pre>"},{"location":"reference/spotoptim/utils/eval/","title":"eval","text":""},{"location":"reference/spotoptim/utils/eval/#spotoptim.utils.eval.mo_cv_models","title":"<code>mo_cv_models(X, y, model_define_func, cv=5, scores=None)</code>","text":"<p>Performs cross-validation for separate models for each target in a multi-output problem.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame or ndarray</code> <p>Feature matrix.</p> required <code>y</code> <code>DataFrame or ndarray</code> <p>Target matrix with multiple columns.</p> required <code>model_define_func</code> <code>Callable</code> <p>Function that returns a fresh model or pipeline instance.</p> required <code>cv</code> <code>int or cross-validation generator, default=5</code> <p>Number of folds or CV object.</p> <code>5</code> <code>scores</code> <code>Union[Dict[str, str / Callable], str, Callable, None]</code> <p>Scoring metric(s). - If None (default): Uses default scorer of the estimator. Returns List[np.ndarray]. - If str/Callable: Single scorer. Returns List[np.ndarray]. - If Dict: Dictionary of {name: scorer}. Returns Dict[str, List[np.ndarray]].   Note: Unlike mo_eval_models which takes raw functions, cross_val_score expects   strings (e.g. \u2018neg_mean_squared_error\u2019) or make_scorer callables, or raw callables that fit the sklearn signature.   To maintain similarity with mo_eval_models, we rely on cross_val_score\u2019s flexibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>Union[List[ndarray], Dict[str, List[ndarray]]]</code> <ul> <li>List of arrays (one array of CV scores per target) if <code>scores</code> is None or single.</li> <li>Dictionary of {name: List[np.ndarray]} if <code>scores</code> is a dictionary.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.utils.eval import mo_cv_models\n</code></pre> <pre><code>&gt;&gt;&gt; # Generate dummy data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; X = pd.DataFrame(np.random.rand(100, 5), columns=[f'x{i}' for i in range(5)])\n&gt;&gt;&gt; y = pd.DataFrame(np.random.rand(100, 3), columns=[f'y{i}' for i in range(3)])\n</code></pre> <pre><code>&gt;&gt;&gt; # Example 1: Default behavior (Default scorer, e.g. R2)\n&gt;&gt;&gt; def make_model():\n...     from sklearn.linear_model import Ridge\n...     return Ridge()\n&gt;&gt;&gt; cv_scores = mo_cv_models(X, y, make_model, cv=3)\nCross-validating target 1/3...\nCross-validating target 2/3...\nCross-validating target 3/3...\nCV Scores Mean: ['-0.14', '-0.07', '-0.20']\n</code></pre> <pre><code>&gt;&gt;&gt; # Example 2: Custom single score (NMSE - Negative Mean Squared Error)\n&gt;&gt;&gt; # Note: sklearn scoring strings are preferred for cross_val_score\n&gt;&gt;&gt; nmse_scores = mo_cv_models(X, y, make_model, cv=3, scores='neg_mean_squared_error')\nCross-validating target 1/3...\nCross-validating target 2/3...\nCross-validating target 3/3...\nCV Scores Mean: ['-0.09', '-0.08', '-0.10']\n</code></pre> <pre><code>&gt;&gt;&gt; # Example 3: Multiple custom scores\n&gt;&gt;&gt; my_scores = {'R2': 'r2', 'NMSE': 'neg_mean_squared_error'}\n&gt;&gt;&gt; all_cv_scores = mo_cv_models(X, y, make_model, cv=3, scores=my_scores)\nCross-validating target 1/3...\nCross-validating target 2/3...\nCross-validating target 3/3...\nCV Scores Mean:\n  R2: ['-0.14', '-0.07', '-0.20']\n  NMSE: ['-0.09', '-0.08', '-0.10']\n</code></pre> Source code in <code>src/spotoptim/utils/eval.py</code> <pre><code>def mo_cv_models(X, y, model_define_func, cv=5, scores=None):\n    \"\"\"\n    Performs cross-validation for separate models for each target in a multi-output problem.\n\n    Args:\n        X (pd.DataFrame or np.ndarray): Feature matrix.\n        y (pd.DataFrame or np.ndarray): Target matrix with multiple columns.\n        model_define_func (Callable): Function that returns a fresh model or pipeline instance.\n        cv (int or cross-validation generator, default=5): Number of folds or CV object.\n        scores (Union[Dict[str, str/Callable], str, Callable, None]): Scoring metric(s).\n            - If None (default): Uses default scorer of the estimator. Returns List[np.ndarray].\n            - If str/Callable: Single scorer. Returns List[np.ndarray].\n            - If Dict: Dictionary of {name: scorer}. Returns Dict[str, List[np.ndarray]].\n              Note: Unlike mo_eval_models which takes raw functions, cross_val_score expects\n              strings (e.g. 'neg_mean_squared_error') or make_scorer callables, or raw callables that fit the sklearn signature.\n              To maintain similarity with mo_eval_models, we rely on cross_val_score's flexibility.\n\n    Returns:\n        scores (Union[List[np.ndarray], Dict[str, List[np.ndarray]]]):\n            - List of arrays (one array of CV scores per target) if `scores` is None or single.\n            - Dictionary of {name: List[np.ndarray]} if `scores` is a dictionary.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.utils.eval import mo_cv_models\n\n        &gt;&gt;&gt; # Generate dummy data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; X = pd.DataFrame(np.random.rand(100, 5), columns=[f'x{i}' for i in range(5)])\n        &gt;&gt;&gt; y = pd.DataFrame(np.random.rand(100, 3), columns=[f'y{i}' for i in range(3)])\n\n        &gt;&gt;&gt; # Example 1: Default behavior (Default scorer, e.g. R2)\n        &gt;&gt;&gt; def make_model():\n        ...     from sklearn.linear_model import Ridge\n        ...     return Ridge()\n        &gt;&gt;&gt; cv_scores = mo_cv_models(X, y, make_model, cv=3)\n        Cross-validating target 1/3...\n        Cross-validating target 2/3...\n        Cross-validating target 3/3...\n        CV Scores Mean: ['-0.14', '-0.07', '-0.20']\n\n        &gt;&gt;&gt; # Example 2: Custom single score (NMSE - Negative Mean Squared Error)\n        &gt;&gt;&gt; # Note: sklearn scoring strings are preferred for cross_val_score\n        &gt;&gt;&gt; nmse_scores = mo_cv_models(X, y, make_model, cv=3, scores='neg_mean_squared_error')\n        Cross-validating target 1/3...\n        Cross-validating target 2/3...\n        Cross-validating target 3/3...\n        CV Scores Mean: ['-0.09', '-0.08', '-0.10']\n\n        &gt;&gt;&gt; # Example 3: Multiple custom scores\n        &gt;&gt;&gt; my_scores = {'R2': 'r2', 'NMSE': 'neg_mean_squared_error'}\n        &gt;&gt;&gt; all_cv_scores = mo_cv_models(X, y, make_model, cv=3, scores=my_scores)\n        Cross-validating target 1/3...\n        Cross-validating target 2/3...\n        Cross-validating target 3/3...\n        CV Scores Mean:\n          R2: ['-0.14', '-0.07', '-0.20']\n          NMSE: ['-0.09', '-0.08', '-0.10']\n    \"\"\"\n    target_count = y.shape[1]\n\n    # Determine scoring mode\n    if scores is None:\n        scoring_items = {\"Score\": None}  # None uses default estimator scorer\n        return_dict = False\n    elif isinstance(scores, (str, object)) and not isinstance(scores, dict):\n        # 'object' to cover callable, but check not dict first\n        scoring_items = {\"Score\": scores}\n        return_dict = False\n    elif isinstance(scores, dict):\n        scoring_items = scores\n        return_dict = True\n    else:\n        raise ValueError(\n            \"scores argument must be None, a string/callable, or a dictionary.\"\n        )\n\n    results = {name: [] for name in scoring_items}\n\n    for i in range(target_count):\n        print(f\"Cross-validating target {i+1}/{target_count}...\")\n        y_target = _get_target_column(y, i)\n\n        # Fresh model for each target (and cross_val_score clones it anyway)\n        model = model_define_func()\n\n        for name, scorer in scoring_items.items():\n            # cross_val_score(estimator, X, y, scoring=..., cv=...)\n            cv_res = cross_val_score(model, X, y_target, cv=cv, scoring=scorer)\n            results[name].append(cv_res)\n\n    if not return_dict:\n        final_scores = list(results.values())[0]\n        # Calculate means for display\n        means = [f\"{np.mean(s):.2f}\" for s in final_scores]\n        print(f\"CV Scores Mean: {means}\")\n    else:\n        final_scores = results\n        print(\"CV Scores Mean:\")\n        for name, vals in final_scores.items():\n            means = [f\"{np.mean(s):.2f}\" for s in vals]\n            print(f\"  {name}: {means}\")\n\n    return final_scores\n</code></pre>"},{"location":"reference/spotoptim/utils/eval/#spotoptim.utils.eval.mo_eval_models","title":"<code>mo_eval_models(X_train, y_train, X_test, y_test, model_define_func, scores=None, verbose=False)</code>","text":"<p>Trains and evaluates separate models for each target in a multi-output regression problem.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>DataFrame or ndarray</code> <p>Training feature matrix.</p> required <code>y_train</code> <code>DataFrame or ndarray</code> <p>Training target matrix with multiple columns (one per target).</p> required <code>X_test</code> <code>DataFrame or ndarray</code> <p>Test feature matrix.</p> required <code>y_test</code> <code>DataFrame or ndarray</code> <p>Test target matrix with multiple columns (one per target).</p> required <code>model_define_func</code> <code>Callable</code> <p>Function that returns a fresh model or pipeline instance for training.</p> required <code>scores</code> <code>Union[Dict[str, Callable], Callable, None]</code> <p>Scoring metric(s) to evaluate. - If None (default): Uses R2 score. Returns List[float]. - If Callable: Single scoring function (e.g., mean_squared_error). Returns List[float]. - If Dict: Dictionary of {name: scoring_func}. Returns Dict[str, List[float]].</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>Union[List[float], Dict[str, List[float]]]</code> <ul> <li>List of scores if <code>scores</code> is None or a single callable.</li> <li>Dictionary of scores if <code>scores</code> is a dictionary.</li> </ul> <code>models</code> <code>List</code> <p>List of trained model instances, one per target.</p> <code>preds_stacked</code> <code>ndarray</code> <p>Array of stacked predictions for all targets, shape (n_samples, n_targets).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from spotoptim.utils.eval import mo_eval_models\n</code></pre> <pre><code>&gt;&gt;&gt; # Generate dummy data\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; X_train = pd.DataFrame(np.random.rand(100, 5), columns=[f'x{i}' for i in range(5)])\n&gt;&gt;&gt; y_train = pd.DataFrame(np.random.rand(100, 3), columns=[f'y{i}' for i in range(3)])\n&gt;&gt;&gt; X_test = pd.DataFrame(np.random.rand(20, 5), columns=[f'x{i}' for i in range(5)])\n&gt;&gt;&gt; y_test = pd.DataFrame(np.random.rand(20, 3), columns=[f'y{i}' for i in range(3)])\n</code></pre> <pre><code>&gt;&gt;&gt; # Example 1: Default behavior (R2 score)\n&gt;&gt;&gt; def make_model():\n...     from sklearn.linear_model import Ridge\n...     return Ridge()\n&gt;&gt;&gt; r2_scores, models, preds = mo_eval_models(X_train, y_train, X_test, y_test, make_model)\nTraining model for target 1/3...\nTraining model for target 2/3...\nTraining model for target 3/3...\nModel scores: ['-0.10', '-0.13', '-0.19']\nPredictions shape: (20, 3)\n</code></pre> <pre><code>&gt;&gt;&gt; # Example 2: Custom single score (MSE)\n&gt;&gt;&gt; from sklearn.metrics import mean_squared_error\n&gt;&gt;&gt; mse_scores, _, _ = mo_eval_models(X_train, y_train, X_test, y_test, make_model, scores=mean_squared_error)\nTraining model for target 1/3...\nTraining model for target 2/3...\nTraining model for target 3/3...\nModel scores: ['0.07', '0.09', '0.10']\nPredictions shape: (20, 3)\n</code></pre> <pre><code>&gt;&gt;&gt; # Example 3: Multiple custom scores\n&gt;&gt;&gt; from sklearn.metrics import mean_absolute_error, r2_score\n&gt;&gt;&gt; my_scores = {'R2': r2_score, 'MSE': mean_squared_error, 'MAE': mean_absolute_error}\n&gt;&gt;&gt; all_scores, _, _ = mo_eval_models(X_train, y_train, X_test, y_test, make_model, scores=my_scores)\nTraining model for target 1/3...\nTraining model for target 2/3...\nTraining model for target 3/3...\nModel scores:\n  R2: ['-0.10', '-0.13', '-0.19']\n  MSE: ['0.07', '0.09', '0.10']\n  MAE: ['0.21', '0.27', '0.28']\nPredictions shape: (20, 3)\n</code></pre> Source code in <code>src/spotoptim/utils/eval.py</code> <pre><code>def mo_eval_models(\n    X_train, y_train, X_test, y_test, model_define_func, scores=None, verbose=False\n):\n    \"\"\"\n    Trains and evaluates separate models for each target in a multi-output regression problem.\n\n    Args:\n        X_train (pd.DataFrame or np.ndarray): Training feature matrix.\n        y_train (pd.DataFrame or np.ndarray): Training target matrix with multiple columns (one per target).\n        X_test (pd.DataFrame or np.ndarray): Test feature matrix.\n        y_test (pd.DataFrame or np.ndarray): Test target matrix with multiple columns (one per target).\n        model_define_func (Callable): Function that returns a fresh model or pipeline instance for training.\n        scores (Union[Dict[str, Callable], Callable, None]): Scoring metric(s) to evaluate.\n            - If None (default): Uses R2 score. Returns List[float].\n            - If Callable: Single scoring function (e.g., mean_squared_error). Returns List[float].\n            - If Dict: Dictionary of {name: scoring_func}. Returns Dict[str, List[float]].\n        verbose (bool): Whether to print verbose output.\n\n    Returns:\n        scores (Union[List[float], Dict[str, List[float]]]):\n            - List of scores if `scores` is None or a single callable.\n            - Dictionary of scores if `scores` is a dictionary.\n        models (List): List of trained model instances, one per target.\n        preds_stacked (np.ndarray): Array of stacked predictions for all targets, shape (n_samples, n_targets).\n\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from spotoptim.utils.eval import mo_eval_models\n\n        &gt;&gt;&gt; # Generate dummy data\n        &gt;&gt;&gt; np.random.seed(42)\n        &gt;&gt;&gt; X_train = pd.DataFrame(np.random.rand(100, 5), columns=[f'x{i}' for i in range(5)])\n        &gt;&gt;&gt; y_train = pd.DataFrame(np.random.rand(100, 3), columns=[f'y{i}' for i in range(3)])\n        &gt;&gt;&gt; X_test = pd.DataFrame(np.random.rand(20, 5), columns=[f'x{i}' for i in range(5)])\n        &gt;&gt;&gt; y_test = pd.DataFrame(np.random.rand(20, 3), columns=[f'y{i}' for i in range(3)])\n\n        &gt;&gt;&gt; # Example 1: Default behavior (R2 score)\n        &gt;&gt;&gt; def make_model():\n        ...     from sklearn.linear_model import Ridge\n        ...     return Ridge()\n        &gt;&gt;&gt; r2_scores, models, preds = mo_eval_models(X_train, y_train, X_test, y_test, make_model)\n        Training model for target 1/3...\n        Training model for target 2/3...\n        Training model for target 3/3...\n        Model scores: ['-0.10', '-0.13', '-0.19']\n        Predictions shape: (20, 3)\n\n        &gt;&gt;&gt; # Example 2: Custom single score (MSE)\n        &gt;&gt;&gt; from sklearn.metrics import mean_squared_error\n        &gt;&gt;&gt; mse_scores, _, _ = mo_eval_models(X_train, y_train, X_test, y_test, make_model, scores=mean_squared_error)\n        Training model for target 1/3...\n        Training model for target 2/3...\n        Training model for target 3/3...\n        Model scores: ['0.07', '0.09', '0.10']\n        Predictions shape: (20, 3)\n\n        &gt;&gt;&gt; # Example 3: Multiple custom scores\n        &gt;&gt;&gt; from sklearn.metrics import mean_absolute_error, r2_score\n        &gt;&gt;&gt; my_scores = {'R2': r2_score, 'MSE': mean_squared_error, 'MAE': mean_absolute_error}\n        &gt;&gt;&gt; all_scores, _, _ = mo_eval_models(X_train, y_train, X_test, y_test, make_model, scores=my_scores)\n        Training model for target 1/3...\n        Training model for target 2/3...\n        Training model for target 3/3...\n        Model scores:\n          R2: ['-0.10', '-0.13', '-0.19']\n          MSE: ['0.07', '0.09', '0.10']\n          MAE: ['0.21', '0.27', '0.28']\n        Predictions shape: (20, 3)\n    \"\"\"\n    models = []\n    preds = []\n    target_count = y_train.shape[1]\n\n    # Determine scoring mode\n    if scores is None:\n        scoring_funcs = {\"R2\": r2_score}\n        return_dict = False\n    elif callable(scores):\n        scoring_funcs = {\"Score\": scores}\n        return_dict = False\n    elif isinstance(scores, dict):\n        scoring_funcs = scores\n        return_dict = True\n    else:\n        raise ValueError(\n            \"scores argument must be None, a callable, or a dictionary of callables.\"\n        )\n\n    for i in range(target_count):\n        # Fit pipeline for this target\n        if verbose:\n            print(f\"Training model for target {i+1}/{target_count}...\")\n        model_pipeline = model_define_func()\n\n        y_train_target = _get_target_column(y_train, i)\n\n        model_pipeline.fit(X_train, y_train_target)\n        models.append(model_pipeline)\n        pred = model_pipeline.predict(X_test)\n        preds.append(pred)\n\n    # stack predictions for multi-output compatibility\n    preds_stacked = np.column_stack(preds)\n\n    # Calculate scores\n    results = {name: [] for name in scoring_funcs}\n\n    for i in range(target_count):\n        y_true = _get_target_column(y_test, i)\n        y_pred = preds_stacked[:, i]\n\n        for name, func in scoring_funcs.items():\n            results[name].append(func(y_true, y_pred))\n\n    if not return_dict:\n        # Return list of values for the single metric (backward compatibility)\n        final_scores = list(results.values())[0]\n        score_display = [f\"{s:.2f}\" for s in final_scores]\n        if verbose:\n            print(f\"Model scores: {score_display}\")\n    else:\n        final_scores = results\n        if verbose:\n            print(\"Model scores:\")\n            for name, vals in final_scores.items():\n                score_display = [f\"{s:.2f}\" for s in vals]\n                print(f\"  {name}: {score_display}\")\n\n    if verbose:\n        print(f\"Predictions shape: {preds_stacked.shape}\")\n    return final_scores, models, preds_stacked\n</code></pre>"},{"location":"reference/spotoptim/utils/file/","title":"file","text":""},{"location":"reference/spotoptim/utils/file/#spotoptim.utils.file.get_experiment_filename","title":"<code>get_experiment_filename(PREFIX=None, fun_name=None, dim=None, fun_evals=None, path='experiments', extension='pkl')</code>","text":"<p>Generates a standardized filename for experiments.</p> <p>Parameters:</p> Name Type Description Default <code>PREFIX</code> <code>str</code> <p>Prefix/identifier for the experiment</p> <code>None</code> <code>fun_name</code> <code>str</code> <p>Name of the objective function</p> <code>None</code> <code>dim</code> <code>int</code> <p>Dimensionality of the problem</p> <code>None</code> <code>fun_evals</code> <code>int</code> <p>Number of function evaluations</p> <code>None</code> <code>path</code> <code>str</code> <p>Directory path to save the file</p> <code>'experiments'</code> <code>extension</code> <code>str</code> <p>File extension (default: \u201cpkl\u201d)</p> <code>'pkl'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Absolute path to the experiment file</p> Source code in <code>src/spotoptim/utils/file.py</code> <pre><code>def get_experiment_filename(\n    PREFIX: str = None,\n    fun_name: str = None,\n    dim: int = None,\n    fun_evals: int = None,\n    path: str = \"experiments\",\n    extension: str = \"pkl\",\n) -&gt; str:\n    \"\"\"\n    Generates a standardized filename for experiments.\n\n    Args:\n        PREFIX (str): Prefix/identifier for the experiment\n        fun_name (str): Name of the objective function\n        dim (int): Dimensionality of the problem\n        fun_evals (int): Number of function evaluations\n        path (str): Directory path to save the file\n        extension (str): File extension (default: \"pkl\")\n\n    Returns:\n        str: Absolute path to the experiment file\n    \"\"\"\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    parts = []\n    if PREFIX:\n        parts.append(str(PREFIX))\n    if fun_name:\n        parts.append(str(fun_name))\n    if dim:\n        parts.append(f\"d_{dim}\")\n    if fun_evals:\n        parts.append(f\"n_{fun_evals}\")\n\n    parts.append(timestamp)\n\n    filename = \"_\".join(parts) + f\".{extension}\"\n    return os.path.join(path, filename)\n</code></pre>"},{"location":"reference/spotoptim/utils/mapping/","title":"mapping","text":"<p>Learning rate mapping functions for unified optimizer interface.</p> <p>This module provides utilities to map a unified learning rate scale to optimizer-specific learning rates, accounting for the different default and typical ranges used by different PyTorch optimizers.</p>"},{"location":"reference/spotoptim/utils/mapping/#spotoptim.utils.mapping.map_lr","title":"<code>map_lr(lr_unified, optimizer_name, use_default_scale=True)</code>","text":"<p>Map a unified learning rate to an optimizer-specific learning rate.</p> <p>This function provides a unified interface for learning rates across different PyTorch optimizers. Different optimizers operate on vastly different learning rate scales (e.g., SGD typically uses lr ~ 0.01-0.1, while Adam uses lr ~ 0.0001-0.001).</p> <p>The mapping uses the default learning rates from PyTorch as scaling factors, allowing users to work with a normalized learning rate scale where 1.0 represents the optimizer\u2019s default learning rate.</p> <p>Parameters:</p> Name Type Description Default <code>lr_unified</code> <code>float</code> <p>Unified learning rate multiplier. A value of 1.0 corresponds to the optimizer\u2019s default learning rate. Values &lt; 1.0 reduce the learning rate, values &gt; 1.0 increase it. Typical range: [0.001, 100.0].</p> required <code>optimizer_name</code> <code>str</code> <p>Name of the PyTorch optimizer. Must be one of: \u201cAdadelta\u201d, \u201cAdagrad\u201d, \u201cAdam\u201d, \u201cAdamW\u201d, \u201cSparseAdam\u201d, \u201cAdamax\u201d, \u201cASGD\u201d, \u201cLBFGS\u201d, \u201cNAdam\u201d, \u201cRAdam\u201d, \u201cRMSprop\u201d, \u201cRprop\u201d, \u201cSGD\u201d.</p> required <code>use_default_scale</code> <code>bool</code> <p>Whether to scale by the optimizer\u2019s default learning rate. If True (default), lr_unified is multiplied by the default lr. If False, returns lr_unified directly. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The optimizer-specific learning rate.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If optimizer_name is not supported.</p> <code>ValueError</code> <p>If lr_unified is not positive.</p> <p>Examples:</p> <p>Using unified learning rate with default scaling:</p> <pre><code>&gt;&gt;&gt; # Get Adam's default learning rate (0.001)\n&gt;&gt;&gt; lr = map_lr(1.0, \"Adam\")\n&gt;&gt;&gt; print(lr)\n0.001\n</code></pre> <pre><code>&gt;&gt;&gt; # Get half of SGD's default learning rate (0.01 / 2 = 0.005)\n&gt;&gt;&gt; lr = map_lr(0.5, \"SGD\")\n&gt;&gt;&gt; print(lr)\n0.005\n</code></pre> <pre><code>&gt;&gt;&gt; # Get 10x RMSprop's default learning rate (0.01 * 10 = 0.1)\n&gt;&gt;&gt; lr = map_lr(10.0, \"RMSprop\")\n&gt;&gt;&gt; print(lr)\n0.1\n</code></pre> <p>Using unified learning rate without scaling:</p> <pre><code>&gt;&gt;&gt; # Use lr_unified directly (0.01)\n&gt;&gt;&gt; lr = map_lr(0.01, \"Adam\", use_default_scale=False)\n&gt;&gt;&gt; print(lr)\n0.01\n</code></pre> <p>Practical example with model training:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import torch.nn as nn\n&gt;&gt;&gt; from spotoptim.nn.linear_regressor import LinearRegressor\n&gt;&gt;&gt; from spotoptim.utils.mapping import map_lr\n&gt;&gt;&gt;\n&gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use unified learning rate of 0.5 for Adam (gives 0.0005)\n&gt;&gt;&gt; lr_adam = map_lr(0.5, \"Adam\")\n&gt;&gt;&gt; optimizer_adam = model.get_optimizer(\"Adam\", lr=lr_adam)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use same unified learning rate for SGD (gives 0.005)\n&gt;&gt;&gt; lr_sgd = map_lr(0.5, \"SGD\")\n&gt;&gt;&gt; optimizer_sgd = model.get_optimizer(\"SGD\", lr=lr_sgd)\n</code></pre> <p>Hyperparameter optimization example:</p> <pre><code>&gt;&gt;&gt; from spotoptim import SpotOptim\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt;\n&gt;&gt;&gt; def train_model(X):\n...     results = []\n...     for params in X:\n...         lr_unified = 10 ** params[0]  # Log scale: [-4, 0]\n...         optimizer_name = params[1]     # Factor variable\n...\n...         # Map to optimizer-specific learning rate\n...         lr_actual = map_lr(lr_unified, optimizer_name)\n...\n...         # Train model with this configuration\n...         # ... training code ...\n...         results.append(test_loss)\n...     return np.array(results)\n&gt;&gt;&gt;\n&gt;&gt;&gt; optimizer = SpotOptim(\n...     fun=train_model,\n...     bounds=[(-4, 0), (\"Adam\", \"SGD\", \"RMSprop\")],\n...     var_type=[\"num\", \"factor\"],\n...     max_iter=30\n... )\n</code></pre> Note <ul> <li>The unified learning rate provides a normalized scale across optimizers</li> <li>A value of 1.0 always corresponds to the optimizer\u2019s PyTorch default</li> <li>This enables fair comparison when optimizing over different optimizers</li> <li>For log-scale optimization, use lr_unified = 10^x where x \u2208 [-4, 2]</li> <li>Default scaling is recommended for most use cases</li> </ul> See Also <ul> <li>PyTorch Optimizer Documentation: https://pytorch.org/docs/stable/optim.html</li> <li>LinearRegressor.get_optimizer(): Convenience method using this mapping</li> </ul> Source code in <code>src/spotoptim/utils/mapping.py</code> <pre><code>def map_lr(\n    lr_unified: float, optimizer_name: str, use_default_scale: bool = True\n) -&gt; float:\n    \"\"\"Map a unified learning rate to an optimizer-specific learning rate.\n\n    This function provides a unified interface for learning rates across different\n    PyTorch optimizers. Different optimizers operate on vastly different learning\n    rate scales (e.g., SGD typically uses lr ~ 0.01-0.1, while Adam uses lr ~ 0.0001-0.001).\n\n    The mapping uses the default learning rates from PyTorch as scaling factors,\n    allowing users to work with a normalized learning rate scale where 1.0 represents\n    the optimizer's default learning rate.\n\n    Args:\n        lr_unified (float): Unified learning rate multiplier. A value of 1.0 corresponds\n            to the optimizer's default learning rate. Values &lt; 1.0 reduce the learning\n            rate, values &gt; 1.0 increase it. Typical range: [0.001, 100.0].\n        optimizer_name (str): Name of the PyTorch optimizer. Must be one of:\n            \"Adadelta\", \"Adagrad\", \"Adam\", \"AdamW\", \"SparseAdam\", \"Adamax\",\n            \"ASGD\", \"LBFGS\", \"NAdam\", \"RAdam\", \"RMSprop\", \"Rprop\", \"SGD\".\n        use_default_scale (bool, optional): Whether to scale by the optimizer's default\n            learning rate. If True (default), lr_unified is multiplied by the default\n            lr. If False, returns lr_unified directly. Defaults to True.\n\n    Returns:\n        float: The optimizer-specific learning rate.\n\n    Raises:\n        ValueError: If optimizer_name is not supported.\n        ValueError: If lr_unified is not positive.\n\n    Examples:\n        Using unified learning rate with default scaling:\n\n        &gt;&gt;&gt; # Get Adam's default learning rate (0.001)\n        &gt;&gt;&gt; lr = map_lr(1.0, \"Adam\")\n        &gt;&gt;&gt; print(lr)\n        0.001\n\n        &gt;&gt;&gt; # Get half of SGD's default learning rate (0.01 / 2 = 0.005)\n        &gt;&gt;&gt; lr = map_lr(0.5, \"SGD\")\n        &gt;&gt;&gt; print(lr)\n        0.005\n\n        &gt;&gt;&gt; # Get 10x RMSprop's default learning rate (0.01 * 10 = 0.1)\n        &gt;&gt;&gt; lr = map_lr(10.0, \"RMSprop\")\n        &gt;&gt;&gt; print(lr)\n        0.1\n\n        Using unified learning rate without scaling:\n\n        &gt;&gt;&gt; # Use lr_unified directly (0.01)\n        &gt;&gt;&gt; lr = map_lr(0.01, \"Adam\", use_default_scale=False)\n        &gt;&gt;&gt; print(lr)\n        0.01\n\n        Practical example with model training:\n\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; import torch.nn as nn\n        &gt;&gt;&gt; from spotoptim.nn.linear_regressor import LinearRegressor\n        &gt;&gt;&gt; from spotoptim.utils.mapping import map_lr\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; model = LinearRegressor(input_dim=10, output_dim=1)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use unified learning rate of 0.5 for Adam (gives 0.0005)\n        &gt;&gt;&gt; lr_adam = map_lr(0.5, \"Adam\")\n        &gt;&gt;&gt; optimizer_adam = model.get_optimizer(\"Adam\", lr=lr_adam)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use same unified learning rate for SGD (gives 0.005)\n        &gt;&gt;&gt; lr_sgd = map_lr(0.5, \"SGD\")\n        &gt;&gt;&gt; optimizer_sgd = model.get_optimizer(\"SGD\", lr=lr_sgd)\n\n        Hyperparameter optimization example:\n\n        &gt;&gt;&gt; from spotoptim import SpotOptim\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; def train_model(X):\n        ...     results = []\n        ...     for params in X:\n        ...         lr_unified = 10 ** params[0]  # Log scale: [-4, 0]\n        ...         optimizer_name = params[1]     # Factor variable\n        ...\n        ...         # Map to optimizer-specific learning rate\n        ...         lr_actual = map_lr(lr_unified, optimizer_name)\n        ...\n        ...         # Train model with this configuration\n        ...         # ... training code ...\n        ...         results.append(test_loss)\n        ...     return np.array(results)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; optimizer = SpotOptim(\n        ...     fun=train_model,\n        ...     bounds=[(-4, 0), (\"Adam\", \"SGD\", \"RMSprop\")],\n        ...     var_type=[\"num\", \"factor\"],\n        ...     max_iter=30\n        ... )\n\n    Note:\n        - The unified learning rate provides a normalized scale across optimizers\n        - A value of 1.0 always corresponds to the optimizer's PyTorch default\n        - This enables fair comparison when optimizing over different optimizers\n        - For log-scale optimization, use lr_unified = 10^x where x \u2208 [-4, 2]\n        - Default scaling is recommended for most use cases\n\n    See Also:\n        - PyTorch Optimizer Documentation: https://pytorch.org/docs/stable/optim.html\n        - LinearRegressor.get_optimizer(): Convenience method using this mapping\n    \"\"\"\n    if lr_unified &lt;= 0:\n        raise ValueError(\n            f\"Learning rate must be positive, got {lr_unified}. \"\n            f\"Typical range is [0.001, 100.0] for unified scale.\"\n        )\n\n    if optimizer_name not in OPTIMIZER_DEFAULT_LR:\n        supported = \", \".join(sorted(OPTIMIZER_DEFAULT_LR.keys()))\n        raise ValueError(\n            f\"Optimizer '{optimizer_name}' not supported. \"\n            f\"Supported optimizers: {supported}\"\n        )\n\n    if not use_default_scale:\n        return lr_unified\n\n    # Scale by optimizer's default learning rate\n    default_lr = OPTIMIZER_DEFAULT_LR[optimizer_name]\n    return lr_unified * default_lr\n</code></pre>"},{"location":"reference/spotoptim/utils/pca/","title":"pca","text":""},{"location":"reference/spotoptim/utils/pca/#spotoptim.utils.pca.get_loading_scores","title":"<code>get_loading_scores(pca, feature_names)</code>","text":"<p>Computes the loading scores matrix for Principal Component Analysis (PCA).</p> <p>Creates and returns a DataFrame showing how each original feature contributes to each principal component.</p> <p>Parameters:</p> Name Type Description Default <code>pca</code> <code>PCA</code> <p>Fitted PCA object containing the components_ attribute with the principal components.</p> required <code>feature_names</code> <code>list - like</code> <p>Names of the original features, must match the order of features used in PCA fitting.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing the loading scores matrix with features as rows and principal components as columns.</p> Example <p>from sklearn.decomposition import PCA from sklearn.datasets import load_iris from spotpython.utils.pca import print_loading_scores,</p> Source code in <code>src/spotoptim/utils/pca.py</code> <pre><code>def get_loading_scores(pca, feature_names) -&gt; pd.DataFrame:\n    \"\"\"Computes the loading scores matrix for Principal Component Analysis (PCA).\n\n    Creates and returns a DataFrame showing how each original feature contributes\n    to each principal component.\n\n    Args:\n        pca (sklearn.decomposition.PCA): Fitted PCA object containing the components_\n            attribute with the principal components.\n        feature_names (list-like): Names of the original features, must match the\n            order of features used in PCA fitting.\n\n    Returns:\n        pd.DataFrame: DataFrame containing the loading scores matrix with features\n            as rows and principal components as columns.\n\n    Example:\n        &gt;&gt;&gt; from sklearn.decomposition import PCA\n        &gt;&gt;&gt; from sklearn.datasets import load_iris\n        &gt;&gt;&gt; from spotpython.utils.pca import print_loading_scores,\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load and prepare iris dataset\n        &gt;&gt;&gt; iris = load_iris()\n        &gt;&gt;&gt; X = iris.data\n        &gt;&gt;&gt; feature_names = iris.feature_names\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit PCA\n        &gt;&gt;&gt; pca = PCA()\n        &gt;&gt;&gt; pca.fit(X)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Print loading scores\n        &gt;&gt;&gt; scores_df = print_loading_scores(pca, feature_names)\n        &gt;&gt;&gt; print(scores_df)\n    \"\"\"\n    loading_scores = pd.DataFrame(\n        pca.components_.T,\n        columns=[f\"PC{i+1}\" for i in range(pca.n_components_)],\n        index=feature_names,\n    )\n    return loading_scores\n</code></pre>"},{"location":"reference/spotoptim/utils/pca/#spotoptim.utils.pca.get_loading_scores--load-and-prepare-iris-dataset","title":"Load and prepare iris dataset","text":"<p>iris = load_iris() X = iris.data feature_names = iris.feature_names</p>"},{"location":"reference/spotoptim/utils/pca/#spotoptim.utils.pca.get_loading_scores--fit-pca","title":"Fit PCA","text":"<p>pca = PCA() pca.fit(X)</p>"},{"location":"reference/spotoptim/utils/pca/#spotoptim.utils.pca.get_loading_scores--print-loading-scores","title":"Print loading scores","text":"<p>scores_df = print_loading_scores(pca, feature_names) print(scores_df)</p>"},{"location":"reference/spotoptim/utils/pca/#spotoptim.utils.pca.get_pca","title":"<code>get_pca(df, n_components=3)</code>","text":"<p>Scale the numeric data and perform PCA.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>n_components</code> <code>int</code> <p>Number of principal components to compute. Defaults to 3.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <ul> <li>pca (PCA): Fitted PCA object.</li> <li>scaled_data (np.ndarray): Scaled numeric data.</li> <li>feature_names (pd.Index): Names of the numeric features.</li> <li>sample_names (pd.Index): Index of the samples.</li> <li>pca_data (np.ndarray): PCA-transformed data.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotpython.utils.pca import get_pca\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"A\": [1, 2, 3],\n...     \"B\": [4, 5, 6],\n...     \"C\": [\"x\", \"y\", \"z\"]  # Non-numeric column will be ignored\n... })\n&gt;&gt;&gt; pca, scaled_data, feature_names, sample_names, pca_data = get_pca(df)\n&gt;&gt;&gt; print(feature_names)\nIndex(['A', 'B'], dtype='object')\n&gt;&gt;&gt; print(pca_data.shape)\n(3, 2)\n</code></pre> Source code in <code>src/spotoptim/utils/pca.py</code> <pre><code>def get_pca(df, n_components=3) -&gt; tuple:\n    \"\"\"\n    Scale the numeric data and perform PCA.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        n_components (int):\n            Number of principal components to compute.\n            Defaults to 3.\n\n    Returns:\n        tuple:\n            - pca (PCA): Fitted PCA object.\n            - scaled_data (np.ndarray): Scaled numeric data.\n            - feature_names (pd.Index): Names of the numeric features.\n            - sample_names (pd.Index): Index of the samples.\n            - pca_data (np.ndarray): PCA-transformed data.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotpython.utils.pca import get_pca\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"A\": [1, 2, 3],\n        ...     \"B\": [4, 5, 6],\n        ...     \"C\": [\"x\", \"y\", \"z\"]  # Non-numeric column will be ignored\n        ... })\n        &gt;&gt;&gt; pca, scaled_data, feature_names, sample_names, pca_data = get_pca(df)\n        &gt;&gt;&gt; print(feature_names)\n        Index(['A', 'B'], dtype='object')\n        &gt;&gt;&gt; print(pca_data.shape)\n        (3, 2)\n    \"\"\"\n    numeric_df = df.select_dtypes(include=[np.number])\n    feature_names = numeric_df.columns\n    pca = PCA(n_components=n_components)\n    pca_scores = pca.fit_transform(numeric_df)\n    pca_columns = [f\"PC{i+1}\" for i in range(pca_scores.shape[1])]\n    df_pca_components = pd.DataFrame(data=pca_scores, columns=pca_columns)\n    sample_names = df.index\n    return pca, pca_scores, feature_names, sample_names, df_pca_components\n</code></pre>"},{"location":"reference/spotoptim/utils/pca/#spotoptim.utils.pca.get_pca_topk","title":"<code>get_pca_topk(pca, feature_names, k=10)</code>","text":"<p>Identify the top k features that have the strongest influence on PC1 and PC2.</p> <p>This function analyzes the loading scores (coefficients) of the first two principal components to determine which original features contribute most strongly to these components. The absolute values of the loading scores are used to rank feature importance.</p> <p>Parameters:</p> Name Type Description Default <code>pca</code> <code>PCA</code> <p>Fitted PCA object containing the components_ attribute with the principal components.</p> required <code>feature_names</code> <code>list - like</code> <p>Names of the original features, must match the order of features used in PCA fitting.</p> required <code>k</code> <code>int</code> <p>Number of top features to select for each principal component. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing two lists: - list[str]: Names of the k features most influential on PC1 - list[str]: Names of the k features most influential on PC2</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.decomposition import PCA\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from spotpython.utils.pca import get_pca_topk\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare the iris dataset\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X = iris.data\n&gt;&gt;&gt; feature_names = iris.feature_names\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit PCA\n&gt;&gt;&gt; pca = PCA()\n&gt;&gt;&gt; pca.fit(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get top 2 most influential features for PC1 and PC2\n&gt;&gt;&gt; top_pc1, top_pc2 = get_pca_topk(pca,\n...                                 feature_names=feature_names,\n...                                 k=2)\n&gt;&gt;&gt; print(\"Top PC1 features:\", top_pc1)\n&gt;&gt;&gt; print(\"Top PC2 features:\", top_pc2)\n</code></pre> Note <ul> <li>The function assumes that PCA has been fitted on standardized data</li> <li>The length of feature_names must match the number of features in the PCA input</li> <li>k should not exceed the total number of features</li> </ul> Source code in <code>src/spotoptim/utils/pca.py</code> <pre><code>def get_pca_topk(pca, feature_names, k=10) -&gt; tuple:\n    \"\"\"Identify the top k features that have the strongest influence on PC1 and PC2.\n\n    This function analyzes the loading scores (coefficients) of the first two principal\n    components to determine which original features contribute most strongly to these\n    components. The absolute values of the loading scores are used to rank feature\n    importance.\n\n    Args:\n        pca (sklearn.decomposition.PCA): Fitted PCA object containing the components_\n            attribute with the principal components.\n        feature_names (list-like): Names of the original features, must match the\n            order of features used in PCA fitting.\n        k (int, optional): Number of top features to select for each principal\n            component. Defaults to 10.\n\n    Returns:\n        tuple: A tuple containing two lists:\n            - list[str]: Names of the k features most influential on PC1\n            - list[str]: Names of the k features most influential on PC2\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.decomposition import PCA\n        &gt;&gt;&gt; from sklearn.datasets import load_iris\n        &gt;&gt;&gt; from spotpython.utils.pca import get_pca_topk\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load and prepare the iris dataset\n        &gt;&gt;&gt; iris = load_iris()\n        &gt;&gt;&gt; X = iris.data\n        &gt;&gt;&gt; feature_names = iris.feature_names\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit PCA\n        &gt;&gt;&gt; pca = PCA()\n        &gt;&gt;&gt; pca.fit(X)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get top 2 most influential features for PC1 and PC2\n        &gt;&gt;&gt; top_pc1, top_pc2 = get_pca_topk(pca,\n        ...                                 feature_names=feature_names,\n        ...                                 k=2)\n        &gt;&gt;&gt; print(\"Top PC1 features:\", top_pc1)\n        &gt;&gt;&gt; print(\"Top PC2 features:\", top_pc2)\n\n    Note:\n        - The function assumes that PCA has been fitted on standardized data\n        - The length of feature_names must match the number of features in the PCA input\n        - k should not exceed the total number of features\n    \"\"\"\n    loading_scores_pc1 = pd.Series(pca.components_[0], index=feature_names)\n    loading_scores_pc2 = pd.Series(pca.components_[1], index=feature_names)\n\n    sorted_loading_scores_pc1 = loading_scores_pc1.abs().sort_values(ascending=False)\n    sorted_loading_scores_pc2 = loading_scores_pc2.abs().sort_values(ascending=False)\n\n    top_k_features_pc1 = sorted_loading_scores_pc1.head(k).index.tolist()\n    top_k_features_pc2 = sorted_loading_scores_pc2.head(k).index.tolist()\n\n    return top_k_features_pc1, top_k_features_pc2\n</code></pre>"},{"location":"reference/spotoptim/utils/pca/#spotoptim.utils.pca.plot_loading_scores","title":"<code>plot_loading_scores(loading_scores, figsize=(12, 8))</code>","text":"<p>Creates a heatmap visualization of PCA loading scores.</p> <p>Generates a heatmap showing the relationship between original features and principal components, with color intensity indicating the strength and direction of the relationship.</p> <p>Parameters:</p> Name Type Description Default <code>loading_scores</code> <code>DataFrame</code> <p>DataFrame containing the loading scores matrix with features as rows and principal components as columns.</p> required <code>figsize</code> <code>tuple</code> <p>Size of the figure as (width, height). Defaults to (12, 8).</p> <code>(12, 8)</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The function creates and displays a matplotlib plot.</p> Example <p>from sklearn.decomposition import PCA from sklearn.datasets import load_iris from spotpython.utils.pca import print_loading_scores, plot_loading_scores</p> Source code in <code>src/spotoptim/utils/pca.py</code> <pre><code>def plot_loading_scores(loading_scores, figsize=(12, 8)) -&gt; None:\n    \"\"\"Creates a heatmap visualization of PCA loading scores.\n\n    Generates a heatmap showing the relationship between original features and\n    principal components, with color intensity indicating the strength and\n    direction of the relationship.\n\n    Args:\n        loading_scores (pd.DataFrame): DataFrame containing the loading scores\n            matrix with features as rows and principal components as columns.\n        figsize (tuple, optional): Size of the figure as (width, height).\n            Defaults to (12, 8).\n\n    Returns:\n        None: The function creates and displays a matplotlib plot.\n\n    Example:\n        &gt;&gt;&gt; from sklearn.decomposition import PCA\n        &gt;&gt;&gt; from sklearn.datasets import load_iris\n        &gt;&gt;&gt; from spotpython.utils.pca import print_loading_scores, plot_loading_scores\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load and prepare iris dataset\n        &gt;&gt;&gt; iris = load_iris()\n        &gt;&gt;&gt; X = iris.data\n        &gt;&gt;&gt; feature_names = iris.feature_names\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit PCA and get loading scores\n        &gt;&gt;&gt; pca = PCA()\n        &gt;&gt;&gt; pca.fit(X)\n        &gt;&gt;&gt; scores_df = print_loading_scores(pca, feature_names)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create heatmap\n        &gt;&gt;&gt; plot_loading_scores(scores_df, figsize=(10, 6))\n    \"\"\"\n    plt.figure(figsize=figsize)\n    sns.heatmap(\n        loading_scores,\n        annot=True,\n        fmt=\".2f\",\n        cmap=\"coolwarm\",\n        cbar_kws={\"label\": \"Loading Score\"},\n        linewidths=0.5,\n    )\n    plt.title(\"PCA Loading Scores Heatmap\")\n    plt.xlabel(\"Principal Components\")\n    plt.ylabel(\"Original Features\")\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/spotoptim/utils/pca/#spotoptim.utils.pca.plot_loading_scores--load-and-prepare-iris-dataset","title":"Load and prepare iris dataset","text":"<p>iris = load_iris() X = iris.data feature_names = iris.feature_names</p>"},{"location":"reference/spotoptim/utils/pca/#spotoptim.utils.pca.plot_loading_scores--fit-pca-and-get-loading-scores","title":"Fit PCA and get loading scores","text":"<p>pca = PCA() pca.fit(X) scores_df = print_loading_scores(pca, feature_names)</p>"},{"location":"reference/spotoptim/utils/pca/#spotoptim.utils.pca.plot_loading_scores--create-heatmap","title":"Create heatmap","text":"<p>plot_loading_scores(scores_df, figsize=(10, 6))</p>"},{"location":"reference/spotoptim/utils/pca/#spotoptim.utils.pca.plot_pca1vs2","title":"<code>plot_pca1vs2(pca, pca_data, df_name='', figsize=(12, 6))</code>","text":"<p>Create a scatter plot of the first two principal components from PCA.</p> <p>This function visualizes the first two principal components (PC1 vs PC2) from a PCA analysis, creating a scatter plot where each point represents a sample in the transformed space. The percentage of variance explained by each component is shown on the axes.</p> <p>Parameters:</p> Name Type Description Default <code>pca</code> <code>PCA</code> <p>Fitted PCA object containing the explained variance ratios and components.</p> required <code>pca_data</code> <code>array - like</code> <p>PCA-transformed data, where each row represents a sample and each column represents a principal component.</p> required <code>df_name</code> <code>str</code> <p>Name of the dataset to be displayed in the plot title. Defaults to empty string.</p> <code>''</code> <code>figsize</code> <code>tuple</code> <p>Size of the figure as (width, height). Defaults to (12, 6).</p> <code>(12, 6)</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The function creates and displays a matplotlib plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.decomposition import PCA\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from spotpython.utils.pca import plot_pca1vs2\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load and prepare the iris dataset\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X = iris.data\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit PCA and transform the data\n&gt;&gt;&gt; pca = PCA()\n&gt;&gt;&gt; pca_data = pca.fit_transform(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create PCA scatter plot\n&gt;&gt;&gt; plot_pca1vs2(pca,\n...             pca_data,\n...             df_name=\"Iris Dataset\",\n...             figsize=(10, 5))\n</code></pre> Note <ul> <li>The function assumes that the input data has at least two principal components</li> <li>Sample names are taken from the index of the created DataFrame</li> <li>The percentage of variance explained is rounded to 1 decimal place</li> </ul> Source code in <code>src/spotoptim/utils/pca.py</code> <pre><code>def plot_pca1vs2(pca, pca_data, df_name=\"\", figsize=(12, 6)) -&gt; None:\n    \"\"\"Create a scatter plot of the first two principal components from PCA.\n\n    This function visualizes the first two principal components (PC1 vs PC2) from a PCA analysis,\n    creating a scatter plot where each point represents a sample in the transformed space.\n    The percentage of variance explained by each component is shown on the axes.\n\n    Args:\n        pca (sklearn.decomposition.PCA): Fitted PCA object containing the explained\n            variance ratios and components.\n        pca_data (array-like): PCA-transformed data, where each row represents a sample\n            and each column represents a principal component.\n        df_name (str, optional): Name of the dataset to be displayed in the plot title.\n            Defaults to empty string.\n        figsize (tuple, optional): Size of the figure as (width, height).\n            Defaults to (12, 6).\n\n    Returns:\n        None: The function creates and displays a matplotlib plot.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.decomposition import PCA\n        &gt;&gt;&gt; from sklearn.datasets import load_iris\n        &gt;&gt;&gt; from spotpython.utils.pca import plot_pca1vs2\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load and prepare the iris dataset\n        &gt;&gt;&gt; iris = load_iris()\n        &gt;&gt;&gt; X = iris.data\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit PCA and transform the data\n        &gt;&gt;&gt; pca = PCA()\n        &gt;&gt;&gt; pca_data = pca.fit_transform(X)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create PCA scatter plot\n        &gt;&gt;&gt; plot_pca1vs2(pca,\n        ...             pca_data,\n        ...             df_name=\"Iris Dataset\",\n        ...             figsize=(10, 5))\n\n    Note:\n        - The function assumes that the input data has at least two principal components\n        - Sample names are taken from the index of the created DataFrame\n        - The percentage of variance explained is rounded to 1 decimal place\n    \"\"\"\n    pca_df = pd.DataFrame(\n        pca_data, columns=[\"PC\" + str(i + 1) for i in range(pca_data.shape[1])]\n    )\n\n    plt.figure(figsize=figsize)\n    plt.scatter(pca_df[\"PC1\"], pca_df[\"PC2\"])\n    for sample in pca_df.index:\n        plt.annotate(sample, (pca_df.PC1.loc[sample], pca_df.PC2.loc[sample]))\n    plt.title(f\"PCA Graph. {df_name}\")\n    plt.xlabel(f\"PC1 - {np.round(pca.explained_variance_ratio_[0] * 100, 1)}%\")\n    plt.ylabel(f\"PC2 - {np.round(pca.explained_variance_ratio_[1] * 100, 1)}%\")\n    plt.grid(True)\n    plt.show()\n</code></pre>"},{"location":"reference/spotoptim/utils/pca/#spotoptim.utils.pca.plot_pca_scree","title":"<code>plot_pca_scree(pca, df_name='', max_scree=None, figsize=(12, 6))</code>","text":"<p>Plot the scree plot for Principal Component Analysis (PCA).</p> <p>A scree plot shows the percentage of variance explained by each principal component in descending order. It helps in determining the optimal number of components to retain.</p> <p>Parameters:</p> Name Type Description Default <code>pca</code> <code>PCA</code> <p>Fitted PCA object containing the explained variance ratios.</p> required <code>df_name</code> <code>str</code> <p>Name of the dataset to be displayed in the plot title. Defaults to empty string.</p> <code>''</code> <code>max_scree</code> <code>int</code> <p>Maximum number of principal components to plot. If None, all components are plotted. Defaults to None.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Size of the figure as (width, height). Defaults to (12, 6).</p> <code>(12, 6)</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The function creates and displays a matplotlib plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.decomposition import PCA\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from spotpython.utils.pca import plot_pca_scree\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load iris dataset\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X = iris.data\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Fit PCA\n&gt;&gt;&gt; pca = PCA()\n&gt;&gt;&gt; pca.fit(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create scree plot\n&gt;&gt;&gt; plot_pca_scree(pca,\n...                df_name=\"Iris Dataset\",\n...                max_scree=4,\n...                figsize=(10, 5))\n</code></pre> Source code in <code>src/spotoptim/utils/pca.py</code> <pre><code>def plot_pca_scree(pca, df_name=\"\", max_scree=None, figsize=(12, 6)) -&gt; None:\n    \"\"\"Plot the scree plot for Principal Component Analysis (PCA).\n\n    A scree plot shows the percentage of variance explained by each principal\n    component in descending order. It helps in determining the optimal number\n    of components to retain.\n\n    Args:\n        pca (sklearn.decomposition.PCA): Fitted PCA object containing the\n            explained variance ratios.\n        df_name (str, optional): Name of the dataset to be displayed in the plot title.\n            Defaults to empty string.\n        max_scree (int, optional): Maximum number of principal components to plot.\n            If None, all components are plotted. Defaults to None.\n        figsize (tuple, optional): Size of the figure as (width, height).\n            Defaults to (12, 6).\n\n    Returns:\n        None: The function creates and displays a matplotlib plot.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sklearn.decomposition import PCA\n        &gt;&gt;&gt; from sklearn.datasets import load_iris\n        &gt;&gt;&gt; from spotpython.utils.pca import plot_pca_scree\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load iris dataset\n        &gt;&gt;&gt; iris = load_iris()\n        &gt;&gt;&gt; X = iris.data\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Fit PCA\n        &gt;&gt;&gt; pca = PCA()\n        &gt;&gt;&gt; pca.fit(X)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create scree plot\n        &gt;&gt;&gt; plot_pca_scree(pca,\n        ...                df_name=\"Iris Dataset\",\n        ...                max_scree=4,\n        ...                figsize=(10, 5))\n    \"\"\"\n    per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)\n    full_labels = [\"PC\" + str(x) for x in range(1, len(per_var) + 1)]\n\n    # Limit the number of PCs in the scree plot\n    if max_scree is not None:\n        per_var = per_var[:max_scree]\n        scree_labels = full_labels[:max_scree]\n    else:\n        scree_labels = full_labels\n\n    plt.figure(figsize=figsize)\n    plt.plot(range(1, len(per_var) + 1), per_var, marker=\"o\", linestyle=\"--\")\n    plt.xticks(range(1, len(per_var) + 1), scree_labels)\n    plt.ylabel(\"Percentage of Explained Variance\")\n    plt.xlabel(\"Principal Component\")\n    plt.title(f\"Scree Plot. {df_name}\")\n    plt.grid(True)\n    plt.show()\n</code></pre>"},{"location":"reference/spotoptim/utils/scaler/","title":"scaler","text":""},{"location":"reference/spotoptim/utils/scaler/#spotoptim.utils.scaler.TorchStandardScaler","title":"<code>TorchStandardScaler</code>","text":"<p>A class for scaling data using standardization with torch tensors. This scaler computes the mean and standard deviation on a dataset so that it can later be used to scale the data using the computed mean and standard deviation.</p> <p>Attributes:</p> Name Type Description <code>mean</code> <code>Tensor</code> <p>The mean value computed over the fitted data.</p> <code>std</code> <code>Tensor</code> <p>The standard deviation computed over the fitted data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from spotoptim.utils.scaler import TorchStandardScaler\n&gt;&gt;&gt; # Create a sample tensor\n&gt;&gt;&gt; tensor = torch.rand((10, 3))  # Random tensor with shape (10, 3)\n&gt;&gt;&gt; scaler = TorchStandardScaler()\n&gt;&gt;&gt; # Fit the scaler to the data\n&gt;&gt;&gt; scaler.fit(tensor)\n&gt;&gt;&gt; # Transform the data using the fitted scaler\n&gt;&gt;&gt; transformed_tensor = scaler.transform(tensor)\n&gt;&gt;&gt; print(transformed_tensor.shape)\ntorch.Size([10, 3])\n&gt;&gt;&gt; # Using fit_transform method to fit and transform in one step\n&gt;&gt;&gt; another_tensor = torch.rand((10, 3))\n&gt;&gt;&gt; scaled_tensor = scaler.fit_transform(another_tensor)\n</code></pre> Source code in <code>src/spotoptim/utils/scaler.py</code> <pre><code>class TorchStandardScaler:\n    \"\"\"\n    A class for scaling data using standardization with torch tensors.\n    This scaler computes the mean and standard deviation on a dataset so that\n    it can later be used to scale the data using the computed mean and standard deviation.\n\n    Attributes:\n        mean (torch.Tensor): The mean value computed over the fitted data.\n        std (torch.Tensor): The standard deviation computed over the fitted data.\n\n    Examples:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from spotoptim.utils.scaler import TorchStandardScaler\n        &gt;&gt;&gt; # Create a sample tensor\n        &gt;&gt;&gt; tensor = torch.rand((10, 3))  # Random tensor with shape (10, 3)\n        &gt;&gt;&gt; scaler = TorchStandardScaler()\n        &gt;&gt;&gt; # Fit the scaler to the data\n        &gt;&gt;&gt; scaler.fit(tensor)\n        &gt;&gt;&gt; # Transform the data using the fitted scaler\n        &gt;&gt;&gt; transformed_tensor = scaler.transform(tensor)\n        &gt;&gt;&gt; print(transformed_tensor.shape)\n        torch.Size([10, 3])\n        &gt;&gt;&gt; # Using fit_transform method to fit and transform in one step\n        &gt;&gt;&gt; another_tensor = torch.rand((10, 3))\n        &gt;&gt;&gt; scaled_tensor = scaler.fit_transform(another_tensor)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the TorchStandardScaler class without any pre-defined mean and std.\n        \"\"\"\n        self.mean = None\n        self.std = None\n\n    def fit(self, x: torch.Tensor) -&gt; None:\n        \"\"\"\n        Compute the mean and standard deviation of the input tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor, expected shape [n_samples, n_features]\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n        \"\"\"\n        if not torch.is_tensor(x):\n            raise TypeError(\"Input should be a torch tensor\")\n        self.mean = x.mean(dim=0, keepdim=True)\n        self.std = x.std(dim=0, unbiased=False, keepdim=True)\n\n    def transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Scale the input tensor using the computed mean and standard deviation.\n\n        Args:\n            x (torch.Tensor): The input tensor to be transformed, expected shape [n_samples, n_features]\n\n        Returns:\n            torch.Tensor: The scaled tensor.\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n            RuntimeError: If the scaler has not been fitted before transforming data.\n        \"\"\"\n        if not torch.is_tensor(x):\n            raise TypeError(\"Input should be a torch tensor\")\n        if self.mean is None or self.std is None:\n            raise RuntimeError(\"Must fit scaler before transforming data\")\n        x = (x - self.mean) / (self.std + 1e-7)\n        return x\n\n    def fit_transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Fit the scaler to the input tensor and then scale the tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor, expected shape [n_samples, n_features]\n\n        Returns:\n            torch.Tensor: The scaled tensor.\n\n        Raises:\n            TypeError: If the input is not a torch tensor.\n        \"\"\"\n        self.fit(x)\n        return self.transform(x)\n</code></pre>"},{"location":"reference/spotoptim/utils/scaler/#spotoptim.utils.scaler.TorchStandardScaler.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the TorchStandardScaler class without any pre-defined mean and std.</p> Source code in <code>src/spotoptim/utils/scaler.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the TorchStandardScaler class without any pre-defined mean and std.\n    \"\"\"\n    self.mean = None\n    self.std = None\n</code></pre>"},{"location":"reference/spotoptim/utils/scaler/#spotoptim.utils.scaler.TorchStandardScaler.fit","title":"<code>fit(x)</code>","text":"<p>Compute the mean and standard deviation of the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor, expected shape [n_samples, n_features]</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> Source code in <code>src/spotoptim/utils/scaler.py</code> <pre><code>def fit(self, x: torch.Tensor) -&gt; None:\n    \"\"\"\n    Compute the mean and standard deviation of the input tensor.\n\n    Args:\n        x (torch.Tensor): The input tensor, expected shape [n_samples, n_features]\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n    \"\"\"\n    if not torch.is_tensor(x):\n        raise TypeError(\"Input should be a torch tensor\")\n    self.mean = x.mean(dim=0, keepdim=True)\n    self.std = x.std(dim=0, unbiased=False, keepdim=True)\n</code></pre>"},{"location":"reference/spotoptim/utils/scaler/#spotoptim.utils.scaler.TorchStandardScaler.fit_transform","title":"<code>fit_transform(x)</code>","text":"<p>Fit the scaler to the input tensor and then scale the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor, expected shape [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The scaled tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> Source code in <code>src/spotoptim/utils/scaler.py</code> <pre><code>def fit_transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Fit the scaler to the input tensor and then scale the tensor.\n\n    Args:\n        x (torch.Tensor): The input tensor, expected shape [n_samples, n_features]\n\n    Returns:\n        torch.Tensor: The scaled tensor.\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n    \"\"\"\n    self.fit(x)\n    return self.transform(x)\n</code></pre>"},{"location":"reference/spotoptim/utils/scaler/#spotoptim.utils.scaler.TorchStandardScaler.transform","title":"<code>transform(x)</code>","text":"<p>Scale the input tensor using the computed mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to be transformed, expected shape [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The scaled tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a torch tensor.</p> <code>RuntimeError</code> <p>If the scaler has not been fitted before transforming data.</p> Source code in <code>src/spotoptim/utils/scaler.py</code> <pre><code>def transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Scale the input tensor using the computed mean and standard deviation.\n\n    Args:\n        x (torch.Tensor): The input tensor to be transformed, expected shape [n_samples, n_features]\n\n    Returns:\n        torch.Tensor: The scaled tensor.\n\n    Raises:\n        TypeError: If the input is not a torch tensor.\n        RuntimeError: If the scaler has not been fitted before transforming data.\n    \"\"\"\n    if not torch.is_tensor(x):\n        raise TypeError(\"Input should be a torch tensor\")\n    if self.mean is None or self.std is None:\n        raise RuntimeError(\"Must fit scaler before transforming data\")\n    x = (x - self.mean) / (self.std + 1e-7)\n    return x\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/","title":"stats","text":""},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.calculate_outliers","title":"<code>calculate_outliers(series_or_df, irqmultiplier=1.5)</code>","text":"<p>Calculate the number of outliers using the IQR method.</p> <p>Accepts either a pandas Series or a pandas DataFrame. For a DataFrame, counts outliers across all numeric columns and returns the total count.</p> <p>Parameters:</p> Name Type Description Default <code>series_or_df</code> <code>Union[Series, DataFrame]</code> <p>pd.Series or pd.DataFrame containing numeric data.</p> required <code>irqmultiplier</code> <code>float</code> <p>Multiplier for IQR to define fences. Defaults to 1.5.</p> <code>1.5</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of outliers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from spotoptim.utils.stats import calculate_outliers\n&gt;&gt;&gt; s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100])\n&gt;&gt;&gt; calculate_outliers(s)\n1\n</code></pre> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({\n...     'a': [1, 2, 3, 100],\n...     'b': [10, 12, 11, 10]\n... })\n&gt;&gt;&gt; calculate_outliers(df)\n1\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def calculate_outliers(\n    series_or_df: Union[pd.Series, pd.DataFrame], irqmultiplier: float = 1.5\n) -&gt; int:\n    \"\"\"\n    Calculate the number of outliers using the IQR method.\n\n    Accepts either a pandas Series or a pandas DataFrame. For a DataFrame,\n    counts outliers across all numeric columns and returns the total count.\n\n    Args:\n        series_or_df: pd.Series or pd.DataFrame containing numeric data.\n        irqmultiplier (float, optional): Multiplier for IQR to define fences. Defaults to 1.5.\n\n    Returns:\n        int: The number of outliers.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from spotoptim.utils.stats import calculate_outliers\n        &gt;&gt;&gt; s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100])\n        &gt;&gt;&gt; calculate_outliers(s)\n        1\n\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'a': [1, 2, 3, 100],\n        ...     'b': [10, 12, 11, 10]\n        ... })\n        &gt;&gt;&gt; calculate_outliers(df)\n        1\n    \"\"\"\n    if isinstance(series_or_df, pd.Series):\n        s = series_or_df.dropna()\n        q1 = s.quantile(0.25)\n        q3 = s.quantile(0.75)\n        iqr = q3 - q1\n        lower_fence = q1 - irqmultiplier * iqr\n        upper_fence = q3 + irqmultiplier * iqr\n        return int(s[(s &lt; lower_fence) | (s &gt; upper_fence)].shape[0])\n\n    if isinstance(series_or_df, pd.DataFrame):\n        df = series_or_df.select_dtypes(include=[\"number\"]).dropna(how=\"all\")\n        total_outliers = 0\n        for col in df.columns:\n            s = df[col].dropna()\n            if s.empty:\n                continue\n            q1 = s.quantile(0.25)\n            q3 = s.quantile(0.75)\n            iqr = q3 - q1\n            lower_fence = q1 - irqmultiplier * iqr\n            upper_fence = q3 + irqmultiplier * iqr\n            total_outliers += int(s[(s &lt; lower_fence) | (s &gt; upper_fence)].shape[0])\n        return total_outliers\n\n    raise TypeError(\"Input must be a pandas Series or DataFrame\")\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.compute_coefficients_table","title":"<code>compute_coefficients_table(model, X_encoded, y, vif_table=None)</code>","text":"Compute a coefficients table containing <ol> <li>Variable name</li> <li>Zero-order correlation</li> <li>Partial correlation</li> <li>Semipartial (part) correlation</li> <li>Tolerance (1 / VIF)</li> <li>VIF</li> </ol> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>RegressionResultsWrapper</code> <p>A fitted OLS model from statsmodels.</p> required <code>X_encoded</code> <code>DataFrame</code> <p>The DataFrame used to fit the model, including \u2018const\u2019.</p> required <code>y</code> <code>Series</code> <p>Dependent variable used in fitting the model.</p> required <code>vif_table</code> <code>DataFrame</code> <p>A DataFrame with columns [\u201cfeature\u201d, \u201cVIF\u201d] for each column in X_encoded (typ. from statsmodels.stats.outliers_influence.variance_inflation_factor). Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame with columns: - \u201cVariable\u201d - \u201cZero-Order r\u201d - \u201cPartial r\u201d - \u201cSemipartial r\u201d - \u201cTolerance\u201d - \u201cVIF\u201d</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import compute_coefficients_table\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import statsmodels.api as sm\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; X = sm.add_constant(data)\n&gt;&gt;&gt; model = sm.OLS(y, X).fit()\n&gt;&gt;&gt; vif_table = pd.DataFrame({\n...     'feature': ['x1', 'x2', 'x3'],\n...     'VIF': [1, 2, 3]\n... })\n&gt;&gt;&gt; compute_coefficients_table(model, data, y, vif_table)\n   Variable  Zero-Order r  Partial r  Semipartial r  Tolerance  VIF\n0       x1           0.0        0.0            0.0        1.0  1.0\n1       x2           0.0        0.0            0.0        0.5  2.0\n2       x3           0.0        0.0            0.0        0.333333  3.0\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def compute_coefficients_table(model, X_encoded, y, vif_table=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute a coefficients table containing:\n      1. Variable name\n      2. Zero-order correlation\n      3. Partial correlation\n      4. Semipartial (part) correlation\n      5. Tolerance (1 / VIF)\n      6. VIF\n\n    Args:\n        model (statsmodels.regression.linear_model.RegressionResultsWrapper):\n            A fitted OLS model from statsmodels.\n        X_encoded (pd.DataFrame):\n            The DataFrame used to fit the model, including 'const'.\n        y (pd.Series):\n            Dependent variable used in fitting the model.\n        vif_table (pd.DataFrame):\n            A DataFrame with columns [\"feature\", \"VIF\"] for each column in X_encoded\n            (typ. from statsmodels.stats.outliers_influence.variance_inflation_factor).\n            Default is None.\n\n    Returns:\n        pd.DataFrame with columns:\n            - \"Variable\"\n            - \"Zero-Order r\"\n            - \"Partial r\"\n            - \"Semipartial r\"\n            - \"Tolerance\"\n            - \"VIF\"\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import compute_coefficients_table\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import statsmodels.api as sm\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'x1': [1, 2, 3, 4, 5],\n        ...     'x2': [2, 4, 6, 8, 10],\n        ...     'x3': [1, 3, 5, 7, 9]\n        ... })\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; X = sm.add_constant(data)\n        &gt;&gt;&gt; model = sm.OLS(y, X).fit()\n        &gt;&gt;&gt; vif_table = pd.DataFrame({\n        ...     'feature': ['x1', 'x2', 'x3'],\n        ...     'VIF': [1, 2, 3]\n        ... })\n        &gt;&gt;&gt; compute_coefficients_table(model, data, y, vif_table)\n           Variable  Zero-Order r  Partial r  Semipartial r  Tolerance  VIF\n        0       x1           0.0        0.0            0.0        1.0  1.0\n        1       x2           0.0        0.0            0.0        0.5  2.0\n        2       x3           0.0        0.0            0.0        0.333333  3.0\n\n    \"\"\"\n\n    # Full-model R^2 and residual df\n    r2_full = model.rsquared\n\n    # We want to iterate over each predictor except the intercept\n    predictors = [col for col in X_encoded.columns if col != \"const\"]\n\n    results = []\n\n    for var in predictors:\n        # -------------------------------------------------------------------\n        # 1) Zero-order correlation: Pearson correlation of var with y\n        # -------------------------------------------------------------------\n        zero_order_r = X_encoded[var].corr(y)\n\n        # -------------------------------------------------------------------\n        # 2) Partial Correlation &amp; 3) Semipartial Correlation\n        #    We compare a 'full' model vs. a 'reduced' model (without var)\n        # -------------------------------------------------------------------\n        X_reduced = X_encoded.drop(columns=[var])\n        reduced_model = sm.OLS(y, X_reduced).fit()\n        r2_reduced = reduced_model.rsquared\n\n        # The difference in R^2 contributed by this predictor\n        delta_r2 = r2_full - r2_reduced\n\n        # Determine sign from the unstandardized coefficient in the full model\n        coeff_sign = np.sign(model.params.get(var, 0.0))\n\n        # If numeric issues occur (e.g., delta_r2 &lt; 0), set correlations to NaN\n        if delta_r2 &lt;= 0.0 or (1 - r2_reduced) &lt;= 0.0:\n            partial_r = np.nan\n            semipartial_r = np.nan\n        else:\n            # partial correlation\n            # partial_r\u00b2 = (R\u00b2_full - R\u00b2_reduced) / (1 - R\u00b2_reduced)\n            partial_r = coeff_sign * np.sqrt(delta_r2 / (1 - r2_reduced))\n\n            # semipartial correlation (also called part correlation)\n            # semipartial_r\u00b2 = (R\u00b2_full - R\u00b2_reduced)\n            # By definition, semipartial_r = sqrt( delta_r2 ), but we treat R\u00b2 as a fraction\n            # Because the base R\u00b2 is SSR / TSS, so:\n            semipartial_r = coeff_sign * np.sqrt(delta_r2)\n\n        # -------------------------------------------------------------------\n        # 4) Tolerance &amp; 5) VIF\n        # -------------------------------------------------------------------\n        if vif_table is None:\n            vif_table = vif(X_encoded)\n            # results.append({\"Variable\": var, \"Zero-Order r\": zero_order_r, \"Partial r\": partial_r, \"Semipartial r\": semipartial_r})\n        # Get the VIF for this predictor\n        vif_row = vif_table.loc[vif_table[\"feature\"] == var, \"VIF\"]\n        if len(vif_row) == 0:\n            var_vif = np.nan\n        else:\n            var_vif = vif_row.iloc[0]\n        if var_vif &lt;= 0 or np.isnan(var_vif):\n            tolerance = np.nan\n        else:\n            tolerance = 1.0 / var_vif\n        # Collect results\n        results.append(\n            {\n                \"Variable\": var,\n                \"Zero-Order r\": zero_order_r,\n                \"Partial r\": partial_r,\n                \"Semipartial r\": semipartial_r,\n                \"Tolerance\": tolerance,\n                \"VIF\": var_vif,\n            }\n        )\n\n    return pd.DataFrame(results)\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.compute_standardized_betas","title":"<code>compute_standardized_betas(model, X_encoded, y)</code>","text":"<p>Computes standardized (beta) coefficients for a fitted statsmodels OLS model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>RegressionResultsWrapper</code> <p>The fitted OLS model.</p> required <code>X_encoded</code> <code>DataFrame</code> <p>The design matrix of independent variables.</p> required <code>y</code> <code>Series</code> <p>The dependent variable.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: A DataFrame containing the standardized beta coefficients.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import compute_standardized_betas\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import statsmodels.api as sm\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n&gt;&gt;&gt; X = sm.add_constant(data)\n&gt;&gt;&gt; model = sm.OLS(y, X).fit()\n&gt;&gt;&gt; compute_standardized_betas(model, data, y)\n   Variable  Standardized Beta\n0     const           0.000000\n1       x1           0.000000\n2       x2           0.000000\n3       x3           0.000000\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def compute_standardized_betas(model, X_encoded, y) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes standardized (beta) coefficients for a fitted statsmodels OLS model.\n\n    Args:\n        model (statsmodels.regression.linear_model.RegressionResultsWrapper): The fitted OLS model.\n        X_encoded (pandas.DataFrame): The design matrix of independent variables.\n        y (pandas.Series): The dependent variable.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the standardized beta coefficients.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import compute_standardized_betas\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import statsmodels.api as sm\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'x1': [1, 2, 3, 4, 5],\n        ...     'x2': [2, 4, 6, 8, 10],\n        ...     'x3': [1, 3, 5, 7, 9]\n        ... })\n        &gt;&gt;&gt; y = pd.Series([1, 2, 3, 4, 5])\n        &gt;&gt;&gt; X = sm.add_constant(data)\n        &gt;&gt;&gt; model = sm.OLS(y, X).fit()\n        &gt;&gt;&gt; compute_standardized_betas(model, data, y)\n           Variable  Standardized Beta\n        0     const           0.000000\n        1       x1           0.000000\n        2       x2           0.000000\n        3       x3           0.000000\n\n    \"\"\"\n    coeffs_unstd = model.params\n    std_X = X_encoded.drop(columns=[\"const\"], errors=\"ignore\").std()\n    std_y = y.std()\n    beta_std = coeffs_unstd.drop(\"const\", errors=\"ignore\") * (std_X / std_y)\n    beta_std_df = pd.DataFrame(\n        {\"Variable\": beta_std.index, \"Standardized Beta\": beta_std.values}\n    )\n    return beta_std_df\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.condition_index","title":"<code>condition_index(df)</code>","text":"<p>Calculates the Condition Index for a DataFrame to assess multicollinearity.</p> <p>The Condition Index is computed based on the eigenvalues of the covariance matrix of the standardized data. High condition indices suggest potential multicollinearity issues.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame containing the independent variables.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: A DataFrame with the following columns: - \u2018Index\u2019: The index of the eigenvalue. - \u2018Eigenvalue\u2019: The eigenvalue of the covariance matrix. - \u2018Condition Index\u2019: The Condition Index for the eigenvalue.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import condition_index\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; condition_index(data)\n   Index  Eigenvalue  Condition Index\n0      0    1.140000         1.000000\n1      1    0.000000              inf\n2      2    0.002857        20.000000\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def condition_index(df) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculates the Condition Index for a DataFrame to assess multicollinearity.\n\n    The Condition Index is computed based on the eigenvalues of the covariance matrix\n    of the standardized data. High condition indices suggest potential multicollinearity issues.\n\n    Args:\n        df (pandas.DataFrame): A DataFrame containing the independent variables.\n\n    Returns:\n        pandas.DataFrame: A DataFrame with the following columns:\n            - 'Index': The index of the eigenvalue.\n            - 'Eigenvalue': The eigenvalue of the covariance matrix.\n            - 'Condition Index': The Condition Index for the eigenvalue.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import condition_index\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'x1': [1, 2, 3, 4, 5],\n        ...     'x2': [2, 4, 6, 8, 10],\n        ...     'x3': [1, 3, 5, 7, 9]\n        ... })\n        &gt;&gt;&gt; condition_index(data)\n           Index  Eigenvalue  Condition Index\n        0      0    1.140000         1.000000\n        1      1    0.000000              inf\n        2      2    0.002857        20.000000\n    \"\"\"\n    # Standardize the data\n    X = df.values\n    X_centered = X - np.mean(X, axis=0)\n\n    # Compute the covariance matrix\n    covariance_matrix = np.cov(X_centered, rowvar=False)\n\n    # Compute the eigenvalues of the covariance matrix\n    eigenvalues, _ = np.linalg.eigh(covariance_matrix)\n\n    # Handle division by zero for eigenvalues\n    max_eigenvalue = max(eigenvalues)\n    condition_indices = np.array(\n        [np.sqrt(max_eigenvalue / ev) if ev &gt; 0 else np.inf for ev in eigenvalues]\n    )\n\n    # Create a DataFrame for the results\n    condition_index_df = pd.DataFrame(\n        {\n            \"Index\": range(len(eigenvalues)),\n            \"Eigenvalue\": eigenvalues,\n            \"Condition Index\": condition_indices,\n        }\n    )\n\n    return condition_index_df\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.cov_to_cor","title":"<code>cov_to_cor(covariance_matrix)</code>","text":"<p>Convert a covariance matrix to a correlation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>covariance_matrix</code> <code>ndarray</code> <p>A square matrix of covariance values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: A corresponding square matrix of correlation coefficients.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import cov_to_cor\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; cov_matrix = np.array([[1, 0.8], [0.8, 1]])\n&gt;&gt;&gt; cov_to_cor(cov_matrix)\narray([[1. , 0.8],\n       [0.8, 1. ]])\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def cov_to_cor(covariance_matrix) -&gt; np.ndarray:\n    \"\"\"Convert a covariance matrix to a correlation matrix.\n\n    Args:\n        covariance_matrix (numpy.ndarray): A square matrix of covariance values.\n\n    Returns:\n        numpy.ndarray: A corresponding square matrix of correlation coefficients.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import cov_to_cor\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; cov_matrix = np.array([[1, 0.8], [0.8, 1]])\n        &gt;&gt;&gt; cov_to_cor(cov_matrix)\n        array([[1. , 0.8],\n               [0.8, 1. ]])\n    \"\"\"\n    d = np.sqrt(np.diag(covariance_matrix))\n    return covariance_matrix / np.outer(d, d)\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.fit_all_lm","title":"<code>fit_all_lm(basic, xlist, data, remove_na=True)</code>","text":"<p>Fit a linear regression model for all possible combinations of independent variables.</p> <p>Parameters:</p> Name Type Description Default <code>basic</code> <code>str</code> <p>The basic model formula.</p> required <code>xlist</code> <code>list</code> <p>A list of independent variables.</p> required <code>data</code> <code>DataFrame</code> <p>The data frame containing the variables.</p> required <code>remove_na</code> <code>bool</code> <p>Whether to remove missing values from the data frame.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the estimated coefficients, confidence intervals, p-values, AIC values, sample size, and the basic model formula.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import fit_all_lm\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'y': [1, 2, 3],\n&gt;&gt;&gt;     'x1': [4, 5, 6],\n&gt;&gt;&gt;     'x2': [7, 8, 9]\n&gt;&gt;&gt; })\n&gt;&gt;&gt; fit_all_lm(\"y ~ x1\", [\"x2\"], data)\n{'estimate':   variables  estimate  conf_low  conf_high    p         aic  n\n0    basic  1.000000  1.000000   1.000000  0.0  0.000000  3\n1       x2  1.000000  1.000000   1.000000  0.0  0.000000  3}\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def fit_all_lm(basic, xlist, data, remove_na=True) -&gt; dict:\n    \"\"\"Fit a linear regression model for all possible combinations of independent variables.\n\n    Args:\n        basic (str): The basic model formula.\n        xlist (list): A list of independent variables.\n        data (pandas.DataFrame): The data frame containing the variables.\n        remove_na (bool): Whether to remove missing values from the data frame.\n\n    Returns:\n        dict: A dictionary containing the estimated coefficients, confidence intervals,\n            p-values, AIC values, sample size, and the basic model formula.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import fit_all_lm\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        &gt;&gt;&gt;     'y': [1, 2, 3],\n        &gt;&gt;&gt;     'x1': [4, 5, 6],\n        &gt;&gt;&gt;     'x2': [7, 8, 9]\n        &gt;&gt;&gt; })\n        &gt;&gt;&gt; fit_all_lm(\"y ~ x1\", [\"x2\"], data)\n        {'estimate':   variables  estimate  conf_low  conf_high    p         aic  n\n        0    basic  1.000000  1.000000   1.000000  0.0  0.000000  3\n        1       x2  1.000000  1.000000   1.000000  0.0  0.000000  3}\n    \"\"\"\n    # Prepare the data frame\n    data = copy.deepcopy(data)\n    data_cols = get_all_vars_from_formula(basic) + xlist\n    # make sure that no duplicates are present in the data_cols\n    data_cols = list(set(data_cols))\n    data = data[data_cols]\n    if remove_na:\n        data = data.dropna()\n    print(f\"The basic model is: {basic}\")\n    print(\n        f\"The following features will be used for fitting the basic model: {data.columns}\"\n    )\n    mod_0 = ols(basic, data=data).fit()\n    p = mod_0.pvalues.iloc[1]\n    print(f\"p-values: {p}\")\n    estimate = mod_0.params.iloc[1]\n    print(f\"estimate: {estimate}\")\n    conf_int = mod_0.conf_int().iloc[1]\n    print(f\"conf_int: {conf_int}\")\n    aic_value = mod_0.aic\n    print(f\"aic: {aic_value}\")\n    n = len(mod_0.resid)\n    df_0 = pd.DataFrame(\n        [[\"basic\", estimate, conf_int[0], conf_int[1], p, aic_value, n]],\n        columns=[\"variables\", \"estimate\", \"conf_low\", \"conf_high\", \"p\", \"aic\", \"n\"],\n    )\n\n    # All combinations model\n    comb_lst = list(\n        itertools.chain.from_iterable(\n            itertools.combinations(xlist, r) for r in range(1, len(xlist) + 1)\n        )\n    )\n    n_comb = len(comb_lst)\n    # if more than 100 combinations, exit\n    if n_comb &gt; 100:\n        print(f\"Number of combinations is {n_comb}. Exiting.\")\n        return None\n    print(f\"Combinations: {comb_lst}\")\n    models = [\n        ols(f\"{basic} + {' + '.join(comb)}\", data=data).fit() for comb in comb_lst\n    ]\n\n    df_list = []\n    for i, model in enumerate(models):\n        p = model.pvalues.iloc[1]\n        estimate = model.params.iloc[1]\n        conf_int = model.conf_int().iloc[1]\n        aic_value = model.aic\n        n = len(model.resid)\n        comb_str = \", \".join(comb_lst[i])\n        df_list.append([comb_str, estimate, conf_int[0], conf_int[1], p, aic_value, n])\n\n    df_coef = pd.DataFrame(\n        df_list,\n        columns=[\"variables\", \"estimate\", \"conf_low\", \"conf_high\", \"p\", \"aic\", \"n\"],\n    )\n    estimates = pd.concat([df_0, df_coef], ignore_index=True)\n    return {\n        \"estimate\": estimates,\n        \"xlist\": xlist,\n        \"fun\": \"all_lm\",\n        \"basic\": basic,\n        \"family\": \"lm\",\n    }\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.get_all_vars_from_formula","title":"<code>get_all_vars_from_formula(formula)</code>","text":"<p>Utility function to extract variables from a formula.</p> <p>Parameters:</p> Name Type Description Default <code>formula</code> <code>str</code> <p>A formula.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of variables.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import get_all_vars_from_formula\n    get_all_vars_from_formula(\"y ~ x1 + x2\")\n        ['y', 'x1', 'x2']\n    get_all_vars_from_formula(\"y ~ \")\n        ['y']\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def get_all_vars_from_formula(formula) -&gt; list:\n    \"\"\"Utility function to extract variables from a formula.\n\n    Args:\n        formula (str): A formula.\n\n    Returns:\n        list: A list of variables.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import get_all_vars_from_formula\n            get_all_vars_from_formula(\"y ~ x1 + x2\")\n                ['y', 'x1', 'x2']\n            get_all_vars_from_formula(\"y ~ \")\n                ['y']\n    \"\"\"\n    # Split the formula into the dependent and independent variables\n    dependent, independent = formula.split(\"~\")\n    # Strip whitespace and split the independent variables by '+'\n    independent_vars = independent.strip().split(\"+\") if independent.strip() else []\n    # Combine the dependent variable with the independent variables\n    return [dependent.strip()] + [var.strip() for var in independent_vars]\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.get_combinations","title":"<code>get_combinations(ind_list, type='indices')</code>","text":"<p>Generates all possible combinations of two values from a list of values. Order is not important.</p> <p>Parameters:</p> Name Type Description Default <code>ind_list</code> <code>list</code> <p>A list of target indices.</p> required <code>type</code> <code>str</code> <p>The type of output, either \u2018values\u2019 or \u2018indices\u2019. Default is \u2018indices\u2019.</p> <code>'indices'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of tuples, where each tuple contains a combination of two values. The order of the values within a tuple is not important, and each combination appears only once.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.utils import get_combinations\n&gt;&gt;&gt; ind_list = [0, 10, 20, 30]\n&gt;&gt;&gt; combinations = get_combinations(ind_list)\n&gt;&gt;&gt; combinations = get_combinations(ind_list, type='indices')\n    [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n&gt;&gt;&gt; print(combinations, type='values')\n    [(0, 10), (0, 20), (0, 30), (1, 20), (1, 30), (2, 30)]\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def get_combinations(ind_list: list, type=\"indices\") -&gt; list:\n    \"\"\"\n    Generates all possible combinations of two values from a list of values. Order is not important.\n\n    Args:\n        ind_list (list): A list of target indices.\n        type (str): The type of output, either 'values' or 'indices'. Default is 'indices'.\n\n    Returns:\n        list: A list of tuples, where each tuple contains a combination of two values.\n            The order of the values within a tuple is not important, and each combination appears only once.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.utils import get_combinations\n        &gt;&gt;&gt; ind_list = [0, 10, 20, 30]\n        &gt;&gt;&gt; combinations = get_combinations(ind_list)\n        &gt;&gt;&gt; combinations = get_combinations(ind_list, type='indices')\n            [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n        &gt;&gt;&gt; print(combinations, type='values')\n            [(0, 10), (0, 20), (0, 30), (1, 20), (1, 30), (2, 30)]\n    \"\"\"\n    # check that ind_list is a list\n    if not isinstance(ind_list, list):\n        raise ValueError(\"ind_list must be a list.\")\n    m = len(ind_list)\n    if type == \"values\":\n        combinations = [\n            (ind_list[i], ind_list[j]) for i in range(m) for j in range(i + 1, m)\n        ]\n    elif type == \"indices\":\n        combinations = [(i, j) for i in range(m) for j in range(i + 1, m)]\n    else:\n        raise ValueError(\"type must be either 'values' or 'indices'.\")\n    return combinations\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.get_sample_size","title":"<code>get_sample_size(alpha, beta, sigma, delta)</code>","text":"<p>Calculate sample size n for comparing two means.</p> <p>Formula: n = 2 * sigma^2 * (z_{1-alpha/2} + z_{1-beta})^2 / delta^2 This corresponds to a two-sided test with equal variance.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Significance level (Type I error probability).</p> required <code>beta</code> <code>float</code> <p>Type II error probability (1 - Power).</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the population (assumed equal for both groups).</p> required <code>delta</code> <code>float</code> <p>Minimum detectable difference (effect size to detect).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The required sample size n per group.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.utils.stats import get_sample_size\n&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; beta = 0.2  # Power = 80%\n&gt;&gt;&gt; sigma = 1.0\n&gt;&gt;&gt; delta = 1.0\n&gt;&gt;&gt; n = get_sample_size(alpha, beta, sigma, delta)\n&gt;&gt;&gt; print(f\"{n:.4f}\")\n15.6978\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def get_sample_size(alpha: float, beta: float, sigma: float, delta: float) -&gt; float:\n    \"\"\"\n    Calculate sample size n for comparing two means.\n\n    Formula: n = 2 * sigma^2 * (z_{1-alpha/2} + z_{1-beta})^2 / delta^2\n    This corresponds to a two-sided test with equal variance.\n\n    Args:\n        alpha (float): Significance level (Type I error probability).\n        beta (float): Type II error probability (1 - Power).\n        sigma (float): Standard deviation of the population (assumed equal for both groups).\n        delta (float): Minimum detectable difference (effect size to detect).\n\n    Returns:\n        float: The required sample size n per group.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.utils.stats import get_sample_size\n        &gt;&gt;&gt; alpha = 0.05\n        &gt;&gt;&gt; beta = 0.2  # Power = 80%\n        &gt;&gt;&gt; sigma = 1.0\n        &gt;&gt;&gt; delta = 1.0\n        &gt;&gt;&gt; n = get_sample_size(alpha, beta, sigma, delta)\n        &gt;&gt;&gt; print(f\"{n:.4f}\")\n        15.6978\n    \"\"\"\n    z_alpha = norm.ppf(1 - alpha / 2)\n    z_beta = norm.ppf(1 - beta)\n    n = 2 * sigma**2 * (z_alpha + z_beta) ** 2 / delta**2\n    return n\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.normalize_X","title":"<code>normalize_X(X, eps=1e-12)</code>","text":"<p>Normalize array X to [0, 1] in each dimension.</p> <p>For dimensions where all values are identical (X_max == X_min), the normalized value is set to 0.5 to avoid division by zero.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input array of shape (n, d) to normalize.</p> required <code>eps</code> <code>float</code> <p>Small value to avoid division by zero when range is very small. Defaults to 1e-12.</p> <code>1e-12</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Normalized array with values in [0, 1] for each dimension. For constant dimensions, values are set to 0.5.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotoptim.utils.stats import normalize_X\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; normalize_X(X)\narray([[0. , 0. ],\n       [0.5, 0.5],\n       [1. , 1. ]])\n</code></pre> <pre><code>&gt;&gt;&gt; # Constant dimension example\n&gt;&gt;&gt; X_const = np.array([[1, 5], [1, 5], [1, 5]])\n&gt;&gt;&gt; normalize_X(X_const)\narray([[0.5, 0.5],\n       [0.5, 0.5],\n       [0.5, 0.5]])\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def normalize_X(X: np.ndarray, eps: float = 1e-12) -&gt; np.ndarray:\n    \"\"\"\n    Normalize array X to [0, 1] in each dimension.\n\n    For dimensions where all values are identical (X_max == X_min), the normalized\n    value is set to 0.5 to avoid division by zero.\n\n    Args:\n        X (np.ndarray): Input array of shape (n, d) to normalize.\n        eps (float, optional): Small value to avoid division by zero when range is very small.\n            Defaults to 1e-12.\n\n    Returns:\n        np.ndarray: Normalized array with values in [0, 1] for each dimension.\n            For constant dimensions, values are set to 0.5.\n\n    Examples:\n        &gt;&gt;&gt; from spotoptim.utils.stats import normalize_X\n        &gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; normalize_X(X)\n        array([[0. , 0. ],\n               [0.5, 0.5],\n               [1. , 1. ]])\n\n        &gt;&gt;&gt; # Constant dimension example\n        &gt;&gt;&gt; X_const = np.array([[1, 5], [1, 5], [1, 5]])\n        &gt;&gt;&gt; normalize_X(X_const)\n        array([[0.5, 0.5],\n               [0.5, 0.5],\n               [0.5, 0.5]])\n    \"\"\"\n    # Handle empty array\n    if X.size == 0:\n        return X.copy()\n\n    # Ensure X is a numpy array (handles DataFrames)\n    X = np.asarray(X)\n\n    X_min = np.min(X, axis=0)\n    X_max = np.max(X, axis=0)\n    X_range = X_max - X_min\n\n    # Handle constant dimensions (where range is effectively zero)\n    constant_dims = X_range &lt; eps\n\n    # Normalize non-constant dimensions\n    X_normalized = np.zeros_like(X, dtype=float)\n    X_normalized[:, ~constant_dims] = (X[:, ~constant_dims] - X_min[~constant_dims]) / (\n        X_range[~constant_dims]\n    )\n\n    # Set constant dimensions to 0.5\n    X_normalized[:, constant_dims] = 0.5\n\n    return X_normalized\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.partial_correlation","title":"<code>partial_correlation(x, method='pearson')</code>","text":"<p>Calculate the partial correlation matrix for a given data set.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataFrame or ndarray</code> <p>The data matrix with variables as columns.</p> required <code>method</code> <code>str</code> <p>Correlation method, one of \u2018pearson\u2019, \u2018kendall\u2019, or \u2018spearman\u2019.</p> <code>'pearson'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the partial correlation estimates, p-values, statistics, sample size (n), number of given parameters (gp), and method used.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input is not a matrix-like structure or not numeric.</p> References <ol> <li>Kim, S. ppcor: An R package for a fast calculation to semi-partial correlation coefficients. Commun Stat Appl Methods 22, 6 (Nov 2015), 665\u2013674.</li> </ol> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import partial_correlation\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'A': [1, 2, 3],\n&gt;&gt;&gt;     'B': [4, 5, 6],\n&gt;&gt;&gt;     'C': [7, 8, 9]\n&gt;&gt;&gt; })\n&gt;&gt;&gt; partial_correlation(data, method='pearson')\n{'estimate': array([[ 1. , -1. ,  1. ],\n                    [-1. ,  1. , -1. ],\n                    [ 1. , -1. ,  1. ]]),\n'p_value': array([[0. , 0. , 0. ],\n                  [0. , 0. , 0. ],\n                  [0. , 0. , 0. ]]), ...\n}\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def partial_correlation(x, method=\"pearson\") -&gt; dict:\n    \"\"\"Calculate the partial correlation matrix for a given data set.\n\n    Args:\n        x (pandas.DataFrame or numpy.ndarray): The data matrix with variables as columns.\n        method (str): Correlation method, one of 'pearson', 'kendall', or 'spearman'.\n\n    Returns:\n        dict: A dictionary containing the partial correlation estimates, p-values,\n            statistics, sample size (n), number of given parameters (gp), and method used.\n\n    Raises:\n        ValueError: If input is not a matrix-like structure or not numeric.\n\n    References:\n        1. Kim, S. ppcor: An R package for a fast calculation to semi-partial correlation coefficients.\n        Commun Stat Appl Methods 22, 6 (Nov 2015), 665\u2013674.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import partial_correlation\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        &gt;&gt;&gt;     'A': [1, 2, 3],\n        &gt;&gt;&gt;     'B': [4, 5, 6],\n        &gt;&gt;&gt;     'C': [7, 8, 9]\n        &gt;&gt;&gt; })\n        &gt;&gt;&gt; partial_correlation(data, method='pearson')\n        {'estimate': array([[ 1. , -1. ,  1. ],\n                            [-1. ,  1. , -1. ],\n                            [ 1. , -1. ,  1. ]]),\n        'p_value': array([[0. , 0. , 0. ],\n                          [0. , 0. , 0. ],\n                          [0. , 0. , 0. ]]), ...\n        }\n    \"\"\"\n    eps = 1e-6\n    if isinstance(x, pd.DataFrame):\n        x = x.to_numpy()\n    if not isinstance(x, np.ndarray):\n        raise ValueError(\"Supply a matrix-like 'x'\")\n    if not np.issubdtype(x.dtype, np.number):\n        raise ValueError(\"'x' must be numeric\")\n\n    n = x.shape[0]\n    gp = x.shape[1] - 2\n    cvx = np.cov(x, rowvar=False, bias=True)\n\n    try:\n        if np.linalg.det(cvx) &lt; np.finfo(float).eps:\n            icvx = pinv(cvx)\n        else:\n            icvx = inv(cvx)\n    except LinAlgError:\n        icvx = pinv(cvx)\n\n    p_cor = -cov_to_cor(icvx)\n    np.fill_diagonal(p_cor, 1)\n\n    if method == \"kendall\":\n        denominator = np.sqrt(2 * (2 * (n - gp) + 5) / (9 * (n - gp) * (n - 1 - gp)))\n        statistic = p_cor / denominator\n        p_value = 2 * norm.cdf(-np.abs(statistic))\n    else:\n        factor = np.sqrt((n - 2 - gp) / (1 + eps - p_cor**2))\n        statistic = p_cor * factor\n        p_value = 2 * t.cdf(-np.abs(statistic), df=n - 2 - gp)\n\n    np.fill_diagonal(statistic, 0)\n    np.fill_diagonal(p_value, 0)\n\n    return {\n        \"estimate\": p_cor,\n        \"p_value\": p_value,\n        \"statistic\": statistic,\n        \"n\": n,\n        \"gp\": gp,\n        \"method\": method,\n    }\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.partial_correlation_test","title":"<code>partial_correlation_test(x, y, z, method='pearson')</code>","text":"<p>The partial correlation coefficient between x and y given z.     x and y should be arrays (vectors) of the same length, and z should be a data frame (matrix).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>The first variable as a 1-dimensional array or list.</p> required <code>y</code> <code>array - like</code> <p>The second variable as a 1-dimensional array or list.</p> required <code>z</code> <code>DataFrame</code> <p>A data frame containing other conditional variables.</p> required <code>method</code> <code>str</code> <p>Correlation method, one of \u2018pearson\u2019, \u2018kendall\u2019, or \u2018spearman\u2019.</p> <code>'pearson'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the partial correlation estimate, p-value, statistic, sample size (n), number of given parameters (gp), and method used.</p> References <ol> <li>Kim, S. ppcor: An R package for a fast calculation to semi-partial correlation coefficients. Commun Stat Appl Methods 22, 6 (Nov 2015), 665\u2013674.</li> </ol> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import pairwise_partial_correlation\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; x = [1, 2, 3]\n&gt;&gt;&gt; y = [4, 5, 6]\n&gt;&gt;&gt; z = pd.DataFrame({'C': [7, 8, 9]})\n&gt;&gt;&gt; pairwise_partial_correlation(x, y, z)\n{'estimate': -1.0, 'p_value': 0.0, 'statistic': -inf, 'n': 3, 'gp': 1, 'method': 'pearson'}\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def partial_correlation_test(x, y, z, method=\"pearson\") -&gt; dict:\n    \"\"\"The partial correlation coefficient between x and y given z.\n        x and y should be arrays (vectors) of the same length, and z should be a data frame (matrix).\n\n    Args:\n        x (array-like): The first variable as a 1-dimensional array or list.\n        y (array-like): The second variable as a 1-dimensional array or list.\n        z (pandas.DataFrame): A data frame containing other conditional variables.\n        method (str): Correlation method, one of 'pearson', 'kendall', or 'spearman'.\n\n    Returns:\n        dict: A dictionary with the partial correlation estimate, p-value, statistic,\n            sample size (n), number of given parameters (gp), and method used.\n\n    References:\n        1. Kim, S. ppcor: An R package for a fast calculation to semi-partial correlation coefficients.\n        Commun Stat Appl Methods 22, 6 (Nov 2015), 665\u2013674.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import pairwise_partial_correlation\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; x = [1, 2, 3]\n        &gt;&gt;&gt; y = [4, 5, 6]\n        &gt;&gt;&gt; z = pd.DataFrame({'C': [7, 8, 9]})\n        &gt;&gt;&gt; pairwise_partial_correlation(x, y, z)\n        {'estimate': -1.0, 'p_value': 0.0, 'statistic': -inf, 'n': 3, 'gp': 1, 'method': 'pearson'}\n    \"\"\"\n    x = np.asarray(x)\n    y = np.asarray(y)\n    z = pd.DataFrame(z)\n\n    xyz = pd.concat([pd.Series(x), pd.Series(y), z], axis=1)\n\n    pcor_result = partial_correlation(xyz, method=method)\n\n    return {\n        \"estimate\": pcor_result[\"estimate\"][0, 1],\n        \"p_value\": pcor_result[\"p_value\"][0, 1],\n        \"statistic\": pcor_result[\"statistic\"][0, 1],\n        \"n\": pcor_result[\"n\"],\n        \"gp\": pcor_result[\"gp\"],\n        \"method\": method,\n    }\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.plot_coeff_vs_pvals","title":"<code>plot_coeff_vs_pvals(data, xlabels=None, xlim=(0, 1), xlab='p-value', ylim=None, ylab=None, xscale_log=True, yscale_log=False, title=None, show=True, y_scaler=1.1)</code>","text":"<p>Plot the coefficient estimates from fit_all_lm against the corresponding p-values.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dictionary containing the estimated coefficients, p-values, and other information. Generated by the fit_all_lm function.</p> required <code>xlabels</code> <code>list</code> <p>A list of x-axis labels.</p> <code>None</code> <code>xlim</code> <code>tuple</code> <p>A tuple of the x-axis limits.</p> <code>(0, 1)</code> <code>xlab</code> <code>str</code> <p>The x-axis label.</p> <code>'p-value'</code> <code>ylim</code> <code>tuple</code> <p>A tuple of the y-axis limits.</p> <code>None</code> <code>ylab</code> <code>str</code> <p>The y-axis label.</p> <code>None</code> <code>xscale_log</code> <code>bool</code> <p>Whether to use a log scale on the x-axis.</p> <code>True</code> <code>yscale_log</code> <code>bool</code> <p>Whether to use a log scale on the y-axis.</p> <code>False</code> <code>title</code> <code>str</code> <p>The plot title.</p> <code>None</code> <code>show</code> <code>bool</code> <p>Whether to display the plot.</p> <code>True</code> <code>y_scaler</code> <code>float</code> <p>A scaling factor for the y-axis limits. Default is 1.1, i.e., 10% more than the maximum value.</p> <code>1.1</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Notes <ul> <li>Based on the R package \u2018allestimates\u2019 by Zhiqiang Wang, see https://cran.r-project.org/package=allestimates</li> </ul> References <p>Wang, Z. (2007). Two Postestimation Commands for Assessing Confounding Effects in Epidemiological Studies. The Stata Journal, 7(2), 183-196. https://doi.org/10.1177/1536867X0700700203</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import plot_coeff_vs_pvals, fit_all_lm\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'y': [1, 2, 3],\n&gt;&gt;&gt;     'x1': [4, 5, 6],\n&gt;&gt;&gt;     'x2': [7, 8, 9]\n&gt;&gt;&gt; })\n&gt;&gt;&gt; estimates = fit_all_lm(\"y ~ x1\", [\"x2\"], data)\n&gt;&gt;&gt; plot_coeff_vs_pvals(estimates)\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def plot_coeff_vs_pvals(\n    data,\n    xlabels=None,\n    xlim=(0, 1),\n    xlab=\"p-value\",\n    ylim=None,\n    ylab=None,\n    xscale_log=True,\n    yscale_log=False,\n    title=None,\n    show=True,\n    y_scaler=1.1,\n) -&gt; None:\n    \"\"\"Plot the coefficient estimates from fit_all_lm against the corresponding p-values.\n\n    Args:\n        data (dict):\n            A dictionary containing the estimated coefficients, p-values, and other information.\n            Generated by the fit_all_lm function.\n        xlabels (list):\n            A list of x-axis labels.\n        xlim (tuple):\n            A tuple of the x-axis limits.\n        xlab (str):\n            The x-axis label.\n        ylim (tuple):\n            A tuple of the y-axis limits.\n        ylab (str):\n            The y-axis label.\n        xscale_log (bool):\n            Whether to use a log scale on the x-axis.\n        yscale_log (bool):\n            Whether to use a log scale on the y-axis.\n        title (str):\n            The plot title.\n        show (bool):\n            Whether to display the plot.\n        y_scaler (float):\n            A scaling factor for the y-axis limits. Default is 1.1, i.e., 10% more than the maximum value.\n\n    Returns:\n        None\n\n    Notes:\n        * Based on the R package 'allestimates' by Zhiqiang Wang,\n        see https://cran.r-project.org/package=allestimates\n\n    References:\n        Wang, Z. (2007). Two Postestimation Commands for Assessing Confounding Effects in Epidemiological Studies.\n        The Stata Journal, 7(2), 183-196. https://doi.org/10.1177/1536867X0700700203\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import plot_coeff_vs_pvals, fit_all_lm\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        &gt;&gt;&gt;     'y': [1, 2, 3],\n        &gt;&gt;&gt;     'x1': [4, 5, 6],\n        &gt;&gt;&gt;     'x2': [7, 8, 9]\n        &gt;&gt;&gt; })\n        &gt;&gt;&gt; estimates = fit_all_lm(\"y ~ x1\", [\"x2\"], data)\n        &gt;&gt;&gt; plot_coeff_vs_pvals(estimates)\n    \"\"\"\n    data = copy.deepcopy(data)\n    if xlabels is None:\n        xlabels = [0, 0.001, 0.01, 0.05, 0.2, 0.5, 1]\n    xbreaks = np.power(xlabels, np.log(0.5) / np.log(0.05))\n\n    result_df = data[\"estimate\"]\n    if ylab is None:\n        ylab = \"Coefficient\" if data[\"fun\"] == \"all_lm\" else \"Effect estimates\"\n    hline = 0 if data[\"fun\"] == \"all_lm\" else 1\n\n    result_df[\"p_value\"] = np.power(result_df[\"p\"], np.log(0.5) / np.log(0.05))\n    if ylim is None:\n        maxv = max(result_df[\"estimate\"].max(), abs(result_df[\"estimate\"].min()))\n        maxv = maxv * y_scaler\n        ylim = (-maxv, maxv) if data[\"fun\"] == \"all_lm\" else (1 / maxv, maxv)\n\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(data=result_df, x=\"p_value\", y=\"estimate\")\n    if xscale_log:\n        plt.xscale(\"log\")\n    if yscale_log:\n        plt.yscale(\"log\")\n    plt.xticks(ticks=xbreaks, labels=xlabels)\n    plt.axvline(x=0.5, linestyle=\"--\")\n    plt.axhline(y=hline, linestyle=\"--\")\n    plt.xlim(xlim)\n    plt.ylim(ylim)\n    plt.xlabel(xlab)\n    plt.ylabel(ylab)\n    if title:\n        plt.title(title)\n    plt.grid(True)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.plot_coeff_vs_pvals_by_included","title":"<code>plot_coeff_vs_pvals_by_included(data, xlabels=None, xlim=(0, 1), xlab='P value', ylim=None, ylab=None, yscale_log=False, title=None, grid=True, ncol=2, show=True, y_scaler=1.1)</code>","text":"<p>Generates a panel of scatter plots with effect estimates of all possible models against p-values. Uses a dictionry generated by the fit_all_lm function. Each plot includes effect estimates from all models including a specific variable.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dictionary, generated by the fit_all_lm function, containing the following keys: - estimate (pd.DataFrame): A DataFrame containing the estimates. - xlist (list): A list of variables. - fun (str): The function name. - family (str): The family of the model.</p> required <code>xlabels</code> <code>list</code> <p>A list of x-axis labels.</p> <code>None</code> <code>xlim</code> <code>tuple</code> <p>The x-axis limits.</p> <code>(0, 1)</code> <code>xlab</code> <code>str</code> <p>The x-axis label.</p> <code>'P value'</code> <code>ylim</code> <code>tuple</code> <p>The y-axis limits.</p> <code>None</code> <code>ylab</code> <code>str</code> <p>The y-axis label.</p> <code>None</code> <code>yscale_log</code> <code>bool</code> <p>Whether to scale y-axis to log10. Default is False.</p> <code>False</code> <code>title</code> <code>str</code> <p>The title of the plot.</p> <code>None</code> <code>grid</code> <code>bool</code> <p>Whether to display gridlines. Default is True.</p> <code>True</code> <code>ncol</code> <code>int</code> <p>Number of columns in the plot grid. Default is 2.</p> <code>2</code> <code>show</code> <code>bool</code> <p>Whether to display the plot. Default is True.</p> <code>True</code> <code>y_scaler</code> <code>float</code> <p>A scaling factor for the y-axis limits. Default is 1.1, i.e., 10% more than the maximum value.</p> <code>1.1</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Notes <ul> <li>Based on the R package \u2018allestimates\u2019 by Zhiqiang Wang, see https://cran.r-project.org/package=allestimates</li> </ul> References <p>Wang, Z. (2007). Two Postestimation Commands for Assessing Confounding Effects in Epidemiological Studies. The Stata Journal, 7(2), 183-196. https://doi.org/10.1177/1536867X0700700203</p> <p>Examples:</p> <p>data = {     \u201cestimate\u201d: pd.DataFrame({         \u201cvariables\u201d: [\u201cCrude\u201d, \u201cAL\u201d, \u201cAM\u201d, \u201cAN\u201d, \u201cAO\u201d],         \u201cestimate\u201d: [0.5, 0.6, 0.7, 0.8, 0.9],         \u201cconf_low\u201d: [0.1, 0.2, 0.3, 0.4, 0.5],         \u201cconf_high\u201d: [0.9, 1.0, 1.1, 1.2, 1.3],         \u201cp\u201d: [0.01, 0.02, 0.03, 0.04, 0.05],         \u201caic\u201d: [100, 200, 300, 400, 500],         \u201cn\u201d: [10, 20, 30, 40, 50]     }),     \u201cxlist\u201d: [\u201cAL\u201d, \u201cAM\u201d, \u201cAN\u201d, \u201cAO\u201d],     \u201cfun\u201d: \u201call_lm\u201d } plot_coeff_vs_pvals_by_included(data)</p> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def plot_coeff_vs_pvals_by_included(\n    data,\n    xlabels=None,\n    xlim=(0, 1),\n    xlab=\"P value\",\n    ylim=None,\n    ylab=None,\n    yscale_log=False,\n    title=None,\n    grid=True,\n    ncol=2,\n    show=True,\n    y_scaler=1.1,\n) -&gt; None:\n    \"\"\"\n    Generates a panel of scatter plots with effect estimates of all possible models against p-values.\n    Uses a dictionry generated by the fit_all_lm function.\n    Each plot includes effect estimates from all models including a specific variable.\n\n    Args:\n        data (dict): A dictionary, generated by the fit_all_lm function, containing the following keys:\n            - estimate (pd.DataFrame): A DataFrame containing the estimates.\n            - xlist (list): A list of variables.\n            - fun (str): The function name.\n            - family (str): The family of the model.\n        xlabels (list): A list of x-axis labels.\n        xlim (tuple): The x-axis limits.\n        xlab (str): The x-axis label.\n        ylim (tuple): The y-axis limits.\n        ylab (str): The y-axis label.\n        yscale_log (bool): Whether to scale y-axis to log10. Default is False.\n        title (str): The title of the plot.\n        grid (bool): Whether to display gridlines. Default is True.\n        ncol (int): Number of columns in the plot grid. Default is 2.\n        show (bool): Whether to display the plot. Default is True.\n        y_scaler (float): A scaling factor for the y-axis limits. Default is 1.1, i.e., 10% more than the maximum value.\n\n    Returns:\n        None\n\n    Notes:\n        * Based on the R package 'allestimates' by Zhiqiang Wang, see https://cran.r-project.org/package=allestimates\n\n    References:\n        Wang, Z. (2007). Two Postestimation Commands for Assessing Confounding Effects in Epidemiological Studies.\n        The Stata Journal, 7(2), 183-196. https://doi.org/10.1177/1536867X0700700203\n\n\n    Examples:\n        data = {\n            \"estimate\": pd.DataFrame({\n                \"variables\": [\"Crude\", \"AL\", \"AM\", \"AN\", \"AO\"],\n                \"estimate\": [0.5, 0.6, 0.7, 0.8, 0.9],\n                \"conf_low\": [0.1, 0.2, 0.3, 0.4, 0.5],\n                \"conf_high\": [0.9, 1.0, 1.1, 1.2, 1.3],\n                \"p\": [0.01, 0.02, 0.03, 0.04, 0.05],\n                \"aic\": [100, 200, 300, 400, 500],\n                \"n\": [10, 20, 30, 40, 50]\n            }),\n            \"xlist\": [\"AL\", \"AM\", \"AN\", \"AO\"],\n            \"fun\": \"all_lm\"\n        }\n        plot_coeff_vs_pvals_by_included(data)\n    \"\"\"\n    if xlabels is None:\n        xlabels = [0, 0.001, 0.01, 0.05, 0.2, 0.5, 1]\n    xbreaks = np.power(xlabels, np.log(0.5) / np.log(0.05))\n\n    result_df = data[\"estimate\"]\n    if ylab is None:\n        ylab = {\n            \"all_lm\": \"Coefficient\",\n            \"poisson\": \"Rate ratio\",\n            \"binomial\": \"Odds ratio\",\n        }.get(data.get(\"fun\"), \"Effect estimates\")\n\n    hline = 0 if data[\"fun\"] == \"all_lm\" else 1\n\n    result_df[\"p_value\"] = np.power(result_df[\"p\"], np.log(0.5) / np.log(0.05))\n    if ylim is None:\n        maxv = max(result_df[\"estimate\"].max(), abs(result_df[\"estimate\"].min()))\n        maxv = maxv * y_scaler\n        if data[\"fun\"] == \"all_lm\":\n            ylim = (-maxv, maxv)\n        else:\n            ylim = (1 / maxv, maxv)\n\n    # Create a DataFrame to mark inclusion of variables\n    mark_df = pd.DataFrame(\n        {x: result_df[\"variables\"].str.contains(x).astype(int) for x in data[\"xlist\"]}\n    )\n    df_scatter = pd.concat([result_df, mark_df], axis=1)\n\n    # Melt the DataFrame for plotting\n    df_long = df_scatter.melt(\n        id_vars=[\n            \"variables\",\n            \"estimate\",\n            \"conf_low\",\n            \"conf_high\",\n            \"p\",\n            \"aic\",\n            \"n\",\n            \"p_value\",\n        ],\n        value_vars=data[\"xlist\"],\n        var_name=\"variable\",\n        value_name=\"inclusion\",\n    )\n    df_long[\"inclusion\"] = df_long[\"inclusion\"].apply(\n        lambda x: \"Included\" if x &gt; 0 else \"Not included\"\n    )\n\n    # Plotting\n    g = sns.FacetGrid(\n        df_long,\n        col=\"variable\",\n        hue=\"inclusion\",\n        palette={\"Included\": \"blue\", \"Not included\": \"orange\"},\n        col_wrap=ncol,\n        height=4,\n        sharex=False,\n        sharey=False,\n    )\n    g.map(sns.scatterplot, \"p_value\", \"estimate\")\n    g.add_legend()\n    for ax in g.axes.flat:\n        ax.set_xticks(xbreaks)\n        ax.set_xticklabels(xlabels)\n        ax.set_xlim(xlim)\n        ax.set_ylim(ylim)\n        ax.axvline(\n            x=0.5, linestyle=\"--\", linewidth=1.5, color=\"black\"\n        )  # Black dashed vertical line\n        ax.axhline(\n            y=hline, linestyle=\"--\", linewidth=1.5, color=\"black\"\n        )  # Black dashed horizontal line\n        if grid:\n            ax.grid(True)\n    if yscale_log:\n        g.set(yscale=\"log\")\n    g.set_axis_labels(xlab, ylab)\n    g.set_titles(\"{col_name}\")\n    if title:\n        plt.subplots_adjust(top=0.9)\n        g.figure.suptitle(title)\n    if show:\n        plt.show()\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.preprocess_df_for_ols","title":"<code>preprocess_df_for_ols(df, independent_var_columns, target_col)</code>","text":"<p>Preprocesses a df for fiitting an OLS regression model using the specified target column and predictors.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing the data.</p> required <code>independent_var_columns</code> <code>list of str</code> <p>List of names for predictor columns.</p> required <code>target_col</code> <code>str</code> <p>Name of the target/dependent variable column.</p> required <p>Returns:</p> Name Type Description <code>X_encoded</code> <code>DataFrame</code> <p>Encoded predictors with a constant term.</p> <code>y</code> <code>Series</code> <p>Target variable.</p> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def preprocess_df_for_ols(df, independent_var_columns, target_col) -&gt; tuple:\n    \"\"\"\n    Preprocesses a df for fiitting an OLS regression model using the specified target column and predictors.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        independent_var_columns (list of str): List of names for predictor columns.\n        target_col (str): Name of the target/dependent variable column.\n\n    Returns:\n        X_encoded (pd.DataFrame): Encoded predictors with a constant term.\n        y (pd.Series): Target variable.\n\n    \"\"\"\n    # Ensure the target column is numeric and 1D\n    y = pd.to_numeric(df[target_col], errors=\"coerce\").fillna(0).squeeze()\n    if y.ndim != 1:\n        raise ValueError(f\"Target column '{target_col}' must be 1-dimensional.\")\n\n    # Ensure predictors are numeric\n    X = df[independent_var_columns].apply(pd.to_numeric, errors=\"coerce\")\n    # Impute missing values\n    X = X.fillna(X.median())\n\n    # Identify categorical columns (replace with your actual categorical list if needed)\n    categorical_cols = [\"type\"]\n    encoder = OneHotEncoder(drop=\"first\", sparse_output=False)\n    X_categorical_encoded = encoder.fit_transform(df[categorical_cols])\n\n    # Convert encoded data into a DataFrame\n    X_categorical_encoded_df = pd.DataFrame(\n        X_categorical_encoded,\n        columns=encoder.get_feature_names_out(categorical_cols),\n        index=df.index,\n    )  # Ensure alignment with the original DataFrame\n\n    # Combine numeric and categorical (encoded) parts\n    X_encoded = pd.concat([X, X_categorical_encoded_df], axis=1)\n\n    # Add a constant term\n    X_encoded = sm.add_constant(X_encoded)\n\n    # Ensure alignment between X_encoded and y\n    if X_encoded.shape[0] != y.shape[0]:\n        raise ValueError(\n            f\"Mismatch in rows: predictors (X_encoded) have {X_encoded.shape[0]} rows, \"\n            f\"but target (y) has {y.shape[0]} rows.\"\n        )\n\n    return X_encoded, y\n</code></pre>"},{"location":"reference/spotoptim/utils/stats/#spotoptim.utils.stats.vif","title":"<code>vif(X, sorted=True)</code>","text":"<p>Calculates the Variance Inflation Factor (VIF) for each feature in a DataFrame.</p> <p>VIF measures the multicollinearity among independent variables within a regression model. High VIF values indicate high multicollinearity, which can cause issues with model interpretation and stability.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>A DataFrame containing the independent variables.</p> required <code>sorted</code> <code>bool</code> <p>Whether to sort the output DataFrame by VIF values.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: A DataFrame with two columns: - \u201cfeature\u201d: The name of the feature. - \u201cVIF\u201d: The Variance Inflation Factor for the feature.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spotpython.utils.stats import vif\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'x1': [1, 2, 3, 4, 5],\n...     'x2': [2, 4, 6, 8, 10],\n...     'x3': [1, 3, 5, 7, 9]\n... })\n&gt;&gt;&gt; vif(data)\n   feature          VIF\n0      x1  1260.000000\n1      x2         0.000000\n2      x3   630.000000\n</code></pre> Source code in <code>src/spotoptim/utils/stats.py</code> <pre><code>def vif(X, sorted=True) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculates the Variance Inflation Factor (VIF) for each feature in a DataFrame.\n\n    VIF measures the multicollinearity among independent variables within a regression model.\n    High VIF values indicate high multicollinearity, which can cause issues with model\n    interpretation and stability.\n\n    Args:\n        X (pandas.DataFrame): A DataFrame containing the independent variables.\n        sorted (bool): Whether to sort the output DataFrame by VIF values.\n\n    Returns:\n        pandas.DataFrame: A DataFrame with two columns:\n            - \"feature\": The name of the feature.\n            - \"VIF\": The Variance Inflation Factor for the feature.\n\n    Examples:\n        &gt;&gt;&gt; from spotpython.utils.stats import vif\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     'x1': [1, 2, 3, 4, 5],\n        ...     'x2': [2, 4, 6, 8, 10],\n        ...     'x3': [1, 3, 5, 7, 9]\n        ... })\n        &gt;&gt;&gt; vif(data)\n           feature          VIF\n        0      x1  1260.000000\n        1      x2         0.000000\n        2      x3   630.000000\n    \"\"\"\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = X.columns\n    vif_data[\"VIF\"] = [\n        variance_inflation_factor(X.values, i) for i in range(X.shape[1])\n    ]\n    if sorted:\n        vif_data = vif_data.sort_values(by=\"VIF\", ascending=False).reset_index(\n            drop=True\n        )\n    return vif_data\n</code></pre>"}]}